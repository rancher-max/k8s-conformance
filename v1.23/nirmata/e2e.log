I0121 09:19:39.596819      18 e2e.go:132] Starting e2e run "bfa569a5-f1e6-42e1-949e-9522ff0644bc" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1642756779 - Will randomize all specs
Will run 346 of 7042 specs

Jan 21 09:19:46.463: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:19:46.467: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 21 09:19:46.504: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 21 09:19:46.548: INFO: 6 / 6 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 21 09:19:46.548: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jan 21 09:19:46.548: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 21 09:19:46.560: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 21 09:19:46.560: INFO: e2e test version: v1.23.1
Jan 21 09:19:46.564: INFO: kube-apiserver version: v1.23.1
Jan 21 09:19:46.564: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:19:46.574: INFO: Cluster IP family: ipv4
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:19:46.575: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename deployment
Jan 21 09:19:46.680: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
W0121 09:19:46.680311      18 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jan 21 09:19:46.756: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 21 09:19:46.756: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 21 09:19:46.781: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 21 09:19:46.781: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 21 09:19:46.812: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 21 09:19:46.812: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 21 09:19:47.028: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 21 09:19:47.028: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 21 09:19:51.007: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 21 09:19:51.007: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 21 09:19:52.478: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jan 21 09:19:52.556: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jan 21 09:19:52.610: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0
Jan 21 09:19:52.610: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0
Jan 21 09:19:52.611: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0
Jan 21 09:19:52.611: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0
Jan 21 09:19:52.642: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0
Jan 21 09:19:52.642: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0
Jan 21 09:19:52.642: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0
Jan 21 09:19:52.642: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 0
Jan 21 09:19:52.642: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:19:52.642: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:19:52.648: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:19:52.648: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:19:52.648: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:19:52.648: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:19:52.685: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:19:52.686: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:19:53.100: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:19:53.100: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:19:53.170: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:19:53.172: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:20:00.932: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:20:00.932: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:20:01.133: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
STEP: listing Deployments
Jan 21 09:20:01.171: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jan 21 09:20:01.262: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jan 21 09:20:01.308: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:01.489: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:01.548: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:01.872: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:02.071: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:05.132: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:09.164: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:09.589: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:09.838: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 21 09:20:13.029: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jan 21 09:20:13.380: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:20:13.392: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:20:13.392: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:20:13.405: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:20:13.405: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 1
Jan 21 09:20:13.409: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:20:13.409: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 3
Jan 21 09:20:13.416: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:20:13.416: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 2
Jan 21 09:20:13.420: INFO: observed Deployment test-deployment in namespace deployment-1197 with ReadyReplicas 3
STEP: deleting the Deployment
Jan 21 09:20:13.489: INFO: observed event type MODIFIED
Jan 21 09:20:13.496: INFO: observed event type MODIFIED
Jan 21 09:20:13.497: INFO: observed event type MODIFIED
Jan 21 09:20:13.588: INFO: observed event type MODIFIED
Jan 21 09:20:13.589: INFO: observed event type MODIFIED
Jan 21 09:20:13.602: INFO: observed event type MODIFIED
Jan 21 09:20:13.603: INFO: observed event type MODIFIED
Jan 21 09:20:13.618: INFO: observed event type MODIFIED
Jan 21 09:20:13.618: INFO: observed event type MODIFIED
Jan 21 09:20:13.631: INFO: observed event type MODIFIED
Jan 21 09:20:13.631: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 21 09:20:13.739: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 21 09:20:13.800: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-1197  ee778ec5-28fc-4090-98a5-518834046d9f 6296 4 2022-01-21 09:19:52 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e95f3830-fc33-49af-b850-c801b1f4ab8c 0xc00239eab7 0xc00239eab8}] []  [{kube-controller-manager Update apps/v1 2022-01-21 09:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e95f3830-fc33-49af-b850-c801b1f4ab8c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:20:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239eb40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 21 09:20:13.832: INFO: pod: "test-deployment-5ddd8b47d8-h465k":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-h465k test-deployment-5ddd8b47d8- deployment-1197  d0d8a7b9-4ba9-45e3-87de-ad3db5d690f3 6292 0 2022-01-21 09:20:01 +0000 UTC 2022-01-21 09:20:14 +0000 UTC 0xc00239efc0 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:b1122651d4383ce9c2976ccaa28dffdbdf2623dc5eb65f70d204886cc4fd7f7c cni.projectcalico.org/podIP:172.16.106.135/32 cni.projectcalico.org/podIPs:172.16.106.135/32] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 ee778ec5-28fc-4090-98a5-518834046d9f 0xc00239eff7 0xc00239eff8}] []  [{kube-controller-manager Update v1 2022-01-21 09:20:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee778ec5-28fc-4090-98a5-518834046d9f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:20:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lh7jw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lh7jw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:172.16.106.135,StartTime:2022-01-21 09:20:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:20:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:docker-pullable://k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:docker://635d23604cb53f63ffdbc417de6a5ccdd4237287cd0d8bb602bdd934104fe5fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.106.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 21 09:20:13.833: INFO: ReplicaSet "test-deployment-6cdc5bc678":
&ReplicaSet{ObjectMeta:{test-deployment-6cdc5bc678  deployment-1197  ba9bb903-bd46-41a2-be1d-6e82f56a2f32 6203 3 2022-01-21 09:19:46 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e95f3830-fc33-49af-b850-c801b1f4ab8c 0xc00239eba7 0xc00239eba8}] []  [{kube-controller-manager Update apps/v1 2022-01-21 09:19:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e95f3830-fc33-49af-b850-c801b1f4ab8c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:20:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6cdc5bc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239ec30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 21 09:20:13.882: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-1197  218ee157-7c9a-450a-896c-f6d7921eb9cc 6288 2 2022-01-21 09:20:01 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e95f3830-fc33-49af-b850-c801b1f4ab8c 0xc00239ec97 0xc00239ec98}] []  [{kube-controller-manager Update apps/v1 2022-01-21 09:20:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e95f3830-fc33-49af-b850-c801b1f4ab8c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:20:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239ed20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 21 09:20:13.902: INFO: pod: "test-deployment-854fdc678-nl79d":
&Pod{ObjectMeta:{test-deployment-854fdc678-nl79d test-deployment-854fdc678- deployment-1197  65b18fcc-8da2-47cc-bf4a-e3d11724f9bb 6287 0 2022-01-21 09:20:09 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:49e8ee00390a8a32298635a5ff43a5fc03cfe70987ca091eb26116abf0ff73a5 cni.projectcalico.org/podIP:172.16.106.136/32 cni.projectcalico.org/podIPs:172.16.106.136/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 218ee157-7c9a-450a-896c-f6d7921eb9cc 0xc0026d86b7 0xc0026d86b8}] []  [{kube-controller-manager Update v1 2022-01-21 09:20:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"218ee157-7c9a-450a-896c-f6d7921eb9cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:20:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:20:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6tt69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6tt69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:172.16.106.136,StartTime:2022-01-21 09:20:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:20:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://71fd25a8c6a4c1ec9ef5f1276c4affe250581dcd4e3424d5ea42be0d6f0d86f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.106.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 21 09:20:13.902: INFO: pod: "test-deployment-854fdc678-p4g9k":
&Pod{ObjectMeta:{test-deployment-854fdc678-p4g9k test-deployment-854fdc678- deployment-1197  698b6839-3263-43f1-a287-bd7682d350b6 6253 0 2022-01-21 09:20:01 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:f1cd0db81ef6647cc71277866f27941573c96bf158b0efac93c8f492b952ffff cni.projectcalico.org/podIP:172.16.209.73/32 cni.projectcalico.org/podIPs:172.16.209.73/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 218ee157-7c9a-450a-896c-f6d7921eb9cc 0xc0026d88f7 0xc0026d88f8}] []  [{kube-controller-manager Update v1 2022-01-21 09:20:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"218ee157-7c9a-450a-896c-f6d7921eb9cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:20:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:20:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4gxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4gxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:20:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:172.16.209.73,StartTime:2022-01-21 09:20:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:20:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://8052481ff8cdfd6d6dc987b0d453e0c63a398ef8eb74d40e36313ff064a165a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.209.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:20:13.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1197" for this suite.

• [SLOW TEST:27.653 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":1,"skipped":11,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:20:14.228: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jan 21 09:20:14.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5591 create -f -'
Jan 21 09:20:17.690: INFO: stderr: ""
Jan 21 09:20:17.690: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 21 09:20:18.774: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:18.774: INFO: Found 0 / 1
Jan 21 09:20:19.865: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:19.865: INFO: Found 0 / 1
Jan 21 09:20:20.817: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:20.817: INFO: Found 0 / 1
Jan 21 09:20:21.804: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:21.805: INFO: Found 0 / 1
Jan 21 09:20:22.768: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:22.787: INFO: Found 0 / 1
Jan 21 09:20:23.769: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:23.772: INFO: Found 0 / 1
Jan 21 09:20:24.854: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:24.854: INFO: Found 0 / 1
Jan 21 09:20:25.749: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:25.749: INFO: Found 0 / 1
Jan 21 09:20:26.734: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:26.734: INFO: Found 1 / 1
Jan 21 09:20:26.734: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 21 09:20:26.758: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:26.759: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 21 09:20:26.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5591 patch pod agnhost-primary-wbwh4 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 21 09:20:27.003: INFO: stderr: ""
Jan 21 09:20:27.003: INFO: stdout: "pod/agnhost-primary-wbwh4 patched\n"
STEP: checking annotations
Jan 21 09:20:27.035: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:20:27.036: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:20:27.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5591" for this suite.

• [SLOW TEST:12.853 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1483
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":2,"skipped":18,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:20:27.096: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:25:27.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7724" for this suite.

• [SLOW TEST:300.372 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":3,"skipped":30,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:25:27.469: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8956 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8956;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8956 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8956;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8956.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8956.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8956.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8956.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8956.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8956.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8956.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8956.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8956.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8956.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8956.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8956.svc;check="$$(dig +notcp +noall +answer +search 202.255.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.255.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.255.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.255.202_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8956 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8956;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8956 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8956;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8956.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8956.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8956.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8956.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8956.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8956.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8956.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8956.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8956.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8956.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8956.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8956.svc;check="$$(dig +notcp +noall +answer +search 202.255.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.255.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.255.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.255.202_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 09:25:33.873: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:33.882: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:33.890: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:33.899: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:33.907: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:33.917: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:34.020: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:34.035: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:34.052: INFO: Unable to read jessie_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:34.064: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:34.073: INFO: Unable to read jessie_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:34.082: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:34.142: INFO: Lookups using dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8956 wheezy_tcp@dns-test-service.dns-8956 wheezy_udp@dns-test-service.dns-8956.svc wheezy_tcp@dns-test-service.dns-8956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8956 jessie_tcp@dns-test-service.dns-8956 jessie_udp@dns-test-service.dns-8956.svc jessie_tcp@dns-test-service.dns-8956.svc]

Jan 21 09:25:39.158: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.173: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.179: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.186: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.194: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.202: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.270: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.279: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.289: INFO: Unable to read jessie_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.299: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.310: INFO: Unable to read jessie_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.327: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:39.416: INFO: Lookups using dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8956 wheezy_tcp@dns-test-service.dns-8956 wheezy_udp@dns-test-service.dns-8956.svc wheezy_tcp@dns-test-service.dns-8956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8956 jessie_tcp@dns-test-service.dns-8956 jessie_udp@dns-test-service.dns-8956.svc jessie_tcp@dns-test-service.dns-8956.svc]

Jan 21 09:25:44.157: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.164: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.172: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.179: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.256: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.263: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.270: INFO: Unable to read jessie_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.286: INFO: Unable to read jessie_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.294: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:44.336: INFO: Lookups using dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8956 wheezy_tcp@dns-test-service.dns-8956 wheezy_udp@dns-test-service.dns-8956.svc wheezy_tcp@dns-test-service.dns-8956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8956 jessie_tcp@dns-test-service.dns-8956 jessie_udp@dns-test-service.dns-8956.svc jessie_tcp@dns-test-service.dns-8956.svc]

Jan 21 09:25:49.156: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.166: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.200: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.211: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.220: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.285: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.297: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.306: INFO: Unable to read jessie_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.317: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.326: INFO: Unable to read jessie_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.335: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:49.402: INFO: Lookups using dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8956 wheezy_tcp@dns-test-service.dns-8956 wheezy_udp@dns-test-service.dns-8956.svc wheezy_tcp@dns-test-service.dns-8956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8956 jessie_tcp@dns-test-service.dns-8956 jessie_udp@dns-test-service.dns-8956.svc jessie_tcp@dns-test-service.dns-8956.svc]

Jan 21 09:25:54.151: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.159: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.166: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.174: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.183: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.271: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.279: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.288: INFO: Unable to read jessie_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.297: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.306: INFO: Unable to read jessie_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.317: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:54.434: INFO: Lookups using dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8956 wheezy_tcp@dns-test-service.dns-8956 wheezy_udp@dns-test-service.dns-8956.svc wheezy_tcp@dns-test-service.dns-8956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8956 jessie_tcp@dns-test-service.dns-8956 jessie_udp@dns-test-service.dns-8956.svc jessie_tcp@dns-test-service.dns-8956.svc]

Jan 21 09:25:59.156: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.164: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.179: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.266: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.274: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.283: INFO: Unable to read jessie_udp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.289: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956 from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.297: INFO: Unable to read jessie_udp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.303: INFO: Unable to read jessie_tcp@dns-test-service.dns-8956.svc from pod dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03: the server could not find the requested resource (get pods dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03)
Jan 21 09:25:59.349: INFO: Lookups using dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8956 wheezy_tcp@dns-test-service.dns-8956 wheezy_udp@dns-test-service.dns-8956.svc wheezy_tcp@dns-test-service.dns-8956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8956 jessie_tcp@dns-test-service.dns-8956 jessie_udp@dns-test-service.dns-8956.svc jessie_tcp@dns-test-service.dns-8956.svc]

Jan 21 09:26:04.323: INFO: DNS probes using dns-8956/dns-test-f1f0812e-ba0a-43ff-93da-a2c43082bd03 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:26:04.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8956" for this suite.

• [SLOW TEST:37.504 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":4,"skipped":89,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:26:04.974: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-75ff6ab0-d0e9-4b7c-9877-6f1bef332b16
STEP: Creating the pod
Jan 21 09:26:05.235: INFO: The status of Pod pod-configmaps-54b67caf-b01f-4542-9b6d-ad507fef4b63 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:26:07.249: INFO: The status of Pod pod-configmaps-54b67caf-b01f-4542-9b6d-ad507fef4b63 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:26:09.246: INFO: The status of Pod pod-configmaps-54b67caf-b01f-4542-9b6d-ad507fef4b63 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:26:11.267: INFO: The status of Pod pod-configmaps-54b67caf-b01f-4542-9b6d-ad507fef4b63 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-75ff6ab0-d0e9-4b7c-9877-6f1bef332b16
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:27:38.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1389" for this suite.

• [SLOW TEST:93.774 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":5,"skipped":124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:27:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 09:27:38.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5fd5f1a7-443f-4cfa-aabd-28f5a3db7b75" in namespace "downward-api-3756" to be "Succeeded or Failed"
Jan 21 09:27:38.895: INFO: Pod "downwardapi-volume-5fd5f1a7-443f-4cfa-aabd-28f5a3db7b75": Phase="Pending", Reason="", readiness=false. Elapsed: 19.394681ms
Jan 21 09:27:40.910: INFO: Pod "downwardapi-volume-5fd5f1a7-443f-4cfa-aabd-28f5a3db7b75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034713338s
Jan 21 09:27:42.926: INFO: Pod "downwardapi-volume-5fd5f1a7-443f-4cfa-aabd-28f5a3db7b75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050545702s
STEP: Saw pod success
Jan 21 09:27:42.926: INFO: Pod "downwardapi-volume-5fd5f1a7-443f-4cfa-aabd-28f5a3db7b75" satisfied condition "Succeeded or Failed"
Jan 21 09:27:42.933: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-5fd5f1a7-443f-4cfa-aabd-28f5a3db7b75 container client-container: <nil>
STEP: delete the pod
Jan 21 09:27:42.979: INFO: Waiting for pod downwardapi-volume-5fd5f1a7-443f-4cfa-aabd-28f5a3db7b75 to disappear
Jan 21 09:27:42.985: INFO: Pod downwardapi-volume-5fd5f1a7-443f-4cfa-aabd-28f5a3db7b75 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:27:42.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3756" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:27:43.014: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:27:45.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3821" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":7,"skipped":170,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:27:45.909: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-ff841bf2-eb1a-4b1e-b79f-4c06ad3e8c94 in namespace container-probe-3919
Jan 21 09:27:52.141: INFO: Started pod test-webserver-ff841bf2-eb1a-4b1e-b79f-4c06ad3e8c94 in namespace container-probe-3919
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 09:27:52.147: INFO: Initial restart count of pod test-webserver-ff841bf2-eb1a-4b1e-b79f-4c06ad3e8c94 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:31:53.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3919" for this suite.

• [SLOW TEST:247.955 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":174,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:31:53.866: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 21 09:31:54.468: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:31:54.468: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:31:55.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:31:55.540: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:31:56.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:31:56.487: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:31:57.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:31:57.539: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:31:58.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 09:31:58.501: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 21 09:31:58.617: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:31:58.618: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:31:59.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:31:59.637: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:32:00.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:32:00.636: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:32:01.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:32:01.638: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:32:02.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:32:02.641: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:32:03.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:32:03.640: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:32:04.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:32:04.640: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:32:05.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:32:05.634: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 09:32:06.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 09:32:06.639: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3422, will wait for the garbage collector to delete the pods
Jan 21 09:32:06.716: INFO: Deleting DaemonSet.extensions daemon-set took: 13.270577ms
Jan 21 09:32:06.942: INFO: Terminating DaemonSet.extensions daemon-set pods took: 226.043167ms
Jan 21 09:32:10.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:32:10.451: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 21 09:32:10.460: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7055"},"items":null}

Jan 21 09:32:10.465: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7055"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:32:10.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3422" for this suite.

• [SLOW TEST:16.642 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":9,"skipped":177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:32:10.508: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:32:26.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7199" for this suite.

• [SLOW TEST:16.140 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":10,"skipped":208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:32:26.674: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 21 09:32:26.911: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 21 09:32:26.952: INFO: Waiting for terminating namespaces to be deleted...
Jan 21 09:32:27.036: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
Jan 21 09:32:27.057: INFO: haproxy-ingress-6968c464f7-sc6ml from ingress-haproxy started at 2022-01-21 06:57:22 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.057: INFO: 	Container haproxy-ingress ready: false, restart count 55
Jan 21 09:32:27.057: INFO: ingress-default-backend-6f6c8556b8-nvd8n from ingress-haproxy started at 2022-01-21 06:57:22 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.057: INFO: 	Container ingress-default-backend ready: true, restart count 0
Jan 21 09:32:27.057: INFO: fail-once-local-47mm7 from job-7199 started at 2022-01-21 09:32:10 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.057: INFO: 	Container c ready: false, restart count 1
Jan 21 09:32:27.057: INFO: fail-once-local-578mk from job-7199 started at 2022-01-21 09:32:18 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.057: INFO: 	Container c ready: false, restart count 1
Jan 21 09:32:27.057: INFO: fail-once-local-5bc5b from job-7199 started at 2022-01-21 09:32:19 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.057: INFO: 	Container c ready: false, restart count 1
Jan 21 09:32:27.057: INFO: fail-once-local-hj6v7 from job-7199 started at 2022-01-21 09:32:10 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.057: INFO: 	Container c ready: false, restart count 1
Jan 21 09:32:27.059: INFO: calico-node-q458w from kube-system started at 2022-01-21 06:54:48 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.059: INFO: 	Container calico-node ready: true, restart count 0
Jan 21 09:32:27.059: INFO: nirmata-cni-installer-6tvqk from nirmata started at 2022-01-21 06:57:57 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.059: INFO: 	Container install-cni ready: true, restart count 0
Jan 21 09:32:27.059: INFO: otel-agent-857cc46474-4msf2 from nirmata started at 2022-01-21 06:57:23 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.059: INFO: 	Container otel-agent ready: true, restart count 0
Jan 21 09:32:27.059: INFO: sonobuoy from sonobuoy started at 2022-01-21 09:19:31 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.059: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 21 09:32:27.059: INFO: sonobuoy-e2e-job-9425c772f26a4979 from sonobuoy started at 2022-01-21 09:19:34 +0000 UTC (2 container statuses recorded)
Jan 21 09:32:27.059: INFO: 	Container e2e ready: true, restart count 0
Jan 21 09:32:27.059: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:32:27.059: INFO: sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-9kb9z from sonobuoy started at 2022-01-21 09:19:35 +0000 UTC (2 container statuses recorded)
Jan 21 09:32:27.059: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:32:27.059: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 21 09:32:27.059: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
Jan 21 09:32:27.081: INFO: calico-kube-controllers-7bf9dd85d9-xd27g from kube-system started at 2022-01-21 06:55:04 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.082: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 21 09:32:27.082: INFO: calico-node-fg2tt from kube-system started at 2022-01-21 06:54:48 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.082: INFO: 	Container calico-node ready: true, restart count 0
Jan 21 09:32:27.083: INFO: coredns-6779677b89-ldbmt from kube-system started at 2022-01-21 06:55:08 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.083: INFO: 	Container coredns ready: true, restart count 0
Jan 21 09:32:27.083: INFO: coredns-6779677b89-mr5vk from kube-system started at 2022-01-21 06:55:07 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.083: INFO: 	Container coredns ready: true, restart count 0
Jan 21 09:32:27.084: INFO: metrics-server-646d5db6cc-nmv44 from kube-system started at 2022-01-21 06:55:09 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.084: INFO: 	Container metrics-server ready: true, restart count 0
Jan 21 09:32:27.085: INFO: nirmata-cni-installer-jq2k2 from nirmata started at 2022-01-21 06:57:57 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.086: INFO: 	Container install-cni ready: true, restart count 0
Jan 21 09:32:27.086: INFO: nirmata-kube-controller-666bcc6bbc-zss9n from nirmata started at 2022-01-21 06:55:04 +0000 UTC (1 container statuses recorded)
Jan 21 09:32:27.088: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Jan 21 09:32:27.088: INFO: sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-rw7d8 from sonobuoy started at 2022-01-21 09:19:34 +0000 UTC (2 container statuses recorded)
Jan 21 09:32:27.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:32:27.088: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node conformance1
STEP: verifying the node has the label node dc-hg-2
Jan 21 09:32:27.219: INFO: Pod haproxy-ingress-6968c464f7-sc6ml requesting resource cpu=10m on Node conformance1
Jan 21 09:32:27.219: INFO: Pod ingress-default-backend-6f6c8556b8-nvd8n requesting resource cpu=250m on Node conformance1
Jan 21 09:32:27.219: INFO: Pod calico-kube-controllers-7bf9dd85d9-xd27g requesting resource cpu=0m on Node dc-hg-2
Jan 21 09:32:27.219: INFO: Pod calico-node-fg2tt requesting resource cpu=250m on Node dc-hg-2
Jan 21 09:32:27.219: INFO: Pod calico-node-q458w requesting resource cpu=250m on Node conformance1
Jan 21 09:32:27.219: INFO: Pod coredns-6779677b89-ldbmt requesting resource cpu=100m on Node dc-hg-2
Jan 21 09:32:27.219: INFO: Pod coredns-6779677b89-mr5vk requesting resource cpu=100m on Node dc-hg-2
Jan 21 09:32:27.219: INFO: Pod metrics-server-646d5db6cc-nmv44 requesting resource cpu=0m on Node dc-hg-2
Jan 21 09:32:27.219: INFO: Pod nirmata-cni-installer-6tvqk requesting resource cpu=250m on Node conformance1
Jan 21 09:32:27.219: INFO: Pod nirmata-cni-installer-jq2k2 requesting resource cpu=250m on Node dc-hg-2
Jan 21 09:32:27.219: INFO: Pod nirmata-kube-controller-666bcc6bbc-zss9n requesting resource cpu=250m on Node dc-hg-2
Jan 21 09:32:27.219: INFO: Pod otel-agent-857cc46474-4msf2 requesting resource cpu=100m on Node conformance1
Jan 21 09:32:27.220: INFO: Pod sonobuoy requesting resource cpu=0m on Node conformance1
Jan 21 09:32:27.220: INFO: Pod sonobuoy-e2e-job-9425c772f26a4979 requesting resource cpu=0m on Node conformance1
Jan 21 09:32:27.220: INFO: Pod sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-9kb9z requesting resource cpu=0m on Node conformance1
Jan 21 09:32:27.220: INFO: Pod sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-rw7d8 requesting resource cpu=0m on Node dc-hg-2
STEP: Starting Pods to consume most of the cluster CPU.
Jan 21 09:32:27.220: INFO: Creating a pod which consumes cpu=798m on Node conformance1
Jan 21 09:32:27.245: INFO: Creating a pod which consumes cpu=735m on Node dc-hg-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ac5c08b-4020-42b1-829d-6cb21702d11a.16cc3f491aaec65d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8263/filler-pod-5ac5c08b-4020-42b1-829d-6cb21702d11a to dc-hg-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ac5c08b-4020-42b1-829d-6cb21702d11a.16cc3f49ab674524], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ac5c08b-4020-42b1-829d-6cb21702d11a.16cc3f49b0371007], Reason = [Created], Message = [Created container filler-pod-5ac5c08b-4020-42b1-829d-6cb21702d11a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ac5c08b-4020-42b1-829d-6cb21702d11a.16cc3f49c3583437], Reason = [Started], Message = [Started container filler-pod-5ac5c08b-4020-42b1-829d-6cb21702d11a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6ee8099c-8805-44d9-83bc-85e84851b85c.16cc3f491a05362c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8263/filler-pod-6ee8099c-8805-44d9-83bc-85e84851b85c to conformance1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6ee8099c-8805-44d9-83bc-85e84851b85c.16cc3f49c713f063], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6ee8099c-8805-44d9-83bc-85e84851b85c.16cc3f49caf93398], Reason = [Created], Message = [Created container filler-pod-6ee8099c-8805-44d9-83bc-85e84851b85c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6ee8099c-8805-44d9-83bc-85e84851b85c.16cc3f49de22ca89], Reason = [Started], Message = [Started container filler-pod-6ee8099c-8805-44d9-83bc-85e84851b85c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16cc3f4a849e0bb9], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node dc-hg-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node conformance1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:32:34.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8263" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:7.825 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":11,"skipped":247,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:32:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:32:41.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3694" for this suite.

• [SLOW TEST:7.192 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":12,"skipped":258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:32:41.703: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 09:32:43.564: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 09:32:45.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 32, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 09:32:47.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 32, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 09:32:50.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:32:50.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3051" for this suite.
STEP: Destroying namespace "webhook-3051-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.212 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":13,"skipped":280,"failed":0}
SSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:32:50.916: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:32:55.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7767" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":14,"skipped":283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:32:55.328: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-401dc6f6-218e-48c1-94c8-fce4ae78047e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:33:01.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6141" for this suite.

• [SLOW TEST:6.219 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":15,"skipped":305,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:33:01.548: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-dgl6
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 09:33:01.690: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dgl6" in namespace "subpath-6629" to be "Succeeded or Failed"
Jan 21 09:33:01.714: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Pending", Reason="", readiness=false. Elapsed: 23.404425ms
Jan 21 09:33:03.727: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036560034s
Jan 21 09:33:05.737: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 4.047187374s
Jan 21 09:33:07.754: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 6.064051655s
Jan 21 09:33:09.770: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 8.079651684s
Jan 21 09:33:11.779: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 10.089081229s
Jan 21 09:33:13.793: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 12.102658818s
Jan 21 09:33:15.804: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 14.113764832s
Jan 21 09:33:17.820: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 16.129824406s
Jan 21 09:33:19.831: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 18.14031161s
Jan 21 09:33:21.844: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 20.153795007s
Jan 21 09:33:23.866: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Running", Reason="", readiness=true. Elapsed: 22.176209272s
Jan 21 09:33:25.881: INFO: Pod "pod-subpath-test-configmap-dgl6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.190320601s
STEP: Saw pod success
Jan 21 09:33:25.881: INFO: Pod "pod-subpath-test-configmap-dgl6" satisfied condition "Succeeded or Failed"
Jan 21 09:33:25.890: INFO: Trying to get logs from node conformance1 pod pod-subpath-test-configmap-dgl6 container test-container-subpath-configmap-dgl6: <nil>
STEP: delete the pod
Jan 21 09:33:25.938: INFO: Waiting for pod pod-subpath-test-configmap-dgl6 to disappear
Jan 21 09:33:25.961: INFO: Pod pod-subpath-test-configmap-dgl6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dgl6
Jan 21 09:33:25.961: INFO: Deleting pod "pod-subpath-test-configmap-dgl6" in namespace "subpath-6629"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:33:25.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6629" for this suite.

• [SLOW TEST:24.449 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":16,"skipped":308,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:33:25.997: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 21 09:33:26.187: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 21 09:33:26.204: INFO: Waiting for terminating namespaces to be deleted...
Jan 21 09:33:26.213: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
Jan 21 09:33:26.230: INFO: haproxy-ingress-6968c464f7-sc6ml from ingress-haproxy started at 2022-01-21 06:57:22 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.231: INFO: 	Container haproxy-ingress ready: false, restart count 55
Jan 21 09:33:26.231: INFO: ingress-default-backend-6f6c8556b8-nvd8n from ingress-haproxy started at 2022-01-21 06:57:22 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.231: INFO: 	Container ingress-default-backend ready: true, restart count 0
Jan 21 09:33:26.232: INFO: calico-node-q458w from kube-system started at 2022-01-21 06:54:48 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.232: INFO: 	Container calico-node ready: true, restart count 0
Jan 21 09:33:26.232: INFO: nirmata-cni-installer-6tvqk from nirmata started at 2022-01-21 06:57:57 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.232: INFO: 	Container install-cni ready: true, restart count 0
Jan 21 09:33:26.232: INFO: otel-agent-857cc46474-4msf2 from nirmata started at 2022-01-21 06:57:23 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.233: INFO: 	Container otel-agent ready: true, restart count 0
Jan 21 09:33:26.233: INFO: sonobuoy from sonobuoy started at 2022-01-21 09:19:31 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.233: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 21 09:33:26.233: INFO: sonobuoy-e2e-job-9425c772f26a4979 from sonobuoy started at 2022-01-21 09:19:34 +0000 UTC (2 container statuses recorded)
Jan 21 09:33:26.233: INFO: 	Container e2e ready: true, restart count 0
Jan 21 09:33:26.234: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:33:26.234: INFO: sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-9kb9z from sonobuoy started at 2022-01-21 09:19:35 +0000 UTC (2 container statuses recorded)
Jan 21 09:33:26.234: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:33:26.234: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 21 09:33:26.234: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
Jan 21 09:33:26.249: INFO: calico-kube-controllers-7bf9dd85d9-xd27g from kube-system started at 2022-01-21 06:55:04 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.250: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 21 09:33:26.250: INFO: calico-node-fg2tt from kube-system started at 2022-01-21 06:54:48 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.250: INFO: 	Container calico-node ready: true, restart count 0
Jan 21 09:33:26.250: INFO: coredns-6779677b89-ldbmt from kube-system started at 2022-01-21 06:55:08 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.250: INFO: 	Container coredns ready: true, restart count 0
Jan 21 09:33:26.250: INFO: coredns-6779677b89-mr5vk from kube-system started at 2022-01-21 06:55:07 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.251: INFO: 	Container coredns ready: true, restart count 0
Jan 21 09:33:26.251: INFO: metrics-server-646d5db6cc-nmv44 from kube-system started at 2022-01-21 06:55:09 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.251: INFO: 	Container metrics-server ready: true, restart count 0
Jan 21 09:33:26.251: INFO: nirmata-cni-installer-jq2k2 from nirmata started at 2022-01-21 06:57:57 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.252: INFO: 	Container install-cni ready: true, restart count 0
Jan 21 09:33:26.252: INFO: nirmata-kube-controller-666bcc6bbc-zss9n from nirmata started at 2022-01-21 06:55:04 +0000 UTC (1 container statuses recorded)
Jan 21 09:33:26.252: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Jan 21 09:33:26.253: INFO: sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-rw7d8 from sonobuoy started at 2022-01-21 09:19:34 +0000 UTC (2 container statuses recorded)
Jan 21 09:33:26.253: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:33:26.253: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e37d349e-35cb-4486-b5ef-6a639398037b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e37d349e-35cb-4486-b5ef-6a639398037b off the node conformance1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e37d349e-35cb-4486-b5ef-6a639398037b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:33:34.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8084" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.605 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":17,"skipped":319,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:33:34.605: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:33:34.691: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:33:38.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9657" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":18,"skipped":327,"failed":0}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:33:38.622: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-t7wmq in namespace proxy-3674
I0121 09:33:38.999986      18 runners.go:193] Created replication controller with name: proxy-service-t7wmq, namespace: proxy-3674, replica count: 1
I0121 09:33:40.051680      18 runners.go:193] proxy-service-t7wmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:33:41.052067      18 runners.go:193] proxy-service-t7wmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:33:42.052854      18 runners.go:193] proxy-service-t7wmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:33:43.095733      18 runners.go:193] proxy-service-t7wmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:33:44.096501      18 runners.go:193] proxy-service-t7wmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:33:45.099743      18 runners.go:193] proxy-service-t7wmq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 09:33:45.110: INFO: setup took 6.171855154s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 21 09:33:45.167: INFO: (0) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 54.954331ms)
Jan 21 09:33:45.178: INFO: (0) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 65.251422ms)
Jan 21 09:33:45.203: INFO: (0) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 90.26223ms)
Jan 21 09:33:45.203: INFO: (0) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 89.154643ms)
Jan 21 09:33:45.204: INFO: (0) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 89.485906ms)
Jan 21 09:33:45.223: INFO: (0) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 111.029668ms)
Jan 21 09:33:45.224: INFO: (0) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 109.291871ms)
Jan 21 09:33:45.231: INFO: (0) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 117.076606ms)
Jan 21 09:33:45.231: INFO: (0) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 118.240553ms)
Jan 21 09:33:45.232: INFO: (0) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 120.020741ms)
Jan 21 09:33:45.235: INFO: (0) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 121.696503ms)
Jan 21 09:33:45.238: INFO: (0) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 124.980886ms)
Jan 21 09:33:45.239: INFO: (0) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 127.177952ms)
Jan 21 09:33:45.253: INFO: (0) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 138.632974ms)
Jan 21 09:33:45.253: INFO: (0) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 139.302411ms)
Jan 21 09:33:45.253: INFO: (0) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 139.684506ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 36.785807ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 32.625319ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 37.206399ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 36.515489ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 36.731159ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 36.951279ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 30.551336ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 21.878272ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 21.721121ms)
Jan 21 09:33:45.293: INFO: (1) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 33.862142ms)
Jan 21 09:33:45.296: INFO: (1) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 24.217753ms)
Jan 21 09:33:45.317: INFO: (1) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 44.66352ms)
Jan 21 09:33:45.318: INFO: (1) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 45.701603ms)
Jan 21 09:33:45.319: INFO: (1) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 46.524992ms)
Jan 21 09:33:45.319: INFO: (1) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 46.857505ms)
Jan 21 09:33:45.322: INFO: (1) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 56.960295ms)
Jan 21 09:33:45.333: INFO: (2) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 10.153715ms)
Jan 21 09:33:45.333: INFO: (2) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 10.125335ms)
Jan 21 09:33:45.361: INFO: (2) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 38.217259ms)
Jan 21 09:33:45.382: INFO: (2) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 59.209505ms)
Jan 21 09:33:45.383: INFO: (2) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 59.972462ms)
Jan 21 09:33:45.384: INFO: (2) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 59.930115ms)
Jan 21 09:33:45.384: INFO: (2) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 60.56237ms)
Jan 21 09:33:45.384: INFO: (2) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 60.301313ms)
Jan 21 09:33:45.384: INFO: (2) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 60.827001ms)
Jan 21 09:33:45.384: INFO: (2) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 60.515963ms)
Jan 21 09:33:45.384: INFO: (2) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 60.207014ms)
Jan 21 09:33:45.384: INFO: (2) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 60.254859ms)
Jan 21 09:33:45.385: INFO: (2) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 61.401011ms)
Jan 21 09:33:45.385: INFO: (2) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 62.139854ms)
Jan 21 09:33:45.385: INFO: (2) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 61.421753ms)
Jan 21 09:33:45.385: INFO: (2) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 61.348642ms)
Jan 21 09:33:45.396: INFO: (3) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 10.146798ms)
Jan 21 09:33:45.406: INFO: (3) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 20.363541ms)
Jan 21 09:33:45.406: INFO: (3) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 20.162768ms)
Jan 21 09:33:45.413: INFO: (3) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 25.306391ms)
Jan 21 09:33:45.426: INFO: (3) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 38.241147ms)
Jan 21 09:33:45.427: INFO: (3) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 38.274387ms)
Jan 21 09:33:45.427: INFO: (3) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 38.613548ms)
Jan 21 09:33:45.427: INFO: (3) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 38.001317ms)
Jan 21 09:33:45.427: INFO: (3) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 37.890636ms)
Jan 21 09:33:45.427: INFO: (3) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 38.94535ms)
Jan 21 09:33:45.427: INFO: (3) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 38.512461ms)
Jan 21 09:33:45.428: INFO: (3) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 39.719151ms)
Jan 21 09:33:45.431: INFO: (3) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 42.482107ms)
Jan 21 09:33:45.431: INFO: (3) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 42.821253ms)
Jan 21 09:33:45.435: INFO: (3) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 46.182582ms)
Jan 21 09:33:45.435: INFO: (3) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 46.209058ms)
Jan 21 09:33:45.463: INFO: (4) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 26.408446ms)
Jan 21 09:33:45.463: INFO: (4) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 27.465056ms)
Jan 21 09:33:45.466: INFO: (4) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 29.884582ms)
Jan 21 09:33:45.467: INFO: (4) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 30.372431ms)
Jan 21 09:33:45.467: INFO: (4) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 30.309088ms)
Jan 21 09:33:45.467: INFO: (4) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 31.00346ms)
Jan 21 09:33:45.467: INFO: (4) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 30.666904ms)
Jan 21 09:33:45.467: INFO: (4) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 30.633517ms)
Jan 21 09:33:45.467: INFO: (4) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 31.98349ms)
Jan 21 09:33:45.467: INFO: (4) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 31.808201ms)
Jan 21 09:33:45.468: INFO: (4) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 31.926524ms)
Jan 21 09:33:45.469: INFO: (4) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 33.467349ms)
Jan 21 09:33:45.470: INFO: (4) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 34.42494ms)
Jan 21 09:33:45.472: INFO: (4) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 36.184108ms)
Jan 21 09:33:45.475: INFO: (4) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 38.293787ms)
Jan 21 09:33:45.475: INFO: (4) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 38.74488ms)
Jan 21 09:33:45.499: INFO: (5) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 22.893859ms)
Jan 21 09:33:45.503: INFO: (5) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 27.066399ms)
Jan 21 09:33:45.503: INFO: (5) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 27.538054ms)
Jan 21 09:33:45.505: INFO: (5) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 24.253594ms)
Jan 21 09:33:45.507: INFO: (5) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 30.223236ms)
Jan 21 09:33:45.511: INFO: (5) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 30.583632ms)
Jan 21 09:33:45.512: INFO: (5) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 31.365016ms)
Jan 21 09:33:45.516: INFO: (5) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 35.191752ms)
Jan 21 09:33:45.517: INFO: (5) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 40.685626ms)
Jan 21 09:33:45.518: INFO: (5) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 38.105186ms)
Jan 21 09:33:45.521: INFO: (5) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 39.604487ms)
Jan 21 09:33:45.522: INFO: (5) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 40.491104ms)
Jan 21 09:33:45.531: INFO: (5) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 49.078095ms)
Jan 21 09:33:45.531: INFO: (5) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 49.449636ms)
Jan 21 09:33:45.531: INFO: (5) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 49.938427ms)
Jan 21 09:33:45.532: INFO: (5) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 50.686127ms)
Jan 21 09:33:45.565: INFO: (6) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 32.874943ms)
Jan 21 09:33:45.566: INFO: (6) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 32.970513ms)
Jan 21 09:33:45.570: INFO: (6) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 36.457897ms)
Jan 21 09:33:45.577: INFO: (6) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 43.973722ms)
Jan 21 09:33:45.578: INFO: (6) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 44.925755ms)
Jan 21 09:33:45.588: INFO: (6) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 54.920504ms)
Jan 21 09:33:45.633: INFO: (6) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 100.002333ms)
Jan 21 09:33:45.634: INFO: (6) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 100.815247ms)
Jan 21 09:33:45.634: INFO: (6) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 100.329366ms)
Jan 21 09:33:45.634: INFO: (6) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 101.144709ms)
Jan 21 09:33:45.635: INFO: (6) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 101.195554ms)
Jan 21 09:33:45.637: INFO: (6) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 103.551021ms)
Jan 21 09:33:45.638: INFO: (6) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 105.397444ms)
Jan 21 09:33:45.638: INFO: (6) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 104.768979ms)
Jan 21 09:33:45.639: INFO: (6) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 105.547385ms)
Jan 21 09:33:45.639: INFO: (6) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 105.669539ms)
Jan 21 09:33:45.692: INFO: (7) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 48.764002ms)
Jan 21 09:33:45.692: INFO: (7) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 46.000714ms)
Jan 21 09:33:45.692: INFO: (7) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 48.974667ms)
Jan 21 09:33:45.692: INFO: (7) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 52.74702ms)
Jan 21 09:33:45.695: INFO: (7) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 52.216754ms)
Jan 21 09:33:45.709: INFO: (7) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 62.880574ms)
Jan 21 09:33:45.723: INFO: (7) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 72.121153ms)
Jan 21 09:33:45.723: INFO: (7) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 72.384778ms)
Jan 21 09:33:45.723: INFO: (7) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 72.676858ms)
Jan 21 09:33:45.727: INFO: (7) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 76.813179ms)
Jan 21 09:33:45.727: INFO: (7) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 76.426423ms)
Jan 21 09:33:45.727: INFO: (7) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 80.852377ms)
Jan 21 09:33:45.727: INFO: (7) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 85.460707ms)
Jan 21 09:33:45.728: INFO: (7) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 81.440908ms)
Jan 21 09:33:45.728: INFO: (7) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 77.225872ms)
Jan 21 09:33:45.728: INFO: (7) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 77.134108ms)
Jan 21 09:33:45.770: INFO: (8) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 39.567512ms)
Jan 21 09:33:45.776: INFO: (8) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 46.529883ms)
Jan 21 09:33:45.776: INFO: (8) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 42.274437ms)
Jan 21 09:33:45.776: INFO: (8) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 45.665485ms)
Jan 21 09:33:45.776: INFO: (8) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 45.381962ms)
Jan 21 09:33:45.776: INFO: (8) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 45.7079ms)
Jan 21 09:33:45.777: INFO: (8) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 45.87651ms)
Jan 21 09:33:45.777: INFO: (8) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 46.616769ms)
Jan 21 09:33:45.777: INFO: (8) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 44.231211ms)
Jan 21 09:33:45.777: INFO: (8) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 48.457427ms)
Jan 21 09:33:45.777: INFO: (8) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 48.366072ms)
Jan 21 09:33:45.779: INFO: (8) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 48.07534ms)
Jan 21 09:33:45.782: INFO: (8) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 49.340344ms)
Jan 21 09:33:45.783: INFO: (8) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 53.583274ms)
Jan 21 09:33:45.786: INFO: (8) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 57.476876ms)
Jan 21 09:33:45.787: INFO: (8) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 54.062002ms)
Jan 21 09:33:45.817: INFO: (9) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 29.484412ms)
Jan 21 09:33:45.822: INFO: (9) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 33.978637ms)
Jan 21 09:33:45.824: INFO: (9) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 34.867513ms)
Jan 21 09:33:45.824: INFO: (9) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 34.984507ms)
Jan 21 09:33:45.824: INFO: (9) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 34.952846ms)
Jan 21 09:33:45.828: INFO: (9) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 39.117427ms)
Jan 21 09:33:45.830: INFO: (9) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 42.123616ms)
Jan 21 09:33:45.830: INFO: (9) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 40.717781ms)
Jan 21 09:33:45.833: INFO: (9) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 43.728278ms)
Jan 21 09:33:45.835: INFO: (9) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 46.02559ms)
Jan 21 09:33:45.837: INFO: (9) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 48.86491ms)
Jan 21 09:33:45.837: INFO: (9) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 47.873486ms)
Jan 21 09:33:45.838: INFO: (9) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 48.352754ms)
Jan 21 09:33:45.838: INFO: (9) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 48.263431ms)
Jan 21 09:33:45.838: INFO: (9) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 49.539115ms)
Jan 21 09:33:45.840: INFO: (9) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 51.255342ms)
Jan 21 09:33:45.866: INFO: (10) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 25.384273ms)
Jan 21 09:33:45.868: INFO: (10) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 26.964055ms)
Jan 21 09:33:45.888: INFO: (10) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 45.248489ms)
Jan 21 09:33:45.889: INFO: (10) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 46.86057ms)
Jan 21 09:33:45.889: INFO: (10) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 47.419599ms)
Jan 21 09:33:45.889: INFO: (10) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 46.616694ms)
Jan 21 09:33:45.889: INFO: (10) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 45.739ms)
Jan 21 09:33:45.890: INFO: (10) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 48.882652ms)
Jan 21 09:33:45.890: INFO: (10) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 47.14673ms)
Jan 21 09:33:45.890: INFO: (10) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 46.745186ms)
Jan 21 09:33:45.890: INFO: (10) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 49.992746ms)
Jan 21 09:33:45.890: INFO: (10) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 49.834494ms)
Jan 21 09:33:45.891: INFO: (10) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 47.764925ms)
Jan 21 09:33:45.891: INFO: (10) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 48.49266ms)
Jan 21 09:33:45.892: INFO: (10) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 47.953322ms)
Jan 21 09:33:45.892: INFO: (10) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 49.643493ms)
Jan 21 09:33:45.923: INFO: (11) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 30.262516ms)
Jan 21 09:33:45.930: INFO: (11) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 35.885879ms)
Jan 21 09:33:45.931: INFO: (11) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 38.370302ms)
Jan 21 09:33:45.933: INFO: (11) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 38.30944ms)
Jan 21 09:33:45.951: INFO: (11) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 58.522348ms)
Jan 21 09:33:45.951: INFO: (11) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 57.456664ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 58.305167ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 57.573456ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 57.251753ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 56.905349ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 58.722976ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 57.473318ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 57.359236ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 57.22375ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 58.263081ms)
Jan 21 09:33:45.952: INFO: (11) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 58.917031ms)
Jan 21 09:33:45.975: INFO: (12) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 22.157545ms)
Jan 21 09:33:45.977: INFO: (12) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 23.630086ms)
Jan 21 09:33:45.977: INFO: (12) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 23.122926ms)
Jan 21 09:33:45.985: INFO: (12) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 31.073699ms)
Jan 21 09:33:45.986: INFO: (12) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 32.8282ms)
Jan 21 09:33:45.986: INFO: (12) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 33.160168ms)
Jan 21 09:33:45.987: INFO: (12) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 33.386159ms)
Jan 21 09:33:45.992: INFO: (12) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 38.39571ms)
Jan 21 09:33:45.993: INFO: (12) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 39.194113ms)
Jan 21 09:33:45.997: INFO: (12) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 42.827079ms)
Jan 21 09:33:46.000: INFO: (12) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 45.19596ms)
Jan 21 09:33:46.001: INFO: (12) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 46.254058ms)
Jan 21 09:33:46.004: INFO: (12) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 49.004317ms)
Jan 21 09:33:46.004: INFO: (12) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 48.434847ms)
Jan 21 09:33:46.004: INFO: (12) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 49.023938ms)
Jan 21 09:33:46.004: INFO: (12) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 49.689851ms)
Jan 21 09:33:46.048: INFO: (13) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 42.769674ms)
Jan 21 09:33:46.048: INFO: (13) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 43.178088ms)
Jan 21 09:33:46.048: INFO: (13) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 43.63137ms)
Jan 21 09:33:46.049: INFO: (13) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 42.619146ms)
Jan 21 09:33:46.054: INFO: (13) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 48.030043ms)
Jan 21 09:33:46.054: INFO: (13) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 48.371036ms)
Jan 21 09:33:46.056: INFO: (13) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 49.887205ms)
Jan 21 09:33:46.056: INFO: (13) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 50.084242ms)
Jan 21 09:33:46.065: INFO: (13) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 59.78129ms)
Jan 21 09:33:46.065: INFO: (13) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 58.470655ms)
Jan 21 09:33:46.065: INFO: (13) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 58.902876ms)
Jan 21 09:33:46.065: INFO: (13) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 61.084577ms)
Jan 21 09:33:46.067: INFO: (13) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 60.9825ms)
Jan 21 09:33:46.069: INFO: (13) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 63.520994ms)
Jan 21 09:33:46.070: INFO: (13) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 64.641821ms)
Jan 21 09:33:46.070: INFO: (13) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 64.958321ms)
Jan 21 09:33:46.112: INFO: (14) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 41.119432ms)
Jan 21 09:33:46.122: INFO: (14) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 51.591596ms)
Jan 21 09:33:46.148: INFO: (14) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 76.219372ms)
Jan 21 09:33:46.148: INFO: (14) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 77.63926ms)
Jan 21 09:33:46.148: INFO: (14) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 77.772771ms)
Jan 21 09:33:46.149: INFO: (14) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 78.458078ms)
Jan 21 09:33:46.161: INFO: (14) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 90.24458ms)
Jan 21 09:33:46.162: INFO: (14) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 90.285034ms)
Jan 21 09:33:46.162: INFO: (14) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 90.256534ms)
Jan 21 09:33:46.171: INFO: (14) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 99.662399ms)
Jan 21 09:33:46.171: INFO: (14) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 100.641518ms)
Jan 21 09:33:46.172: INFO: (14) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 101.366139ms)
Jan 21 09:33:46.172: INFO: (14) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 101.698125ms)
Jan 21 09:33:46.172: INFO: (14) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 100.645417ms)
Jan 21 09:33:46.172: INFO: (14) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 100.901338ms)
Jan 21 09:33:46.172: INFO: (14) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 101.889583ms)
Jan 21 09:33:46.214: INFO: (15) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 33.980203ms)
Jan 21 09:33:46.214: INFO: (15) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 34.318356ms)
Jan 21 09:33:46.222: INFO: (15) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 42.377483ms)
Jan 21 09:33:46.223: INFO: (15) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 43.65649ms)
Jan 21 09:33:46.229: INFO: (15) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 55.216144ms)
Jan 21 09:33:46.229: INFO: (15) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 56.303885ms)
Jan 21 09:33:46.232: INFO: (15) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 59.627165ms)
Jan 21 09:33:46.232: INFO: (15) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 59.084041ms)
Jan 21 09:33:46.251: INFO: (15) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 70.897491ms)
Jan 21 09:33:46.252: INFO: (15) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 72.108428ms)
Jan 21 09:33:46.252: INFO: (15) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 71.814907ms)
Jan 21 09:33:46.261: INFO: (15) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 81.348855ms)
Jan 21 09:33:46.261: INFO: (15) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 81.513898ms)
Jan 21 09:33:46.262: INFO: (15) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 81.628862ms)
Jan 21 09:33:46.262: INFO: (15) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 88.754975ms)
Jan 21 09:33:46.262: INFO: (15) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 81.815242ms)
Jan 21 09:33:46.310: INFO: (16) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 47.477354ms)
Jan 21 09:33:46.315: INFO: (16) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 53.087235ms)
Jan 21 09:33:46.326: INFO: (16) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 60.294003ms)
Jan 21 09:33:46.327: INFO: (16) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 61.129132ms)
Jan 21 09:33:46.331: INFO: (16) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 66.205853ms)
Jan 21 09:33:46.341: INFO: (16) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 75.858552ms)
Jan 21 09:33:46.345: INFO: (16) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 80.495758ms)
Jan 21 09:33:46.356: INFO: (16) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 90.749661ms)
Jan 21 09:33:46.359: INFO: (16) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 94.070369ms)
Jan 21 09:33:46.360: INFO: (16) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 94.543664ms)
Jan 21 09:33:46.360: INFO: (16) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 94.178001ms)
Jan 21 09:33:46.360: INFO: (16) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 94.612543ms)
Jan 21 09:33:46.362: INFO: (16) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 96.257056ms)
Jan 21 09:33:46.364: INFO: (16) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 101.104979ms)
Jan 21 09:33:46.365: INFO: (16) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 98.914722ms)
Jan 21 09:33:46.365: INFO: (16) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 100.555545ms)
Jan 21 09:33:46.421: INFO: (17) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 54.997105ms)
Jan 21 09:33:46.422: INFO: (17) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 55.547229ms)
Jan 21 09:33:46.423: INFO: (17) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 56.016498ms)
Jan 21 09:33:46.423: INFO: (17) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 57.955346ms)
Jan 21 09:33:46.428: INFO: (17) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 62.413342ms)
Jan 21 09:33:46.429: INFO: (17) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 62.912009ms)
Jan 21 09:33:46.430: INFO: (17) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 63.956909ms)
Jan 21 09:33:46.431: INFO: (17) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 64.271379ms)
Jan 21 09:33:46.431: INFO: (17) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 64.277361ms)
Jan 21 09:33:46.431: INFO: (17) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 64.549271ms)
Jan 21 09:33:46.431: INFO: (17) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 64.408802ms)
Jan 21 09:33:46.431: INFO: (17) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 65.029094ms)
Jan 21 09:33:46.435: INFO: (17) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 68.918106ms)
Jan 21 09:33:46.436: INFO: (17) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 69.938483ms)
Jan 21 09:33:46.444: INFO: (17) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 77.979858ms)
Jan 21 09:33:46.445: INFO: (17) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 77.927994ms)
Jan 21 09:33:46.490: INFO: (18) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 45.340902ms)
Jan 21 09:33:46.494: INFO: (18) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 48.458945ms)
Jan 21 09:33:46.495: INFO: (18) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 48.296265ms)
Jan 21 09:33:46.495: INFO: (18) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 49.809673ms)
Jan 21 09:33:46.495: INFO: (18) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 48.546423ms)
Jan 21 09:33:46.508: INFO: (18) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 62.155809ms)
Jan 21 09:33:46.510: INFO: (18) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 61.665685ms)
Jan 21 09:33:46.510: INFO: (18) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 62.029478ms)
Jan 21 09:33:46.510: INFO: (18) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 62.4285ms)
Jan 21 09:33:46.510: INFO: (18) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 62.456984ms)
Jan 21 09:33:46.510: INFO: (18) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 65.283533ms)
Jan 21 09:33:46.510: INFO: (18) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 62.4776ms)
Jan 21 09:33:46.510: INFO: (18) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 62.782106ms)
Jan 21 09:33:46.513: INFO: (18) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 67.041098ms)
Jan 21 09:33:46.515: INFO: (18) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 69.621023ms)
Jan 21 09:33:46.530: INFO: (18) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 82.928278ms)
Jan 21 09:33:46.585: INFO: (19) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname1/proxy/: foo (200; 54.00012ms)
Jan 21 09:33:46.593: INFO: (19) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 61.992748ms)
Jan 21 09:33:46.600: INFO: (19) /api/v1/namespaces/proxy-3674/services/proxy-service-t7wmq:portname2/proxy/: bar (200; 68.644208ms)
Jan 21 09:33:46.600: INFO: (19) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 67.912854ms)
Jan 21 09:33:46.600: INFO: (19) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname1/proxy/: foo (200; 68.373256ms)
Jan 21 09:33:46.604: INFO: (19) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:160/proxy/: foo (200; 71.649191ms)
Jan 21 09:33:46.604: INFO: (19) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname2/proxy/: tls qux (200; 72.058917ms)
Jan 21 09:33:46.604: INFO: (19) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:462/proxy/: tls qux (200; 71.913319ms)
Jan 21 09:33:46.604: INFO: (19) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">test<... (200; 71.804248ms)
Jan 21 09:33:46.604: INFO: (19) /api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/http:proxy-service-t7wmq-tmtg2:1080/proxy/rewriteme">... (200; 72.213923ms)
Jan 21 09:33:46.609: INFO: (19) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2:162/proxy/: bar (200; 78.847365ms)
Jan 21 09:33:46.609: INFO: (19) /api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/proxy-service-t7wmq-tmtg2/proxy/rewriteme">test</a> (200; 78.256639ms)
Jan 21 09:33:46.611: INFO: (19) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:460/proxy/: tls baz (200; 79.237194ms)
Jan 21 09:33:46.616: INFO: (19) /api/v1/namespaces/proxy-3674/services/https:proxy-service-t7wmq:tlsportname1/proxy/: tls baz (200; 85.505187ms)
Jan 21 09:33:46.617: INFO: (19) /api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/: <a href="/api/v1/namespaces/proxy-3674/pods/https:proxy-service-t7wmq-tmtg2:443/proxy/tlsrewritem... (200; 85.261513ms)
Jan 21 09:33:46.616: INFO: (19) /api/v1/namespaces/proxy-3674/services/http:proxy-service-t7wmq:portname2/proxy/: bar (200; 85.780059ms)
STEP: deleting ReplicationController proxy-service-t7wmq in namespace proxy-3674, will wait for the garbage collector to delete the pods
Jan 21 09:33:46.709: INFO: Deleting ReplicationController proxy-service-t7wmq took: 23.79467ms
Jan 21 09:33:46.916: INFO: Terminating ReplicationController proxy-service-t7wmq pods took: 207.5464ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:33:50.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3674" for this suite.

• [SLOW TEST:11.722 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":19,"skipped":333,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:33:50.347: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jan 21 09:33:54.564: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2211 PodName:pod-sharedvolume-e241144e-33db-4052-9f60-a589cbf6548c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:33:54.564: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:33:54.565: INFO: ExecWithOptions: Clientset creation
Jan 21 09:33:54.565: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/emptydir-2211/pods/pod-sharedvolume-e241144e-33db-4052-9f60-a589cbf6548c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Jan 21 09:33:54.772: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:33:54.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2211" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":20,"skipped":338,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:33:54.804: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 21 09:33:54.997: INFO: Waiting up to 5m0s for pod "pod-9f642887-eda1-4433-88b0-7688d35c9e7a" in namespace "emptydir-777" to be "Succeeded or Failed"
Jan 21 09:33:55.016: INFO: Pod "pod-9f642887-eda1-4433-88b0-7688d35c9e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.272808ms
Jan 21 09:33:57.033: INFO: Pod "pod-9f642887-eda1-4433-88b0-7688d35c9e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03529012s
Jan 21 09:33:59.044: INFO: Pod "pod-9f642887-eda1-4433-88b0-7688d35c9e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046364778s
Jan 21 09:34:01.053: INFO: Pod "pod-9f642887-eda1-4433-88b0-7688d35c9e7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055373363s
STEP: Saw pod success
Jan 21 09:34:01.053: INFO: Pod "pod-9f642887-eda1-4433-88b0-7688d35c9e7a" satisfied condition "Succeeded or Failed"
Jan 21 09:34:01.058: INFO: Trying to get logs from node conformance1 pod pod-9f642887-eda1-4433-88b0-7688d35c9e7a container test-container: <nil>
STEP: delete the pod
Jan 21 09:34:01.104: INFO: Waiting for pod pod-9f642887-eda1-4433-88b0-7688d35c9e7a to disappear
Jan 21 09:34:01.112: INFO: Pod pod-9f642887-eda1-4433-88b0-7688d35c9e7a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:34:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-777" for this suite.

• [SLOW TEST:6.341 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":351,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:34:01.146: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jan 21 09:34:01.308: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 21 09:34:06.370: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jan 21 09:34:08.555: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:34:08.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4325" for this suite.

• [SLOW TEST:7.742 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":22,"skipped":358,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:34:08.896: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 21 09:34:09.033: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:34:16.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2823" for this suite.

• [SLOW TEST:7.432 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":23,"skipped":361,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:34:16.328: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jan 21 09:34:16.464: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:34:18.497: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:34:20.472: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 21 09:34:20.504: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:34:22.523: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:34:24.518: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 21 09:34:24.534: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 21 09:34:24.543: INFO: Pod pod-with-prestop-http-hook still exists
Jan 21 09:34:26.543: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 21 09:34:26.713: INFO: Pod pod-with-prestop-http-hook still exists
Jan 21 09:34:28.544: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 21 09:34:28.583: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:34:28.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-542" for this suite.

• [SLOW TEST:12.405 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":363,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:34:28.740: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 21 09:34:28.875: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 21 09:35:28.930: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:35:28.935: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:35:29.055: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jan 21 09:35:29.062: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:35:29.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-909" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:35:29.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4053" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.494 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":25,"skipped":368,"failed":0}
SSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:35:29.235: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:35:33.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6243" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":374,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:35:33.513: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 21 09:35:33.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:35:33.656: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:34.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:35:34.689: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:35.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:35:35.690: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:36.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:35:36.703: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:37.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:35:37.684: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:38.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 09:35:38.725: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 21 09:35:38.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:35:38.852: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:39.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:35:39.897: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:40.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:35:40.882: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:41.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:35:41.924: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:42.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:35:42.900: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:43.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:35:43.895: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:44.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 09:35:44.879: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 09:35:45.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 09:35:45.896: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8796, will wait for the garbage collector to delete the pods
Jan 21 09:35:46.000: INFO: Deleting DaemonSet.extensions daemon-set took: 11.010512ms
Jan 21 09:35:46.300: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.615854ms
Jan 21 09:35:49.921: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 09:35:49.921: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 21 09:35:49.926: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8130"},"items":null}

Jan 21 09:35:49.931: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8130"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:35:49.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8796" for this suite.

• [SLOW TEST:16.452 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":27,"skipped":387,"failed":0}
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:35:49.967: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:35:50.077: INFO: Creating deployment "webserver-deployment"
Jan 21 09:35:50.087: INFO: Waiting for observed generation 1
Jan 21 09:35:52.131: INFO: Waiting for all required pods to come up
Jan 21 09:35:52.151: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 21 09:36:04.193: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 21 09:36:04.204: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 21 09:36:04.225: INFO: Updating deployment webserver-deployment
Jan 21 09:36:04.225: INFO: Waiting for observed generation 2
Jan 21 09:36:06.260: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 21 09:36:06.265: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 21 09:36:06.271: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 21 09:36:06.320: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 21 09:36:06.320: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 21 09:36:06.331: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 21 09:36:06.354: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 21 09:36:06.355: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 21 09:36:06.424: INFO: Updating deployment webserver-deployment
Jan 21 09:36:06.425: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 21 09:36:06.466: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 21 09:36:06.534: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 21 09:36:06.629: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2106  6b8f52bd-c1e9-4ba3-95fd-e8fb5e49f62f 8377 3 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00441e828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-21 09:36:01 +0000 UTC,LastTransitionTime:2022-01-21 09:36:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-01-21 09:36:05 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 21 09:36:06.741: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-2106  5da4fb2c-ad8f-436a-8089-4269922abb2c 8381 3 2022-01-21 09:36:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6b8f52bd-c1e9-4ba3-95fd-e8fb5e49f62f 0xc0048ef237 0xc0048ef238}] []  [{kube-controller-manager Update apps/v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b8f52bd-c1e9-4ba3-95fd-e8fb5e49f62f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048ef428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 21 09:36:06.741: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 21 09:36:06.741: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-2106  e1edd722-2aec-4120-b214-d552116dfcf6 8378 3 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6b8f52bd-c1e9-4ba3-95fd-e8fb5e49f62f 0xc0048ef4a7 0xc0048ef4a8}] []  [{kube-controller-manager Update apps/v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b8f52bd-c1e9-4ba3-95fd-e8fb5e49f62f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:35:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048ef678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 21 09:36:07.104: INFO: Pod "webserver-deployment-566f96c878-bgllv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-bgllv webserver-deployment-566f96c878- deployment-2106  41394b85-00ff-4715-b5f4-c8b5336d5032 8368 0 2022-01-21 09:36:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 5da4fb2c-ad8f-436a-8089-4269922abb2c 0xc00441ebe0 0xc00441ebe1}] []  [{Go-http-client Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5da4fb2c-ad8f-436a-8089-4269922abb2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrnqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrnqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2022-01-21 09:36:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.104: INFO: Pod "webserver-deployment-566f96c878-bshsp" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-bshsp webserver-deployment-566f96c878- deployment-2106  cac3c08b-f870-4070-b2d8-618510aaa9d9 8348 0 2022-01-21 09:36:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 5da4fb2c-ad8f-436a-8089-4269922abb2c 0xc00441edc0 0xc00441edc1}] []  [{Go-http-client Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5da4fb2c-ad8f-436a-8089-4269922abb2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-chtg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chtg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2022-01-21 09:36:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.104: INFO: Pod "webserver-deployment-566f96c878-h95cs" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-h95cs webserver-deployment-566f96c878- deployment-2106  d5f76cbd-3970-4a13-97ac-7a013b1246e0 8340 0 2022-01-21 09:36:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 5da4fb2c-ad8f-436a-8089-4269922abb2c 0xc00441efa0 0xc00441efa1}] []  [{Go-http-client Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5da4fb2c-ad8f-436a-8089-4269922abb2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8p4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8p4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2022-01-21 09:36:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.104: INFO: Pod "webserver-deployment-566f96c878-mbjnv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-mbjnv webserver-deployment-566f96c878- deployment-2106  87a04b61-bfcc-4c54-bdbd-f48bc7ccfe6e 8388 0 2022-01-21 09:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 5da4fb2c-ad8f-436a-8089-4269922abb2c 0xc00441f180 0xc00441f181}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5da4fb2c-ad8f-436a-8089-4269922abb2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vnjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vnjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.105: INFO: Pod "webserver-deployment-566f96c878-wv5nx" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-wv5nx webserver-deployment-566f96c878- deployment-2106  405f9f7d-f706-404a-8668-ba3a748ebf81 8367 0 2022-01-21 09:36:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 5da4fb2c-ad8f-436a-8089-4269922abb2c 0xc00441f2c7 0xc00441f2c8}] []  [{Go-http-client Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5da4fb2c-ad8f-436a-8089-4269922abb2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dczx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dczx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2022-01-21 09:36:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.105: INFO: Pod "webserver-deployment-566f96c878-zvjgj" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-zvjgj webserver-deployment-566f96c878- deployment-2106  46c923d9-d246-4e24-bad5-bf9f7921c5c2 8336 0 2022-01-21 09:36:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 5da4fb2c-ad8f-436a-8089-4269922abb2c 0xc00441f4b0 0xc00441f4b1}] []  [{Go-http-client Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-21 09:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5da4fb2c-ad8f-436a-8089-4269922abb2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzvc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzvc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2022-01-21 09:36:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.105: INFO: Pod "webserver-deployment-5d9fdcc779-26xhz" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-26xhz webserver-deployment-5d9fdcc779- deployment-2106  d6a701b1-e4df-4e77-b187-5938c55e4927 8320 0 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:1603f33d5fe8ad72974c18d7b444e17deada5cd40649c00e697b16a1bfa38500 cni.projectcalico.org/podIP:172.16.209.103/32 cni.projectcalico.org/podIPs:172.16.209.103/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc00441f690 0xc00441f691}] []  [{kube-controller-manager Update v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:36:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:36:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kbqnj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kbqnj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:172.16.209.103,StartTime:2022-01-21 09:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:36:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://de8d264ccdf85df5745b75255b7e9ff1746feed433cf4eb99f3f7f05f6a8219d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.209.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.106: INFO: Pod "webserver-deployment-5d9fdcc779-9c26w" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9c26w webserver-deployment-5d9fdcc779- deployment-2106  88276678-83fe-4a6a-9cbd-7d6a9a8d72ba 8390 0 2022-01-21 09:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc00441f8a0 0xc00441f8a1}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mjwt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mjwt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.106: INFO: Pod "webserver-deployment-5d9fdcc779-9fdr6" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9fdr6 webserver-deployment-5d9fdcc779- deployment-2106  3ebaca9c-048e-43bd-93a9-9ae87ea027ba 8394 0 2022-01-21 09:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc00441fa00 0xc00441fa01}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zl2wp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zl2wp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.106: INFO: Pod "webserver-deployment-5d9fdcc779-d5d6r" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-d5d6r webserver-deployment-5d9fdcc779- deployment-2106  28adf1ad-05d0-4aff-9693-4f58106632cd 8308 0 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:fdccae7edbaff364e27a16c19594af5d01ede122791be10485fc53ced162b846 cni.projectcalico.org/podIP:172.16.209.102/32 cni.projectcalico.org/podIPs:172.16.209.102/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc00441fb37 0xc00441fb38}] []  [{kube-controller-manager Update v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:35:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:36:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-492qh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-492qh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:172.16.209.102,StartTime:2022-01-21 09:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:36:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://6c80ac3d9555478232d9d453d483e4b9529bfade60ca6897b14f98ac3c9ac285,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.209.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.108: INFO: Pod "webserver-deployment-5d9fdcc779-ddqtx" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ddqtx webserver-deployment-5d9fdcc779- deployment-2106  346fdf4b-a0fd-42c3-8168-7fe4cde97a17 8391 0 2022-01-21 09:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc00441fd40 0xc00441fd41}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmzxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmzxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.108: INFO: Pod "webserver-deployment-5d9fdcc779-fntkp" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-fntkp webserver-deployment-5d9fdcc779- deployment-2106  13b2184c-f989-4c88-8a3b-24e502dedab1 8249 0 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:23898c28ac642a600febd1eefc485c08680eb3b2f92bc07b210d613a3bc97a66 cni.projectcalico.org/podIP:172.16.106.144/32 cni.projectcalico.org/podIPs:172.16.106.144/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc00441fea0 0xc00441fea1}] []  [{kube-controller-manager Update v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:35:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:35:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjt8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjt8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:172.16.106.144,StartTime:2022-01-21 09:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:35:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://7f5d984c2dd6e0097f8c15a8d68d417d57c463f302e4944baa43a7954bd7982a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.106.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.108: INFO: Pod "webserver-deployment-5d9fdcc779-gtcbz" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gtcbz webserver-deployment-5d9fdcc779- deployment-2106  0da3ea44-77a0-4012-adbc-5388dbfe6ef0 8270 0 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:f58d3d53a64cfbde41a102a2078e559bfb913a2cac4c34559802604251e00848 cni.projectcalico.org/podIP:172.16.106.146/32 cni.projectcalico.org/podIPs:172.16.106.146/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6e0b0 0xc001f6e0b1}] []  [{kube-controller-manager Update v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:35:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:35:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85v2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85v2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:172.16.106.146,StartTime:2022-01-21 09:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:35:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://07e24115c2854085027cb9ca77545f792fa02cced80891cab98d715b1adff792,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.106.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.109: INFO: Pod "webserver-deployment-5d9fdcc779-jfvtk" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-jfvtk webserver-deployment-5d9fdcc779- deployment-2106  dd1ba995-9f47-47df-a818-74f62e6c5bd9 8311 0 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:1a7111b19be662228d31ea3e9e31f754bce6c3534c0fd59a8895dcc2cb82bad5 cni.projectcalico.org/podIP:172.16.209.101/32 cni.projectcalico.org/podIPs:172.16.209.101/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6e2c0 0xc001f6e2c1}] []  [{kube-controller-manager Update v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:35:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:36:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhr4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhr4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:172.16.209.101,StartTime:2022-01-21 09:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:36:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://d9c9cb03d3d5593d8f905a38a4617bf12096ea6e832246127ca43e52be7dbae9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.209.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.109: INFO: Pod "webserver-deployment-5d9fdcc779-kl7pl" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-kl7pl webserver-deployment-5d9fdcc779- deployment-2106  ac9bb666-47c4-4a3e-87d4-431903330f7f 8282 0 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:fa627f6259465672ef9d1026184250cdb16ce95b4d4e7752262330a5b72e49ac cni.projectcalico.org/podIP:172.16.106.145/32 cni.projectcalico.org/podIPs:172.16.106.145/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6e4d0 0xc001f6e4d1}] []  [{kube-controller-manager Update v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:35:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:35:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68rxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68rxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:172.16.106.145,StartTime:2022-01-21 09:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:35:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://97d10bacdfa7c55aa65845aef591ee3fa60647d4c5810fdab3d7ec3beb2d1236,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.106.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.109: INFO: Pod "webserver-deployment-5d9fdcc779-ljsjz" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ljsjz webserver-deployment-5d9fdcc779- deployment-2106  bd0df1c8-6da9-470e-ad81-a58280449897 8392 0 2022-01-21 09:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6e6e0 0xc001f6e6e1}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lc2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lc2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.109: INFO: Pod "webserver-deployment-5d9fdcc779-nlvsj" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-nlvsj webserver-deployment-5d9fdcc779- deployment-2106  c4a92091-267b-4926-b440-8e39c2b49dd3 8274 0 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:d10cd116432cab32d56e5c3811102c64399396243a166fdecee75e26090a6a73 cni.projectcalico.org/podIP:172.16.106.148/32 cni.projectcalico.org/podIPs:172.16.106.148/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6e827 0xc001f6e828}] []  [{kube-controller-manager Update v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:35:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:35:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xz8zj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xz8zj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:172.16.106.148,StartTime:2022-01-21 09:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:35:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://ff10d7f433c33fceb54d7cd86a6d3e6e13a391ad2d24d02f9b4e79d970bf1679,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.106.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.110: INFO: Pod "webserver-deployment-5d9fdcc779-p9sr8" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-p9sr8 webserver-deployment-5d9fdcc779- deployment-2106  8d0b2f85-a531-4b66-bd47-9c48f44b1de0 8393 0 2022-01-21 09:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6ea30 0xc001f6ea31}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q2j9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q2j9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.110: INFO: Pod "webserver-deployment-5d9fdcc779-v7cfm" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-v7cfm webserver-deployment-5d9fdcc779- deployment-2106  58a31618-0d66-451f-8241-c957c78fcc74 8396 0 2022-01-21 09:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6eb67 0xc001f6eb68}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jl677,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jl677,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.110: INFO: Pod "webserver-deployment-5d9fdcc779-wkp7m" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-wkp7m webserver-deployment-5d9fdcc779- deployment-2106  86e5eaad-f5a4-46af-9517-5d447b3eb409 8278 0 2022-01-21 09:35:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:8be266004330f22dce293a9e294cc549ee9f54787e26e564a0ec8e945e264428 cni.projectcalico.org/podIP:172.16.106.147/32 cni.projectcalico.org/podIPs:172.16.106.147/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6eca7 0xc001f6eca8}] []  [{kube-controller-manager Update v1 2022-01-21 09:35:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:35:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:35:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ncvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ncvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:172.16.106.147,StartTime:2022-01-21 09:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:35:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://8f4113f879de17bab09ba23ceabb8fcc5512ad450fa9431c2f43e92998844aa3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.106.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:36:07.111: INFO: Pod "webserver-deployment-5d9fdcc779-x95kz" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-x95kz webserver-deployment-5d9fdcc779- deployment-2106  4b451904-4ba7-4464-bc28-91449a83af33 8382 0 2022-01-21 09:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 e1edd722-2aec-4120-b214-d552116dfcf6 0xc001f6eeb0 0xc001f6eeb1}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1edd722-2aec-4120-b214-d552116dfcf6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xglj7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xglj7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:36:07.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2106" for this suite.

• [SLOW TEST:18.277 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":28,"skipped":387,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:36:08.244: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1537
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 21 09:36:09.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8696 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jan 21 09:36:19.704: INFO: stderr: ""
Jan 21 09:36:19.704: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
Jan 21 09:36:20.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8696 delete pods e2e-test-httpd-pod'
Jan 21 09:36:33.936: INFO: stderr: ""
Jan 21 09:36:33.936: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:36:33.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8696" for this suite.

• [SLOW TEST:25.720 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1534
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":29,"skipped":405,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:36:33.964: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jan 21 09:36:34.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 create -f -'
Jan 21 09:36:36.708: INFO: stderr: ""
Jan 21 09:36:36.708: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 09:36:36.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 21 09:36:37.007: INFO: stderr: ""
Jan 21 09:36:37.007: INFO: stdout: "update-demo-nautilus-ln8jb update-demo-nautilus-t8h8h "
Jan 21 09:36:37.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get pods update-demo-nautilus-ln8jb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 09:36:37.211: INFO: stderr: ""
Jan 21 09:36:37.211: INFO: stdout: ""
Jan 21 09:36:37.211: INFO: update-demo-nautilus-ln8jb is created but not running
Jan 21 09:36:42.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 21 09:36:42.354: INFO: stderr: ""
Jan 21 09:36:42.354: INFO: stdout: "update-demo-nautilus-ln8jb update-demo-nautilus-t8h8h "
Jan 21 09:36:42.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get pods update-demo-nautilus-ln8jb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 09:36:42.500: INFO: stderr: ""
Jan 21 09:36:42.500: INFO: stdout: "true"
Jan 21 09:36:42.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get pods update-demo-nautilus-ln8jb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 21 09:36:42.662: INFO: stderr: ""
Jan 21 09:36:42.662: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 21 09:36:42.662: INFO: validating pod update-demo-nautilus-ln8jb
Jan 21 09:36:42.675: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 09:36:42.675: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 09:36:42.675: INFO: update-demo-nautilus-ln8jb is verified up and running
Jan 21 09:36:42.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get pods update-demo-nautilus-t8h8h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 09:36:42.812: INFO: stderr: ""
Jan 21 09:36:42.812: INFO: stdout: "true"
Jan 21 09:36:42.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get pods update-demo-nautilus-t8h8h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 21 09:36:42.943: INFO: stderr: ""
Jan 21 09:36:42.943: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 21 09:36:42.943: INFO: validating pod update-demo-nautilus-t8h8h
Jan 21 09:36:42.955: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 09:36:42.955: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 09:36:42.955: INFO: update-demo-nautilus-t8h8h is verified up and running
STEP: using delete to clean up resources
Jan 21 09:36:42.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 delete --grace-period=0 --force -f -'
Jan 21 09:36:43.108: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 09:36:43.108: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 21 09:36:43.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get rc,svc -l name=update-demo --no-headers'
Jan 21 09:36:43.355: INFO: stderr: "No resources found in kubectl-4443 namespace.\n"
Jan 21 09:36:43.355: INFO: stdout: ""
Jan 21 09:36:43.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-4443 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 21 09:36:43.643: INFO: stderr: ""
Jan 21 09:36:43.643: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:36:43.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4443" for this suite.

• [SLOW TEST:9.700 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":30,"skipped":405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:36:43.665: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jan 21 09:36:43.856: INFO: pods: 0 < 3
Jan 21 09:36:45.869: INFO: running pods: 0 < 3
Jan 21 09:36:47.871: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 21 09:36:52.334: INFO: running pods: 2 < 3
Jan 21 09:36:54.345: INFO: running pods: 2 < 3
Jan 21 09:36:56.344: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:36:58.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7154" for this suite.

• [SLOW TEST:15.244 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":31,"skipped":438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:36:58.910: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Jan 21 09:36:59.035: INFO: Creating simple deployment test-deployment-b4rfk
Jan 21 09:36:59.076: INFO: deployment "test-deployment-b4rfk" doesn't have the required revision set
Jan 21 09:37:01.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b4rfk-764bc7c4b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 09:37:03.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b4rfk-764bc7c4b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Jan 21 09:37:05.207: INFO: Deployment test-deployment-b4rfk has Conditions: [{Available True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:37:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4rfk-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Jan 21 09:37:05.313: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 37, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 37, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 37, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 36, 59, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-b4rfk-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jan 21 09:37:05.338: INFO: Observed &Deployment event: ADDED
Jan 21 09:37:05.338: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4rfk-764bc7c4b7"}
Jan 21 09:37:05.341: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.344: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4rfk-764bc7c4b7"}
Jan 21 09:37:05.345: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 21 09:37:05.348: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.348: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 21 09:37:05.348: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b4rfk-764bc7c4b7" is progressing.}
Jan 21 09:37:05.355: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.355: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:37:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 21 09:37:05.356: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4rfk-764bc7c4b7" has successfully progressed.}
Jan 21 09:37:05.356: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.356: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:37:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 21 09:37:05.356: INFO: Observed Deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4rfk-764bc7c4b7" has successfully progressed.}
Jan 21 09:37:05.356: INFO: Found Deployment test-deployment-b4rfk in namespace deployment-9233 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 21 09:37:05.356: INFO: Deployment test-deployment-b4rfk has an updated status
STEP: patching the Statefulset Status
Jan 21 09:37:05.356: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 21 09:37:05.420: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jan 21 09:37:05.471: INFO: Observed &Deployment event: ADDED
Jan 21 09:37:05.471: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4rfk-764bc7c4b7"}
Jan 21 09:37:05.476: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.479: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4rfk-764bc7c4b7"}
Jan 21 09:37:05.481: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 21 09:37:05.485: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.491: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 21 09:37:05.491: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:36:59 +0000 UTC 2022-01-21 09:36:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b4rfk-764bc7c4b7" is progressing.}
Jan 21 09:37:05.491: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.494: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:37:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 21 09:37:05.497: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4rfk-764bc7c4b7" has successfully progressed.}
Jan 21 09:37:05.498: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.503: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:37:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 21 09:37:05.504: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-21 09:37:04 +0000 UTC 2022-01-21 09:36:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4rfk-764bc7c4b7" has successfully progressed.}
Jan 21 09:37:05.504: INFO: Observed deployment test-deployment-b4rfk in namespace deployment-9233 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 21 09:37:05.507: INFO: Observed &Deployment event: MODIFIED
Jan 21 09:37:05.511: INFO: Found deployment test-deployment-b4rfk in namespace deployment-9233 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 21 09:37:05.511: INFO: Deployment test-deployment-b4rfk has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 21 09:37:05.541: INFO: Deployment "test-deployment-b4rfk":
&Deployment{ObjectMeta:{test-deployment-b4rfk  deployment-9233  36665013-8935-4128-8f6d-8fde19c3112c 9008 1 2022-01-21 09:36:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-01-21 09:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-01-21 09:37:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-01-21 09:37:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b2a0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-21 09:37:05 +0000 UTC,LastTransitionTime:2022-01-21 09:37:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-b4rfk-764bc7c4b7" has successfully progressed.,LastUpdateTime:2022-01-21 09:37:05 +0000 UTC,LastTransitionTime:2022-01-21 09:37:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 21 09:37:05.569: INFO: New ReplicaSet "test-deployment-b4rfk-764bc7c4b7" of Deployment "test-deployment-b4rfk":
&ReplicaSet{ObjectMeta:{test-deployment-b4rfk-764bc7c4b7  deployment-9233  e4148d35-4bc3-4697-b54e-7b7b51a02b3e 8975 1 2022-01-21 09:36:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-b4rfk 36665013-8935-4128-8f6d-8fde19c3112c 0xc00423d667 0xc00423d668}] []  [{kube-controller-manager Update apps/v1 2022-01-21 09:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36665013-8935-4128-8f6d-8fde19c3112c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:37:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00423d808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 21 09:37:05.599: INFO: Pod "test-deployment-b4rfk-764bc7c4b7-kmbgd" is available:
&Pod{ObjectMeta:{test-deployment-b4rfk-764bc7c4b7-kmbgd test-deployment-b4rfk-764bc7c4b7- deployment-9233  dc82db6b-fda8-4c8d-aba0-22a9f45d28bc 8973 0 2022-01-21 09:36:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/containerID:f9490c4ade01d777b707acf7bf4bc03e9282404c6b9441da57be0b82751784a7 cni.projectcalico.org/podIP:172.16.209.110/32 cni.projectcalico.org/podIPs:172.16.209.110/32] [{apps/v1 ReplicaSet test-deployment-b4rfk-764bc7c4b7 e4148d35-4bc3-4697-b54e-7b7b51a02b3e 0xc00423dd97 0xc00423dd98}] []  [{kube-controller-manager Update v1 2022-01-21 09:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4148d35-4bc3-4697-b54e-7b7b51a02b3e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:37:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:37:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-svdfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-svdfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:37:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:37:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:36:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:172.16.209.110,StartTime:2022-01-21 09:36:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:37:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://c1630ccfc15b4f43f43c87b40633acee452020b829b12377128732d27c5103f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.209.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:37:05.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9233" for this suite.

• [SLOW TEST:6.731 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":32,"skipped":471,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:37:05.710: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:37:05.927: INFO: The status of Pod busybox-host-aliases525c73b4-df9b-4251-9553-ee0c6ffcf70d is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:37:07.985: INFO: The status of Pod busybox-host-aliases525c73b4-df9b-4251-9553-ee0c6ffcf70d is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:37:09.938: INFO: The status of Pod busybox-host-aliases525c73b4-df9b-4251-9553-ee0c6ffcf70d is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:37:09.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1782" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":483,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:37:10.003: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8837
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8837
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8837
Jan 21 09:37:10.210: INFO: Found 0 stateful pods, waiting for 1
Jan 21 09:37:20.221: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 21 09:37:20.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8837 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 09:37:20.612: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 09:37:20.612: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 09:37:20.612: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 09:37:20.620: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 21 09:37:30.633: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 09:37:30.633: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 09:37:30.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996685s
Jan 21 09:37:31.678: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992058849s
Jan 21 09:37:32.690: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.980870639s
Jan 21 09:37:33.725: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.969599181s
Jan 21 09:37:34.734: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.934579997s
Jan 21 09:37:35.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.924266552s
Jan 21 09:37:36.762: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.91104714s
Jan 21 09:37:37.792: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.892460302s
Jan 21 09:37:38.803: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.867377323s
Jan 21 09:37:39.866: INFO: Verifying statefulset ss doesn't scale past 1 for another 808.428994ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8837
Jan 21 09:37:40.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8837 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 09:37:41.262: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 21 09:37:41.262: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 09:37:41.262: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 21 09:37:41.311: INFO: Found 1 stateful pods, waiting for 3
Jan 21 09:37:51.326: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 09:37:51.326: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 09:37:51.327: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 21 09:37:51.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8837 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 09:37:51.713: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 09:37:51.713: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 09:37:51.713: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 09:37:51.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8837 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 09:37:52.079: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 09:37:52.079: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 09:37:52.079: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 09:37:52.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8837 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 09:37:52.630: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 09:37:52.630: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 09:37:52.630: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 09:37:52.630: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 09:37:52.653: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 09:37:52.653: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 09:37:52.653: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 09:37:52.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996678s
Jan 21 09:37:53.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986063065s
Jan 21 09:37:54.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.970756059s
Jan 21 09:37:55.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.959114042s
Jan 21 09:37:56.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945587511s
Jan 21 09:37:57.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.918601638s
Jan 21 09:37:58.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.905867197s
Jan 21 09:37:59.808: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.89327022s
Jan 21 09:38:00.822: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.880896784s
Jan 21 09:38:01.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 866.281472ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8837
Jan 21 09:38:02.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8837 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 09:38:03.253: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 21 09:38:03.253: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 09:38:03.253: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 21 09:38:03.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8837 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 09:38:03.662: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 21 09:38:03.662: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 09:38:03.662: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 21 09:38:03.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8837 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 09:38:04.003: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 21 09:38:04.003: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 09:38:04.003: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 21 09:38:04.003: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 21 09:38:14.052: INFO: Deleting all statefulset in ns statefulset-8837
Jan 21 09:38:14.057: INFO: Scaling statefulset ss to 0
Jan 21 09:38:14.077: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 09:38:14.082: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:38:14.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8837" for this suite.

• [SLOW TEST:64.169 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":34,"skipped":497,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:38:14.176: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Jan 21 09:38:14.293: INFO: created test-pod-1
Jan 21 09:38:18.308: INFO: running and ready test-pod-1
Jan 21 09:38:18.333: INFO: created test-pod-2
Jan 21 09:38:24.416: INFO: running and ready test-pod-2
Jan 21 09:38:24.435: INFO: created test-pod-3
Jan 21 09:38:28.491: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Jan 21 09:38:28.731: INFO: Pod quantity 3 is different from expected quantity 0
Jan 21 09:38:29.751: INFO: Pod quantity 3 is different from expected quantity 0
Jan 21 09:38:30.802: INFO: Pod quantity 2 is different from expected quantity 0
Jan 21 09:38:31.748: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:38:32.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3151" for this suite.

• [SLOW TEST:18.596 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":35,"skipped":515,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:38:32.779: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 21 09:38:32.880: INFO: Waiting up to 5m0s for pod "pod-f708d970-83df-4f93-b75e-7dc803ddc793" in namespace "emptydir-698" to be "Succeeded or Failed"
Jan 21 09:38:32.891: INFO: Pod "pod-f708d970-83df-4f93-b75e-7dc803ddc793": Phase="Pending", Reason="", readiness=false. Elapsed: 10.706647ms
Jan 21 09:38:34.909: INFO: Pod "pod-f708d970-83df-4f93-b75e-7dc803ddc793": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028347773s
Jan 21 09:38:36.925: INFO: Pod "pod-f708d970-83df-4f93-b75e-7dc803ddc793": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044072526s
STEP: Saw pod success
Jan 21 09:38:36.925: INFO: Pod "pod-f708d970-83df-4f93-b75e-7dc803ddc793" satisfied condition "Succeeded or Failed"
Jan 21 09:38:36.931: INFO: Trying to get logs from node conformance1 pod pod-f708d970-83df-4f93-b75e-7dc803ddc793 container test-container: <nil>
STEP: delete the pod
Jan 21 09:38:37.025: INFO: Waiting for pod pod-f708d970-83df-4f93-b75e-7dc803ddc793 to disappear
Jan 21 09:38:37.036: INFO: Pod pod-f708d970-83df-4f93-b75e-7dc803ddc793 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:38:37.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-698" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":519,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:38:37.099: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:38:37.264: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 21 09:38:42.276: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 21 09:38:42.276: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 21 09:38:42.351: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4800  88b0896d-5d80-4d08-8d87-93bffbf633ae 9494 1 2022-01-21 09:38:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-01-21 09:38:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049fcfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 21 09:38:42.361: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 21 09:38:42.361: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 21 09:38:42.361: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4800  eccc3a6d-e1a2-4b5c-be84-b3e7690cd76b 9496 1 2022-01-21 09:38:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 88b0896d-5d80-4d08-8d87-93bffbf633ae 0xc004998a0f 0xc004998a20}] []  [{e2e.test Update apps/v1 2022-01-21 09:38:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:38:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-01-21 09:38:42 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"88b0896d-5d80-4d08-8d87-93bffbf633ae\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004998ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 21 09:38:42.387: INFO: Pod "test-cleanup-controller-88nd9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-88nd9 test-cleanup-controller- deployment-4800  3838ef62-71cc-45a6-bce9-99d4cb7dc37a 9489 0 2022-01-21 09:38:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:39532c43a7fb6ef0aceaab4e93822ff7c5e42b78a11b8f827ab2eadd5cb4ff68 cni.projectcalico.org/podIP:172.16.209.117/32 cni.projectcalico.org/podIPs:172.16.209.117/32] [{apps/v1 ReplicaSet test-cleanup-controller eccc3a6d-e1a2-4b5c-be84-b3e7690cd76b 0xc004998d9f 0xc004998db0}] []  [{kube-controller-manager Update v1 2022-01-21 09:38:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eccc3a6d-e1a2-4b5c-be84-b3e7690cd76b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-01-21 09:38:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-01-21 09:38:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hghck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hghck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:38:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:38:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:172.16.209.117,StartTime:2022-01-21 09:38:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:38:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://2e7ea417b0c4653e5e0a8706640e7ca1e6149f49cda9f3a0e939ceb2c9e893eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.209.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:38:42.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4800" for this suite.

• [SLOW TEST:5.431 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":37,"skipped":533,"failed":0}
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:38:42.522: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-fa06055f-e6d1-4828-8bf6-cd3835a41bb4
STEP: Creating a pod to test consume configMaps
Jan 21 09:38:42.768: INFO: Waiting up to 5m0s for pod "pod-configmaps-09e2cc52-8edd-40d4-b356-25784fc00df3" in namespace "configmap-5356" to be "Succeeded or Failed"
Jan 21 09:38:42.800: INFO: Pod "pod-configmaps-09e2cc52-8edd-40d4-b356-25784fc00df3": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00655ms
Jan 21 09:38:44.807: INFO: Pod "pod-configmaps-09e2cc52-8edd-40d4-b356-25784fc00df3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038882775s
Jan 21 09:38:46.822: INFO: Pod "pod-configmaps-09e2cc52-8edd-40d4-b356-25784fc00df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053160703s
STEP: Saw pod success
Jan 21 09:38:46.822: INFO: Pod "pod-configmaps-09e2cc52-8edd-40d4-b356-25784fc00df3" satisfied condition "Succeeded or Failed"
Jan 21 09:38:46.827: INFO: Trying to get logs from node dc-hg-2 pod pod-configmaps-09e2cc52-8edd-40d4-b356-25784fc00df3 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 09:38:46.906: INFO: Waiting for pod pod-configmaps-09e2cc52-8edd-40d4-b356-25784fc00df3 to disappear
Jan 21 09:38:46.913: INFO: Pod pod-configmaps-09e2cc52-8edd-40d4-b356-25784fc00df3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:38:46.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5356" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:38:46.934: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:38:55.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1309" for this suite.

• [SLOW TEST:8.209 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:38:55.144: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-tphc
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 09:38:55.324: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tphc" in namespace "subpath-2396" to be "Succeeded or Failed"
Jan 21 09:38:55.369: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Pending", Reason="", readiness=false. Elapsed: 45.237282ms
Jan 21 09:38:57.438: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.113832516s
Jan 21 09:38:59.456: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 4.131654432s
Jan 21 09:39:01.534: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 6.209850688s
Jan 21 09:39:03.544: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 8.219963455s
Jan 21 09:39:05.607: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 10.282918112s
Jan 21 09:39:07.624: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 12.299865923s
Jan 21 09:39:09.631: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 14.307180716s
Jan 21 09:39:11.648: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 16.32407226s
Jan 21 09:39:13.661: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 18.336446797s
Jan 21 09:39:15.674: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 20.349935453s
Jan 21 09:39:17.691: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Running", Reason="", readiness=true. Elapsed: 22.366639756s
Jan 21 09:39:19.723: INFO: Pod "pod-subpath-test-downwardapi-tphc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.398495003s
STEP: Saw pod success
Jan 21 09:39:19.723: INFO: Pod "pod-subpath-test-downwardapi-tphc" satisfied condition "Succeeded or Failed"
Jan 21 09:39:19.740: INFO: Trying to get logs from node conformance1 pod pod-subpath-test-downwardapi-tphc container test-container-subpath-downwardapi-tphc: <nil>
STEP: delete the pod
Jan 21 09:39:19.811: INFO: Waiting for pod pod-subpath-test-downwardapi-tphc to disappear
Jan 21 09:39:19.819: INFO: Pod pod-subpath-test-downwardapi-tphc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tphc
Jan 21 09:39:19.819: INFO: Deleting pod "pod-subpath-test-downwardapi-tphc" in namespace "subpath-2396"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:39:19.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2396" for this suite.

• [SLOW TEST:24.712 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":40,"skipped":600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:39:19.876: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:39:24.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7808" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":41,"skipped":660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:39:24.116: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9272
STEP: creating service affinity-nodeport in namespace services-9272
STEP: creating replication controller affinity-nodeport in namespace services-9272
I0121 09:39:24.261549      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9272, replica count: 3
I0121 09:39:27.366037      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:39:30.366759      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:39:33.367992      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 09:39:33.395: INFO: Creating new exec pod
Jan 21 09:39:38.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9272 exec execpod-affinitydfchv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 21 09:39:38.980: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 21 09:39:38.980: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 09:39:38.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9272 exec execpod-affinitydfchv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.100.37 80'
Jan 21 09:39:39.307: INFO: stderr: "+ nc -v -t -w 2 10.10.100.37 80\nConnection to 10.10.100.37 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 21 09:39:39.307: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 09:39:39.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9272 exec execpod-affinitydfchv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.86 32184'
Jan 21 09:39:39.693: INFO: stderr: "+ nc -v -t -w 2 10.10.1.86 32184\nConnection to 10.10.1.86 32184 port [tcp/*] succeeded!\n+ echo hostName\n"
Jan 21 09:39:39.693: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 09:39:39.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9272 exec execpod-affinitydfchv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.136 32184'
Jan 21 09:39:40.076: INFO: stderr: "+ nc -v -t -w 2 10.10.1.136 32184\n+ echo hostName\nConnection to 10.10.1.136 32184 port [tcp/*] succeeded!\n"
Jan 21 09:39:40.076: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 09:39:40.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9272 exec execpod-affinitydfchv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.86:32184/ ; done'
Jan 21 09:39:40.556: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32184/\n"
Jan 21 09:39:40.556: INFO: stdout: "\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d\naffinity-nodeport-55k5d"
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Received response from host: affinity-nodeport-55k5d
Jan 21 09:39:40.556: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9272, will wait for the garbage collector to delete the pods
Jan 21 09:39:40.687: INFO: Deleting ReplicationController affinity-nodeport took: 15.972821ms
Jan 21 09:39:41.090: INFO: Terminating ReplicationController affinity-nodeport pods took: 403.182454ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:39:44.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9272" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:20.714 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":42,"skipped":696,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:39:44.833: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6141
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6141
STEP: creating replication controller externalsvc in namespace services-6141
I0121 09:39:44.992203      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6141, replica count: 2
I0121 09:39:48.044986      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:39:51.045534      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 21 09:39:51.120: INFO: Creating new exec pod
Jan 21 09:39:55.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6141 exec execpod89bzw -- /bin/sh -x -c nslookup clusterip-service.services-6141.svc.cluster.local'
Jan 21 09:39:55.587: INFO: stderr: "+ nslookup clusterip-service.services-6141.svc.cluster.local\n"
Jan 21 09:39:55.587: INFO: stdout: "Server:\t\t10.10.0.10\nAddress:\t10.10.0.10#53\n\nclusterip-service.services-6141.svc.cluster.local\tcanonical name = externalsvc.services-6141.svc.cluster.local.\nName:\texternalsvc.services-6141.svc.cluster.local\nAddress: 10.10.185.105\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6141, will wait for the garbage collector to delete the pods
Jan 21 09:39:55.665: INFO: Deleting ReplicationController externalsvc took: 17.742812ms
Jan 21 09:39:55.777: INFO: Terminating ReplicationController externalsvc pods took: 112.090201ms
Jan 21 09:39:59.047: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:39:59.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6141" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.337 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":43,"skipped":713,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:39:59.170: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:39:59.336: INFO: The status of Pod busybox-scheduling-8254bf6c-8a43-4b8b-80c6-8c414244bb90 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:40:01.349: INFO: The status of Pod busybox-scheduling-8254bf6c-8a43-4b8b-80c6-8c414244bb90 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:40:03.352: INFO: The status of Pod busybox-scheduling-8254bf6c-8a43-4b8b-80c6-8c414244bb90 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:40:05.361: INFO: The status of Pod busybox-scheduling-8254bf6c-8a43-4b8b-80c6-8c414244bb90 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:40:05.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-352" for this suite.

• [SLOW TEST:6.308 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":725,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:40:05.478: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:40:05.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8984" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":45,"skipped":727,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:40:05.917: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:40:06.047: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3727
I0121 09:40:06.078477      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3727, replica count: 1
I0121 09:40:07.130189      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:40:08.130590      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:40:09.131477      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:40:10.132033      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 09:40:10.283: INFO: Created: latency-svc-hhd22
Jan 21 09:40:10.338: INFO: Got endpoints: latency-svc-hhd22 [106.107443ms]
Jan 21 09:40:10.419: INFO: Created: latency-svc-5vg8n
Jan 21 09:40:10.512: INFO: Created: latency-svc-smskt
Jan 21 09:40:10.513: INFO: Created: latency-svc-wc8zz
Jan 21 09:40:10.567: INFO: Got endpoints: latency-svc-wc8zz [226.313626ms]
Jan 21 09:40:10.604: INFO: Got endpoints: latency-svc-5vg8n [264.566198ms]
Jan 21 09:40:10.606: INFO: Got endpoints: latency-svc-smskt [267.391765ms]
Jan 21 09:40:10.671: INFO: Created: latency-svc-bq5zc
Jan 21 09:40:10.687: INFO: Got endpoints: latency-svc-bq5zc [348.2086ms]
Jan 21 09:40:10.774: INFO: Created: latency-svc-nw5g9
Jan 21 09:40:10.861: INFO: Got endpoints: latency-svc-nw5g9 [521.759256ms]
Jan 21 09:40:11.231: INFO: Created: latency-svc-9sjlb
Jan 21 09:40:11.232: INFO: Created: latency-svc-vf95p
Jan 21 09:40:11.258: INFO: Created: latency-svc-crk4r
Jan 21 09:40:11.259: INFO: Created: latency-svc-prg4x
Jan 21 09:40:11.259: INFO: Created: latency-svc-xnlm4
Jan 21 09:40:11.262: INFO: Created: latency-svc-m2s9z
Jan 21 09:40:11.270: INFO: Created: latency-svc-h4w5n
Jan 21 09:40:11.270: INFO: Created: latency-svc-4k7bt
Jan 21 09:40:11.271: INFO: Created: latency-svc-z29kw
Jan 21 09:40:11.271: INFO: Created: latency-svc-9flt8
Jan 21 09:40:11.271: INFO: Created: latency-svc-9f7c4
Jan 21 09:40:11.272: INFO: Created: latency-svc-b9gcx
Jan 21 09:40:11.276: INFO: Created: latency-svc-2zqvd
Jan 21 09:40:11.281: INFO: Created: latency-svc-l7bdx
Jan 21 09:40:11.281: INFO: Created: latency-svc-zcjs2
Jan 21 09:40:11.290: INFO: Got endpoints: latency-svc-vf95p [685.742323ms]
Jan 21 09:40:11.336: INFO: Got endpoints: latency-svc-h4w5n [648.493434ms]
Jan 21 09:40:11.351: INFO: Got endpoints: latency-svc-9sjlb [1.010956425s]
Jan 21 09:40:11.392: INFO: Got endpoints: latency-svc-crk4r [1.052362339s]
Jan 21 09:40:11.392: INFO: Got endpoints: latency-svc-l7bdx [786.397794ms]
Jan 21 09:40:11.449: INFO: Got endpoints: latency-svc-zcjs2 [1.108831474s]
Jan 21 09:40:11.452: INFO: Got endpoints: latency-svc-4k7bt [1.111045976s]
Jan 21 09:40:11.530: INFO: Created: latency-svc-tb4wn
Jan 21 09:40:11.533: INFO: Got endpoints: latency-svc-xnlm4 [1.193574589s]
Jan 21 09:40:11.540: INFO: Got endpoints: latency-svc-prg4x [679.014231ms]
Jan 21 09:40:11.545: INFO: Got endpoints: latency-svc-m2s9z [1.203233787s]
Jan 21 09:40:11.568: INFO: Got endpoints: latency-svc-9flt8 [1.226369213s]
Jan 21 09:40:11.577: INFO: Got endpoints: latency-svc-z29kw [1.235620355s]
Jan 21 09:40:11.583: INFO: Created: latency-svc-6scd7
Jan 21 09:40:11.626: INFO: Got endpoints: latency-svc-b9gcx [1.0579947s]
Jan 21 09:40:11.626: INFO: Got endpoints: latency-svc-9f7c4 [1.285800291s]
Jan 21 09:40:11.643: INFO: Got endpoints: latency-svc-tb4wn [351.78639ms]
Jan 21 09:40:11.660: INFO: Created: latency-svc-sh4j2
Jan 21 09:40:11.660: INFO: Got endpoints: latency-svc-2zqvd [1.320206579s]
Jan 21 09:40:11.669: INFO: Got endpoints: latency-svc-6scd7 [332.531149ms]
Jan 21 09:40:11.677: INFO: Created: latency-svc-pfdl7
Jan 21 09:40:11.727: INFO: Got endpoints: latency-svc-pfdl7 [375.920082ms]
Jan 21 09:40:11.772: INFO: Got endpoints: latency-svc-sh4j2 [380.082567ms]
Jan 21 09:40:11.801: INFO: Created: latency-svc-2nwgq
Jan 21 09:40:11.835: INFO: Created: latency-svc-fwpjk
Jan 21 09:40:11.842: INFO: Got endpoints: latency-svc-fwpjk [392.286523ms]
Jan 21 09:40:11.840: INFO: Got endpoints: latency-svc-2nwgq [447.42119ms]
Jan 21 09:40:11.887: INFO: Created: latency-svc-f2kds
Jan 21 09:40:11.891: INFO: Created: latency-svc-8hd5r
Jan 21 09:40:11.903: INFO: Got endpoints: latency-svc-f2kds [450.908987ms]
Jan 21 09:40:11.935: INFO: Got endpoints: latency-svc-8hd5r [401.698182ms]
Jan 21 09:40:11.936: INFO: Created: latency-svc-qfgmz
Jan 21 09:40:11.984: INFO: Created: latency-svc-nk44q
Jan 21 09:40:11.999: INFO: Got endpoints: latency-svc-qfgmz [453.610451ms]
Jan 21 09:40:12.025: INFO: Created: latency-svc-hbmsv
Jan 21 09:40:12.049: INFO: Got endpoints: latency-svc-nk44q [504.372798ms]
Jan 21 09:40:12.114: INFO: Created: latency-svc-6f84s
Jan 21 09:40:12.115: INFO: Created: latency-svc-fmqwc
Jan 21 09:40:12.117: INFO: Got endpoints: latency-svc-hbmsv [548.666291ms]
Jan 21 09:40:12.151: INFO: Got endpoints: latency-svc-6f84s [573.757162ms]
Jan 21 09:40:12.181: INFO: Created: latency-svc-sszvc
Jan 21 09:40:12.185: INFO: Got endpoints: latency-svc-fmqwc [558.616117ms]
Jan 21 09:40:12.198: INFO: Created: latency-svc-wmf94
Jan 21 09:40:12.210: INFO: Got endpoints: latency-svc-sszvc [581.606718ms]
Jan 21 09:40:12.262: INFO: Got endpoints: latency-svc-wmf94 [592.844116ms]
Jan 21 09:40:12.263: INFO: Created: latency-svc-p2z7d
Jan 21 09:40:12.272: INFO: Got endpoints: latency-svc-p2z7d [623.388836ms]
Jan 21 09:40:12.318: INFO: Created: latency-svc-sd59d
Jan 21 09:40:12.358: INFO: Created: latency-svc-4xpj6
Jan 21 09:40:12.377: INFO: Got endpoints: latency-svc-4xpj6 [649.357246ms]
Jan 21 09:40:12.384: INFO: Got endpoints: latency-svc-sd59d [721.196062ms]
Jan 21 09:40:12.960: INFO: Created: latency-svc-6qb9f
Jan 21 09:40:12.960: INFO: Created: latency-svc-d657k
Jan 21 09:40:12.961: INFO: Created: latency-svc-vsm6l
Jan 21 09:40:12.961: INFO: Created: latency-svc-8jqn2
Jan 21 09:40:12.961: INFO: Created: latency-svc-r4qnh
Jan 21 09:40:12.961: INFO: Created: latency-svc-dmqw2
Jan 21 09:40:12.962: INFO: Created: latency-svc-6krdc
Jan 21 09:40:12.962: INFO: Created: latency-svc-9tpwj
Jan 21 09:40:12.962: INFO: Created: latency-svc-4dfbc
Jan 21 09:40:12.963: INFO: Created: latency-svc-4l6hk
Jan 21 09:40:12.968: INFO: Created: latency-svc-qvf92
Jan 21 09:40:12.968: INFO: Created: latency-svc-hnvgh
Jan 21 09:40:12.968: INFO: Created: latency-svc-vkxt8
Jan 21 09:40:12.968: INFO: Created: latency-svc-dsjw8
Jan 21 09:40:12.968: INFO: Created: latency-svc-t2cgf
Jan 21 09:40:13.054: INFO: Got endpoints: latency-svc-vkxt8 [781.915654ms]
Jan 21 09:40:13.141: INFO: Got endpoints: latency-svc-6qb9f [756.821548ms]
Jan 21 09:40:13.149: INFO: Got endpoints: latency-svc-9tpwj [938.662524ms]
Jan 21 09:40:13.169: INFO: Created: latency-svc-7xfc4
Jan 21 09:40:13.178: INFO: Got endpoints: latency-svc-r4qnh [916.156752ms]
Jan 21 09:40:13.182: INFO: Got endpoints: latency-svc-vsm6l [1.183565413s]
Jan 21 09:40:13.183: INFO: Got endpoints: latency-svc-4l6hk [991.70424ms]
Jan 21 09:40:13.236: INFO: Got endpoints: latency-svc-4dfbc [1.426537753s]
Jan 21 09:40:13.236: INFO: Got endpoints: latency-svc-qvf92 [1.38930987s]
Jan 21 09:40:13.290: INFO: Got endpoints: latency-svc-6krdc [1.38651364s]
Jan 21 09:40:13.291: INFO: Got endpoints: latency-svc-dsjw8 [1.35559839s]
Jan 21 09:40:13.305: INFO: Got endpoints: latency-svc-8jqn2 [1.449723466s]
Jan 21 09:40:13.369: INFO: Created: latency-svc-n2n9v
Jan 21 09:40:13.381: INFO: Got endpoints: latency-svc-hnvgh [1.003242462s]
Jan 21 09:40:13.399: INFO: Created: latency-svc-t48h6
Jan 21 09:40:13.408: INFO: Got endpoints: latency-svc-t2cgf [1.359172911s]
Jan 21 09:40:13.409: INFO: Got endpoints: latency-svc-dmqw2 [1.289724266s]
Jan 21 09:40:13.415: INFO: Got endpoints: latency-svc-d657k [1.264733607s]
Jan 21 09:40:13.477: INFO: Got endpoints: latency-svc-7xfc4 [422.807013ms]
Jan 21 09:40:13.483: INFO: Created: latency-svc-2njfr
Jan 21 09:40:13.545: INFO: Created: latency-svc-rbjtb
Jan 21 09:40:13.555: INFO: Got endpoints: latency-svc-t48h6 [405.674458ms]
Jan 21 09:40:13.555: INFO: Got endpoints: latency-svc-n2n9v [414.435674ms]
Jan 21 09:40:13.578: INFO: Got endpoints: latency-svc-2njfr [389.260151ms]
Jan 21 09:40:13.579: INFO: Got endpoints: latency-svc-rbjtb [386.643557ms]
Jan 21 09:40:13.603: INFO: Created: latency-svc-sm5fj
Jan 21 09:40:13.635: INFO: Created: latency-svc-29c2w
Jan 21 09:40:13.700: INFO: Got endpoints: latency-svc-sm5fj [514.592951ms]
Jan 21 09:40:13.715: INFO: Created: latency-svc-vtmwq
Jan 21 09:40:13.731: INFO: Got endpoints: latency-svc-29c2w [424.954674ms]
Jan 21 09:40:13.731: INFO: Created: latency-svc-ng7ps
Jan 21 09:40:13.782: INFO: Got endpoints: latency-svc-ng7ps [545.676141ms]
Jan 21 09:40:13.782: INFO: Got endpoints: latency-svc-vtmwq [491.45228ms]
Jan 21 09:40:13.789: INFO: Created: latency-svc-df54g
Jan 21 09:40:13.811: INFO: Created: latency-svc-hzqr2
Jan 21 09:40:13.819: INFO: Got endpoints: latency-svc-df54g [528.549353ms]
Jan 21 09:40:13.833: INFO: Created: latency-svc-nszfp
Jan 21 09:40:13.852: INFO: Got endpoints: latency-svc-hzqr2 [615.96659ms]
Jan 21 09:40:13.876: INFO: Got endpoints: latency-svc-nszfp [495.051338ms]
Jan 21 09:40:13.896: INFO: Created: latency-svc-n2csx
Jan 21 09:40:13.918: INFO: Got endpoints: latency-svc-n2csx [509.246013ms]
Jan 21 09:40:13.929: INFO: Created: latency-svc-f5bwr
Jan 21 09:40:13.945: INFO: Got endpoints: latency-svc-f5bwr [529.959528ms]
Jan 21 09:40:13.967: INFO: Created: latency-svc-h92jh
Jan 21 09:40:13.976: INFO: Got endpoints: latency-svc-h92jh [566.220066ms]
Jan 21 09:40:13.994: INFO: Created: latency-svc-4m8xn
Jan 21 09:40:14.009: INFO: Created: latency-svc-4krxh
Jan 21 09:40:14.039: INFO: Got endpoints: latency-svc-4krxh [484.136944ms]
Jan 21 09:40:14.040: INFO: Got endpoints: latency-svc-4m8xn [563.024882ms]
Jan 21 09:40:14.284: INFO: Created: latency-svc-x5t8z
Jan 21 09:40:14.303: INFO: Created: latency-svc-mswgl
Jan 21 09:40:14.303: INFO: Created: latency-svc-tgxlp
Jan 21 09:40:14.303: INFO: Created: latency-svc-v95xn
Jan 21 09:40:14.304: INFO: Created: latency-svc-mf9mj
Jan 21 09:40:14.394: INFO: Created: latency-svc-dbx6l
Jan 21 09:40:14.396: INFO: Created: latency-svc-wzch8
Jan 21 09:40:14.396: INFO: Created: latency-svc-qwh48
Jan 21 09:40:14.396: INFO: Created: latency-svc-t7qp8
Jan 21 09:40:14.397: INFO: Created: latency-svc-nfgxp
Jan 21 09:40:14.397: INFO: Created: latency-svc-87gxs
Jan 21 09:40:14.403: INFO: Created: latency-svc-nhq6s
Jan 21 09:40:14.405: INFO: Created: latency-svc-zqzsn
Jan 21 09:40:14.405: INFO: Created: latency-svc-t8lj8
Jan 21 09:40:14.406: INFO: Created: latency-svc-vc2cg
Jan 21 09:40:14.480: INFO: Got endpoints: latency-svc-zqzsn [749.646535ms]
Jan 21 09:40:14.482: INFO: Got endpoints: latency-svc-x5t8z [662.546467ms]
Jan 21 09:40:14.482: INFO: Got endpoints: latency-svc-v95xn [903.582362ms]
Jan 21 09:40:14.487: INFO: Got endpoints: latency-svc-mf9mj [787.031344ms]
Jan 21 09:40:14.489: INFO: Got endpoints: latency-svc-mswgl [908.436075ms]
Jan 21 09:40:14.548: INFO: Got endpoints: latency-svc-87gxs [765.176129ms]
Jan 21 09:40:14.599: INFO: Got endpoints: latency-svc-tgxlp [560.049453ms]
Jan 21 09:40:14.600: INFO: Got endpoints: latency-svc-nfgxp [817.762176ms]
Jan 21 09:40:14.600: INFO: Got endpoints: latency-svc-t7qp8 [560.028889ms]
Jan 21 09:40:14.601: INFO: Got endpoints: latency-svc-wzch8 [682.472116ms]
Jan 21 09:40:14.640: INFO: Got endpoints: latency-svc-dbx6l [787.90499ms]
Jan 21 09:40:14.644: INFO: Created: latency-svc-rtqrz
Jan 21 09:40:14.720: INFO: Got endpoints: latency-svc-qwh48 [843.639465ms]
Jan 21 09:40:14.721: INFO: Created: latency-svc-t97rv
Jan 21 09:40:14.769: INFO: Created: latency-svc-99b5c
Jan 21 09:40:14.769: INFO: Got endpoints: latency-svc-nhq6s [823.921229ms]
Jan 21 09:40:14.770: INFO: Got endpoints: latency-svc-vc2cg [794.128097ms]
Jan 21 09:40:14.782: INFO: Got endpoints: latency-svc-t8lj8 [1.226155401s]
Jan 21 09:40:14.799: INFO: Got endpoints: latency-svc-t97rv [309.788963ms]
Jan 21 09:40:14.800: INFO: Got endpoints: latency-svc-rtqrz [318.885957ms]
Jan 21 09:40:14.811: INFO: Created: latency-svc-wb5zc
Jan 21 09:40:14.850: INFO: Got endpoints: latency-svc-wb5zc [368.046324ms]
Jan 21 09:40:14.851: INFO: Got endpoints: latency-svc-99b5c [369.475507ms]
Jan 21 09:40:14.855: INFO: Created: latency-svc-h9k5f
Jan 21 09:40:14.894: INFO: Created: latency-svc-pttpf
Jan 21 09:40:14.895: INFO: Created: latency-svc-thcrw
Jan 21 09:40:14.895: INFO: Got endpoints: latency-svc-thcrw [347.429549ms]
Jan 21 09:40:14.948: INFO: Got endpoints: latency-svc-h9k5f [460.020338ms]
Jan 21 09:40:14.982: INFO: Created: latency-svc-526ll
Jan 21 09:40:14.991: INFO: Got endpoints: latency-svc-pttpf [391.521145ms]
Jan 21 09:40:15.032: INFO: Got endpoints: latency-svc-526ll [432.318972ms]
Jan 21 09:40:15.065: INFO: Created: latency-svc-hdnwn
Jan 21 09:40:15.093: INFO: Got endpoints: latency-svc-hdnwn [493.349507ms]
Jan 21 09:40:15.216: INFO: Created: latency-svc-js6ql
Jan 21 09:40:15.216: INFO: Created: latency-svc-967s5
Jan 21 09:40:15.217: INFO: Created: latency-svc-8x4vc
Jan 21 09:40:15.242: INFO: Created: latency-svc-ljtsc
Jan 21 09:40:15.244: INFO: Created: latency-svc-hbj4b
Jan 21 09:40:15.244: INFO: Created: latency-svc-ksn5v
Jan 21 09:40:15.244: INFO: Created: latency-svc-5m6xj
Jan 21 09:40:15.244: INFO: Created: latency-svc-jldsh
Jan 21 09:40:15.245: INFO: Created: latency-svc-j6xqw
Jan 21 09:40:15.245: INFO: Created: latency-svc-k9v7m
Jan 21 09:40:15.245: INFO: Created: latency-svc-t4llc
Jan 21 09:40:15.246: INFO: Created: latency-svc-96n77
Jan 21 09:40:15.246: INFO: Created: latency-svc-sxdpc
Jan 21 09:40:15.249: INFO: Created: latency-svc-pzv52
Jan 21 09:40:15.249: INFO: Created: latency-svc-vl7zm
Jan 21 09:40:15.332: INFO: Got endpoints: latency-svc-js6ql [298.057503ms]
Jan 21 09:40:15.332: INFO: Got endpoints: latency-svc-8x4vc [612.655291ms]
Jan 21 09:40:15.355: INFO: Got endpoints: latency-svc-hbj4b [753.870194ms]
Jan 21 09:40:15.355: INFO: Got endpoints: latency-svc-ksn5v [504.863189ms]
Jan 21 09:40:15.358: INFO: Got endpoints: latency-svc-967s5 [588.880287ms]
Jan 21 09:40:15.413: INFO: Got endpoints: latency-svc-5m6xj [634.731593ms]
Jan 21 09:40:15.442: INFO: Created: latency-svc-2rb7w
Jan 21 09:40:15.500: INFO: Created: latency-svc-rxt7w
Jan 21 09:40:15.522: INFO: Created: latency-svc-jhr6z
Jan 21 09:40:15.533: INFO: Got endpoints: latency-svc-j6xqw [734.303197ms]
Jan 21 09:40:15.538: INFO: Got endpoints: latency-svc-ljtsc [897.31746ms]
Jan 21 09:40:15.544: INFO: Got endpoints: latency-svc-jldsh [692.398514ms]
Jan 21 09:40:15.550: INFO: Got endpoints: latency-svc-t4llc [601.896536ms]
Jan 21 09:40:15.550: INFO: Got endpoints: latency-svc-sxdpc [750.545742ms]
Jan 21 09:40:15.617: INFO: Created: latency-svc-62wg6
Jan 21 09:40:15.637: INFO: Got endpoints: latency-svc-2rb7w [304.58214ms]
Jan 21 09:40:15.639: INFO: Got endpoints: latency-svc-k9v7m [743.835324ms]
Jan 21 09:40:15.639: INFO: Got endpoints: latency-svc-vl7zm [648.275273ms]
Jan 21 09:40:15.639: INFO: Got endpoints: latency-svc-pzv52 [857.329888ms]
Jan 21 09:40:15.639: INFO: Got endpoints: latency-svc-96n77 [543.27929ms]
Jan 21 09:40:15.671: INFO: Created: latency-svc-qhj27
Jan 21 09:40:15.708: INFO: Got endpoints: latency-svc-jhr6z [375.92443ms]
Jan 21 09:40:15.709: INFO: Got endpoints: latency-svc-rxt7w [353.624217ms]
Jan 21 09:40:15.711: INFO: Got endpoints: latency-svc-62wg6 [356.278709ms]
Jan 21 09:40:15.756: INFO: Got endpoints: latency-svc-qhj27 [397.344325ms]
Jan 21 09:40:15.770: INFO: Created: latency-svc-8clvb
Jan 21 09:40:15.774: INFO: Got endpoints: latency-svc-8clvb [360.52834ms]
Jan 21 09:40:15.779: INFO: Created: latency-svc-rsj7g
Jan 21 09:40:15.804: INFO: Got endpoints: latency-svc-rsj7g [269.821641ms]
Jan 21 09:40:15.908: INFO: Created: latency-svc-pzflh
Jan 21 09:40:15.927: INFO: Created: latency-svc-lbdlk
Jan 21 09:40:15.932: INFO: Got endpoints: latency-svc-pzflh [381.763985ms]
Jan 21 09:40:15.960: INFO: Created: latency-svc-sljzk
Jan 21 09:40:15.960: INFO: Created: latency-svc-qrzr2
Jan 21 09:40:15.962: INFO: Created: latency-svc-hvmcc
Jan 21 09:40:15.963: INFO: Created: latency-svc-5kxsm
Jan 21 09:40:15.964: INFO: Created: latency-svc-xq8gn
Jan 21 09:40:15.967: INFO: Got endpoints: latency-svc-xq8gn [416.941538ms]
Jan 21 09:40:15.968: INFO: Created: latency-svc-nf8vc
Jan 21 09:40:15.969: INFO: Created: latency-svc-xg4hv
Jan 21 09:40:15.970: INFO: Created: latency-svc-b967x
Jan 21 09:40:15.970: INFO: Created: latency-svc-4l94t
Jan 21 09:40:15.971: INFO: Created: latency-svc-7w8gq
Jan 21 09:40:15.972: INFO: Created: latency-svc-lfrcw
Jan 21 09:40:15.973: INFO: Created: latency-svc-kkp47
Jan 21 09:40:15.977: INFO: Created: latency-svc-75kvq
Jan 21 09:40:16.090: INFO: Created: latency-svc-bntns
Jan 21 09:40:16.104: INFO: Got endpoints: latency-svc-75kvq [428.817948ms]
Jan 21 09:40:16.104: INFO: Got endpoints: latency-svc-lfrcw [566.130182ms]
Jan 21 09:40:16.114: INFO: Got endpoints: latency-svc-xg4hv [454.862979ms]
Jan 21 09:40:16.124: INFO: Got endpoints: latency-svc-hvmcc [414.698643ms]
Jan 21 09:40:16.124: INFO: Got endpoints: latency-svc-4l94t [319.865593ms]
Jan 21 09:40:16.147: INFO: Got endpoints: latency-svc-b967x [472.573849ms]
Jan 21 09:40:16.186: INFO: Got endpoints: latency-svc-7w8gq [506.342968ms]
Jan 21 09:40:16.191: INFO: Created: latency-svc-qh9kr
Jan 21 09:40:16.224: INFO: Got endpoints: latency-svc-5kxsm [515.482369ms]
Jan 21 09:40:16.224: INFO: Got endpoints: latency-svc-lbdlk [450.203516ms]
Jan 21 09:40:16.225: INFO: Got endpoints: latency-svc-nf8vc [468.626203ms]
Jan 21 09:40:16.246: INFO: Created: latency-svc-6cp8x
Jan 21 09:40:16.264: INFO: Created: latency-svc-wqm98
Jan 21 09:40:16.272: INFO: Created: latency-svc-lkx7c
Jan 21 09:40:16.280: INFO: Got endpoints: latency-svc-sljzk [620.304068ms]
Jan 21 09:40:16.322: INFO: Got endpoints: latency-svc-bntns [389.718274ms]
Jan 21 09:40:16.322: INFO: Got endpoints: latency-svc-qh9kr [354.788212ms]
Jan 21 09:40:16.346: INFO: Got endpoints: latency-svc-kkp47 [635.089515ms]
Jan 21 09:40:16.347: INFO: Got endpoints: latency-svc-qrzr2 [798.400044ms]
Jan 21 09:40:16.375: INFO: Created: latency-svc-hckn9
Jan 21 09:40:16.405: INFO: Got endpoints: latency-svc-wqm98 [280.317908ms]
Jan 21 09:40:16.404: INFO: Got endpoints: latency-svc-6cp8x [299.683266ms]
Jan 21 09:40:16.451: INFO: Got endpoints: latency-svc-lkx7c [336.583709ms]
Jan 21 09:40:16.451: INFO: Created: latency-svc-4lmwh
Jan 21 09:40:16.460: INFO: Got endpoints: latency-svc-hckn9 [355.709766ms]
Jan 21 09:40:16.483: INFO: Got endpoints: latency-svc-4lmwh [358.640792ms]
Jan 21 09:40:16.499: INFO: Created: latency-svc-bhdnw
Jan 21 09:40:16.525: INFO: Created: latency-svc-mzsfg
Jan 21 09:40:16.591: INFO: Got endpoints: latency-svc-bhdnw [444.103655ms]
Jan 21 09:40:16.609: INFO: Got endpoints: latency-svc-mzsfg [423.140253ms]
Jan 21 09:40:16.630: INFO: Created: latency-svc-lfksx
Jan 21 09:40:16.649: INFO: Got endpoints: latency-svc-lfksx [425.153087ms]
Jan 21 09:40:16.660: INFO: Created: latency-svc-q67p6
Jan 21 09:40:16.680: INFO: Got endpoints: latency-svc-q67p6 [455.123204ms]
Jan 21 09:40:16.712: INFO: Created: latency-svc-75vnx
Jan 21 09:40:16.721: INFO: Created: latency-svc-j2hdg
Jan 21 09:40:16.739: INFO: Got endpoints: latency-svc-75vnx [514.536063ms]
Jan 21 09:40:16.754: INFO: Got endpoints: latency-svc-j2hdg [473.828885ms]
Jan 21 09:40:16.760: INFO: Created: latency-svc-dxn6s
Jan 21 09:40:16.780: INFO: Created: latency-svc-6q8ls
Jan 21 09:40:16.797: INFO: Created: latency-svc-jl2rl
Jan 21 09:40:16.820: INFO: Created: latency-svc-cvmx5
Jan 21 09:40:16.828: INFO: Got endpoints: latency-svc-dxn6s [505.901921ms]
Jan 21 09:40:16.864: INFO: Created: latency-svc-fqqzs
Jan 21 09:40:16.874: INFO: Created: latency-svc-pmq8f
Jan 21 09:40:16.892: INFO: Got endpoints: latency-svc-6q8ls [570.002277ms]
Jan 21 09:40:16.902: INFO: Created: latency-svc-7c7hs
Jan 21 09:40:16.917: INFO: Got endpoints: latency-svc-jl2rl [562.016606ms]
Jan 21 09:40:16.928: INFO: Created: latency-svc-fhwft
Jan 21 09:40:16.941: INFO: Got endpoints: latency-svc-cvmx5 [581.96524ms]
Jan 21 09:40:16.966: INFO: Created: latency-svc-jsptr
Jan 21 09:40:16.981: INFO: Created: latency-svc-tln67
Jan 21 09:40:16.995: INFO: Created: latency-svc-pxksq
Jan 21 09:40:17.014: INFO: Got endpoints: latency-svc-fqqzs [609.235242ms]
Jan 21 09:40:17.050: INFO: Created: latency-svc-sd47k
Jan 21 09:40:17.068: INFO: Created: latency-svc-kdqvh
Jan 21 09:40:17.070: INFO: Got endpoints: latency-svc-pmq8f [658.202148ms]
Jan 21 09:40:17.084: INFO: Created: latency-svc-j5g8m
Jan 21 09:40:17.096: INFO: Created: latency-svc-wfcxd
Jan 21 09:40:17.110: INFO: Got endpoints: latency-svc-7c7hs [649.865837ms]
Jan 21 09:40:17.113: INFO: Created: latency-svc-hrk87
Jan 21 09:40:17.130: INFO: Created: latency-svc-lqqss
Jan 21 09:40:17.162: INFO: Created: latency-svc-ddhgj
Jan 21 09:40:17.165: INFO: Got endpoints: latency-svc-fhwft [701.660337ms]
Jan 21 09:40:17.190: INFO: Created: latency-svc-szdcg
Jan 21 09:40:17.190: INFO: Created: latency-svc-9hqsf
Jan 21 09:40:17.197: INFO: Got endpoints: latency-svc-jsptr [714.074528ms]
Jan 21 09:40:17.216: INFO: Created: latency-svc-z8ksn
Jan 21 09:40:17.228: INFO: Created: latency-svc-6hrxq
Jan 21 09:40:17.243: INFO: Got endpoints: latency-svc-tln67 [651.623223ms]
Jan 21 09:40:17.272: INFO: Created: latency-svc-4n8td
Jan 21 09:40:17.286: INFO: Created: latency-svc-8c87h
Jan 21 09:40:17.302: INFO: Got endpoints: latency-svc-pxksq [692.110596ms]
Jan 21 09:40:17.329: INFO: Created: latency-svc-tncrf
Jan 21 09:40:17.340: INFO: Created: latency-svc-d2lcs
Jan 21 09:40:17.362: INFO: Got endpoints: latency-svc-sd47k [712.243879ms]
Jan 21 09:40:17.401: INFO: Created: latency-svc-b2c5r
Jan 21 09:40:17.456: INFO: Got endpoints: latency-svc-kdqvh [775.686863ms]
Jan 21 09:40:17.474: INFO: Got endpoints: latency-svc-j5g8m [719.678838ms]
Jan 21 09:40:17.531: INFO: Got endpoints: latency-svc-wfcxd [791.584985ms]
Jan 21 09:40:17.565: INFO: Created: latency-svc-2wrrg
Jan 21 09:40:17.601: INFO: Got endpoints: latency-svc-hrk87 [773.801747ms]
Jan 21 09:40:17.646: INFO: Created: latency-svc-pjvl7
Jan 21 09:40:17.664: INFO: Got endpoints: latency-svc-lqqss [772.047456ms]
Jan 21 09:40:17.706: INFO: Created: latency-svc-x5lmk
Jan 21 09:40:17.716: INFO: Got endpoints: latency-svc-ddhgj [798.748036ms]
Jan 21 09:40:17.751: INFO: Got endpoints: latency-svc-szdcg [809.661741ms]
Jan 21 09:40:17.881: INFO: Got endpoints: latency-svc-9hqsf [866.764346ms]
Jan 21 09:40:17.882: INFO: Created: latency-svc-mcvjj
Jan 21 09:40:17.896: INFO: Got endpoints: latency-svc-z8ksn [823.662038ms]
Jan 21 09:40:17.919: INFO: Created: latency-svc-4rtf4
Jan 21 09:40:17.952: INFO: Got endpoints: latency-svc-6hrxq [841.627758ms]
Jan 21 09:40:17.955: INFO: Created: latency-svc-pq5f8
Jan 21 09:40:17.964: INFO: Got endpoints: latency-svc-8c87h [766.777302ms]
Jan 21 09:40:17.964: INFO: Got endpoints: latency-svc-4n8td [799.317929ms]
Jan 21 09:40:18.044: INFO: Created: latency-svc-d57fq
Jan 21 09:40:18.044: INFO: Got endpoints: latency-svc-tncrf [801.262058ms]
Jan 21 09:40:18.046: INFO: Created: latency-svc-ntv6q
Jan 21 09:40:18.075: INFO: Created: latency-svc-cdttq
Jan 21 09:40:18.100: INFO: Got endpoints: latency-svc-d2lcs [797.888605ms]
Jan 21 09:40:18.136: INFO: Got endpoints: latency-svc-b2c5r [773.858257ms]
Jan 21 09:40:18.154: INFO: Created: latency-svc-5cc5s
Jan 21 09:40:18.182: INFO: Got endpoints: latency-svc-2wrrg [725.913108ms]
Jan 21 09:40:18.183: INFO: Created: latency-svc-ct5qc
Jan 21 09:40:18.244: INFO: Created: latency-svc-shgkb
Jan 21 09:40:18.276: INFO: Got endpoints: latency-svc-pjvl7 [801.791205ms]
Jan 21 09:40:18.306: INFO: Created: latency-svc-4hdxr
Jan 21 09:40:18.312: INFO: Created: latency-svc-4wmj2
Jan 21 09:40:18.324: INFO: Got endpoints: latency-svc-x5lmk [792.5923ms]
Jan 21 09:40:18.420: INFO: Got endpoints: latency-svc-mcvjj [818.880483ms]
Jan 21 09:40:18.422: INFO: Created: latency-svc-z8clw
Jan 21 09:40:18.464: INFO: Got endpoints: latency-svc-4rtf4 [799.731462ms]
Jan 21 09:40:18.470: INFO: Got endpoints: latency-svc-ntv6q [718.220994ms]
Jan 21 09:40:18.470: INFO: Got endpoints: latency-svc-pq5f8 [754.071931ms]
Jan 21 09:40:18.531: INFO: Got endpoints: latency-svc-d57fq [650.243689ms]
Jan 21 09:40:18.571: INFO: Got endpoints: latency-svc-cdttq [675.755655ms]
Jan 21 09:40:18.611: INFO: Got endpoints: latency-svc-5cc5s [659.03956ms]
Jan 21 09:40:18.656: INFO: Got endpoints: latency-svc-ct5qc [691.893718ms]
Jan 21 09:40:18.669: INFO: Created: latency-svc-spd59
Jan 21 09:40:18.739: INFO: Created: latency-svc-nbg8t
Jan 21 09:40:18.887: INFO: Got endpoints: latency-svc-shgkb [842.977976ms]
Jan 21 09:40:18.894: INFO: Created: latency-svc-h2mmr
Jan 21 09:40:18.907: INFO: Got endpoints: latency-svc-4hdxr [942.576928ms]
Jan 21 09:40:18.910: INFO: Created: latency-svc-8kjks
Jan 21 09:40:18.910: INFO: Created: latency-svc-fsskb
Jan 21 09:40:18.910: INFO: Created: latency-svc-sh8tm
Jan 21 09:40:18.979: INFO: Got endpoints: latency-svc-4wmj2 [879.611082ms]
Jan 21 09:40:18.989: INFO: Created: latency-svc-pg5vw
Jan 21 09:40:18.989: INFO: Created: latency-svc-f5p8n
Jan 21 09:40:18.989: INFO: Created: latency-svc-bwwjg
Jan 21 09:40:19.023: INFO: Created: latency-svc-bjltf
Jan 21 09:40:19.045: INFO: Created: latency-svc-fnmwk
Jan 21 09:40:19.061: INFO: Got endpoints: latency-svc-z8clw [925.164907ms]
Jan 21 09:40:19.061: INFO: Created: latency-svc-vksjq
Jan 21 09:40:19.100: INFO: Got endpoints: latency-svc-h2mmr [775.888025ms]
Jan 21 09:40:19.102: INFO: Got endpoints: latency-svc-nbg8t [920.173066ms]
Jan 21 09:40:19.107: INFO: Got endpoints: latency-svc-spd59 [830.536744ms]
Jan 21 09:40:19.108: INFO: Got endpoints: latency-svc-fsskb [638.147727ms]
Jan 21 09:40:19.132: INFO: Created: latency-svc-rqrnc
Jan 21 09:40:19.135: INFO: Created: latency-svc-9ksj8
Jan 21 09:40:19.143: INFO: Got endpoints: latency-svc-8kjks [678.435916ms]
Jan 21 09:40:19.147: INFO: Got endpoints: latency-svc-sh8tm [725.840865ms]
Jan 21 09:40:19.220: INFO: Got endpoints: latency-svc-bjltf [564.013999ms]
Jan 21 09:40:19.263: INFO: Got endpoints: latency-svc-bwwjg [651.472335ms]
Jan 21 09:40:19.342: INFO: Got endpoints: latency-svc-pg5vw [871.185624ms]
Jan 21 09:40:19.380: INFO: Got endpoints: latency-svc-f5p8n [848.730411ms]
Jan 21 09:40:19.404: INFO: Got endpoints: latency-svc-fnmwk [832.231011ms]
Jan 21 09:40:19.457: INFO: Got endpoints: latency-svc-vksjq [569.02748ms]
Jan 21 09:40:19.501: INFO: Got endpoints: latency-svc-9ksj8 [511.326643ms]
Jan 21 09:40:19.551: INFO: Got endpoints: latency-svc-rqrnc [642.4777ms]
Jan 21 09:40:19.551: INFO: Latencies: [226.313626ms 264.566198ms 267.391765ms 269.821641ms 280.317908ms 298.057503ms 299.683266ms 304.58214ms 309.788963ms 318.885957ms 319.865593ms 332.531149ms 336.583709ms 347.429549ms 348.2086ms 351.78639ms 353.624217ms 354.788212ms 355.709766ms 356.278709ms 358.640792ms 360.52834ms 368.046324ms 369.475507ms 375.920082ms 375.92443ms 380.082567ms 381.763985ms 386.643557ms 389.260151ms 389.718274ms 391.521145ms 392.286523ms 397.344325ms 401.698182ms 405.674458ms 414.435674ms 414.698643ms 416.941538ms 422.807013ms 423.140253ms 424.954674ms 425.153087ms 428.817948ms 432.318972ms 444.103655ms 447.42119ms 450.203516ms 450.908987ms 453.610451ms 454.862979ms 455.123204ms 460.020338ms 468.626203ms 472.573849ms 473.828885ms 484.136944ms 491.45228ms 493.349507ms 495.051338ms 504.372798ms 504.863189ms 505.901921ms 506.342968ms 509.246013ms 511.326643ms 514.536063ms 514.592951ms 515.482369ms 521.759256ms 528.549353ms 529.959528ms 543.27929ms 545.676141ms 548.666291ms 558.616117ms 560.028889ms 560.049453ms 562.016606ms 563.024882ms 564.013999ms 566.130182ms 566.220066ms 569.02748ms 570.002277ms 573.757162ms 581.606718ms 581.96524ms 588.880287ms 592.844116ms 601.896536ms 609.235242ms 612.655291ms 615.96659ms 620.304068ms 623.388836ms 634.731593ms 635.089515ms 638.147727ms 642.4777ms 648.275273ms 648.493434ms 649.357246ms 649.865837ms 650.243689ms 651.472335ms 651.623223ms 658.202148ms 659.03956ms 662.546467ms 675.755655ms 678.435916ms 679.014231ms 682.472116ms 685.742323ms 691.893718ms 692.110596ms 692.398514ms 701.660337ms 712.243879ms 714.074528ms 718.220994ms 719.678838ms 721.196062ms 725.840865ms 725.913108ms 734.303197ms 743.835324ms 749.646535ms 750.545742ms 753.870194ms 754.071931ms 756.821548ms 765.176129ms 766.777302ms 772.047456ms 773.801747ms 773.858257ms 775.686863ms 775.888025ms 781.915654ms 786.397794ms 787.031344ms 787.90499ms 791.584985ms 792.5923ms 794.128097ms 797.888605ms 798.400044ms 798.748036ms 799.317929ms 799.731462ms 801.262058ms 801.791205ms 809.661741ms 817.762176ms 818.880483ms 823.662038ms 823.921229ms 830.536744ms 832.231011ms 841.627758ms 842.977976ms 843.639465ms 848.730411ms 857.329888ms 866.764346ms 871.185624ms 879.611082ms 897.31746ms 903.582362ms 908.436075ms 916.156752ms 920.173066ms 925.164907ms 938.662524ms 942.576928ms 991.70424ms 1.003242462s 1.010956425s 1.052362339s 1.0579947s 1.108831474s 1.111045976s 1.183565413s 1.193574589s 1.203233787s 1.226155401s 1.226369213s 1.235620355s 1.264733607s 1.285800291s 1.289724266s 1.320206579s 1.35559839s 1.359172911s 1.38651364s 1.38930987s 1.426537753s 1.449723466s]
Jan 21 09:40:19.552: INFO: 50 %ile: 648.275273ms
Jan 21 09:40:19.552: INFO: 90 %ile: 1.052362339s
Jan 21 09:40:19.552: INFO: 99 %ile: 1.426537753s
Jan 21 09:40:19.552: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:40:19.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3727" for this suite.

• [SLOW TEST:13.683 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":46,"skipped":747,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:40:19.620: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:40:19.741: INFO: Creating simple deployment test-new-deployment
Jan 21 09:40:19.805: INFO: deployment "test-new-deployment" doesn't have the required revision set
Jan 21 09:40:21.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 40, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 40, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 40, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 40, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-5d9fdcc779\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 21 09:40:24.031: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2746  cb06e870-40e9-4049-87b0-e86db3d67c8c 10925 3 2022-01-21 09:40:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-01-21 09:40:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:40:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00428b698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-21 09:40:23 +0000 UTC,LastTransitionTime:2022-01-21 09:40:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-01-21 09:40:23 +0000 UTC,LastTransitionTime:2022-01-21 09:40:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 21 09:40:24.065: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-2746  df792275-6219-4119-b89b-993d94e3cc77 10927 2 2022-01-21 09:40:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment cb06e870-40e9-4049-87b0-e86db3d67c8c 0xc00428baa7 0xc00428baa8}] []  [{kube-controller-manager Update apps/v1 2022-01-21 09:40:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb06e870-40e9-4049-87b0-e86db3d67c8c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:40:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00428bb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 21 09:40:24.083: INFO: Pod "test-new-deployment-5d9fdcc779-5mtt2" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-5mtt2 test-new-deployment-5d9fdcc779- deployment-2746  1bd70aae-0512-4f34-b20c-da6fe7b304ea 10928 0 2022-01-21 09:40:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 df792275-6219-4119-b89b-993d94e3cc77 0xc003cfba17 0xc003cfba18}] []  [{kube-controller-manager Update v1 2022-01-21 09:40:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df792275-6219-4119-b89b-993d94e3cc77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmx9p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmx9p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:40:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 09:40:24.084: INFO: Pod "test-new-deployment-5d9fdcc779-8x5wn" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-8x5wn test-new-deployment-5d9fdcc779- deployment-2746  c260d50c-d8aa-487f-a805-f00b1a35ebb7 10918 0 2022-01-21 09:40:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:00afed6d47b17d82c3647d5d8fadafc1e918f2e7e67cf29283397e2492c985a8 cni.projectcalico.org/podIP:172.16.106.162/32 cni.projectcalico.org/podIPs:172.16.106.162/32] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 df792275-6219-4119-b89b-993d94e3cc77 0xc003cfbb80 0xc003cfbb81}] []  [{kube-controller-manager Update v1 2022-01-21 09:40:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df792275-6219-4119-b89b-993d94e3cc77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:40:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:40:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptxtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptxtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:40:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:40:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:40:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:40:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:172.16.106.162,StartTime:2022-01-21 09:40:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:40:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://91e3a15a8c8580b021af9b0b8a6bee64a80e0209774026368ef5b9cf1cd67a73,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.106.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:40:24.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2746" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":47,"skipped":753,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:40:24.156: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Jan 21 09:40:24.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-1697 create -f -'
Jan 21 09:40:25.526: INFO: stderr: ""
Jan 21 09:40:25.526: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 21 09:40:25.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-1697 diff -f -'
Jan 21 09:40:27.347: INFO: rc: 1
Jan 21 09:40:27.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-1697 delete -f -'
Jan 21 09:40:28.100: INFO: stderr: ""
Jan 21 09:40:28.100: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:40:28.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1697" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":48,"skipped":774,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:40:28.280: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 09:40:34.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 09:40:36.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 40, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 40, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 40, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 40, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 09:40:38.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 40, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 40, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 40, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 40, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 09:40:42.098: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:40:52.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9396" for this suite.
STEP: Destroying namespace "webhook-9396-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:24.809 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":49,"skipped":776,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:40:53.098: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 21 09:40:53.300: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1389  b0d77ff0-13a5-4e23-bffc-ed7fc9049a84 11927 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:40:53.300: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1389  b0d77ff0-13a5-4e23-bffc-ed7fc9049a84 11927 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 21 09:40:53.335: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1389  b0d77ff0-13a5-4e23-bffc-ed7fc9049a84 11928 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:40:53.341: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1389  b0d77ff0-13a5-4e23-bffc-ed7fc9049a84 11928 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 21 09:40:53.411: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1389  b0d77ff0-13a5-4e23-bffc-ed7fc9049a84 11929 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:40:53.411: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1389  b0d77ff0-13a5-4e23-bffc-ed7fc9049a84 11929 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 21 09:40:53.445: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1389  b0d77ff0-13a5-4e23-bffc-ed7fc9049a84 11930 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:40:53.445: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1389  b0d77ff0-13a5-4e23-bffc-ed7fc9049a84 11930 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 21 09:40:53.470: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1389  3f87387d-a1bf-40f7-82b4-7e1cdc1b4d01 11931 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:40:53.471: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1389  3f87387d-a1bf-40f7-82b4-7e1cdc1b4d01 11931 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 21 09:41:03.502: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1389  3f87387d-a1bf-40f7-82b4-7e1cdc1b4d01 11964 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:41:03.503: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1389  3f87387d-a1bf-40f7-82b4-7e1cdc1b4d01 11964 0 2022-01-21 09:40:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-21 09:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:41:13.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1389" for this suite.

• [SLOW TEST:20.461 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":50,"skipped":788,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:41:13.559: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jan 21 09:41:13.739: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 21 09:41:18.763: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jan 21 09:41:18.784: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jan 21 09:41:18.802: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jan 21 09:41:18.811: INFO: Observed &ReplicaSet event: ADDED
Jan 21 09:41:18.812: INFO: Observed &ReplicaSet event: MODIFIED
Jan 21 09:41:18.823: INFO: Observed &ReplicaSet event: MODIFIED
Jan 21 09:41:18.825: INFO: Observed &ReplicaSet event: MODIFIED
Jan 21 09:41:18.826: INFO: Found replicaset test-rs in namespace replicaset-7488 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 21 09:41:18.826: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jan 21 09:41:18.827: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 21 09:41:18.848: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jan 21 09:41:18.872: INFO: Observed &ReplicaSet event: ADDED
Jan 21 09:41:18.874: INFO: Observed &ReplicaSet event: MODIFIED
Jan 21 09:41:18.875: INFO: Observed &ReplicaSet event: MODIFIED
Jan 21 09:41:18.875: INFO: Observed &ReplicaSet event: MODIFIED
Jan 21 09:41:18.876: INFO: Observed replicaset test-rs in namespace replicaset-7488 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 21 09:41:18.877: INFO: Observed &ReplicaSet event: MODIFIED
Jan 21 09:41:18.877: INFO: Found replicaset test-rs in namespace replicaset-7488 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 21 09:41:18.877: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:41:18.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7488" for this suite.

• [SLOW TEST:5.351 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":51,"skipped":790,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:41:18.919: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:41:19.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3815" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":52,"skipped":801,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:41:19.240: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 09:41:20.578: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 21 09:41:22.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 41, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 41, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 41, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 41, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 09:41:25.870: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
Jan 21 09:41:26.000: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 21 09:41:26.150: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:41:26.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4779" for this suite.
STEP: Destroying namespace "webhook-4779-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.248 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":53,"skipped":815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:41:26.488: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 21 09:41:26.811: INFO: The status of Pod pod-update-2a24026e-e5a0-4a65-8599-682a8aebc313 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:41:29.030: INFO: The status of Pod pod-update-2a24026e-e5a0-4a65-8599-682a8aebc313 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:41:30.826: INFO: The status of Pod pod-update-2a24026e-e5a0-4a65-8599-682a8aebc313 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:41:32.822: INFO: The status of Pod pod-update-2a24026e-e5a0-4a65-8599-682a8aebc313 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 21 09:41:33.378: INFO: Successfully updated pod "pod-update-2a24026e-e5a0-4a65-8599-682a8aebc313"
STEP: verifying the updated pod is in kubernetes
Jan 21 09:41:33.393: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:41:33.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1287" for this suite.

• [SLOW TEST:6.934 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:41:33.440: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3243
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3243
STEP: Waiting until pod test-pod will start running in namespace statefulset-3243
STEP: Creating statefulset with conflicting port in namespace statefulset-3243
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3243
Jan 21 09:41:37.700: INFO: Observed stateful pod in namespace: statefulset-3243, name: ss-0, uid: 2a7640fd-5878-4330-9987-c29d86a1b35f, status phase: Pending. Waiting for statefulset controller to delete.
Jan 21 09:41:37.764: INFO: Observed stateful pod in namespace: statefulset-3243, name: ss-0, uid: 2a7640fd-5878-4330-9987-c29d86a1b35f, status phase: Failed. Waiting for statefulset controller to delete.
Jan 21 09:41:37.783: INFO: Observed stateful pod in namespace: statefulset-3243, name: ss-0, uid: 2a7640fd-5878-4330-9987-c29d86a1b35f, status phase: Failed. Waiting for statefulset controller to delete.
Jan 21 09:41:37.798: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3243
STEP: Removing pod with conflicting port in namespace statefulset-3243
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3243 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 21 09:41:45.974: INFO: Deleting all statefulset in ns statefulset-3243
Jan 21 09:41:45.984: INFO: Scaling statefulset ss to 0
Jan 21 09:41:56.026: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 09:41:56.038: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:41:56.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3243" for this suite.

• [SLOW TEST:22.693 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":55,"skipped":919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:41:56.148: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 21 09:41:56.386: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5311  5813dc67-359f-4cf8-97f3-53d35d393147 12254 0 2022-01-21 09:41:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-01-21 09:41:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:41:56.387: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5311  5813dc67-359f-4cf8-97f3-53d35d393147 12255 0 2022-01-21 09:41:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-01-21 09:41:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:41:56.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5311" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":56,"skipped":941,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:41:56.427: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-0cc6304b-2c48-4e6f-9bf0-8e828beba596
STEP: Creating a pod to test consume configMaps
Jan 21 09:41:56.541: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8241e83b-9808-4f12-bfe1-db98f4ecaae8" in namespace "projected-9249" to be "Succeeded or Failed"
Jan 21 09:41:56.587: INFO: Pod "pod-projected-configmaps-8241e83b-9808-4f12-bfe1-db98f4ecaae8": Phase="Pending", Reason="", readiness=false. Elapsed: 45.869059ms
Jan 21 09:41:58.634: INFO: Pod "pod-projected-configmaps-8241e83b-9808-4f12-bfe1-db98f4ecaae8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092649086s
Jan 21 09:42:00.641: INFO: Pod "pod-projected-configmaps-8241e83b-9808-4f12-bfe1-db98f4ecaae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.100346964s
STEP: Saw pod success
Jan 21 09:42:00.641: INFO: Pod "pod-projected-configmaps-8241e83b-9808-4f12-bfe1-db98f4ecaae8" satisfied condition "Succeeded or Failed"
Jan 21 09:42:00.650: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-8241e83b-9808-4f12-bfe1-db98f4ecaae8 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 09:42:00.745: INFO: Waiting for pod pod-projected-configmaps-8241e83b-9808-4f12-bfe1-db98f4ecaae8 to disappear
Jan 21 09:42:00.753: INFO: Pod pod-projected-configmaps-8241e83b-9808-4f12-bfe1-db98f4ecaae8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:42:00.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9249" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":57,"skipped":948,"failed":0}

------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:42:00.803: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:42:01.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-717" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":58,"skipped":948,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:42:01.153: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 21 09:42:01.455: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1924  14ea9a26-f545-4a1f-9cb7-28bcbdbbeeb1 12302 0 2022-01-21 09:42:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-21 09:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:42:01.455: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1924  14ea9a26-f545-4a1f-9cb7-28bcbdbbeeb1 12303 0 2022-01-21 09:42:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-21 09:42:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:42:01.456: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1924  14ea9a26-f545-4a1f-9cb7-28bcbdbbeeb1 12304 0 2022-01-21 09:42:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-21 09:42:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 21 09:42:11.563: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1924  14ea9a26-f545-4a1f-9cb7-28bcbdbbeeb1 12357 0 2022-01-21 09:42:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-21 09:42:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:42:11.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1924  14ea9a26-f545-4a1f-9cb7-28bcbdbbeeb1 12358 0 2022-01-21 09:42:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-21 09:42:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 09:42:11.564: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1924  14ea9a26-f545-4a1f-9cb7-28bcbdbbeeb1 12359 0 2022-01-21 09:42:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-21 09:42:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:42:11.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1924" for this suite.

• [SLOW TEST:10.432 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":59,"skipped":968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:42:11.598: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Jan 21 09:42:11.745: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jan 21 09:42:13.786: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jan 21 09:42:15.849: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:42:17.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3329" for this suite.

• [SLOW TEST:6.295 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":60,"skipped":1029,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:42:17.895: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 21 09:42:24.760: INFO: Successfully updated pod "adopt-release-8x7gl"
STEP: Checking that the Job readopts the Pod
Jan 21 09:42:24.760: INFO: Waiting up to 15m0s for pod "adopt-release-8x7gl" in namespace "job-7881" to be "adopted"
Jan 21 09:42:24.789: INFO: Pod "adopt-release-8x7gl": Phase="Running", Reason="", readiness=true. Elapsed: 29.332631ms
Jan 21 09:42:26.797: INFO: Pod "adopt-release-8x7gl": Phase="Running", Reason="", readiness=true. Elapsed: 2.037318029s
Jan 21 09:42:26.797: INFO: Pod "adopt-release-8x7gl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 21 09:42:27.331: INFO: Successfully updated pod "adopt-release-8x7gl"
STEP: Checking that the Job releases the Pod
Jan 21 09:42:27.331: INFO: Waiting up to 15m0s for pod "adopt-release-8x7gl" in namespace "job-7881" to be "released"
Jan 21 09:42:27.343: INFO: Pod "adopt-release-8x7gl": Phase="Running", Reason="", readiness=true. Elapsed: 11.351176ms
Jan 21 09:42:29.377: INFO: Pod "adopt-release-8x7gl": Phase="Running", Reason="", readiness=true. Elapsed: 2.045964103s
Jan 21 09:42:29.377: INFO: Pod "adopt-release-8x7gl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:42:29.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7881" for this suite.

• [SLOW TEST:11.526 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":61,"skipped":1050,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:42:29.422: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:42:29.647: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 21 09:42:38.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-2987 --namespace=crd-publish-openapi-2987 create -f -'
Jan 21 09:42:40.769: INFO: stderr: ""
Jan 21 09:42:40.769: INFO: stdout: "e2e-test-crd-publish-openapi-9274-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 21 09:42:40.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-2987 --namespace=crd-publish-openapi-2987 delete e2e-test-crd-publish-openapi-9274-crds test-cr'
Jan 21 09:42:40.955: INFO: stderr: ""
Jan 21 09:42:40.955: INFO: stdout: "e2e-test-crd-publish-openapi-9274-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 21 09:42:40.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-2987 --namespace=crd-publish-openapi-2987 apply -f -'
Jan 21 09:42:41.733: INFO: stderr: ""
Jan 21 09:42:41.733: INFO: stdout: "e2e-test-crd-publish-openapi-9274-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 21 09:42:41.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-2987 --namespace=crd-publish-openapi-2987 delete e2e-test-crd-publish-openapi-9274-crds test-cr'
Jan 21 09:42:41.926: INFO: stderr: ""
Jan 21 09:42:41.926: INFO: stdout: "e2e-test-crd-publish-openapi-9274-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 21 09:42:41.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-2987 explain e2e-test-crd-publish-openapi-9274-crds'
Jan 21 09:42:42.495: INFO: stderr: ""
Jan 21 09:42:42.495: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9274-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:42:50.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2987" for this suite.

• [SLOW TEST:21.575 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":62,"skipped":1060,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:42:50.998: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3659
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 21 09:42:51.106: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 21 09:42:51.314: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:42:53.346: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:42:55.324: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:42:57.325: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:42:59.320: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:43:01.325: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:43:03.329: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:43:05.324: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:43:07.417: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:43:09.393: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:43:11.357: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 21 09:43:11.470: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 21 09:43:15.686: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 21 09:43:15.687: INFO: Going to poll 172.16.209.79 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 21 09:43:15.697: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.209.79 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3659 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:43:15.698: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:43:15.700: INFO: ExecWithOptions: Clientset creation
Jan 21 09:43:15.700: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/pod-network-test-3659/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.209.79+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 21 09:43:16.961: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 21 09:43:16.961: INFO: Going to poll 172.16.106.164 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 21 09:43:16.971: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.106.164 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3659 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:43:16.971: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:43:16.974: INFO: ExecWithOptions: Clientset creation
Jan 21 09:43:16.974: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/pod-network-test-3659/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.106.164+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 21 09:43:18.216: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:43:18.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3659" for this suite.

• [SLOW TEST:27.305 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1070,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:43:18.332: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0121 09:44:10.988917      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 21 09:44:11.944: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 21 09:44:11.955: INFO: Deleting pod "simpletest.rc-22vzs" in namespace "gc-2203"
Jan 21 09:44:13.535: INFO: Deleting pod "simpletest.rc-2r5t9" in namespace "gc-2203"
Jan 21 09:44:13.957: INFO: Deleting pod "simpletest.rc-2v9bx" in namespace "gc-2203"
Jan 21 09:44:14.620: INFO: Deleting pod "simpletest.rc-4bjnr" in namespace "gc-2203"
Jan 21 09:44:16.559: INFO: Deleting pod "simpletest.rc-4chqf" in namespace "gc-2203"
Jan 21 09:44:17.224: INFO: Deleting pod "simpletest.rc-4fskp" in namespace "gc-2203"
Jan 21 09:44:18.440: INFO: Deleting pod "simpletest.rc-4nzpc" in namespace "gc-2203"
Jan 21 09:44:20.810: INFO: Deleting pod "simpletest.rc-5hlt2" in namespace "gc-2203"
Jan 21 09:44:21.219: INFO: Deleting pod "simpletest.rc-5hzq8" in namespace "gc-2203"
Jan 21 09:44:21.503: INFO: Deleting pod "simpletest.rc-5ssxt" in namespace "gc-2203"
Jan 21 09:44:22.499: INFO: Deleting pod "simpletest.rc-6k9zt" in namespace "gc-2203"
Jan 21 09:44:23.413: INFO: Deleting pod "simpletest.rc-6mqpm" in namespace "gc-2203"
Jan 21 09:44:23.763: INFO: Deleting pod "simpletest.rc-7bcm4" in namespace "gc-2203"
Jan 21 09:44:24.359: INFO: Deleting pod "simpletest.rc-7g9sb" in namespace "gc-2203"
Jan 21 09:44:26.207: INFO: Deleting pod "simpletest.rc-7r469" in namespace "gc-2203"
Jan 21 09:44:27.866: INFO: Deleting pod "simpletest.rc-8ptgk" in namespace "gc-2203"
Jan 21 09:44:28.648: INFO: Deleting pod "simpletest.rc-8qfh5" in namespace "gc-2203"
Jan 21 09:44:29.872: INFO: Deleting pod "simpletest.rc-97rg4" in namespace "gc-2203"
Jan 21 09:44:30.266: INFO: Deleting pod "simpletest.rc-b2k47" in namespace "gc-2203"
Jan 21 09:44:30.353: INFO: Deleting pod "simpletest.rc-b42wh" in namespace "gc-2203"
Jan 21 09:44:30.408: INFO: Deleting pod "simpletest.rc-bc6nb" in namespace "gc-2203"
Jan 21 09:44:30.475: INFO: Deleting pod "simpletest.rc-bwsfq" in namespace "gc-2203"
Jan 21 09:44:30.611: INFO: Deleting pod "simpletest.rc-c9vkc" in namespace "gc-2203"
Jan 21 09:44:30.832: INFO: Deleting pod "simpletest.rc-ckfks" in namespace "gc-2203"
Jan 21 09:44:30.927: INFO: Deleting pod "simpletest.rc-dnlc2" in namespace "gc-2203"
Jan 21 09:44:31.039: INFO: Deleting pod "simpletest.rc-dsblj" in namespace "gc-2203"
Jan 21 09:44:31.106: INFO: Deleting pod "simpletest.rc-dwqp6" in namespace "gc-2203"
Jan 21 09:44:31.157: INFO: Deleting pod "simpletest.rc-dwtqn" in namespace "gc-2203"
Jan 21 09:44:31.248: INFO: Deleting pod "simpletest.rc-dxnfd" in namespace "gc-2203"
Jan 21 09:44:31.314: INFO: Deleting pod "simpletest.rc-dzp92" in namespace "gc-2203"
Jan 21 09:44:31.553: INFO: Deleting pod "simpletest.rc-f4ftw" in namespace "gc-2203"
Jan 21 09:44:31.636: INFO: Deleting pod "simpletest.rc-f5bhw" in namespace "gc-2203"
Jan 21 09:44:31.845: INFO: Deleting pod "simpletest.rc-f8tmp" in namespace "gc-2203"
Jan 21 09:44:31.911: INFO: Deleting pod "simpletest.rc-f94lt" in namespace "gc-2203"
Jan 21 09:44:32.101: INFO: Deleting pod "simpletest.rc-f9r8s" in namespace "gc-2203"
Jan 21 09:44:32.272: INFO: Deleting pod "simpletest.rc-fcdt5" in namespace "gc-2203"
Jan 21 09:44:32.544: INFO: Deleting pod "simpletest.rc-fsdgn" in namespace "gc-2203"
Jan 21 09:44:32.597: INFO: Deleting pod "simpletest.rc-fw8wp" in namespace "gc-2203"
Jan 21 09:44:32.812: INFO: Deleting pod "simpletest.rc-gb7qf" in namespace "gc-2203"
Jan 21 09:44:33.119: INFO: Deleting pod "simpletest.rc-gsqhs" in namespace "gc-2203"
Jan 21 09:44:33.251: INFO: Deleting pod "simpletest.rc-h5lf8" in namespace "gc-2203"
Jan 21 09:44:33.358: INFO: Deleting pod "simpletest.rc-hj95b" in namespace "gc-2203"
Jan 21 09:44:33.441: INFO: Deleting pod "simpletest.rc-hjnc6" in namespace "gc-2203"
Jan 21 09:44:33.545: INFO: Deleting pod "simpletest.rc-hk97p" in namespace "gc-2203"
Jan 21 09:44:33.700: INFO: Deleting pod "simpletest.rc-jckx8" in namespace "gc-2203"
Jan 21 09:44:33.840: INFO: Deleting pod "simpletest.rc-jfcm6" in namespace "gc-2203"
Jan 21 09:44:34.017: INFO: Deleting pod "simpletest.rc-jqd8h" in namespace "gc-2203"
Jan 21 09:44:34.188: INFO: Deleting pod "simpletest.rc-jxbbn" in namespace "gc-2203"
Jan 21 09:44:34.406: INFO: Deleting pod "simpletest.rc-k9dxq" in namespace "gc-2203"
Jan 21 09:44:34.872: INFO: Deleting pod "simpletest.rc-kcfqj" in namespace "gc-2203"
Jan 21 09:44:34.953: INFO: Deleting pod "simpletest.rc-kfp8z" in namespace "gc-2203"
Jan 21 09:44:35.262: INFO: Deleting pod "simpletest.rc-kzczh" in namespace "gc-2203"
Jan 21 09:44:35.378: INFO: Deleting pod "simpletest.rc-l7p74" in namespace "gc-2203"
Jan 21 09:44:35.456: INFO: Deleting pod "simpletest.rc-lkjvk" in namespace "gc-2203"
Jan 21 09:44:35.705: INFO: Deleting pod "simpletest.rc-lnxp7" in namespace "gc-2203"
Jan 21 09:44:35.808: INFO: Deleting pod "simpletest.rc-lr2l2" in namespace "gc-2203"
Jan 21 09:44:36.233: INFO: Deleting pod "simpletest.rc-ltvks" in namespace "gc-2203"
Jan 21 09:44:36.434: INFO: Deleting pod "simpletest.rc-lv97m" in namespace "gc-2203"
Jan 21 09:44:36.581: INFO: Deleting pod "simpletest.rc-lwsfr" in namespace "gc-2203"
Jan 21 09:44:36.628: INFO: Deleting pod "simpletest.rc-md7sr" in namespace "gc-2203"
Jan 21 09:44:36.733: INFO: Deleting pod "simpletest.rc-mglc4" in namespace "gc-2203"
Jan 21 09:44:36.906: INFO: Deleting pod "simpletest.rc-mm6b2" in namespace "gc-2203"
Jan 21 09:44:37.066: INFO: Deleting pod "simpletest.rc-mrjvl" in namespace "gc-2203"
Jan 21 09:44:37.381: INFO: Deleting pod "simpletest.rc-n7fbs" in namespace "gc-2203"
Jan 21 09:44:37.540: INFO: Deleting pod "simpletest.rc-ngb2k" in namespace "gc-2203"
Jan 21 09:44:37.908: INFO: Deleting pod "simpletest.rc-ngjv7" in namespace "gc-2203"
Jan 21 09:44:38.889: INFO: Deleting pod "simpletest.rc-nglr6" in namespace "gc-2203"
Jan 21 09:44:39.156: INFO: Deleting pod "simpletest.rc-p5zxf" in namespace "gc-2203"
Jan 21 09:44:39.678: INFO: Deleting pod "simpletest.rc-p82zh" in namespace "gc-2203"
Jan 21 09:44:40.096: INFO: Deleting pod "simpletest.rc-pc2jh" in namespace "gc-2203"
Jan 21 09:44:40.595: INFO: Deleting pod "simpletest.rc-pwm4m" in namespace "gc-2203"
Jan 21 09:44:40.761: INFO: Deleting pod "simpletest.rc-q4xhf" in namespace "gc-2203"
Jan 21 09:44:40.866: INFO: Deleting pod "simpletest.rc-q7vzj" in namespace "gc-2203"
Jan 21 09:44:40.938: INFO: Deleting pod "simpletest.rc-qdfg8" in namespace "gc-2203"
Jan 21 09:44:41.063: INFO: Deleting pod "simpletest.rc-qg9nh" in namespace "gc-2203"
Jan 21 09:44:41.255: INFO: Deleting pod "simpletest.rc-qrwvn" in namespace "gc-2203"
Jan 21 09:44:41.387: INFO: Deleting pod "simpletest.rc-qsfgr" in namespace "gc-2203"
Jan 21 09:44:41.859: INFO: Deleting pod "simpletest.rc-rbvzj" in namespace "gc-2203"
Jan 21 09:44:42.333: INFO: Deleting pod "simpletest.rc-rfxh4" in namespace "gc-2203"
Jan 21 09:44:42.435: INFO: Deleting pod "simpletest.rc-rhqs8" in namespace "gc-2203"
Jan 21 09:44:42.510: INFO: Deleting pod "simpletest.rc-rprjb" in namespace "gc-2203"
Jan 21 09:44:42.820: INFO: Deleting pod "simpletest.rc-rvbrc" in namespace "gc-2203"
Jan 21 09:44:43.015: INFO: Deleting pod "simpletest.rc-rztbg" in namespace "gc-2203"
Jan 21 09:44:43.087: INFO: Deleting pod "simpletest.rc-sk5ng" in namespace "gc-2203"
Jan 21 09:44:43.324: INFO: Deleting pod "simpletest.rc-snfsr" in namespace "gc-2203"
Jan 21 09:44:43.429: INFO: Deleting pod "simpletest.rc-tmlb9" in namespace "gc-2203"
Jan 21 09:44:43.528: INFO: Deleting pod "simpletest.rc-tqpfh" in namespace "gc-2203"
Jan 21 09:44:43.601: INFO: Deleting pod "simpletest.rc-twvmb" in namespace "gc-2203"
Jan 21 09:44:43.872: INFO: Deleting pod "simpletest.rc-vh5s6" in namespace "gc-2203"
Jan 21 09:44:43.936: INFO: Deleting pod "simpletest.rc-vtmpz" in namespace "gc-2203"
Jan 21 09:44:44.080: INFO: Deleting pod "simpletest.rc-vtpjl" in namespace "gc-2203"
Jan 21 09:44:44.462: INFO: Deleting pod "simpletest.rc-vx4nw" in namespace "gc-2203"
Jan 21 09:44:44.670: INFO: Deleting pod "simpletest.rc-w289w" in namespace "gc-2203"
Jan 21 09:44:45.800: INFO: Deleting pod "simpletest.rc-wd5c4" in namespace "gc-2203"
Jan 21 09:44:46.305: INFO: Deleting pod "simpletest.rc-xg8ng" in namespace "gc-2203"
Jan 21 09:44:46.444: INFO: Deleting pod "simpletest.rc-xjqpb" in namespace "gc-2203"
Jan 21 09:44:46.549: INFO: Deleting pod "simpletest.rc-xn6m2" in namespace "gc-2203"
Jan 21 09:44:46.675: INFO: Deleting pod "simpletest.rc-xwjt9" in namespace "gc-2203"
Jan 21 09:44:46.833: INFO: Deleting pod "simpletest.rc-z68tg" in namespace "gc-2203"
Jan 21 09:44:46.999: INFO: Deleting pod "simpletest.rc-z78ln" in namespace "gc-2203"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:44:47.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2203" for this suite.

• [SLOW TEST:89.267 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":64,"skipped":1084,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:44:47.801: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3051
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3051
STEP: creating replication controller externalsvc in namespace services-3051
I0121 09:44:57.148140      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3051, replica count: 2
I0121 09:45:01.216641      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:04.217111      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:07.218510      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:10.218918      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:13.222638      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:16.224730      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:19.233146      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:22.255713      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:25.256705      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:28.257286      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:31.259801      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:34.261122      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:37.267661      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:40.271708      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 09:45:43.274181      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 21 09:45:43.382: INFO: Creating new exec pod
Jan 21 09:45:59.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-3051 exec execpodbkg7r -- /bin/sh -x -c nslookup nodeport-service.services-3051.svc.cluster.local'
Jan 21 09:46:00.723: INFO: stderr: "+ nslookup nodeport-service.services-3051.svc.cluster.local\n"
Jan 21 09:46:00.723: INFO: stdout: "Server:\t\t10.10.0.10\nAddress:\t10.10.0.10#53\n\nnodeport-service.services-3051.svc.cluster.local\tcanonical name = externalsvc.services-3051.svc.cluster.local.\nName:\texternalsvc.services-3051.svc.cluster.local\nAddress: 10.10.56.161\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3051, will wait for the garbage collector to delete the pods
Jan 21 09:46:00.802: INFO: Deleting ReplicationController externalsvc took: 16.397247ms
Jan 21 09:46:01.007: INFO: Terminating ReplicationController externalsvc pods took: 204.86223ms
Jan 21 09:46:04.542: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:46:04.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3051" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:76.896 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":65,"skipped":1093,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:46:04.657: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:46:04.839: INFO: created pod
Jan 21 09:46:04.839: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5835" to be "Succeeded or Failed"
Jan 21 09:46:04.871: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 31.306376ms
Jan 21 09:46:06.911: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07203395s
Jan 21 09:46:08.922: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083052001s
Jan 21 09:46:10.968: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.128592076s
STEP: Saw pod success
Jan 21 09:46:10.983: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 21 09:46:40.991: INFO: polling logs
Jan 21 09:46:41.050: INFO: Pod logs: 
2022/01/21 09:46:07 OK: Got token
2022/01/21 09:46:07 validating with in-cluster discovery
2022/01/21 09:46:07 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/01/21 09:46:07 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5835:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1642758965, NotBefore:1642758365, IssuedAt:1642758365, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5835", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ef6986a2-6db4-429b-9114-fa86871408a3"}}}
2022/01/21 09:46:08 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/01/21 09:46:08 OK: Validated signature on JWT
2022/01/21 09:46:08 OK: Got valid claims from token!
2022/01/21 09:46:08 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5835:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1642758965, NotBefore:1642758365, IssuedAt:1642758365, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5835", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ef6986a2-6db4-429b-9114-fa86871408a3"}}}

Jan 21 09:46:41.050: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:46:41.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5835" for this suite.

• [SLOW TEST:36.427 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":66,"skipped":1097,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:46:41.085: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2801
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2801
Jan 21 09:46:41.205: INFO: Found 0 stateful pods, waiting for 1
Jan 21 09:46:51.218: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jan 21 09:46:51.259: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jan 21 09:46:51.284: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jan 21 09:46:51.296: INFO: Observed &StatefulSet event: ADDED
Jan 21 09:46:51.296: INFO: Found Statefulset ss in namespace statefulset-2801 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 21 09:46:51.296: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jan 21 09:46:51.296: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 21 09:46:51.316: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jan 21 09:46:51.322: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 21 09:46:51.322: INFO: Deleting all statefulset in ns statefulset-2801
Jan 21 09:46:51.333: INFO: Scaling statefulset ss to 0
Jan 21 09:47:01.383: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 09:47:01.390: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:47:01.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2801" for this suite.

• [SLOW TEST:20.442 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":67,"skipped":1105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:47:01.539: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:47:01.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3915" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":68,"skipped":1157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:47:01.697: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-619ba289-58db-45b5-b5d9-a059c097c16d
STEP: Creating a pod to test consume secrets
Jan 21 09:47:01.823: INFO: Waiting up to 5m0s for pod "pod-secrets-045c6c68-28b6-4dd1-be06-74c0b4841f5c" in namespace "secrets-8304" to be "Succeeded or Failed"
Jan 21 09:47:01.865: INFO: Pod "pod-secrets-045c6c68-28b6-4dd1-be06-74c0b4841f5c": Phase="Pending", Reason="", readiness=false. Elapsed: 41.346277ms
Jan 21 09:47:03.895: INFO: Pod "pod-secrets-045c6c68-28b6-4dd1-be06-74c0b4841f5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072114488s
Jan 21 09:47:05.907: INFO: Pod "pod-secrets-045c6c68-28b6-4dd1-be06-74c0b4841f5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083752554s
STEP: Saw pod success
Jan 21 09:47:05.907: INFO: Pod "pod-secrets-045c6c68-28b6-4dd1-be06-74c0b4841f5c" satisfied condition "Succeeded or Failed"
Jan 21 09:47:05.914: INFO: Trying to get logs from node conformance1 pod pod-secrets-045c6c68-28b6-4dd1-be06-74c0b4841f5c container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 09:47:05.961: INFO: Waiting for pod pod-secrets-045c6c68-28b6-4dd1-be06-74c0b4841f5c to disappear
Jan 21 09:47:05.979: INFO: Pod pod-secrets-045c6c68-28b6-4dd1-be06-74c0b4841f5c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:47:05.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8304" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1201,"failed":0}

------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:47:06.017: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:47:08.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2389" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":70,"skipped":1201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:47:08.477: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9916
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 21 09:47:08.931: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 21 09:47:09.308: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:47:11.322: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:47:13.337: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:47:15.315: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:47:17.322: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:47:19.322: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:47:21.325: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:47:23.324: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:47:25.315: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:47:27.326: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:47:29.394: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 21 09:47:29.441: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 21 09:47:35.543: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 21 09:47:35.543: INFO: Going to poll 172.16.209.96 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 21 09:47:35.554: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.209.96:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9916 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:47:35.555: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:47:35.556: INFO: ExecWithOptions: Clientset creation
Jan 21 09:47:35.557: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/pod-network-test-9916/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.209.96%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 21 09:47:35.809: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 21 09:47:35.812: INFO: Going to poll 172.16.106.158 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 21 09:47:35.828: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.106.158:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9916 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:47:35.828: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:47:35.829: INFO: ExecWithOptions: Clientset creation
Jan 21 09:47:35.829: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/pod-network-test-9916/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.106.158%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 21 09:47:36.055: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:47:36.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9916" for this suite.

• [SLOW TEST:27.628 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:47:36.107: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:47:36.206: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:47:36.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8024" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":72,"skipped":1277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:47:36.909: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:47:48.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9837" for this suite.

• [SLOW TEST:11.830 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":73,"skipped":1324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:47:48.756: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 09:47:50.111: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 09:47:52.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 47, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 47, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 47, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 47, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 09:47:54.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 47, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 47, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 47, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 47, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 09:47:57.201: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:47:57.214: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-721-crds.webhook.example.com via the AdmissionRegistration API
Jan 21 09:47:57.929: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:48:00.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7643" for this suite.
STEP: Destroying namespace "webhook-7643-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.600 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":74,"skipped":1353,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:48:01.359: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a collection of services
Jan 21 09:48:01.678: INFO: Creating e2e-svc-a-m8776
Jan 21 09:48:01.707: INFO: Creating e2e-svc-b-kgknt
Jan 21 09:48:01.744: INFO: Creating e2e-svc-c-hcj9z
STEP: deleting service collection
Jan 21 09:48:01.866: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:48:01.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7905" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":75,"skipped":1365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:48:01.916: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Jan 21 09:48:02.290: INFO: The status of Pod pod-hostip-6b5c5bf4-5afd-4f41-bca7-48fb7fd6daa2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:48:04.309: INFO: The status of Pod pod-hostip-6b5c5bf4-5afd-4f41-bca7-48fb7fd6daa2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:48:06.312: INFO: The status of Pod pod-hostip-6b5c5bf4-5afd-4f41-bca7-48fb7fd6daa2 is Running (Ready = true)
Jan 21 09:48:06.336: INFO: Pod pod-hostip-6b5c5bf4-5afd-4f41-bca7-48fb7fd6daa2 has hostIP: 10.10.1.86
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:48:06.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9353" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1414,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:48:06.378: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-6f6e77f1-d3da-44c3-81f3-ad1d5873a6b4
STEP: Creating a pod to test consume configMaps
Jan 21 09:48:06.651: INFO: Waiting up to 5m0s for pod "pod-configmaps-f20edc84-9e1f-4c73-8a56-778876d66cf4" in namespace "configmap-1221" to be "Succeeded or Failed"
Jan 21 09:48:06.702: INFO: Pod "pod-configmaps-f20edc84-9e1f-4c73-8a56-778876d66cf4": Phase="Pending", Reason="", readiness=false. Elapsed: 51.328876ms
Jan 21 09:48:08.722: INFO: Pod "pod-configmaps-f20edc84-9e1f-4c73-8a56-778876d66cf4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070741539s
Jan 21 09:48:10.734: INFO: Pod "pod-configmaps-f20edc84-9e1f-4c73-8a56-778876d66cf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082945984s
STEP: Saw pod success
Jan 21 09:48:10.734: INFO: Pod "pod-configmaps-f20edc84-9e1f-4c73-8a56-778876d66cf4" satisfied condition "Succeeded or Failed"
Jan 21 09:48:10.748: INFO: Trying to get logs from node conformance1 pod pod-configmaps-f20edc84-9e1f-4c73-8a56-778876d66cf4 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 09:48:10.807: INFO: Waiting for pod pod-configmaps-f20edc84-9e1f-4c73-8a56-778876d66cf4 to disappear
Jan 21 09:48:10.813: INFO: Pod pod-configmaps-f20edc84-9e1f-4c73-8a56-778876d66cf4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:48:10.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1221" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":77,"skipped":1421,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:48:10.869: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 09:48:11.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771" in namespace "downward-api-6275" to be "Succeeded or Failed"
Jan 21 09:48:11.048: INFO: Pod "downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771": Phase="Pending", Reason="", readiness=false. Elapsed: 7.97112ms
Jan 21 09:48:13.095: INFO: Pod "downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055519652s
Jan 21 09:48:15.124: INFO: Pod "downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084409257s
Jan 21 09:48:17.142: INFO: Pod "downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.101598045s
STEP: Saw pod success
Jan 21 09:48:17.142: INFO: Pod "downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771" satisfied condition "Succeeded or Failed"
Jan 21 09:48:17.150: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771 container client-container: <nil>
STEP: delete the pod
Jan 21 09:48:17.225: INFO: Waiting for pod downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771 to disappear
Jan 21 09:48:17.239: INFO: Pod downwardapi-volume-0e33bf23-dfc7-4f32-a9be-d9d84f436771 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:48:17.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6275" for this suite.

• [SLOW TEST:6.407 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":78,"skipped":1429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:48:17.276: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:48:17.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4835" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":79,"skipped":1471,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:48:17.584: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 09:48:21.157: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 09:48:23.261: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 09:48:25.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 09:48:28.430: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:48:28.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-962" for this suite.
STEP: Destroying namespace "webhook-962-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":80,"skipped":1478,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:48:29.244: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3441
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jan 21 09:48:29.707: INFO: Found 0 stateful pods, waiting for 3
Jan 21 09:48:39.724: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 09:48:39.724: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 09:48:39.724: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 21 09:48:49.735: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 09:48:49.735: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 09:48:49.736: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 21 09:48:49.810: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 21 09:48:59.987: INFO: Updating stateful set ss2
Jan 21 09:49:00.076: INFO: Waiting for Pod statefulset-3441/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jan 21 09:49:10.436: INFO: Found 2 stateful pods, waiting for 3
Jan 21 09:49:20.444: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 09:49:20.444: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 09:49:20.444: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 21 09:49:20.493: INFO: Updating stateful set ss2
Jan 21 09:49:20.536: INFO: Waiting for Pod statefulset-3441/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 21 09:49:30.581: INFO: Updating stateful set ss2
Jan 21 09:49:30.593: INFO: Waiting for StatefulSet statefulset-3441/ss2 to complete update
Jan 21 09:49:30.593: INFO: Waiting for Pod statefulset-3441/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 21 09:49:40.616: INFO: Deleting all statefulset in ns statefulset-3441
Jan 21 09:49:40.624: INFO: Scaling statefulset ss2 to 0
Jan 21 09:49:50.723: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 09:49:50.730: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:49:50.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3441" for this suite.

• [SLOW TEST:81.599 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":81,"skipped":1479,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:49:50.844: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 21 09:49:51.983: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 21 09:49:54.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 49, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 49, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 09:49:57.155: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:49:57.177: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:50:00.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9428" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.297 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":82,"skipped":1493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:50:01.142: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:50:01.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4227" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":83,"skipped":1552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:50:01.780: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:50:02.184: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 21 09:50:07.211: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jan 21 09:50:07.234: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jan 21 09:50:07.308: INFO: observed ReplicaSet test-rs in namespace replicaset-4021 with ReadyReplicas 1, AvailableReplicas 1
Jan 21 09:50:07.378: INFO: observed ReplicaSet test-rs in namespace replicaset-4021 with ReadyReplicas 1, AvailableReplicas 1
Jan 21 09:50:07.481: INFO: observed ReplicaSet test-rs in namespace replicaset-4021 with ReadyReplicas 1, AvailableReplicas 1
Jan 21 09:50:07.520: INFO: observed ReplicaSet test-rs in namespace replicaset-4021 with ReadyReplicas 1, AvailableReplicas 1
Jan 21 09:50:11.737: INFO: observed ReplicaSet test-rs in namespace replicaset-4021 with ReadyReplicas 2, AvailableReplicas 2
Jan 21 09:50:12.531: INFO: observed Replicaset test-rs in namespace replicaset-4021 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:50:12.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4021" for this suite.

• [SLOW TEST:10.779 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":84,"skipped":1578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:50:12.565: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Jan 21 09:50:12.721: INFO: Waiting up to 5m0s for pod "var-expansion-166b3b72-7833-4b38-8770-d79385cfab42" in namespace "var-expansion-9767" to be "Succeeded or Failed"
Jan 21 09:50:12.763: INFO: Pod "var-expansion-166b3b72-7833-4b38-8770-d79385cfab42": Phase="Pending", Reason="", readiness=false. Elapsed: 41.585599ms
Jan 21 09:50:14.808: INFO: Pod "var-expansion-166b3b72-7833-4b38-8770-d79385cfab42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086462922s
Jan 21 09:50:16.840: INFO: Pod "var-expansion-166b3b72-7833-4b38-8770-d79385cfab42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.118664574s
STEP: Saw pod success
Jan 21 09:50:16.840: INFO: Pod "var-expansion-166b3b72-7833-4b38-8770-d79385cfab42" satisfied condition "Succeeded or Failed"
Jan 21 09:50:16.862: INFO: Trying to get logs from node conformance1 pod var-expansion-166b3b72-7833-4b38-8770-d79385cfab42 container dapi-container: <nil>
STEP: delete the pod
Jan 21 09:50:16.986: INFO: Waiting for pod var-expansion-166b3b72-7833-4b38-8770-d79385cfab42 to disappear
Jan 21 09:50:16.993: INFO: Pod var-expansion-166b3b72-7833-4b38-8770-d79385cfab42 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:50:16.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9767" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1615,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:50:17.044: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-e3ded530-203b-4e26-8ac0-0b06cc23d873
STEP: Creating secret with name s-test-opt-upd-30420415-9996-4c63-8ca8-f3c85f3f550b
STEP: Creating the pod
Jan 21 09:50:17.280: INFO: The status of Pod pod-secrets-5027b527-80ad-41de-a409-66eca4804f79 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:50:19.587: INFO: The status of Pod pod-secrets-5027b527-80ad-41de-a409-66eca4804f79 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:50:21.351: INFO: The status of Pod pod-secrets-5027b527-80ad-41de-a409-66eca4804f79 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:50:23.339: INFO: The status of Pod pod-secrets-5027b527-80ad-41de-a409-66eca4804f79 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:50:25.300: INFO: The status of Pod pod-secrets-5027b527-80ad-41de-a409-66eca4804f79 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:50:27.295: INFO: The status of Pod pod-secrets-5027b527-80ad-41de-a409-66eca4804f79 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-e3ded530-203b-4e26-8ac0-0b06cc23d873
STEP: Updating secret s-test-opt-upd-30420415-9996-4c63-8ca8-f3c85f3f550b
STEP: Creating secret with name s-test-opt-create-9ab8a4c6-50c9-42b3-ac95-9df0c8f25a5b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:51:48.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8756" for this suite.

• [SLOW TEST:91.572 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1617,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:51:48.618: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-8ec3b7f1-c06c-425d-b497-f14e75cdc374
STEP: Creating a pod to test consume secrets
Jan 21 09:51:48.838: INFO: Waiting up to 5m0s for pod "pod-secrets-1349b414-2771-4819-bd68-0ae9ddc7d9fc" in namespace "secrets-5237" to be "Succeeded or Failed"
Jan 21 09:51:48.872: INFO: Pod "pod-secrets-1349b414-2771-4819-bd68-0ae9ddc7d9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 29.121693ms
Jan 21 09:51:50.882: INFO: Pod "pod-secrets-1349b414-2771-4819-bd68-0ae9ddc7d9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039509375s
Jan 21 09:51:52.897: INFO: Pod "pod-secrets-1349b414-2771-4819-bd68-0ae9ddc7d9fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054414773s
STEP: Saw pod success
Jan 21 09:51:52.897: INFO: Pod "pod-secrets-1349b414-2771-4819-bd68-0ae9ddc7d9fc" satisfied condition "Succeeded or Failed"
Jan 21 09:51:52.904: INFO: Trying to get logs from node dc-hg-2 pod pod-secrets-1349b414-2771-4819-bd68-0ae9ddc7d9fc container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 09:51:52.969: INFO: Waiting for pod pod-secrets-1349b414-2771-4819-bd68-0ae9ddc7d9fc to disappear
Jan 21 09:51:52.985: INFO: Pod pod-secrets-1349b414-2771-4819-bd68-0ae9ddc7d9fc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:51:52.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5237" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1622,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:51:53.010: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
STEP: creating an pod
Jan 21 09:51:53.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 21 09:51:53.357: INFO: stderr: ""
Jan 21 09:51:53.357: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Jan 21 09:51:53.357: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 21 09:51:53.357: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5178" to be "running and ready, or succeeded"
Jan 21 09:51:53.370: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.027968ms
Jan 21 09:51:55.378: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021226116s
Jan 21 09:51:57.391: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.033933948s
Jan 21 09:51:57.391: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 21 09:51:57.391: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 21 09:51:57.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 logs logs-generator logs-generator'
Jan 21 09:51:57.650: INFO: stderr: ""
Jan 21 09:51:57.650: INFO: stdout: "I0121 09:51:56.065334       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/9fs 435\nI0121 09:51:56.265515       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/qhn 516\nI0121 09:51:56.465248       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/x4c 339\nI0121 09:51:56.666423       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/c7d 418\nI0121 09:51:56.866043       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/n4r 452\nI0121 09:51:57.065415       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/2r64 272\nI0121 09:51:57.266040       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/m56k 318\nI0121 09:51:57.465402       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/sklf 243\n"
Jan 21 09:51:59.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 logs logs-generator logs-generator'
Jan 21 09:52:00.357: INFO: stderr: ""
Jan 21 09:52:00.357: INFO: stdout: "I0121 09:51:56.065334       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/9fs 435\nI0121 09:51:56.265515       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/qhn 516\nI0121 09:51:56.465248       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/x4c 339\nI0121 09:51:56.666423       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/c7d 418\nI0121 09:51:56.866043       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/n4r 452\nI0121 09:51:57.065415       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/2r64 272\nI0121 09:51:57.266040       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/m56k 318\nI0121 09:51:57.465402       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/sklf 243\nI0121 09:51:57.665892       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/w4v8 253\nI0121 09:51:57.866383       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/hww2 501\nI0121 09:51:58.066009       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/2s2 397\nI0121 09:51:58.271417       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/gjv 561\nI0121 09:51:58.466579       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/4mw 405\nI0121 09:51:58.666346       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/zf86 561\nI0121 09:51:58.865919       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/l9k 435\nI0121 09:51:59.065456       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/xk6t 375\nI0121 09:51:59.265888       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/9qbv 413\nI0121 09:51:59.466402       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/8t5k 252\nI0121 09:51:59.665886       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/nwx6 257\nI0121 09:51:59.866437       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/5dn 205\nI0121 09:52:00.065908       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/sk7k 346\nI0121 09:52:00.265453       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/ttg 459\n"
STEP: limiting log lines
Jan 21 09:52:00.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 logs logs-generator logs-generator --tail=1'
Jan 21 09:52:00.784: INFO: stderr: ""
Jan 21 09:52:00.784: INFO: stdout: "I0121 09:52:00.666413       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/lvhh 490\n"
Jan 21 09:52:00.784: INFO: got output "I0121 09:52:00.666413       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/lvhh 490\n"
STEP: limiting log bytes
Jan 21 09:52:00.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 logs logs-generator logs-generator --limit-bytes=1'
Jan 21 09:52:00.971: INFO: stderr: ""
Jan 21 09:52:00.971: INFO: stdout: "I"
Jan 21 09:52:00.971: INFO: got output "I"
STEP: exposing timestamps
Jan 21 09:52:00.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 21 09:52:01.162: INFO: stderr: ""
Jan 21 09:52:01.162: INFO: stdout: "2022-01-21T09:52:01.066169881Z I0121 09:52:01.065904       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/nt96 396\n"
Jan 21 09:52:01.162: INFO: got output "2022-01-21T09:52:01.066169881Z I0121 09:52:01.065904       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/nt96 396\n"
STEP: restricting to a time range
Jan 21 09:52:03.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 logs logs-generator logs-generator --since=1s'
Jan 21 09:52:03.834: INFO: stderr: ""
Jan 21 09:52:03.834: INFO: stdout: "I0121 09:52:02.865478       1 logs_generator.go:76] 34 GET /api/v1/namespaces/kube-system/pods/gkb 368\nI0121 09:52:03.065430       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/default/pods/vzsj 483\nI0121 09:52:03.265990       1 logs_generator.go:76] 36 GET /api/v1/namespaces/default/pods/q25r 219\nI0121 09:52:03.465412       1 logs_generator.go:76] 37 PUT /api/v1/namespaces/default/pods/vvjb 331\nI0121 09:52:03.665449       1 logs_generator.go:76] 38 GET /api/v1/namespaces/kube-system/pods/dmp 585\n"
Jan 21 09:52:03.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 logs logs-generator logs-generator --since=24h'
Jan 21 09:52:04.016: INFO: stderr: ""
Jan 21 09:52:04.016: INFO: stdout: "I0121 09:51:56.065334       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/9fs 435\nI0121 09:51:56.265515       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/qhn 516\nI0121 09:51:56.465248       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/x4c 339\nI0121 09:51:56.666423       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/c7d 418\nI0121 09:51:56.866043       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/n4r 452\nI0121 09:51:57.065415       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/2r64 272\nI0121 09:51:57.266040       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/m56k 318\nI0121 09:51:57.465402       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/sklf 243\nI0121 09:51:57.665892       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/w4v8 253\nI0121 09:51:57.866383       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/hww2 501\nI0121 09:51:58.066009       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/2s2 397\nI0121 09:51:58.271417       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/gjv 561\nI0121 09:51:58.466579       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/4mw 405\nI0121 09:51:58.666346       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/zf86 561\nI0121 09:51:58.865919       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/l9k 435\nI0121 09:51:59.065456       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/xk6t 375\nI0121 09:51:59.265888       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/9qbv 413\nI0121 09:51:59.466402       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/8t5k 252\nI0121 09:51:59.665886       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/nwx6 257\nI0121 09:51:59.866437       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/5dn 205\nI0121 09:52:00.065908       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/sk7k 346\nI0121 09:52:00.265453       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/ttg 459\nI0121 09:52:00.465886       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/75cr 449\nI0121 09:52:00.666413       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/lvhh 490\nI0121 09:52:00.865948       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/25p 560\nI0121 09:52:01.065904       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/nt96 396\nI0121 09:52:01.265386       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/9kc6 319\nI0121 09:52:01.465945       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/vvw5 431\nI0121 09:52:01.665440       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/dxsp 223\nI0121 09:52:01.866032       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/v6f 474\nI0121 09:52:02.065545       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/xx2m 500\nI0121 09:52:02.266167       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/t6vq 427\nI0121 09:52:02.465609       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/wcz8 400\nI0121 09:52:02.666141       1 logs_generator.go:76] 33 PUT /api/v1/namespaces/ns/pods/frn 442\nI0121 09:52:02.865478       1 logs_generator.go:76] 34 GET /api/v1/namespaces/kube-system/pods/gkb 368\nI0121 09:52:03.065430       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/default/pods/vzsj 483\nI0121 09:52:03.265990       1 logs_generator.go:76] 36 GET /api/v1/namespaces/default/pods/q25r 219\nI0121 09:52:03.465412       1 logs_generator.go:76] 37 PUT /api/v1/namespaces/default/pods/vvjb 331\nI0121 09:52:03.665449       1 logs_generator.go:76] 38 GET /api/v1/namespaces/kube-system/pods/dmp 585\nI0121 09:52:03.865362       1 logs_generator.go:76] 39 GET /api/v1/namespaces/ns/pods/ssj 388\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
Jan 21 09:52:04.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5178 delete pod logs-generator'
Jan 21 09:52:06.415: INFO: stderr: ""
Jan 21 09:52:06.416: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:52:06.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5178" for this suite.

• [SLOW TEST:13.436 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1406
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":88,"skipped":1630,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:52:06.447: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jan 21 09:52:06.552: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:52:47.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8896" for this suite.

• [SLOW TEST:41.313 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":89,"skipped":1635,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:52:47.771: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 21 09:52:47.947: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 21 09:52:47.958: INFO: starting watch
STEP: patching
STEP: updating
Jan 21 09:52:48.011: INFO: waiting for watch events with expected annotations
Jan 21 09:52:48.018: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:52:48.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5810" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":90,"skipped":1645,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:52:48.200: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Jan 21 09:52:48.402: INFO: Waiting up to 5m0s for pod "var-expansion-6a0f3694-098b-4025-ad82-968e37acff05" in namespace "var-expansion-1578" to be "Succeeded or Failed"
Jan 21 09:52:48.419: INFO: Pod "var-expansion-6a0f3694-098b-4025-ad82-968e37acff05": Phase="Pending", Reason="", readiness=false. Elapsed: 16.893907ms
Jan 21 09:52:50.435: INFO: Pod "var-expansion-6a0f3694-098b-4025-ad82-968e37acff05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03287179s
Jan 21 09:52:52.447: INFO: Pod "var-expansion-6a0f3694-098b-4025-ad82-968e37acff05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044494341s
Jan 21 09:52:54.467: INFO: Pod "var-expansion-6a0f3694-098b-4025-ad82-968e37acff05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064731203s
STEP: Saw pod success
Jan 21 09:52:54.467: INFO: Pod "var-expansion-6a0f3694-098b-4025-ad82-968e37acff05" satisfied condition "Succeeded or Failed"
Jan 21 09:52:54.476: INFO: Trying to get logs from node conformance1 pod var-expansion-6a0f3694-098b-4025-ad82-968e37acff05 container dapi-container: <nil>
STEP: delete the pod
Jan 21 09:52:54.537: INFO: Waiting for pod var-expansion-6a0f3694-098b-4025-ad82-968e37acff05 to disappear
Jan 21 09:52:54.551: INFO: Pod var-expansion-6a0f3694-098b-4025-ad82-968e37acff05 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:52:54.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1578" for this suite.

• [SLOW TEST:6.404 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1655,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:52:54.604: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:53:24.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8341" for this suite.

• [SLOW TEST:30.217 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":92,"skipped":1662,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:53:24.828: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0121 09:53:26.457681      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 21 09:53:26.458: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:53:26.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9313" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":93,"skipped":1665,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:53:26.512: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Jan 21 09:53:26.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5459 cluster-info'
Jan 21 09:53:27.641: INFO: stderr: ""
Jan 21 09:53:27.641: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:53:27.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5459" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":94,"skipped":1673,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:53:27.693: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0121 09:53:37.905169      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 21 09:53:37.905: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:53:37.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3010" for this suite.

• [SLOW TEST:10.234 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":95,"skipped":1675,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:53:37.929: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:53:38.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-7724 create -f -'
Jan 21 09:53:39.747: INFO: stderr: ""
Jan 21 09:53:39.747: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 21 09:53:39.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-7724 create -f -'
Jan 21 09:53:40.755: INFO: stderr: ""
Jan 21 09:53:40.755: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 21 09:53:41.787: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:53:41.787: INFO: Found 0 / 1
Jan 21 09:53:42.771: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:53:42.772: INFO: Found 0 / 1
Jan 21 09:53:43.764: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:53:43.764: INFO: Found 0 / 1
Jan 21 09:53:44.776: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:53:44.776: INFO: Found 1 / 1
Jan 21 09:53:44.776: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 21 09:53:44.783: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 09:53:44.783: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 21 09:53:44.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-7724 describe pod agnhost-primary-dwmhv'
Jan 21 09:53:44.947: INFO: stderr: ""
Jan 21 09:53:44.947: INFO: stdout: "Name:         agnhost-primary-dwmhv\nNamespace:    kubectl-7724\nPriority:     0\nNode:         conformance1/10.10.1.86\nStart Time:   Fri, 21 Jan 2022 09:53:39 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: ed525fcefaf069b27ebb46090066f21531696975aae5986456426ac3e62df243\n              cni.projectcalico.org/podIP: 172.16.209.122/32\n              cni.projectcalico.org/podIPs: 172.16.209.122/32\nStatus:       Running\nIP:           172.16.209.122\nIPs:\n  IP:           172.16.209.122\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://f1f2c687e6d7d2d23e3927829cebbb80145ce41d09fbb0bb180f0f1f8d29d0c4\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 Jan 2022 09:53:43 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2ghmk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2ghmk:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  5s    default-scheduler  Successfully assigned kubectl-7724/agnhost-primary-dwmhv to conformance1\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 21 09:53:44.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-7724 describe rc agnhost-primary'
Jan 21 09:53:45.119: INFO: stderr: ""
Jan 21 09:53:45.119: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7724\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  6s    replication-controller  Created pod: agnhost-primary-dwmhv\n"
Jan 21 09:53:45.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-7724 describe service agnhost-primary'
Jan 21 09:53:45.268: INFO: stderr: ""
Jan 21 09:53:45.268: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7724\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.10.72.252\nIPs:               10.10.72.252\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.209.122:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 21 09:53:45.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-7724 describe node conformance1'
Jan 21 09:53:45.588: INFO: stderr: ""
Jan 21 09:53:45.588: INFO: stdout: "Name:               conformance1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=conformance1\n                    kubernetes.io/os=linux\n                    nirmata.io/cluster.name=k8s-123-test\n                    nirmata.io/cluster.role=control-plane\n                    nirmata.io/node.name=conformance1\n                    node=conformance1\n                    node-role.kubernetes.io/master=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.1.86/16\n                    projectcalico.org/IPv4VXLANTunnelAddr: 172.16.209.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 21 Jan 2022 06:54:01 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  conformance1\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 21 Jan 2022 09:53:42 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 21 Jan 2022 06:55:08 +0000   Fri, 21 Jan 2022 06:55:08 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 21 Jan 2022 09:52:12 +0000   Fri, 21 Jan 2022 06:54:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 21 Jan 2022 09:52:12 +0000   Fri, 21 Jan 2022 06:54:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 21 Jan 2022 09:52:12 +0000   Fri, 21 Jan 2022 06:54:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 21 Jan 2022 09:52:12 +0000   Fri, 21 Jan 2022 06:55:13 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.10.1.86\n  Hostname:    conformance1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  31166436Ki\n  hugepages-2Mi:      0\n  memory:             8149868Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  28722987371\n  hugepages-2Mi:      0\n  memory:             8047468Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 cf9039846e817bf110c3933d5c3e0c56\n  System UUID:                6965D524-A02C-12ED-9FCA-91C7F0430F4E\n  Boot ID:                    1e20909f-748e-437e-8769-32c891468f2a\n  Kernel Version:             4.4.0-131-generic\n  OS Image:                   Debian GNU/Linux 9 (stretch)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.6\n  Kubelet Version:            v1.23.1\n  Kube-Proxy Version:         v1.23.1\nPodCIDR:                      192.168.1.0/24\nPodCIDRs:                     192.168.1.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-haproxy             haproxy-ingress-6968c464f7-sc6ml                           10m (0%)      0 (0%)      20Mi (0%)        100Mi (1%)     176m\n  ingress-haproxy             ingress-default-backend-6f6c8556b8-nvd8n                   250m (12%)    0 (0%)      200Mi (2%)       250Mi (3%)     176m\n  kube-system                 calico-node-q458w                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         178m\n  kubectl-7724                agnhost-primary-dwmhv                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         6s\n  nirmata                     nirmata-cni-installer-6tvqk                                250m (12%)    0 (0%)      100Mi (1%)       200Mi (2%)     175m\n  nirmata                     otel-agent-857cc46474-4msf2                                100m (5%)     100m (5%)   200Mi (2%)       200Mi (2%)     176m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  sonobuoy                    sonobuoy-e2e-job-9425c772f26a4979                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-9kb9z    0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                860m (43%)  100m (5%)\n  memory             520Mi (6%)  750Mi (9%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                                            Age    From        Message\n  ----     ------                                            ----   ----        -------\n  Warning  listen tcp4 :32184: bind: address already in use  14m    kube-proxy  can't open port \"nodePort for services-9272/affinity-nodeport\" (:32184/tcp4), skipping it\n  Warning  listen tcp4 :31486: bind: address already in use  8m54s  kube-proxy  can't open port \"nodePort for services-3051/nodeport-service\" (:31486/tcp4), skipping it\n  Warning  listen tcp4 :32233: bind: address already in use  6m     kube-proxy  can't open port \"nodePort for resourcequota-9837/test-service-np\" (:32233/tcp4), skipping it\n"
Jan 21 09:53:45.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-7724 describe namespace kubectl-7724'
Jan 21 09:53:45.746: INFO: stderr: ""
Jan 21 09:53:45.746: INFO: stdout: "Name:         kubectl-7724\nLabels:       e2e-framework=kubectl\n              e2e-run=bfa569a5-f1e6-42e1-949e-9522ff0644bc\n              kubernetes.io/metadata.name=kubectl-7724\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:53:45.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7724" for this suite.

• [SLOW TEST:7.843 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1107
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":96,"skipped":1676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:53:45.773: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-7a117d04-ea24-467e-a8fe-a6d79d2b88ba
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:53:45.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4144" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":97,"skipped":1712,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:53:45.901: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-7893d13a-6300-4b91-9283-3e9adcfdc158
STEP: Creating configMap with name cm-test-opt-upd-ad45d2c2-900c-4d9f-b92a-d932e344a870
STEP: Creating the pod
Jan 21 09:53:46.138: INFO: The status of Pod pod-projected-configmaps-6b1dbb11-e181-4214-8fb0-71f7bf50da8e is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:53:48.147: INFO: The status of Pod pod-projected-configmaps-6b1dbb11-e181-4214-8fb0-71f7bf50da8e is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:53:50.163: INFO: The status of Pod pod-projected-configmaps-6b1dbb11-e181-4214-8fb0-71f7bf50da8e is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:53:52.169: INFO: The status of Pod pod-projected-configmaps-6b1dbb11-e181-4214-8fb0-71f7bf50da8e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-7893d13a-6300-4b91-9283-3e9adcfdc158
STEP: Updating configmap cm-test-opt-upd-ad45d2c2-900c-4d9f-b92a-d932e344a870
STEP: Creating configMap with name cm-test-opt-create-b325313a-a4c1-427c-90f7-6b1499eee1ac
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:53:57.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8592" for this suite.

• [SLOW TEST:11.402 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:53:57.315: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:54:08.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-155" for this suite.

• [SLOW TEST:11.435 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":99,"skipped":1739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:54:08.752: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 21 09:54:08.891: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:54:13.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-902" for this suite.

• [SLOW TEST:5.320 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":100,"skipped":1787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:54:14.073: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1571
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 21 09:54:14.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2612 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 21 09:54:14.894: INFO: stderr: ""
Jan 21 09:54:14.894: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 21 09:54:19.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2612 get pod e2e-test-httpd-pod -o json'
Jan 21 09:54:20.105: INFO: stderr: ""
Jan 21 09:54:20.105: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"8a6cdccd6d581ba4767fa9cafd0e4cee1a092197f34d10676a0020484231de59\",\n            \"cni.projectcalico.org/podIP\": \"172.16.209.123/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.209.123/32\"\n        },\n        \"creationTimestamp\": \"2022-01-21T09:54:14Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2612\",\n        \"resourceVersion\": \"16678\",\n        \"uid\": \"bb248572-c229-4eb8-8214-2d719509d071\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zncjw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"conformance1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zncjw\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-21T09:54:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-21T09:54:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-21T09:54:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-21T09:54:14Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2607c19002cc8d7f1207f22a34eecc5a178d71fd64ae68d1927c0ca436eff60a\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-01-21T09:54:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.1.86\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.209.123\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.209.123\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-01-21T09:54:14Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 21 09:54:20.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2612 replace -f -'
Jan 21 09:54:20.661: INFO: stderr: ""
Jan 21 09:54:20.661: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1575
Jan 21 09:54:20.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2612 delete pods e2e-test-httpd-pod'
Jan 21 09:54:23.781: INFO: stderr: ""
Jan 21 09:54:23.781: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:54:23.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2612" for this suite.

• [SLOW TEST:9.738 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":101,"skipped":1814,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:54:23.852: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jan 21 09:54:23.997: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:26.020: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:28.018: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:30.026: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.10.1.86 on the node which pod1 resides and expect scheduled
Jan 21 09:54:30.078: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:32.090: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:34.087: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:36.090: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.10.1.86 but use UDP protocol on the node which pod2 resides
Jan 21 09:54:36.261: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:38.276: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:40.270: INFO: The status of Pod pod3 is Running (Ready = false)
Jan 21 09:54:42.272: INFO: The status of Pod pod3 is Running (Ready = true)
Jan 21 09:54:42.321: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:44.346: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:54:46.346: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jan 21 09:54:46.359: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.10.1.86 http://127.0.0.1:54323/hostname] Namespace:hostport-2635 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:54:46.359: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:54:46.360: INFO: ExecWithOptions: Clientset creation
Jan 21 09:54:46.360: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/hostport-2635/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.10.1.86+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.1.86, port: 54323
Jan 21 09:54:47.000: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.10.1.86:54323/hostname] Namespace:hostport-2635 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:54:47.001: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:54:47.002: INFO: ExecWithOptions: Clientset creation
Jan 21 09:54:47.002: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/hostport-2635/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.10.1.86%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.1.86, port: 54323 UDP
Jan 21 09:54:47.295: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.10.1.86 54323] Namespace:hostport-2635 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:54:47.295: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:54:47.303: INFO: ExecWithOptions: Clientset creation
Jan 21 09:54:47.304: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/hostport-2635/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.10.1.86+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:54:52.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-2635" for this suite.

• [SLOW TEST:28.712 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":102,"skipped":1836,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:54:52.566: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 21 09:54:52.740: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:54:58.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8237" for this suite.

• [SLOW TEST:5.844 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":103,"skipped":1840,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:54:58.414: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jan 21 09:54:59.117: INFO: running pods: 0 < 1
Jan 21 09:55:01.129: INFO: running pods: 0 < 1
Jan 21 09:55:03.138: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:55:05.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9607" for this suite.

• [SLOW TEST:6.908 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":104,"skipped":1847,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:55:05.324: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jan 21 09:55:05.499: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:55:07.513: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:55:09.521: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 21 09:55:09.592: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:55:11.610: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:55:13.604: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:55:15.601: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 21 09:55:15.626: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 09:55:15.659: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 09:55:17.661: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 09:55:17.692: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 09:55:19.659: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 09:55:19.671: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:55:19.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6883" for this suite.

• [SLOW TEST:14.409 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":1864,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:55:19.738: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 09:55:19.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2" in namespace "projected-1995" to be "Succeeded or Failed"
Jan 21 09:55:19.894: INFO: Pod "downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.296894ms
Jan 21 09:55:21.904: INFO: Pod "downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03433635s
Jan 21 09:55:23.947: INFO: Pod "downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077482974s
Jan 21 09:55:25.964: INFO: Pod "downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.093991805s
STEP: Saw pod success
Jan 21 09:55:25.964: INFO: Pod "downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2" satisfied condition "Succeeded or Failed"
Jan 21 09:55:25.972: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2 container client-container: <nil>
STEP: delete the pod
Jan 21 09:55:26.021: INFO: Waiting for pod downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2 to disappear
Jan 21 09:55:26.037: INFO: Pod downwardapi-volume-48ba4085-02d5-4a79-af9f-7d607e7c3af2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:55:26.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1995" for this suite.

• [SLOW TEST:6.317 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":106,"skipped":1873,"failed":0}
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:55:26.056: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9288
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 21 09:55:26.196: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 21 09:55:26.333: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:55:28.406: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:55:30.340: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 09:55:32.352: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:55:34.349: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:55:36.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:55:38.349: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:55:40.341: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:55:42.350: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:55:44.354: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 09:55:46.360: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 21 09:55:46.380: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 21 09:55:48.416: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 21 09:55:52.565: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 21 09:55:52.565: INFO: Breadth first check of 172.16.209.72 on host 10.10.1.86...
Jan 21 09:55:52.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.209.73:9080/dial?request=hostname&protocol=http&host=172.16.209.72&port=8083&tries=1'] Namespace:pod-network-test-9288 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:55:52.573: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:55:52.574: INFO: ExecWithOptions: Clientset creation
Jan 21 09:55:52.574: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/pod-network-test-9288/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.209.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.209.72%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 21 09:55:52.788: INFO: Waiting for responses: map[]
Jan 21 09:55:52.788: INFO: reached 172.16.209.72 after 0/1 tries
Jan 21 09:55:52.788: INFO: Breadth first check of 172.16.106.141 on host 10.10.1.136...
Jan 21 09:55:52.798: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.209.73:9080/dial?request=hostname&protocol=http&host=172.16.106.141&port=8083&tries=1'] Namespace:pod-network-test-9288 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 09:55:52.798: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 09:55:52.799: INFO: ExecWithOptions: Clientset creation
Jan 21 09:55:52.799: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/pod-network-test-9288/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.209.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.106.141%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 21 09:55:53.051: INFO: Waiting for responses: map[]
Jan 21 09:55:53.052: INFO: reached 172.16.106.141 after 0/1 tries
Jan 21 09:55:53.052: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:55:53.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9288" for this suite.

• [SLOW TEST:27.047 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":1873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:55:53.110: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:55:53.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5077" for this suite.
STEP: Destroying namespace "nspatchtest-82582462-f679-4d7e-b957-b149076653e4-8982" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":108,"skipped":1949,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:55:53.370: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-24ac04a8-c659-460e-a243-16ee0e93dce4
STEP: Creating a pod to test consume secrets
Jan 21 09:55:53.522: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-55de15d6-19ba-4a3e-8ec8-ffa96bfd3063" in namespace "projected-3256" to be "Succeeded or Failed"
Jan 21 09:55:53.556: INFO: Pod "pod-projected-secrets-55de15d6-19ba-4a3e-8ec8-ffa96bfd3063": Phase="Pending", Reason="", readiness=false. Elapsed: 33.908986ms
Jan 21 09:55:55.588: INFO: Pod "pod-projected-secrets-55de15d6-19ba-4a3e-8ec8-ffa96bfd3063": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065242853s
Jan 21 09:55:57.603: INFO: Pod "pod-projected-secrets-55de15d6-19ba-4a3e-8ec8-ffa96bfd3063": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080474238s
STEP: Saw pod success
Jan 21 09:55:57.603: INFO: Pod "pod-projected-secrets-55de15d6-19ba-4a3e-8ec8-ffa96bfd3063" satisfied condition "Succeeded or Failed"
Jan 21 09:55:57.612: INFO: Trying to get logs from node conformance1 pod pod-projected-secrets-55de15d6-19ba-4a3e-8ec8-ffa96bfd3063 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 09:55:57.683: INFO: Waiting for pod pod-projected-secrets-55de15d6-19ba-4a3e-8ec8-ffa96bfd3063 to disappear
Jan 21 09:55:57.693: INFO: Pod pod-projected-secrets-55de15d6-19ba-4a3e-8ec8-ffa96bfd3063 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:55:57.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3256" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":109,"skipped":1955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:55:57.729: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 09:55:57.971: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c" in namespace "downward-api-3888" to be "Succeeded or Failed"
Jan 21 09:55:58.013: INFO: Pod "downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c": Phase="Pending", Reason="", readiness=false. Elapsed: 39.979501ms
Jan 21 09:56:00.085: INFO: Pod "downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111878797s
Jan 21 09:56:02.136: INFO: Pod "downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16257797s
Jan 21 09:56:04.153: INFO: Pod "downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.180031322s
Jan 21 09:56:06.170: INFO: Pod "downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.196675551s
STEP: Saw pod success
Jan 21 09:56:06.170: INFO: Pod "downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c" satisfied condition "Succeeded or Failed"
Jan 21 09:56:06.191: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c container client-container: <nil>
STEP: delete the pod
Jan 21 09:56:06.290: INFO: Waiting for pod downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c to disappear
Jan 21 09:56:06.296: INFO: Pod downwardapi-volume-90ef035b-85a3-4759-a4a3-914c84ddc73c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:56:06.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3888" for this suite.

• [SLOW TEST:8.588 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":2039,"failed":0}
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:56:06.323: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:56:06.425: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c8f12ad6-4283-4d50-8346-e15b2c1fda5d" in namespace "security-context-test-7111" to be "Succeeded or Failed"
Jan 21 09:56:06.441: INFO: Pod "busybox-privileged-false-c8f12ad6-4283-4d50-8346-e15b2c1fda5d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.169879ms
Jan 21 09:56:08.487: INFO: Pod "busybox-privileged-false-c8f12ad6-4283-4d50-8346-e15b2c1fda5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06155389s
Jan 21 09:56:10.515: INFO: Pod "busybox-privileged-false-c8f12ad6-4283-4d50-8346-e15b2c1fda5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090091532s
Jan 21 09:56:10.515: INFO: Pod "busybox-privileged-false-c8f12ad6-4283-4d50-8346-e15b2c1fda5d" satisfied condition "Succeeded or Failed"
Jan 21 09:56:10.544: INFO: Got logs for pod "busybox-privileged-false-c8f12ad6-4283-4d50-8346-e15b2c1fda5d": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:56:10.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7111" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":111,"skipped":2042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:56:10.596: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Jan 21 09:56:10.794: INFO: Waiting up to 5m0s for pod "test-pod-722432f9-9aa6-4198-b64d-6d41409439cb" in namespace "svcaccounts-1318" to be "Succeeded or Failed"
Jan 21 09:56:10.823: INFO: Pod "test-pod-722432f9-9aa6-4198-b64d-6d41409439cb": Phase="Pending", Reason="", readiness=false. Elapsed: 28.679444ms
Jan 21 09:56:12.932: INFO: Pod "test-pod-722432f9-9aa6-4198-b64d-6d41409439cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137433087s
Jan 21 09:56:14.965: INFO: Pod "test-pod-722432f9-9aa6-4198-b64d-6d41409439cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.171377215s
STEP: Saw pod success
Jan 21 09:56:14.966: INFO: Pod "test-pod-722432f9-9aa6-4198-b64d-6d41409439cb" satisfied condition "Succeeded or Failed"
Jan 21 09:56:14.977: INFO: Trying to get logs from node conformance1 pod test-pod-722432f9-9aa6-4198-b64d-6d41409439cb container agnhost-container: <nil>
STEP: delete the pod
Jan 21 09:56:15.054: INFO: Waiting for pod test-pod-722432f9-9aa6-4198-b64d-6d41409439cb to disappear
Jan 21 09:56:15.061: INFO: Pod test-pod-722432f9-9aa6-4198-b64d-6d41409439cb no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:56:15.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1318" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":112,"skipped":2069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:56:15.093: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:56:15.266: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Creating first CR 
Jan 21 09:56:17.923: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-21T09:56:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-21T09:56:17Z]] name:name1 resourceVersion:17288 uid:bf9896cf-d049-4eef-9332-a3acfe4ee2d3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 21 09:56:27.948: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-21T09:56:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-21T09:56:27Z]] name:name2 resourceVersion:17303 uid:88bcebb7-ad37-49d9-a811-5b779b650d9c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 21 09:56:37.970: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-21T09:56:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-21T09:56:37Z]] name:name1 resourceVersion:17307 uid:bf9896cf-d049-4eef-9332-a3acfe4ee2d3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 21 09:56:47.991: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-21T09:56:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-21T09:56:47Z]] name:name2 resourceVersion:17311 uid:88bcebb7-ad37-49d9-a811-5b779b650d9c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 21 09:56:58.019: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-21T09:56:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-21T09:56:37Z]] name:name1 resourceVersion:17315 uid:bf9896cf-d049-4eef-9332-a3acfe4ee2d3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 21 09:57:08.046: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-21T09:56:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-21T09:56:47Z]] name:name2 resourceVersion:17319 uid:88bcebb7-ad37-49d9-a811-5b779b650d9c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:18.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6152" for this suite.

• [SLOW TEST:63.613 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":113,"skipped":2101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:18.703: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 21 09:57:18.923: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 21 09:57:18.938: INFO: Waiting for terminating namespaces to be deleted...
Jan 21 09:57:18.944: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
Jan 21 09:57:18.959: INFO: haproxy-ingress-6968c464f7-sc6ml from ingress-haproxy started at 2022-01-21 06:57:22 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.959: INFO: 	Container haproxy-ingress ready: false, restart count 63
Jan 21 09:57:18.959: INFO: ingress-default-backend-6f6c8556b8-nvd8n from ingress-haproxy started at 2022-01-21 06:57:22 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.959: INFO: 	Container ingress-default-backend ready: true, restart count 0
Jan 21 09:57:18.959: INFO: calico-node-q458w from kube-system started at 2022-01-21 06:54:48 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.959: INFO: 	Container calico-node ready: true, restart count 0
Jan 21 09:57:18.959: INFO: nirmata-cni-installer-6tvqk from nirmata started at 2022-01-21 06:57:57 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.959: INFO: 	Container install-cni ready: true, restart count 1
Jan 21 09:57:18.959: INFO: otel-agent-857cc46474-4msf2 from nirmata started at 2022-01-21 06:57:23 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.959: INFO: 	Container otel-agent ready: true, restart count 0
Jan 21 09:57:18.959: INFO: sonobuoy from sonobuoy started at 2022-01-21 09:19:31 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.959: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 21 09:57:18.959: INFO: sonobuoy-e2e-job-9425c772f26a4979 from sonobuoy started at 2022-01-21 09:19:34 +0000 UTC (2 container statuses recorded)
Jan 21 09:57:18.959: INFO: 	Container e2e ready: true, restart count 0
Jan 21 09:57:18.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:57:18.959: INFO: sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-9kb9z from sonobuoy started at 2022-01-21 09:19:35 +0000 UTC (2 container statuses recorded)
Jan 21 09:57:18.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:57:18.959: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 21 09:57:18.959: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
Jan 21 09:57:18.973: INFO: calico-kube-controllers-7bf9dd85d9-xd27g from kube-system started at 2022-01-21 06:55:04 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.973: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 21 09:57:18.973: INFO: calico-node-fg2tt from kube-system started at 2022-01-21 06:54:48 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.973: INFO: 	Container calico-node ready: true, restart count 0
Jan 21 09:57:18.973: INFO: coredns-6779677b89-ldbmt from kube-system started at 2022-01-21 06:55:08 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.973: INFO: 	Container coredns ready: true, restart count 0
Jan 21 09:57:18.974: INFO: coredns-6779677b89-mr5vk from kube-system started at 2022-01-21 06:55:07 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.974: INFO: 	Container coredns ready: true, restart count 0
Jan 21 09:57:18.974: INFO: metrics-server-646d5db6cc-nmv44 from kube-system started at 2022-01-21 06:55:09 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.974: INFO: 	Container metrics-server ready: true, restart count 0
Jan 21 09:57:18.974: INFO: nirmata-cni-installer-jq2k2 from nirmata started at 2022-01-21 06:57:57 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.974: INFO: 	Container install-cni ready: true, restart count 0
Jan 21 09:57:18.974: INFO: nirmata-kube-controller-666bcc6bbc-zss9n from nirmata started at 2022-01-21 06:55:04 +0000 UTC (1 container statuses recorded)
Jan 21 09:57:18.974: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Jan 21 09:57:18.974: INFO: sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-rw7d8 from sonobuoy started at 2022-01-21 09:19:34 +0000 UTC (2 container statuses recorded)
Jan 21 09:57:18.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 09:57:18.974: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16cc40a46c88f22d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:20.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9172" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":114,"skipped":2141,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:20.129: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jan 21 09:57:20.290: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jan 21 09:57:20.347: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:20.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5571" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":115,"skipped":2266,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:20.438: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:57:22.287: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 21 09:57:22.291: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 21 09:57:22.291: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 21 09:57:22.291: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 21 09:57:22.291: INFO: Checking APIGroup: apps
Jan 21 09:57:22.294: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 21 09:57:22.294: INFO: Versions found [{apps/v1 v1}]
Jan 21 09:57:22.294: INFO: apps/v1 matches apps/v1
Jan 21 09:57:22.294: INFO: Checking APIGroup: events.k8s.io
Jan 21 09:57:22.296: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 21 09:57:22.297: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 21 09:57:22.297: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 21 09:57:22.297: INFO: Checking APIGroup: authentication.k8s.io
Jan 21 09:57:22.300: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 21 09:57:22.301: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 21 09:57:22.301: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 21 09:57:22.301: INFO: Checking APIGroup: authorization.k8s.io
Jan 21 09:57:22.304: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 21 09:57:22.304: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 21 09:57:22.305: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 21 09:57:22.305: INFO: Checking APIGroup: autoscaling
Jan 21 09:57:22.307: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 21 09:57:22.308: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 21 09:57:22.308: INFO: autoscaling/v2 matches autoscaling/v2
Jan 21 09:57:22.308: INFO: Checking APIGroup: batch
Jan 21 09:57:22.311: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 21 09:57:22.311: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 21 09:57:22.311: INFO: batch/v1 matches batch/v1
Jan 21 09:57:22.311: INFO: Checking APIGroup: certificates.k8s.io
Jan 21 09:57:22.314: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 21 09:57:22.314: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 21 09:57:22.314: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 21 09:57:22.314: INFO: Checking APIGroup: networking.k8s.io
Jan 21 09:57:22.316: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 21 09:57:22.316: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 21 09:57:22.316: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 21 09:57:22.316: INFO: Checking APIGroup: policy
Jan 21 09:57:22.319: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 21 09:57:22.319: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jan 21 09:57:22.319: INFO: policy/v1 matches policy/v1
Jan 21 09:57:22.319: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 21 09:57:22.322: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 21 09:57:22.323: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 21 09:57:22.323: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 21 09:57:22.323: INFO: Checking APIGroup: storage.k8s.io
Jan 21 09:57:22.326: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 21 09:57:22.326: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 21 09:57:22.327: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 21 09:57:22.327: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 21 09:57:22.329: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 21 09:57:22.330: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 21 09:57:22.330: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 21 09:57:22.330: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 21 09:57:22.332: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 21 09:57:22.333: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 21 09:57:22.333: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 21 09:57:22.333: INFO: Checking APIGroup: scheduling.k8s.io
Jan 21 09:57:22.336: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 21 09:57:22.336: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 21 09:57:22.336: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 21 09:57:22.337: INFO: Checking APIGroup: coordination.k8s.io
Jan 21 09:57:22.339: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 21 09:57:22.340: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 21 09:57:22.340: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 21 09:57:22.340: INFO: Checking APIGroup: node.k8s.io
Jan 21 09:57:22.343: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 21 09:57:22.343: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jan 21 09:57:22.343: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 21 09:57:22.344: INFO: Checking APIGroup: discovery.k8s.io
Jan 21 09:57:22.346: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 21 09:57:22.346: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jan 21 09:57:22.347: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 21 09:57:22.347: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 21 09:57:22.349: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 21 09:57:22.350: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 21 09:57:22.350: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 21 09:57:22.350: INFO: Checking APIGroup: crd.projectcalico.org
Jan 21 09:57:22.353: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 21 09:57:22.353: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 21 09:57:22.353: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 21 09:57:22.354: INFO: Checking APIGroup: kyverno.io
Jan 21 09:57:22.357: INFO: PreferredVersion.GroupVersion: kyverno.io/v1
Jan 21 09:57:22.358: INFO: Versions found [{kyverno.io/v1 v1} {kyverno.io/v1alpha2 v1alpha2} {kyverno.io/v1alpha1 v1alpha1}]
Jan 21 09:57:22.358: INFO: kyverno.io/v1 matches kyverno.io/v1
Jan 21 09:57:22.358: INFO: Checking APIGroup: wgpolicyk8s.io
Jan 21 09:57:22.360: INFO: PreferredVersion.GroupVersion: wgpolicyk8s.io/v1alpha2
Jan 21 09:57:22.361: INFO: Versions found [{wgpolicyk8s.io/v1alpha2 v1alpha2} {wgpolicyk8s.io/v1alpha1 v1alpha1}]
Jan 21 09:57:22.361: INFO: wgpolicyk8s.io/v1alpha2 matches wgpolicyk8s.io/v1alpha2
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:22.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3128" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":116,"skipped":2280,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:22.388: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-2be67e3d-641a-4dff-b118-70affb276a4d
STEP: Creating a pod to test consume configMaps
Jan 21 09:57:22.518: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2" in namespace "projected-3497" to be "Succeeded or Failed"
Jan 21 09:57:22.547: INFO: Pod "pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.429862ms
Jan 21 09:57:24.586: INFO: Pod "pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064929118s
Jan 21 09:57:26.604: INFO: Pod "pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08248693s
Jan 21 09:57:28.661: INFO: Pod "pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.13975589s
STEP: Saw pod success
Jan 21 09:57:28.662: INFO: Pod "pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2" satisfied condition "Succeeded or Failed"
Jan 21 09:57:28.670: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 09:57:28.855: INFO: Waiting for pod pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2 to disappear
Jan 21 09:57:28.877: INFO: Pod pod-projected-configmaps-82f3fc3d-bff2-4665-94c0-cdea8c0333c2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:28.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3497" for this suite.

• [SLOW TEST:6.573 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":2284,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:28.960: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Jan 21 09:57:29.057: INFO: created test-event-1
Jan 21 09:57:29.076: INFO: created test-event-2
Jan 21 09:57:29.089: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 21 09:57:29.098: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 21 09:57:29.136: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:29.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1960" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":118,"skipped":2295,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:29.181: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 09:57:31.098: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 09:57:33.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 9, 57, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 57, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 57, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 57, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 09:57:36.229: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:57:36.243: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6318-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:39.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8604" for this suite.
STEP: Destroying namespace "webhook-8604-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.959 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":119,"skipped":2306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:40.152: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-163/configmap-test-0441d9e8-c7d6-4e56-b831-0dd54e9b8a18
STEP: Creating a pod to test consume configMaps
Jan 21 09:57:40.601: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c190474-a6a1-4637-ad62-d951a69c3eff" in namespace "configmap-163" to be "Succeeded or Failed"
Jan 21 09:57:40.624: INFO: Pod "pod-configmaps-9c190474-a6a1-4637-ad62-d951a69c3eff": Phase="Pending", Reason="", readiness=false. Elapsed: 22.911556ms
Jan 21 09:57:42.641: INFO: Pod "pod-configmaps-9c190474-a6a1-4637-ad62-d951a69c3eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039825408s
Jan 21 09:57:44.655: INFO: Pod "pod-configmaps-9c190474-a6a1-4637-ad62-d951a69c3eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053561682s
STEP: Saw pod success
Jan 21 09:57:44.655: INFO: Pod "pod-configmaps-9c190474-a6a1-4637-ad62-d951a69c3eff" satisfied condition "Succeeded or Failed"
Jan 21 09:57:44.662: INFO: Trying to get logs from node conformance1 pod pod-configmaps-9c190474-a6a1-4637-ad62-d951a69c3eff container env-test: <nil>
STEP: delete the pod
Jan 21 09:57:44.787: INFO: Waiting for pod pod-configmaps-9c190474-a6a1-4637-ad62-d951a69c3eff to disappear
Jan 21 09:57:44.799: INFO: Pod pod-configmaps-9c190474-a6a1-4637-ad62-d951a69c3eff no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:44.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-163" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":120,"skipped":2346,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:44.856: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:57:44.988: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 21 09:57:45.035: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 21 09:57:50.075: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 21 09:57:50.075: INFO: Creating deployment "test-rolling-update-deployment"
Jan 21 09:57:50.101: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 21 09:57:50.123: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 21 09:57:52.154: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 21 09:57:52.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 57, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 57, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 57, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 57, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-796dbc4547\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 09:57:54.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 57, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 57, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 9, 57, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 9, 57, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-796dbc4547\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 09:57:56.179: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 21 09:57:56.197: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6799  53c20793-f27c-4e71-abf8-7801fd63dbc4 17620 1 2022-01-21 09:57:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-01-21 09:57:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:57:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea0238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-21 09:57:50 +0000 UTC,LastTransitionTime:2022-01-21 09:57:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-01-21 09:57:54 +0000 UTC,LastTransitionTime:2022-01-21 09:57:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 21 09:57:56.206: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-6799  6bbbdd9f-81ee-414a-9999-aa92ede0b5a6 17610 1 2022-01-21 09:57:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 53c20793-f27c-4e71-abf8-7801fd63dbc4 0xc0039dd887 0xc0039dd888}] []  [{kube-controller-manager Update apps/v1 2022-01-21 09:57:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"53c20793-f27c-4e71-abf8-7801fd63dbc4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:57:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039dd948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 21 09:57:56.206: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 21 09:57:56.206: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6799  13c0b757-bd9b-4b43-bdee-d8c12136d0ec 17619 2 2022-01-21 09:57:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 53c20793-f27c-4e71-abf8-7801fd63dbc4 0xc0039dd757 0xc0039dd758}] []  [{e2e.test Update apps/v1 2022-01-21 09:57:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:57:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"53c20793-f27c-4e71-abf8-7801fd63dbc4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-01-21 09:57:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0039dd818 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 21 09:57:56.215: INFO: Pod "test-rolling-update-deployment-796dbc4547-zqwz5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-zqwz5 test-rolling-update-deployment-796dbc4547- deployment-6799  18b56d2e-2373-4649-bb4c-13fd44c5f6c9 17609 0 2022-01-21 09:57:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[cni.projectcalico.org/containerID:453d8048f58827ff451d7a584afe5847cd63bee294ffa775da430336a26cfe1b cni.projectcalico.org/podIP:172.16.209.86/32 cni.projectcalico.org/podIPs:172.16.209.86/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 6bbbdd9f-81ee-414a-9999-aa92ede0b5a6 0xc003ea0ba7 0xc003ea0ba8}] []  [{kube-controller-manager Update v1 2022-01-21 09:57:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6bbbdd9f-81ee-414a-9999-aa92ede0b5a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 09:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 09:57:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vrvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vrvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:57:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:57:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:57:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 09:57:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:172.16.209.86,StartTime:2022-01-21 09:57:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 09:57:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:docker://9946bdcf40cca2ff0e167627c07916702c24c6d2be47c3e012b2de80947006e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.209.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:57:56.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6799" for this suite.

• [SLOW TEST:11.388 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":121,"skipped":2365,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:57:56.245: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-0cd0a245-a785-449e-8c77-66240925c8fb
STEP: Creating a pod to test consume secrets
Jan 21 09:57:56.473: INFO: Waiting up to 5m0s for pod "pod-secrets-aa6883f7-d48e-4ec0-af94-4650ce5ee5b1" in namespace "secrets-1468" to be "Succeeded or Failed"
Jan 21 09:57:56.479: INFO: Pod "pod-secrets-aa6883f7-d48e-4ec0-af94-4650ce5ee5b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.696408ms
Jan 21 09:57:58.585: INFO: Pod "pod-secrets-aa6883f7-d48e-4ec0-af94-4650ce5ee5b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.112309913s
Jan 21 09:58:00.606: INFO: Pod "pod-secrets-aa6883f7-d48e-4ec0-af94-4650ce5ee5b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.133127635s
STEP: Saw pod success
Jan 21 09:58:00.606: INFO: Pod "pod-secrets-aa6883f7-d48e-4ec0-af94-4650ce5ee5b1" satisfied condition "Succeeded or Failed"
Jan 21 09:58:00.618: INFO: Trying to get logs from node conformance1 pod pod-secrets-aa6883f7-d48e-4ec0-af94-4650ce5ee5b1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 09:58:00.707: INFO: Waiting for pod pod-secrets-aa6883f7-d48e-4ec0-af94-4650ce5ee5b1 to disappear
Jan 21 09:58:00.723: INFO: Pod pod-secrets-aa6883f7-d48e-4ec0-af94-4650ce5ee5b1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:00.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1468" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:00.789: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-4cac704d-a156-4161-a338-8c1aa75be598
STEP: Creating a pod to test consume secrets
Jan 21 09:58:00.999: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7" in namespace "projected-15" to be "Succeeded or Failed"
Jan 21 09:58:01.064: INFO: Pod "pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7": Phase="Pending", Reason="", readiness=false. Elapsed: 61.76817ms
Jan 21 09:58:03.119: INFO: Pod "pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.117207101s
Jan 21 09:58:05.132: INFO: Pod "pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.12974523s
Jan 21 09:58:07.169: INFO: Pod "pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.167244653s
STEP: Saw pod success
Jan 21 09:58:07.170: INFO: Pod "pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7" satisfied condition "Succeeded or Failed"
Jan 21 09:58:07.178: INFO: Trying to get logs from node conformance1 pod pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 09:58:07.251: INFO: Waiting for pod pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7 to disappear
Jan 21 09:58:07.262: INFO: Pod pod-projected-secrets-a92255f5-8932-43f9-bce8-b6f4036370a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:07.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-15" for this suite.

• [SLOW TEST:6.513 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2441,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:07.317: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 21 09:58:10.315: INFO: starting watch
STEP: patching
STEP: updating
Jan 21 09:58:10.378: INFO: waiting for watch events with expected annotations
Jan 21 09:58:10.378: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:10.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5672" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":124,"skipped":2458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:10.597: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 09:58:10.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62ed59f7-640b-46df-bb58-bfea72b0e053" in namespace "downward-api-1219" to be "Succeeded or Failed"
Jan 21 09:58:10.740: INFO: Pod "downwardapi-volume-62ed59f7-640b-46df-bb58-bfea72b0e053": Phase="Pending", Reason="", readiness=false. Elapsed: 18.863203ms
Jan 21 09:58:12.761: INFO: Pod "downwardapi-volume-62ed59f7-640b-46df-bb58-bfea72b0e053": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039704313s
Jan 21 09:58:14.789: INFO: Pod "downwardapi-volume-62ed59f7-640b-46df-bb58-bfea72b0e053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067750605s
STEP: Saw pod success
Jan 21 09:58:14.789: INFO: Pod "downwardapi-volume-62ed59f7-640b-46df-bb58-bfea72b0e053" satisfied condition "Succeeded or Failed"
Jan 21 09:58:14.796: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-62ed59f7-640b-46df-bb58-bfea72b0e053 container client-container: <nil>
STEP: delete the pod
Jan 21 09:58:14.860: INFO: Waiting for pod downwardapi-volume-62ed59f7-640b-46df-bb58-bfea72b0e053 to disappear
Jan 21 09:58:14.867: INFO: Pod downwardapi-volume-62ed59f7-640b-46df-bb58-bfea72b0e053 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:14.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1219" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:14.916: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-381f35a8-4761-4ccd-9c7c-359288d69c27
STEP: Creating a pod to test consume secrets
Jan 21 09:58:15.103: INFO: Waiting up to 5m0s for pod "pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a" in namespace "secrets-7385" to be "Succeeded or Failed"
Jan 21 09:58:15.156: INFO: Pod "pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a": Phase="Pending", Reason="", readiness=false. Elapsed: 47.362452ms
Jan 21 09:58:17.169: INFO: Pod "pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060253009s
Jan 21 09:58:19.195: INFO: Pod "pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08591525s
Jan 21 09:58:21.236: INFO: Pod "pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.126954639s
STEP: Saw pod success
Jan 21 09:58:21.236: INFO: Pod "pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a" satisfied condition "Succeeded or Failed"
Jan 21 09:58:21.253: INFO: Trying to get logs from node conformance1 pod pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 09:58:21.348: INFO: Waiting for pod pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a to disappear
Jan 21 09:58:21.355: INFO: Pod pod-secrets-d91ea5d3-e103-4744-b879-85f94276756a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:21.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7385" for this suite.

• [SLOW TEST:6.465 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":126,"skipped":2504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:21.388: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:21.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4265" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":127,"skipped":2530,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:21.670: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 21 09:58:21.849: INFO: Waiting up to 5m0s for pod "pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7" in namespace "emptydir-8221" to be "Succeeded or Failed"
Jan 21 09:58:21.868: INFO: Pod "pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.325847ms
Jan 21 09:58:23.879: INFO: Pod "pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029649859s
Jan 21 09:58:25.901: INFO: Pod "pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052261076s
Jan 21 09:58:27.951: INFO: Pod "pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.101501386s
STEP: Saw pod success
Jan 21 09:58:27.951: INFO: Pod "pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7" satisfied condition "Succeeded or Failed"
Jan 21 09:58:27.985: INFO: Trying to get logs from node conformance1 pod pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7 container test-container: <nil>
STEP: delete the pod
Jan 21 09:58:28.061: INFO: Waiting for pod pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7 to disappear
Jan 21 09:58:28.099: INFO: Pod pod-3b9f061b-a90a-4aa5-b534-f1dcd2913ee7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:28.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8221" for this suite.

• [SLOW TEST:6.493 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2549,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:28.183: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 09:58:28.444: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a" in namespace "projected-5878" to be "Succeeded or Failed"
Jan 21 09:58:28.529: INFO: Pod "downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a": Phase="Pending", Reason="", readiness=false. Elapsed: 79.14506ms
Jan 21 09:58:30.581: INFO: Pod "downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131406269s
Jan 21 09:58:32.610: INFO: Pod "downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.160049328s
Jan 21 09:58:34.630: INFO: Pod "downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.180174877s
STEP: Saw pod success
Jan 21 09:58:34.630: INFO: Pod "downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a" satisfied condition "Succeeded or Failed"
Jan 21 09:58:34.637: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a container client-container: <nil>
STEP: delete the pod
Jan 21 09:58:34.678: INFO: Waiting for pod downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a to disappear
Jan 21 09:58:34.687: INFO: Pod downwardapi-volume-25cb83bd-8d92-4acb-b4ba-d6ffc068798a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:34.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5878" for this suite.

• [SLOW TEST:6.538 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:34.739: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 21 09:58:34.902: INFO: Waiting up to 5m0s for pod "pod-fc5581f9-e880-46a9-9a01-a442c6410c5a" in namespace "emptydir-4956" to be "Succeeded or Failed"
Jan 21 09:58:34.921: INFO: Pod "pod-fc5581f9-e880-46a9-9a01-a442c6410c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.45903ms
Jan 21 09:58:36.936: INFO: Pod "pod-fc5581f9-e880-46a9-9a01-a442c6410c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033499621s
Jan 21 09:58:38.953: INFO: Pod "pod-fc5581f9-e880-46a9-9a01-a442c6410c5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051149704s
STEP: Saw pod success
Jan 21 09:58:38.955: INFO: Pod "pod-fc5581f9-e880-46a9-9a01-a442c6410c5a" satisfied condition "Succeeded or Failed"
Jan 21 09:58:38.964: INFO: Trying to get logs from node conformance1 pod pod-fc5581f9-e880-46a9-9a01-a442c6410c5a container test-container: <nil>
STEP: delete the pod
Jan 21 09:58:39.020: INFO: Waiting for pod pod-fc5581f9-e880-46a9-9a01-a442c6410c5a to disappear
Jan 21 09:58:39.026: INFO: Pod pod-fc5581f9-e880-46a9-9a01-a442c6410c5a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 09:58:39.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4956" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 09:58:39.047: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jan 21 09:58:39.160: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 21 09:59:39.226: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 09:59:39.238: INFO: Starting informer...
STEP: Starting pod...
Jan 21 09:59:39.515: INFO: Pod is running on conformance1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 21 09:59:39.573: INFO: Pod wasn't evicted. Proceeding
Jan 21 09:59:39.573: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 21 10:00:54.758: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:00:54.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6557" for this suite.

• [SLOW TEST:135.737 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":131,"skipped":2643,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:00:54.798: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:00:54.897: INFO: Got root ca configmap in namespace "svcaccounts-9722"
Jan 21 10:00:54.909: INFO: Deleted root ca configmap in namespace "svcaccounts-9722"
STEP: waiting for a new root ca configmap created
Jan 21 10:00:55.418: INFO: Recreated root ca configmap in namespace "svcaccounts-9722"
Jan 21 10:00:55.435: INFO: Updated root ca configmap in namespace "svcaccounts-9722"
STEP: waiting for the root ca configmap reconciled
Jan 21 10:00:55.947: INFO: Reconciled root ca configmap in namespace "svcaccounts-9722"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:00:55.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9722" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":132,"skipped":2674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:00:55.993: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 21 10:00:56.114: INFO: Waiting up to 5m0s for pod "downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac" in namespace "downward-api-74" to be "Succeeded or Failed"
Jan 21 10:00:56.149: INFO: Pod "downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 34.881009ms
Jan 21 10:00:58.161: INFO: Pod "downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04694263s
Jan 21 10:01:00.192: INFO: Pod "downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077895475s
Jan 21 10:01:02.226: INFO: Pod "downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.112147131s
STEP: Saw pod success
Jan 21 10:01:02.226: INFO: Pod "downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac" satisfied condition "Succeeded or Failed"
Jan 21 10:01:02.241: INFO: Trying to get logs from node conformance1 pod downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac container dapi-container: <nil>
STEP: delete the pod
Jan 21 10:01:02.425: INFO: Waiting for pod downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac to disappear
Jan 21 10:01:02.450: INFO: Pod downward-api-0fd74f2c-417c-44c5-a358-f6afaf2ea7ac no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:01:02.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-74" for this suite.

• [SLOW TEST:6.493 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":133,"skipped":2720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:01:02.493: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7669
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-7669
Jan 21 10:01:02.909: INFO: Found 0 stateful pods, waiting for 1
Jan 21 10:01:12.920: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 21 10:01:12.979: INFO: Deleting all statefulset in ns statefulset-7669
Jan 21 10:01:12.989: INFO: Scaling statefulset ss to 0
Jan 21 10:01:23.108: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 10:01:23.114: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:01:23.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7669" for this suite.

• [SLOW TEST:20.697 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":134,"skipped":2745,"failed":0}
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:01:23.208: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:01:23.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6395" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":135,"skipped":2745,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:01:23.338: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jan 21 10:01:38.454: INFO: 70 pods remaining
Jan 21 10:01:38.487: INFO: 65 pods has nil DeletionTimestamp
Jan 21 10:01:38.495: INFO: 
Jan 21 10:01:39.751: INFO: 49 pods remaining
Jan 21 10:01:39.792: INFO: 46 pods has nil DeletionTimestamp
Jan 21 10:01:39.824: INFO: 
Jan 21 10:01:41.528: INFO: 39 pods remaining
Jan 21 10:01:41.542: INFO: 39 pods has nil DeletionTimestamp
Jan 21 10:01:41.542: INFO: 
Jan 21 10:01:42.628: INFO: 17 pods remaining
Jan 21 10:01:42.628: INFO: 16 pods has nil DeletionTimestamp
Jan 21 10:01:42.628: INFO: 
Jan 21 10:01:43.484: INFO: 3 pods remaining
Jan 21 10:01:43.484: INFO: 1 pods has nil DeletionTimestamp
Jan 21 10:01:43.484: INFO: 
Jan 21 10:01:44.191: INFO: 0 pods remaining
Jan 21 10:01:44.191: INFO: 0 pods has nil DeletionTimestamp
Jan 21 10:01:44.191: INFO: 
STEP: Gathering metrics
W0121 10:01:46.480773      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 21 10:01:46.547: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:01:46.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5091" for this suite.

• [SLOW TEST:23.992 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":136,"skipped":2751,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:01:47.420: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:01:48.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7" in namespace "projected-352" to be "Succeeded or Failed"
Jan 21 10:01:48.358: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 45.942882ms
Jan 21 10:01:50.419: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.106623267s
Jan 21 10:01:52.440: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.128278429s
Jan 21 10:01:54.755: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.443243567s
Jan 21 10:01:57.026: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.713543934s
Jan 21 10:02:00.304: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.991349418s
Jan 21 10:02:02.795: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.483265835s
Jan 21 10:02:05.032: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.719669427s
Jan 21 10:02:07.060: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.747819065s
Jan 21 10:02:09.107: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.795149493s
Jan 21 10:02:11.170: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.858315643s
Jan 21 10:02:13.353: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.040800693s
Jan 21 10:02:15.460: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 27.148251722s
Jan 21 10:02:18.306: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 29.99373089s
Jan 21 10:02:21.714: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 33.402019269s
Jan 21 10:02:24.034: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 35.721485357s
Jan 21 10:02:26.108: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 37.795446921s
Jan 21 10:02:28.224: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 39.912104178s
Jan 21 10:02:30.636: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 42.323785382s
Jan 21 10:02:32.650: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 44.338291878s
Jan 21 10:02:34.915: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 46.60295773s
Jan 21 10:02:36.942: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 48.62950323s
Jan 21 10:02:38.992: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 50.679374634s
Jan 21 10:02:41.045: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 52.733149174s
Jan 21 10:02:43.464: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 55.151801749s
Jan 21 10:02:45.586: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 57.274116716s
Jan 21 10:02:47.908: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 59.596040915s
Jan 21 10:02:49.970: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m1.658322609s
Jan 21 10:02:52.020: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m3.707485032s
Jan 21 10:02:54.027: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m5.715161799s
Jan 21 10:02:56.065: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m7.75257502s
Jan 21 10:02:58.103: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m9.790643826s
Jan 21 10:03:00.109: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m11.796796962s
Jan 21 10:03:02.161: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m13.848708868s
STEP: Saw pod success
Jan 21 10:03:02.161: INFO: Pod "downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7" satisfied condition "Succeeded or Failed"
Jan 21 10:03:02.172: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7 container client-container: <nil>
STEP: delete the pod
Jan 21 10:03:02.654: INFO: Waiting for pod downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7 to disappear
Jan 21 10:03:02.673: INFO: Pod downwardapi-volume-80761623-af95-41cd-b377-97b051148bc7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:03:02.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-352" for this suite.

• [SLOW TEST:75.283 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2771,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:03:02.703: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-99541e96-7447-47d4-8959-a673ca97d54c
STEP: Creating a pod to test consume secrets
Jan 21 10:03:02.926: INFO: Waiting up to 5m0s for pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307" in namespace "secrets-2737" to be "Succeeded or Failed"
Jan 21 10:03:02.976: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Pending", Reason="", readiness=false. Elapsed: 37.860399ms
Jan 21 10:03:05.003: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06540323s
Jan 21 10:03:07.015: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076848462s
Jan 21 10:03:09.050: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Pending", Reason="", readiness=false. Elapsed: 6.112337801s
Jan 21 10:03:11.064: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125902919s
Jan 21 10:03:13.080: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Pending", Reason="", readiness=false. Elapsed: 10.141945635s
Jan 21 10:03:15.116: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Pending", Reason="", readiness=false. Elapsed: 12.178365688s
Jan 21 10:03:17.138: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Pending", Reason="", readiness=false. Elapsed: 14.200356175s
Jan 21 10:03:19.184: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.245879655s
STEP: Saw pod success
Jan 21 10:03:19.184: INFO: Pod "pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307" satisfied condition "Succeeded or Failed"
Jan 21 10:03:19.211: INFO: Trying to get logs from node conformance1 pod pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 10:03:19.457: INFO: Waiting for pod pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307 to disappear
Jan 21 10:03:19.471: INFO: Pod pod-secrets-bcb55fd2-af63-484d-b363-c09bb6177307 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:03:19.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2737" for this suite.

• [SLOW TEST:16.808 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:03:19.562: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 21 10:03:19.792: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:03:29.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9187" for this suite.

• [SLOW TEST:9.650 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":139,"skipped":2812,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:03:29.213: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:03:29.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7882" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":140,"skipped":2824,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:03:29.527: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 21 10:05:30.330: INFO: Successfully updated pod "var-expansion-1d0dac43-d720-4d50-977d-e258ed0ffc1d"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 21 10:05:32.364: INFO: Deleting pod "var-expansion-1d0dac43-d720-4d50-977d-e258ed0ffc1d" in namespace "var-expansion-8420"
Jan 21 10:05:32.396: INFO: Wait up to 5m0s for pod "var-expansion-1d0dac43-d720-4d50-977d-e258ed0ffc1d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:06:06.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8420" for this suite.

• [SLOW TEST:156.936 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":141,"skipped":2834,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:06:06.467: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6546.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6546.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6546.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6546.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6546.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6546.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6546.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6546.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6546.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6546.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 111.89.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.89.111_udp@PTR;check="$$(dig +tcp +noall +answer +search 111.89.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.89.111_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6546.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6546.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6546.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6546.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6546.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6546.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6546.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6546.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6546.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6546.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 111.89.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.89.111_udp@PTR;check="$$(dig +tcp +noall +answer +search 111.89.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.89.111_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 10:06:12.722: INFO: Unable to read wheezy_udp@dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:12.735: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:12.756: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:12.773: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:12.865: INFO: Unable to read jessie_udp@dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:12.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:12.920: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:12.940: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:13.060: INFO: Lookups using dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da failed for: [wheezy_udp@dns-test-service.dns-6546.svc.cluster.local wheezy_tcp@dns-test-service.dns-6546.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_udp@dns-test-service.dns-6546.svc.cluster.local jessie_tcp@dns-test-service.dns-6546.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local]

Jan 21 10:06:18.100: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:18.107: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:18.183: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:18.195: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:18.254: INFO: Lookups using dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local]

Jan 21 10:06:23.085: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:23.094: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:23.164: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:23.174: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:23.206: INFO: Lookups using dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local]

Jan 21 10:06:28.128: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:28.135: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:28.265: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:28.277: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:28.324: INFO: Lookups using dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local]

Jan 21 10:06:33.087: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:33.095: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:33.160: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:33.168: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:33.216: INFO: Lookups using dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local]

Jan 21 10:06:38.083: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:38.090: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:38.149: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:38.160: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local from pod dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da: the server could not find the requested resource (get pods dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da)
Jan 21 10:06:38.197: INFO: Lookups using dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6546.svc.cluster.local]

Jan 21 10:06:43.185: INFO: DNS probes using dns-6546/dns-test-c98eef27-b38e-4dc7-b4b4-0dcf2cca63da succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:06:43.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6546" for this suite.

• [SLOW TEST:37.392 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":142,"skipped":2836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:06:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 21 10:06:43.985: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 21 10:06:49.007: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:06:50.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9814" for this suite.

• [SLOW TEST:6.292 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":143,"skipped":2858,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:06:50.166: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 21 10:06:50.280: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:07:01.101: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:07:29.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9500" for this suite.

• [SLOW TEST:39.109 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":144,"skipped":2861,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:07:29.275: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:07:29.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5449" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":145,"skipped":2862,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:07:29.389: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Jan 21 10:07:29.517: INFO: Waiting up to 5m0s for pod "var-expansion-c199606b-ac4c-469d-bff2-5f3bacec0333" in namespace "var-expansion-2909" to be "Succeeded or Failed"
Jan 21 10:07:29.526: INFO: Pod "var-expansion-c199606b-ac4c-469d-bff2-5f3bacec0333": Phase="Pending", Reason="", readiness=false. Elapsed: 9.022879ms
Jan 21 10:07:31.543: INFO: Pod "var-expansion-c199606b-ac4c-469d-bff2-5f3bacec0333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025831882s
Jan 21 10:07:33.588: INFO: Pod "var-expansion-c199606b-ac4c-469d-bff2-5f3bacec0333": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07116935s
STEP: Saw pod success
Jan 21 10:07:33.612: INFO: Pod "var-expansion-c199606b-ac4c-469d-bff2-5f3bacec0333" satisfied condition "Succeeded or Failed"
Jan 21 10:07:33.639: INFO: Trying to get logs from node conformance1 pod var-expansion-c199606b-ac4c-469d-bff2-5f3bacec0333 container dapi-container: <nil>
STEP: delete the pod
Jan 21 10:07:33.787: INFO: Waiting for pod var-expansion-c199606b-ac4c-469d-bff2-5f3bacec0333 to disappear
Jan 21 10:07:33.798: INFO: Pod var-expansion-c199606b-ac4c-469d-bff2-5f3bacec0333 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:07:33.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2909" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2872,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:07:33.832: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 21 10:07:34.031: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6844  e0297c8b-c474-4104-b496-c24ff17d2e7e 19729 0 2022-01-21 10:07:34 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-01-21 10:07:34 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzr82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzr82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 21 10:07:34.048: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:07:36.064: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:07:38.064: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:07:40.061: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 21 10:07:40.061: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6844 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 10:07:40.061: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:07:40.063: INFO: ExecWithOptions: Clientset creation
Jan 21 10:07:40.063: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/dns-6844/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Jan 21 10:07:40.303: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6844 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 10:07:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:07:40.309: INFO: ExecWithOptions: Clientset creation
Jan 21 10:07:40.309: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/dns-6844/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 21 10:07:40.529: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:07:40.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6844" for this suite.

• [SLOW TEST:6.767 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":147,"skipped":2884,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:07:40.609: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 10:07:43.047: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 21 10:07:45.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 7, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 7, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 7, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 7, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:07:47.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 7, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 7, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 7, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 7, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 10:07:50.150: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:07:50.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3152" for this suite.
STEP: Destroying namespace "webhook-3152-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.282 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":148,"skipped":2930,"failed":0}
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:07:50.892: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jan 21 10:07:51.383: INFO: namespace kubectl-2939
Jan 21 10:07:51.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2939 create -f -'
Jan 21 10:07:54.107: INFO: stderr: ""
Jan 21 10:07:54.107: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 21 10:07:55.120: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 10:07:55.120: INFO: Found 0 / 1
Jan 21 10:07:56.134: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 10:07:56.134: INFO: Found 0 / 1
Jan 21 10:07:57.129: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 10:07:57.129: INFO: Found 0 / 1
Jan 21 10:07:58.118: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 10:07:58.118: INFO: Found 1 / 1
Jan 21 10:07:58.118: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 21 10:07:58.123: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 21 10:07:58.124: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 21 10:07:58.124: INFO: wait on agnhost-primary startup in kubectl-2939 
Jan 21 10:07:58.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2939 logs agnhost-primary-fmlt7 agnhost-primary'
Jan 21 10:07:58.335: INFO: stderr: ""
Jan 21 10:07:58.335: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 21 10:07:58.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2939 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 21 10:07:58.898: INFO: stderr: ""
Jan 21 10:07:58.898: INFO: stdout: "service/rm2 exposed\n"
Jan 21 10:07:58.912: INFO: Service rm2 in namespace kubectl-2939 found.
STEP: exposing service
Jan 21 10:08:00.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2939 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 21 10:08:01.210: INFO: stderr: ""
Jan 21 10:08:01.210: INFO: stdout: "service/rm3 exposed\n"
Jan 21 10:08:01.291: INFO: Service rm3 in namespace kubectl-2939 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:08:03.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2939" for this suite.

• [SLOW TEST:12.441 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":149,"skipped":2930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:08:03.342: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:08:03.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be9df97b-9325-4a1a-b611-26c27293960b" in namespace "projected-3392" to be "Succeeded or Failed"
Jan 21 10:08:03.522: INFO: Pod "downwardapi-volume-be9df97b-9325-4a1a-b611-26c27293960b": Phase="Pending", Reason="", readiness=false. Elapsed: 53.200847ms
Jan 21 10:08:05.537: INFO: Pod "downwardapi-volume-be9df97b-9325-4a1a-b611-26c27293960b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06880439s
Jan 21 10:08:07.549: INFO: Pod "downwardapi-volume-be9df97b-9325-4a1a-b611-26c27293960b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080325453s
STEP: Saw pod success
Jan 21 10:08:07.549: INFO: Pod "downwardapi-volume-be9df97b-9325-4a1a-b611-26c27293960b" satisfied condition "Succeeded or Failed"
Jan 21 10:08:07.558: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-be9df97b-9325-4a1a-b611-26c27293960b container client-container: <nil>
STEP: delete the pod
Jan 21 10:08:07.632: INFO: Waiting for pod downwardapi-volume-be9df97b-9325-4a1a-b611-26c27293960b to disappear
Jan 21 10:08:07.656: INFO: Pod downwardapi-volume-be9df97b-9325-4a1a-b611-26c27293960b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:08:07.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3392" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:08:07.699: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1104, will wait for the garbage collector to delete the pods
Jan 21 10:08:16.056: INFO: Deleting Job.batch foo took: 14.480728ms
Jan 21 10:08:16.257: INFO: Terminating Job.batch foo pods took: 200.709546ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:08:50.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1104" for this suite.

• [SLOW TEST:42.787 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":151,"skipped":2980,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:08:50.488: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 10:08:56.687: INFO: DNS probes using dns-6629/dns-test-3ec29a4d-443c-4e29-9873-09b837791f28 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:08:56.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6629" for this suite.

• [SLOW TEST:6.310 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":152,"skipped":2982,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:08:56.797: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-5844
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5844 to expose endpoints map[]
Jan 21 10:08:57.067: INFO: successfully validated that service multi-endpoint-test in namespace services-5844 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5844
Jan 21 10:08:57.176: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:08:59.225: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:09:01.199: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:09:03.191: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5844 to expose endpoints map[pod1:[100]]
Jan 21 10:09:03.235: INFO: successfully validated that service multi-endpoint-test in namespace services-5844 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5844
Jan 21 10:09:03.265: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:09:05.272: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:09:07.277: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:09:09.280: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5844 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 21 10:09:09.316: INFO: successfully validated that service multi-endpoint-test in namespace services-5844 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jan 21 10:09:09.317: INFO: Creating new exec pod
Jan 21 10:09:14.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5844 exec execpod8wxmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 21 10:09:14.738: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 21 10:09:14.738: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:09:14.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5844 exec execpod8wxmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.66.96 80'
Jan 21 10:09:15.155: INFO: stderr: "+ nc -v -t -w 2 10.10.66.96 80\n+ echo hostName\nConnection to 10.10.66.96 80 port [tcp/http] succeeded!\n"
Jan 21 10:09:15.156: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:09:15.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5844 exec execpod8wxmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 21 10:09:15.656: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 21 10:09:15.656: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:09:15.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5844 exec execpod8wxmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.66.96 81'
Jan 21 10:09:16.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.66.96 81\nConnection to 10.10.66.96 81 port [tcp/*] succeeded!\n"
Jan 21 10:09:16.043: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5844
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5844 to expose endpoints map[pod2:[101]]
Jan 21 10:09:17.361: INFO: successfully validated that service multi-endpoint-test in namespace services-5844 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5844
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5844 to expose endpoints map[]
Jan 21 10:09:17.530: INFO: successfully validated that service multi-endpoint-test in namespace services-5844 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:09:17.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5844" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:20.844 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":153,"skipped":2986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:09:17.642: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 21 10:09:17.819: INFO: Waiting up to 5m0s for pod "pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6" in namespace "emptydir-2129" to be "Succeeded or Failed"
Jan 21 10:09:17.856: INFO: Pod "pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6": Phase="Pending", Reason="", readiness=false. Elapsed: 35.635833ms
Jan 21 10:09:19.878: INFO: Pod "pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058415185s
Jan 21 10:09:21.954: INFO: Pod "pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.134548795s
Jan 21 10:09:23.981: INFO: Pod "pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.161377517s
STEP: Saw pod success
Jan 21 10:09:23.981: INFO: Pod "pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6" satisfied condition "Succeeded or Failed"
Jan 21 10:09:24.002: INFO: Trying to get logs from node conformance1 pod pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6 container test-container: <nil>
STEP: delete the pod
Jan 21 10:09:24.068: INFO: Waiting for pod pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6 to disappear
Jan 21 10:09:24.077: INFO: Pod pod-429135e9-fc1e-4ecf-9f1a-b08dfb0f11a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:09:24.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2129" for this suite.

• [SLOW TEST:6.481 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":154,"skipped":3026,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:09:24.128: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2617
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:09:24.443: INFO: Found 0 stateful pods, waiting for 1
Jan 21 10:09:34.458: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jan 21 10:09:34.503: INFO: Found 1 stateful pods, waiting for 2
Jan 21 10:09:44.518: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jan 21 10:09:54.522: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 10:09:54.522: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 21 10:09:54.577: INFO: Deleting all statefulset in ns statefulset-2617
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:09:54.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2617" for this suite.

• [SLOW TEST:30.530 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":155,"skipped":3029,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:09:54.658: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:09:55.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5208" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":156,"skipped":3045,"failed":0}
S
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:09:55.187: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 21 10:09:55.460: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 21 10:09:55.490: INFO: starting watch
STEP: patching
STEP: updating
Jan 21 10:09:55.551: INFO: waiting for watch events with expected annotations
Jan 21 10:09:55.551: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:09:56.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3084" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":157,"skipped":3046,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:09:56.272: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:09:56.771: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506" in namespace "projected-2036" to be "Succeeded or Failed"
Jan 21 10:09:56.830: INFO: Pod "downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506": Phase="Pending", Reason="", readiness=false. Elapsed: 56.412639ms
Jan 21 10:09:58.852: INFO: Pod "downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078382933s
Jan 21 10:10:00.920: INFO: Pod "downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146356769s
Jan 21 10:10:02.935: INFO: Pod "downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.161211519s
STEP: Saw pod success
Jan 21 10:10:02.935: INFO: Pod "downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506" satisfied condition "Succeeded or Failed"
Jan 21 10:10:02.945: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506 container client-container: <nil>
STEP: delete the pod
Jan 21 10:10:03.036: INFO: Waiting for pod downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506 to disappear
Jan 21 10:10:03.051: INFO: Pod downwardapi-volume-7fad6711-bf23-4ffa-8047-ed694be02506 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:10:03.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2036" for this suite.

• [SLOW TEST:6.818 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":3059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:10:03.098: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:10:10.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2994" for this suite.
STEP: Destroying namespace "nsdeletetest-9015" for this suite.
Jan 21 10:10:10.647: INFO: Namespace nsdeletetest-9015 was already deleted
STEP: Destroying namespace "nsdeletetest-5447" for this suite.

• [SLOW TEST:7.567 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":159,"skipped":3099,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:10:10.672: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1897.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1897.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1897.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1897.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 10:10:16.901: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:16.907: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:16.922: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:16.947: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:16.957: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:16.965: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:16.972: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:16.983: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:16.984: INFO: Lookups using dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local]

Jan 21 10:10:21.994: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:22.002: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:22.021: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:22.031: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:22.045: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:22.053: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:22.060: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:22.068: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:22.068: INFO: Lookups using dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local]

Jan 21 10:10:26.994: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:27.006: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:27.012: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:27.018: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:27.025: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:27.033: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:27.040: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:27.047: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:27.048: INFO: Lookups using dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local]

Jan 21 10:10:31.996: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:32.019: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:32.037: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:32.047: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:32.063: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:32.074: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:32.085: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:32.097: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:32.097: INFO: Lookups using dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local]

Jan 21 10:10:36.995: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:37.010: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:37.017: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:37.025: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:37.033: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:37.041: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:37.047: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:37.054: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:37.055: INFO: Lookups using dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local]

Jan 21 10:10:41.992: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:42.000: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:42.013: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:42.021: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:42.037: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:42.066: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:42.078: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:42.087: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local from pod dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d: the server could not find the requested resource (get pods dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d)
Jan 21 10:10:42.088: INFO: Lookups using dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1897.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1897.svc.cluster.local jessie_udp@dns-test-service-2.dns-1897.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1897.svc.cluster.local]

Jan 21 10:10:47.045: INFO: DNS probes using dns-1897/dns-test-712a07b8-c8b5-486f-a4a4-b05fda8a8b1d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:10:47.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1897" for this suite.

• [SLOW TEST:36.730 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":160,"skipped":3102,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:10:47.414: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 21 10:10:47.529: INFO: Waiting up to 5m0s for pod "security-context-21512483-f7e4-4211-8b42-f76d8161af16" in namespace "security-context-2811" to be "Succeeded or Failed"
Jan 21 10:10:47.559: INFO: Pod "security-context-21512483-f7e4-4211-8b42-f76d8161af16": Phase="Pending", Reason="", readiness=false. Elapsed: 17.039941ms
Jan 21 10:10:49.628: INFO: Pod "security-context-21512483-f7e4-4211-8b42-f76d8161af16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088331417s
Jan 21 10:10:51.642: INFO: Pod "security-context-21512483-f7e4-4211-8b42-f76d8161af16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102660078s
Jan 21 10:10:53.655: INFO: Pod "security-context-21512483-f7e4-4211-8b42-f76d8161af16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.115897597s
STEP: Saw pod success
Jan 21 10:10:53.655: INFO: Pod "security-context-21512483-f7e4-4211-8b42-f76d8161af16" satisfied condition "Succeeded or Failed"
Jan 21 10:10:53.665: INFO: Trying to get logs from node conformance1 pod security-context-21512483-f7e4-4211-8b42-f76d8161af16 container test-container: <nil>
STEP: delete the pod
Jan 21 10:10:53.737: INFO: Waiting for pod security-context-21512483-f7e4-4211-8b42-f76d8161af16 to disappear
Jan 21 10:10:53.748: INFO: Pod security-context-21512483-f7e4-4211-8b42-f76d8161af16 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:10:53.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2811" for this suite.

• [SLOW TEST:6.364 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":161,"skipped":3107,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:10:53.780: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jan 21 10:10:53.870: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:11:29.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2304" for this suite.

• [SLOW TEST:36.211 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":162,"skipped":3109,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:11:29.995: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-9478
STEP: creating replication controller nodeport-test in namespace services-9478
I0121 10:11:30.305435      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9478, replica count: 2
I0121 10:11:33.357621      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:11:36.358583      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 10:11:36.358: INFO: Creating new exec pod
Jan 21 10:11:41.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9478 exec execpodnnvlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 21 10:11:41.762: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 21 10:11:41.762: INFO: stdout: "nodeport-test-55wfk"
Jan 21 10:11:41.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9478 exec execpodnnvlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.118.37 80'
Jan 21 10:11:42.111: INFO: stderr: "+ nc -v -t -w 2 10.10.118.37 80\nConnection to 10.10.118.37 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 21 10:11:42.111: INFO: stdout: "nodeport-test-csbcg"
Jan 21 10:11:42.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9478 exec execpodnnvlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.86 32140'
Jan 21 10:11:42.441: INFO: stderr: "+ nc -v -t -w 2 10.10.1.86 32140\nConnection to 10.10.1.86 32140 port [tcp/*] succeeded!\n+ echo hostName\n"
Jan 21 10:11:42.441: INFO: stdout: ""
Jan 21 10:11:43.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9478 exec execpodnnvlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.86 32140'
Jan 21 10:11:43.766: INFO: stderr: "+ + ncecho -v -t hostName -w\n 2 10.10.1.86 32140\nConnection to 10.10.1.86 32140 port [tcp/*] succeeded!\n"
Jan 21 10:11:43.766: INFO: stdout: "nodeport-test-55wfk"
Jan 21 10:11:43.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9478 exec execpodnnvlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.136 32140'
Jan 21 10:11:44.109: INFO: stderr: "+ nc -v -t -w 2 10.10.1.136 32140\n+ echo hostName\nConnection to 10.10.1.136 32140 port [tcp/*] succeeded!\n"
Jan 21 10:11:44.109: INFO: stdout: "nodeport-test-csbcg"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:11:44.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9478" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.144 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":163,"skipped":3122,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:11:44.143: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-fc8d2c7c-dc88-43eb-a86e-1facb3ecc850
Jan 21 10:11:44.265: INFO: Pod name my-hostname-basic-fc8d2c7c-dc88-43eb-a86e-1facb3ecc850: Found 0 pods out of 1
Jan 21 10:11:49.280: INFO: Pod name my-hostname-basic-fc8d2c7c-dc88-43eb-a86e-1facb3ecc850: Found 1 pods out of 1
Jan 21 10:11:49.281: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fc8d2c7c-dc88-43eb-a86e-1facb3ecc850" are running
Jan 21 10:11:49.288: INFO: Pod "my-hostname-basic-fc8d2c7c-dc88-43eb-a86e-1facb3ecc850-9r6fq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-21 10:11:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-21 10:11:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-21 10:11:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-21 10:11:44 +0000 UTC Reason: Message:}])
Jan 21 10:11:49.288: INFO: Trying to dial the pod
Jan 21 10:11:54.331: INFO: Controller my-hostname-basic-fc8d2c7c-dc88-43eb-a86e-1facb3ecc850: Got expected result from replica 1 [my-hostname-basic-fc8d2c7c-dc88-43eb-a86e-1facb3ecc850-9r6fq]: "my-hostname-basic-fc8d2c7c-dc88-43eb-a86e-1facb3ecc850-9r6fq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:11:54.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8681" for this suite.

• [SLOW TEST:10.210 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":164,"skipped":3141,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:11:54.353: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 21 10:11:54.496: INFO: Waiting up to 5m0s for pod "pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d" in namespace "emptydir-4424" to be "Succeeded or Failed"
Jan 21 10:11:54.530: INFO: Pod "pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 34.539942ms
Jan 21 10:11:56.567: INFO: Pod "pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070755921s
Jan 21 10:11:58.716: INFO: Pod "pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219905665s
Jan 21 10:12:00.726: INFO: Pod "pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.230211346s
STEP: Saw pod success
Jan 21 10:12:00.726: INFO: Pod "pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d" satisfied condition "Succeeded or Failed"
Jan 21 10:12:00.737: INFO: Trying to get logs from node conformance1 pod pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d container test-container: <nil>
STEP: delete the pod
Jan 21 10:12:00.797: INFO: Waiting for pod pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d to disappear
Jan 21 10:12:00.828: INFO: Pod pod-8bb6087b-0144-43a8-a8cf-6388b4ee6b7d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:12:00.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4424" for this suite.

• [SLOW TEST:6.519 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":3155,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:12:00.872: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:12:09.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-208" for this suite.

• [SLOW TEST:8.163 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":166,"skipped":3161,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:12:09.039: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 21 10:12:09.180: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 21 10:13:09.244: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:13:09.256: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 21 10:13:13.439: INFO: found a healthy node: conformance1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:13:37.687: INFO: pods created so far: [1 1 1]
Jan 21 10:13:37.687: INFO: length of pods created so far: 3
Jan 21 10:13:41.737: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:13:48.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3794" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:13:48.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1123" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:99.960 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":167,"skipped":3162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:13:49.025: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-7981
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7981 to expose endpoints map[]
Jan 21 10:13:49.170: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 21 10:13:50.184: INFO: successfully validated that service endpoint-test2 in namespace services-7981 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7981
Jan 21 10:13:50.220: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:13:52.238: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:13:54.239: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7981 to expose endpoints map[pod1:[80]]
Jan 21 10:13:54.315: INFO: successfully validated that service endpoint-test2 in namespace services-7981 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jan 21 10:13:54.315: INFO: Creating new exec pod
Jan 21 10:14:03.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7981 exec execpodj6hrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 21 10:14:03.774: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 21 10:14:03.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:14:03.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7981 exec execpodj6hrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.96.119 80'
Jan 21 10:14:04.186: INFO: stderr: "+ nc -v -t -w 2 10.10.96.119 80\nConnection to 10.10.96.119 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 21 10:14:04.186: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-7981
Jan 21 10:14:04.227: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:14:06.239: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:14:08.245: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7981 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 21 10:14:08.297: INFO: successfully validated that service endpoint-test2 in namespace services-7981 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jan 21 10:14:09.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7981 exec execpodj6hrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 21 10:14:09.648: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 21 10:14:09.648: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:14:09.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7981 exec execpodj6hrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.96.119 80'
Jan 21 10:14:10.024: INFO: stderr: "+ nc -v -t -w 2 10.10.96.119 80\nConnection to 10.10.96.119 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 21 10:14:10.024: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7981
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7981 to expose endpoints map[pod2:[80]]
Jan 21 10:14:10.264: INFO: successfully validated that service endpoint-test2 in namespace services-7981 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jan 21 10:14:11.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7981 exec execpodj6hrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 21 10:14:11.717: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 21 10:14:11.718: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:14:11.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7981 exec execpodj6hrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.96.119 80'
Jan 21 10:14:12.159: INFO: stderr: "+ nc -v -t -w 2 10.10.96.119 80\nConnection to 10.10.96.119 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 21 10:14:12.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-7981
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7981 to expose endpoints map[]
Jan 21 10:14:13.258: INFO: successfully validated that service endpoint-test2 in namespace services-7981 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:14:13.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7981" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:24.426 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":168,"skipped":3216,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:14:13.451: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:14:13.552: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:14:20.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5782" for this suite.

• [SLOW TEST:7.376 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":169,"skipped":3227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:14:20.829: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:14:21.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3898" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":170,"skipped":3272,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:14:21.143: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-66ec03fe-05ca-479f-bc38-e0b4978fcc00
STEP: Creating a pod to test consume secrets
Jan 21 10:14:21.548: INFO: Waiting up to 5m0s for pod "pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311" in namespace "secrets-3686" to be "Succeeded or Failed"
Jan 21 10:14:21.576: INFO: Pod "pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311": Phase="Pending", Reason="", readiness=false. Elapsed: 27.983465ms
Jan 21 10:14:23.584: INFO: Pod "pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036729304s
Jan 21 10:14:25.596: INFO: Pod "pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04857811s
Jan 21 10:14:27.614: INFO: Pod "pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066710101s
STEP: Saw pod success
Jan 21 10:14:27.615: INFO: Pod "pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311" satisfied condition "Succeeded or Failed"
Jan 21 10:14:27.625: INFO: Trying to get logs from node conformance1 pod pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311 container secret-env-test: <nil>
STEP: delete the pod
Jan 21 10:14:27.791: INFO: Waiting for pod pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311 to disappear
Jan 21 10:14:27.811: INFO: Pod pod-secrets-8e1d2c98-3304-4b97-9f40-a21d26623311 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:14:27.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3686" for this suite.

• [SLOW TEST:6.717 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3285,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:14:27.865: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:14:39.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4208" for this suite.

• [SLOW TEST:11.288 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":172,"skipped":3285,"failed":0}
SS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:14:39.155: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-b6c0de35-c6fb-4d4e-9fcb-000b26f5e87b
STEP: Creating secret with name secret-projected-all-test-volume-5a609f26-ee42-4a02-ab55-2b798e02ffcd
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 21 10:14:39.302: INFO: Waiting up to 5m0s for pod "projected-volume-b4502ee0-218f-4c8c-8e55-1693ee20f177" in namespace "projected-5838" to be "Succeeded or Failed"
Jan 21 10:14:39.314: INFO: Pod "projected-volume-b4502ee0-218f-4c8c-8e55-1693ee20f177": Phase="Pending", Reason="", readiness=false. Elapsed: 12.768032ms
Jan 21 10:14:41.391: INFO: Pod "projected-volume-b4502ee0-218f-4c8c-8e55-1693ee20f177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089329005s
Jan 21 10:14:43.414: INFO: Pod "projected-volume-b4502ee0-218f-4c8c-8e55-1693ee20f177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.112711816s
STEP: Saw pod success
Jan 21 10:14:43.415: INFO: Pod "projected-volume-b4502ee0-218f-4c8c-8e55-1693ee20f177" satisfied condition "Succeeded or Failed"
Jan 21 10:14:43.420: INFO: Trying to get logs from node conformance1 pod projected-volume-b4502ee0-218f-4c8c-8e55-1693ee20f177 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 21 10:14:43.467: INFO: Waiting for pod projected-volume-b4502ee0-218f-4c8c-8e55-1693ee20f177 to disappear
Jan 21 10:14:43.485: INFO: Pod projected-volume-b4502ee0-218f-4c8c-8e55-1693ee20f177 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:14:43.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5838" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3287,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:14:43.531: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:14:43.697: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 21 10:14:52.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1200 --namespace=crd-publish-openapi-1200 create -f -'
Jan 21 10:14:54.423: INFO: stderr: ""
Jan 21 10:14:54.423: INFO: stdout: "e2e-test-crd-publish-openapi-1278-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 21 10:14:54.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1200 --namespace=crd-publish-openapi-1200 delete e2e-test-crd-publish-openapi-1278-crds test-cr'
Jan 21 10:14:54.560: INFO: stderr: ""
Jan 21 10:14:54.560: INFO: stdout: "e2e-test-crd-publish-openapi-1278-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 21 10:14:54.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1200 --namespace=crd-publish-openapi-1200 apply -f -'
Jan 21 10:14:55.086: INFO: stderr: ""
Jan 21 10:14:55.087: INFO: stdout: "e2e-test-crd-publish-openapi-1278-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 21 10:14:55.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1200 --namespace=crd-publish-openapi-1200 delete e2e-test-crd-publish-openapi-1278-crds test-cr'
Jan 21 10:14:55.272: INFO: stderr: ""
Jan 21 10:14:55.272: INFO: stdout: "e2e-test-crd-publish-openapi-1278-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 21 10:14:55.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1200 explain e2e-test-crd-publish-openapi-1278-crds'
Jan 21 10:14:55.851: INFO: stderr: ""
Jan 21 10:14:55.851: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1278-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:03.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1200" for this suite.

• [SLOW TEST:20.345 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":174,"skipped":3292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:03.880: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 21 10:15:03.993: INFO: Waiting up to 5m0s for pod "downward-api-eef2a4b3-25ed-4cf6-96bf-7990dea3bb21" in namespace "downward-api-4255" to be "Succeeded or Failed"
Jan 21 10:15:04.006: INFO: Pod "downward-api-eef2a4b3-25ed-4cf6-96bf-7990dea3bb21": Phase="Pending", Reason="", readiness=false. Elapsed: 12.479046ms
Jan 21 10:15:06.048: INFO: Pod "downward-api-eef2a4b3-25ed-4cf6-96bf-7990dea3bb21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05424375s
Jan 21 10:15:08.101: INFO: Pod "downward-api-eef2a4b3-25ed-4cf6-96bf-7990dea3bb21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.107618994s
STEP: Saw pod success
Jan 21 10:15:08.101: INFO: Pod "downward-api-eef2a4b3-25ed-4cf6-96bf-7990dea3bb21" satisfied condition "Succeeded or Failed"
Jan 21 10:15:08.107: INFO: Trying to get logs from node conformance1 pod downward-api-eef2a4b3-25ed-4cf6-96bf-7990dea3bb21 container dapi-container: <nil>
STEP: delete the pod
Jan 21 10:15:08.163: INFO: Waiting for pod downward-api-eef2a4b3-25ed-4cf6-96bf-7990dea3bb21 to disappear
Jan 21 10:15:08.171: INFO: Pod downward-api-eef2a4b3-25ed-4cf6-96bf-7990dea3bb21 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:08.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4255" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3353,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:08.254: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:08.421: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-9279
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:12.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7081" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:12.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9279" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":176,"skipped":3370,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:12.966: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-febeb92b-b92a-48ea-940f-e8d6f65d2aba
STEP: Creating secret with name s-test-opt-upd-4f75b1a3-f8da-4c1a-95a6-3d7c83648ffe
STEP: Creating the pod
Jan 21 10:15:13.138: INFO: The status of Pod pod-projected-secrets-b685b3fe-0efc-417b-b48c-40ed9cef186c is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:15:15.150: INFO: The status of Pod pod-projected-secrets-b685b3fe-0efc-417b-b48c-40ed9cef186c is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:15:17.215: INFO: The status of Pod pod-projected-secrets-b685b3fe-0efc-417b-b48c-40ed9cef186c is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:15:19.170: INFO: The status of Pod pod-projected-secrets-b685b3fe-0efc-417b-b48c-40ed9cef186c is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-febeb92b-b92a-48ea-940f-e8d6f65d2aba
STEP: Updating secret s-test-opt-upd-4f75b1a3-f8da-4c1a-95a6-3d7c83648ffe
STEP: Creating secret with name s-test-opt-create-819c7df1-c96b-4bf8-be03-c44acb5bb7c1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:23.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2088" for this suite.

• [SLOW TEST:10.511 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3378,"failed":0}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:23.480: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jan 21 10:15:23.577: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Jan 21 10:15:25.318: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 21 10:15:27.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 15, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 15, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 15, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 15, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:15:29.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 15, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 15, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 15, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 15, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:15:32.993: INFO: Waited 1.249912397s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jan 21 10:15:33.216: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:33.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-590" for this suite.

• [SLOW TEST:10.369 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":178,"skipped":3379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:33.878: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 21 10:15:34.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:15:34.159: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:15:35.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:15:35.196: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:15:36.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:15:36.221: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:15:37.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:15:37.215: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:15:38.178: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:15:38.178: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:15:39.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 10:15:39.192: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status
Jan 21 10:15:39.242: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jan 21 10:15:39.320: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jan 21 10:15:39.346: INFO: Observed &DaemonSet event: ADDED
Jan 21 10:15:39.351: INFO: Observed &DaemonSet event: MODIFIED
Jan 21 10:15:39.359: INFO: Observed &DaemonSet event: MODIFIED
Jan 21 10:15:39.363: INFO: Observed &DaemonSet event: MODIFIED
Jan 21 10:15:39.367: INFO: Found daemon set daemon-set in namespace daemonsets-1069 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 21 10:15:39.373: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jan 21 10:15:39.499: INFO: Observed &DaemonSet event: ADDED
Jan 21 10:15:39.499: INFO: Observed &DaemonSet event: MODIFIED
Jan 21 10:15:39.502: INFO: Observed &DaemonSet event: MODIFIED
Jan 21 10:15:39.511: INFO: Observed &DaemonSet event: MODIFIED
Jan 21 10:15:39.512: INFO: Observed daemon set daemon-set in namespace daemonsets-1069 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 21 10:15:39.515: INFO: Observed &DaemonSet event: MODIFIED
Jan 21 10:15:39.520: INFO: Found daemon set daemon-set in namespace daemonsets-1069 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 21 10:15:39.520: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1069, will wait for the garbage collector to delete the pods
Jan 21 10:15:39.678: INFO: Deleting DaemonSet.extensions daemon-set took: 21.493906ms
Jan 21 10:15:39.779: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.048186ms
Jan 21 10:15:42.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:15:42.997: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 21 10:15:43.005: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21843"},"items":null}

Jan 21 10:15:43.012: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21843"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:43.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1069" for this suite.

• [SLOW TEST:9.179 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":179,"skipped":3406,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:43.057: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-595316f5-f357-42fb-a15a-8d77565fe46f
STEP: Creating a pod to test consume configMaps
Jan 21 10:15:43.197: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1abcbeb-fd66-4b11-85f1-050443477692" in namespace "projected-4821" to be "Succeeded or Failed"
Jan 21 10:15:43.221: INFO: Pod "pod-projected-configmaps-d1abcbeb-fd66-4b11-85f1-050443477692": Phase="Pending", Reason="", readiness=false. Elapsed: 24.102019ms
Jan 21 10:15:45.232: INFO: Pod "pod-projected-configmaps-d1abcbeb-fd66-4b11-85f1-050443477692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035255627s
Jan 21 10:15:47.249: INFO: Pod "pod-projected-configmaps-d1abcbeb-fd66-4b11-85f1-050443477692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051614517s
STEP: Saw pod success
Jan 21 10:15:47.249: INFO: Pod "pod-projected-configmaps-d1abcbeb-fd66-4b11-85f1-050443477692" satisfied condition "Succeeded or Failed"
Jan 21 10:15:47.262: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-d1abcbeb-fd66-4b11-85f1-050443477692 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 10:15:47.308: INFO: Waiting for pod pod-projected-configmaps-d1abcbeb-fd66-4b11-85f1-050443477692 to disappear
Jan 21 10:15:47.324: INFO: Pod pod-projected-configmaps-d1abcbeb-fd66-4b11-85f1-050443477692 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:47.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4821" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3417,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:47.360: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:15:47.500: INFO: Creating deployment "test-recreate-deployment"
Jan 21 10:15:47.510: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 21 10:15:47.531: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 21 10:15:49.576: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 21 10:15:49.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 15, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 15, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d659f7dc9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:15:51.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 15, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 15, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d659f7dc9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:15:53.610: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 21 10:15:53.634: INFO: Updating deployment test-recreate-deployment
Jan 21 10:15:53.634: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 21 10:15:53.981: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1221  06da3336-f6ed-4f15-a9d4-821b075b44c1 21955 2 2022-01-21 10:15:47 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-21 10:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 10:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008c388c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-01-21 10:15:53 +0000 UTC,LastTransitionTime:2022-01-21 10:15:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-01-21 10:15:53 +0000 UTC,LastTransitionTime:2022-01-21 10:15:47 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 21 10:15:53.991: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-1221  16bed18e-0841-4165-ad7e-51fbaebcb880 21953 1 2022-01-21 10:15:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 06da3336-f6ed-4f15-a9d4-821b075b44c1 0xc008c38d57 0xc008c38d58}] []  [{kube-controller-manager Update apps/v1 2022-01-21 10:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06da3336-f6ed-4f15-a9d4-821b075b44c1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 10:15:53 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008c38df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 21 10:15:53.991: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 21 10:15:53.991: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-1221  acfabf58-8c85-4c0a-8edf-fbb406cf131f 21944 2 2022-01-21 10:15:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 06da3336-f6ed-4f15-a9d4-821b075b44c1 0xc008c38e67 0xc008c38e68}] []  [{kube-controller-manager Update apps/v1 2022-01-21 10:15:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06da3336-f6ed-4f15-a9d4-821b075b44c1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 10:15:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008c38f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 21 10:15:53.999: INFO: Pod "test-recreate-deployment-5b99bd5487-dggl6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-dggl6 test-recreate-deployment-5b99bd5487- deployment-1221  4ee2d8fa-f9cc-477a-8fbd-ff3b542e33ac 21956 0 2022-01-21 10:15:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 16bed18e-0841-4165-ad7e-51fbaebcb880 0xc008af46b7 0xc008af46b8}] []  [{Go-http-client Update v1 2022-01-21 10:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-21 10:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16bed18e-0841-4165-ad7e-51fbaebcb880\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldllb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldllb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 10:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 10:15:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 10:15:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 10:15:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2022-01-21 10:15:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:15:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1221" for this suite.

• [SLOW TEST:6.672 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":181,"skipped":3427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:15:54.033: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-eae56a49-69fe-4b7c-94c7-dd20d2fda8a9
STEP: Creating the pod
Jan 21 10:15:54.252: INFO: The status of Pod pod-projected-configmaps-b82645db-d7ad-4f8b-9a31-cbc9ed57539b is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:15:56.314: INFO: The status of Pod pod-projected-configmaps-b82645db-d7ad-4f8b-9a31-cbc9ed57539b is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:15:58.296: INFO: The status of Pod pod-projected-configmaps-b82645db-d7ad-4f8b-9a31-cbc9ed57539b is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:16:00.262: INFO: The status of Pod pod-projected-configmaps-b82645db-d7ad-4f8b-9a31-cbc9ed57539b is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-eae56a49-69fe-4b7c-94c7-dd20d2fda8a9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:17:21.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2652" for this suite.

• [SLOW TEST:87.203 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3500,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:17:21.239: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6677/configmap-test-04f8f3ca-d845-4446-8f68-f8099041790f
STEP: Creating a pod to test consume configMaps
Jan 21 10:17:21.369: INFO: Waiting up to 5m0s for pod "pod-configmaps-946ccf0e-7542-4146-9817-78c0e9d350b8" in namespace "configmap-6677" to be "Succeeded or Failed"
Jan 21 10:17:21.415: INFO: Pod "pod-configmaps-946ccf0e-7542-4146-9817-78c0e9d350b8": Phase="Pending", Reason="", readiness=false. Elapsed: 45.029304ms
Jan 21 10:17:23.490: INFO: Pod "pod-configmaps-946ccf0e-7542-4146-9817-78c0e9d350b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120575928s
Jan 21 10:17:25.511: INFO: Pod "pod-configmaps-946ccf0e-7542-4146-9817-78c0e9d350b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.141672987s
STEP: Saw pod success
Jan 21 10:17:25.512: INFO: Pod "pod-configmaps-946ccf0e-7542-4146-9817-78c0e9d350b8" satisfied condition "Succeeded or Failed"
Jan 21 10:17:25.527: INFO: Trying to get logs from node conformance1 pod pod-configmaps-946ccf0e-7542-4146-9817-78c0e9d350b8 container env-test: <nil>
STEP: delete the pod
Jan 21 10:17:25.599: INFO: Waiting for pod pod-configmaps-946ccf0e-7542-4146-9817-78c0e9d350b8 to disappear
Jan 21 10:17:25.616: INFO: Pod pod-configmaps-946ccf0e-7542-4146-9817-78c0e9d350b8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:17:25.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6677" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:17:25.657: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:17:25.838: INFO: Endpoints addresses: [10.10.1.86] , ports: [6443]
Jan 21 10:17:25.838: INFO: EndpointSlices addresses: [10.10.1.86] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:17:25.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7114" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":184,"skipped":3549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:17:25.894: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-1847/secret-test-bc502abf-51c3-4980-852d-361c7153d88b
STEP: Creating a pod to test consume secrets
Jan 21 10:17:26.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae" in namespace "secrets-1847" to be "Succeeded or Failed"
Jan 21 10:17:26.043: INFO: Pod "pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.4504ms
Jan 21 10:17:28.064: INFO: Pod "pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02742536s
Jan 21 10:17:30.086: INFO: Pod "pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049507144s
Jan 21 10:17:32.141: INFO: Pod "pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.105027215s
STEP: Saw pod success
Jan 21 10:17:32.141: INFO: Pod "pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae" satisfied condition "Succeeded or Failed"
Jan 21 10:17:32.164: INFO: Trying to get logs from node conformance1 pod pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae container env-test: <nil>
STEP: delete the pod
Jan 21 10:17:32.268: INFO: Waiting for pod pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae to disappear
Jan 21 10:17:32.281: INFO: Pod pod-configmaps-de2ccb1f-bec9-44aa-8959-8fa6a9c47aae no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:17:32.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1847" for this suite.

• [SLOW TEST:6.430 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3584,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:17:32.325: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:17:32.541: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 21 10:17:37.574: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 21 10:17:39.604: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 21 10:17:41.620: INFO: Creating deployment "test-rollover-deployment"
Jan 21 10:17:41.647: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 21 10:17:43.668: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 21 10:17:43.688: INFO: Ensure that both replica sets have 1 created replica
Jan 21 10:17:43.702: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 21 10:17:43.775: INFO: Updating deployment test-rollover-deployment
Jan 21 10:17:43.775: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 21 10:17:45.871: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 21 10:17:45.915: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 21 10:17:45.963: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 10:17:45.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:17:47.985: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 10:17:47.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:17:49.989: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 10:17:49.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:17:51.989: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 10:17:51.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:17:53.995: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 10:17:53.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:17:55.989: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 10:17:55.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:17:58.049: INFO: 
Jan 21 10:17:58.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 17, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 17, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:18:00.011: INFO: 
Jan 21 10:18:00.011: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 21 10:18:00.048: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3530  ff53a342-3154-4af4-87d1-2a4c9d7a77fe 22232 2 2022-01-21 10:17:41 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-21 10:17:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 10:17:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039dccc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-21 10:17:41 +0000 UTC,LastTransitionTime:2022-01-21 10:17:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-01-21 10:17:58 +0000 UTC,LastTransitionTime:2022-01-21 10:17:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 21 10:18:00.059: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-3530  127287e3-23a9-4f1a-9354-e5356fc0d265 22222 2 2022-01-21 10:17:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ff53a342-3154-4af4-87d1-2a4c9d7a77fe 0xc0039dd197 0xc0039dd198}] []  [{kube-controller-manager Update apps/v1 2022-01-21 10:17:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff53a342-3154-4af4-87d1-2a4c9d7a77fe\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 10:17:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039dd248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 21 10:18:00.059: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 21 10:18:00.060: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3530  295917f2-a1e8-4d1d-a700-47f9b168d60b 22231 2 2022-01-21 10:17:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ff53a342-3154-4af4-87d1-2a4c9d7a77fe 0xc0039dd067 0xc0039dd068}] []  [{e2e.test Update apps/v1 2022-01-21 10:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 10:17:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff53a342-3154-4af4-87d1-2a4c9d7a77fe\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-01-21 10:17:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0039dd128 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 21 10:18:00.060: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-3530  065a8e0f-4e59-42c3-a33e-5ac712ccd92a 22192 2 2022-01-21 10:17:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ff53a342-3154-4af4-87d1-2a4c9d7a77fe 0xc0039dd2b7 0xc0039dd2b8}] []  [{kube-controller-manager Update apps/v1 2022-01-21 10:17:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff53a342-3154-4af4-87d1-2a4c9d7a77fe\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-21 10:17:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039dd368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 21 10:18:00.069: INFO: Pod "test-rollover-deployment-668b7f667d-sx5p4" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-sx5p4 test-rollover-deployment-668b7f667d- deployment-3530  9815fa7d-a464-41ef-989c-6b86e77ebc8d 22216 0 2022-01-21 10:17:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[cni.projectcalico.org/containerID:61e941f26a94b2e91abb07be338c888e78eaa228d916b548d4cb9a8f29a5c8da cni.projectcalico.org/podIP:172.16.209.85/32 cni.projectcalico.org/podIPs:172.16.209.85/32] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d 127287e3-23a9-4f1a-9354-e5356fc0d265 0xc0039dd8f7 0xc0039dd8f8}] []  [{kube-controller-manager Update v1 2022-01-21 10:17:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"127287e3-23a9-4f1a-9354-e5356fc0d265\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-21 10:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-21 10:17:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxwvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxwvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 10:17:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 10:17:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 10:17:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-21 10:17:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:172.16.209.85,StartTime:2022-01-21 10:17:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-21 10:17:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:docker://2edd4a602576c468b0ce7a79348cba44e37debca5c413dcf5a4c8c87589af690,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.209.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:18:00.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3530" for this suite.

• [SLOW TEST:27.775 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":186,"skipped":3602,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:18:00.119: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 21 10:18:00.233: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:18:10.083: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:18:39.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1892" for this suite.

• [SLOW TEST:38.942 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":187,"skipped":3614,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:18:39.066: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:18:43.226: INFO: Deleting pod "var-expansion-581375a0-b6bc-421c-920b-ccffc29d3b35" in namespace "var-expansion-721"
Jan 21 10:18:43.255: INFO: Wait up to 5m0s for pod "var-expansion-581375a0-b6bc-421c-920b-ccffc29d3b35" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:18:47.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-721" for this suite.

• [SLOW TEST:8.238 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":188,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:18:47.309: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 10:18:49.381: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 10:18:51.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 18, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 18, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 18, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 18, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 10:18:54.481: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:18:54.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2659" for this suite.
STEP: Destroying namespace "webhook-2659-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.516 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":189,"skipped":3651,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:18:54.825: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-vvjt
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 10:18:55.030: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vvjt" in namespace "subpath-9323" to be "Succeeded or Failed"
Jan 21 10:18:55.075: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Pending", Reason="", readiness=false. Elapsed: 44.595674ms
Jan 21 10:18:57.125: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094345471s
Jan 21 10:18:59.141: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.110102276s
Jan 21 10:19:01.158: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 6.126922999s
Jan 21 10:19:03.185: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 8.153998575s
Jan 21 10:19:05.194: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 10.163255511s
Jan 21 10:19:07.211: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 12.180246741s
Jan 21 10:19:09.227: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 14.196678385s
Jan 21 10:19:11.239: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 16.208253938s
Jan 21 10:19:13.257: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 18.226334815s
Jan 21 10:19:15.265: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 20.234311588s
Jan 21 10:19:17.284: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Running", Reason="", readiness=true. Elapsed: 22.253198539s
Jan 21 10:19:19.321: INFO: Pod "pod-subpath-test-configmap-vvjt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.290524697s
STEP: Saw pod success
Jan 21 10:19:19.321: INFO: Pod "pod-subpath-test-configmap-vvjt" satisfied condition "Succeeded or Failed"
Jan 21 10:19:19.335: INFO: Trying to get logs from node conformance1 pod pod-subpath-test-configmap-vvjt container test-container-subpath-configmap-vvjt: <nil>
STEP: delete the pod
Jan 21 10:19:19.476: INFO: Waiting for pod pod-subpath-test-configmap-vvjt to disappear
Jan 21 10:19:19.486: INFO: Pod pod-subpath-test-configmap-vvjt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vvjt
Jan 21 10:19:19.486: INFO: Deleting pod "pod-subpath-test-configmap-vvjt" in namespace "subpath-9323"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:19:19.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9323" for this suite.

• [SLOW TEST:24.742 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":190,"skipped":3661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:19:19.571: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5362
Jan 21 10:19:19.830: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:19:21.841: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:19:23.837: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 21 10:19:23.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5362 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 21 10:19:24.222: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 21 10:19:24.222: INFO: stdout: "iptables"
Jan 21 10:19:24.222: INFO: proxyMode: iptables
Jan 21 10:19:24.279: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 21 10:19:24.294: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5362
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5362
I0121 10:19:24.365113      18 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5362, replica count: 3
I0121 10:19:27.417106      18 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:19:30.418131      18 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:19:33.419251      18 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 10:19:33.447: INFO: Creating new exec pod
Jan 21 10:19:38.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5362 exec execpod-affinitywjfz8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 21 10:19:39.201: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 21 10:19:39.201: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:19:39.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5362 exec execpod-affinitywjfz8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.37.227 80'
Jan 21 10:19:39.554: INFO: stderr: "+ nc -v -t -w 2 10.10.37.227 80\nConnection to 10.10.37.227 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 21 10:19:39.554: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:19:39.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5362 exec execpod-affinitywjfz8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.37.227:80/ ; done'
Jan 21 10:19:40.215: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n"
Jan 21 10:19:40.215: INFO: stdout: "\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks\naffinity-clusterip-timeout-2xvks"
Jan 21 10:19:40.215: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.215: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.215: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.215: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.215: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.215: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.215: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Received response from host: affinity-clusterip-timeout-2xvks
Jan 21 10:19:40.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5362 exec execpod-affinitywjfz8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.37.227:80/'
Jan 21 10:19:40.809: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n"
Jan 21 10:19:40.809: INFO: stdout: "affinity-clusterip-timeout-2xvks"
Jan 21 10:20:00.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-5362 exec execpod-affinitywjfz8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.37.227:80/'
Jan 21 10:20:01.272: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.37.227:80/\n"
Jan 21 10:20:01.272: INFO: stdout: "affinity-clusterip-timeout-j42r9"
Jan 21 10:20:01.272: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5362, will wait for the garbage collector to delete the pods
Jan 21 10:20:01.612: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 47.27359ms
Jan 21 10:20:02.015: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 402.731511ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:20:06.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5362" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:46.600 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":191,"skipped":3684,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:20:06.172: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-4005be2f-5121-4249-ae60-00564458a89c
STEP: Creating a pod to test consume configMaps
Jan 21 10:20:06.408: INFO: Waiting up to 5m0s for pod "pod-configmaps-786cc4e2-f149-4e70-9fb5-4086f32c1b45" in namespace "configmap-6272" to be "Succeeded or Failed"
Jan 21 10:20:06.449: INFO: Pod "pod-configmaps-786cc4e2-f149-4e70-9fb5-4086f32c1b45": Phase="Pending", Reason="", readiness=false. Elapsed: 40.773922ms
Jan 21 10:20:08.544: INFO: Pod "pod-configmaps-786cc4e2-f149-4e70-9fb5-4086f32c1b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136060348s
Jan 21 10:20:10.565: INFO: Pod "pod-configmaps-786cc4e2-f149-4e70-9fb5-4086f32c1b45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.157060896s
STEP: Saw pod success
Jan 21 10:20:10.565: INFO: Pod "pod-configmaps-786cc4e2-f149-4e70-9fb5-4086f32c1b45" satisfied condition "Succeeded or Failed"
Jan 21 10:20:10.591: INFO: Trying to get logs from node conformance1 pod pod-configmaps-786cc4e2-f149-4e70-9fb5-4086f32c1b45 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 10:20:10.644: INFO: Waiting for pod pod-configmaps-786cc4e2-f149-4e70-9fb5-4086f32c1b45 to disappear
Jan 21 10:20:10.658: INFO: Pod pod-configmaps-786cc4e2-f149-4e70-9fb5-4086f32c1b45 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:20:10.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6272" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:20:10.689: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 21 10:20:11.187: INFO: running pods: 0 < 3
Jan 21 10:20:13.280: INFO: running pods: 0 < 3
Jan 21 10:20:15.199: INFO: running pods: 0 < 3
Jan 21 10:20:17.203: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:20:19.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2149" for this suite.

• [SLOW TEST:8.550 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":193,"skipped":3722,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:20:19.240: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 21 10:20:19.372: INFO: Waiting up to 5m0s for pod "pod-3f549762-a0de-4764-91f7-903fb908f658" in namespace "emptydir-6342" to be "Succeeded or Failed"
Jan 21 10:20:19.386: INFO: Pod "pod-3f549762-a0de-4764-91f7-903fb908f658": Phase="Pending", Reason="", readiness=false. Elapsed: 13.804641ms
Jan 21 10:20:21.412: INFO: Pod "pod-3f549762-a0de-4764-91f7-903fb908f658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039662286s
Jan 21 10:20:23.422: INFO: Pod "pod-3f549762-a0de-4764-91f7-903fb908f658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049667385s
STEP: Saw pod success
Jan 21 10:20:23.422: INFO: Pod "pod-3f549762-a0de-4764-91f7-903fb908f658" satisfied condition "Succeeded or Failed"
Jan 21 10:20:23.428: INFO: Trying to get logs from node conformance1 pod pod-3f549762-a0de-4764-91f7-903fb908f658 container test-container: <nil>
STEP: delete the pod
Jan 21 10:20:23.487: INFO: Waiting for pod pod-3f549762-a0de-4764-91f7-903fb908f658 to disappear
Jan 21 10:20:23.497: INFO: Pod pod-3f549762-a0de-4764-91f7-903fb908f658 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:20:23.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6342" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3722,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:20:23.528: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 10:20:27.440: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 10:20:29.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:20:31.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:20:33.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 20, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 10:20:36.606: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:20:37.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9722" for this suite.
STEP: Destroying namespace "webhook-9722-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.901 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":195,"skipped":3732,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:20:37.434: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 21 10:20:37.715: INFO: Waiting up to 5m0s for pod "downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d" in namespace "downward-api-6882" to be "Succeeded or Failed"
Jan 21 10:20:37.812: INFO: Pod "downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d": Phase="Pending", Reason="", readiness=false. Elapsed: 96.107461ms
Jan 21 10:20:39.823: INFO: Pod "downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107494904s
Jan 21 10:20:41.838: INFO: Pod "downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123012997s
Jan 21 10:20:43.854: INFO: Pod "downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.13810776s
STEP: Saw pod success
Jan 21 10:20:43.854: INFO: Pod "downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d" satisfied condition "Succeeded or Failed"
Jan 21 10:20:43.862: INFO: Trying to get logs from node conformance1 pod downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d container dapi-container: <nil>
STEP: delete the pod
Jan 21 10:20:44.070: INFO: Waiting for pod downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d to disappear
Jan 21 10:20:44.097: INFO: Pod downward-api-e4a2fee4-27ed-42b7-bcde-d91f8e1ab59d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:20:44.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6882" for this suite.

• [SLOW TEST:6.694 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3757,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:20:44.128: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2806.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2806.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2806.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2806.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 10:20:50.443: INFO: DNS probes using dns-2806/dns-test-1d85878f-1258-436f-a85b-409339244c71 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:20:50.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2806" for this suite.

• [SLOW TEST:6.556 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":197,"skipped":3777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:20:50.717: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 21 10:20:50.807: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 21 10:21:19.068: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:21:25.858: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:21:55.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1570" for this suite.

• [SLOW TEST:64.350 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":198,"skipped":3817,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:21:55.067: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 21 10:21:59.327: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:21:59.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4843" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:21:59.424: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-d11622b8-e770-4958-af43-8bec61115e89
STEP: Creating a pod to test consume configMaps
Jan 21 10:21:59.589: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5" in namespace "projected-113" to be "Succeeded or Failed"
Jan 21 10:21:59.634: INFO: Pod "pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.720218ms
Jan 21 10:22:01.645: INFO: Pod "pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055487083s
Jan 21 10:22:03.668: INFO: Pod "pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078292239s
Jan 21 10:22:05.678: INFO: Pod "pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088259005s
STEP: Saw pod success
Jan 21 10:22:05.678: INFO: Pod "pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5" satisfied condition "Succeeded or Failed"
Jan 21 10:22:05.685: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 10:22:05.732: INFO: Waiting for pod pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5 to disappear
Jan 21 10:22:05.750: INFO: Pod pod-projected-configmaps-90397feb-e8c2-45f4-8b86-32f03cdb77a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:22:05.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-113" for this suite.

• [SLOW TEST:6.369 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:22:05.802: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-7b535bcf-bd5a-4e44-823d-06bcf62832d8
STEP: Creating a pod to test consume secrets
Jan 21 10:22:05.951: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b" in namespace "projected-648" to be "Succeeded or Failed"
Jan 21 10:22:05.999: INFO: Pod "pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b": Phase="Pending", Reason="", readiness=false. Elapsed: 48.281123ms
Jan 21 10:22:08.038: INFO: Pod "pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086466294s
Jan 21 10:22:10.061: INFO: Pod "pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109912645s
Jan 21 10:22:12.086: INFO: Pod "pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.134684877s
STEP: Saw pod success
Jan 21 10:22:12.087: INFO: Pod "pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b" satisfied condition "Succeeded or Failed"
Jan 21 10:22:12.100: INFO: Trying to get logs from node conformance1 pod pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 10:22:12.161: INFO: Waiting for pod pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b to disappear
Jan 21 10:22:12.177: INFO: Pod pod-projected-secrets-782b7efb-56f2-426a-898d-7106eca9d59b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:22:12.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-648" for this suite.

• [SLOW TEST:6.426 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:22:12.230: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:22:12.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9380" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":202,"skipped":3955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:22:12.583: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:22:12.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9271" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":203,"skipped":4009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:22:12.845: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-db9d228a-cbc4-441b-aca8-dc1f1f977872
STEP: Creating a pod to test consume secrets
Jan 21 10:22:13.002: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f02730c8-19c5-42da-933b-14b09fea57a9" in namespace "projected-988" to be "Succeeded or Failed"
Jan 21 10:22:13.020: INFO: Pod "pod-projected-secrets-f02730c8-19c5-42da-933b-14b09fea57a9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.38829ms
Jan 21 10:22:15.059: INFO: Pod "pod-projected-secrets-f02730c8-19c5-42da-933b-14b09fea57a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053260058s
Jan 21 10:22:17.082: INFO: Pod "pod-projected-secrets-f02730c8-19c5-42da-933b-14b09fea57a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076081322s
STEP: Saw pod success
Jan 21 10:22:17.082: INFO: Pod "pod-projected-secrets-f02730c8-19c5-42da-933b-14b09fea57a9" satisfied condition "Succeeded or Failed"
Jan 21 10:22:17.090: INFO: Trying to get logs from node conformance1 pod pod-projected-secrets-f02730c8-19c5-42da-933b-14b09fea57a9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 10:22:17.145: INFO: Waiting for pod pod-projected-secrets-f02730c8-19c5-42da-933b-14b09fea57a9 to disappear
Jan 21 10:22:17.158: INFO: Pod pod-projected-secrets-f02730c8-19c5-42da-933b-14b09fea57a9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:22:17.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-988" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":4032,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:22:17.201: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 21 10:22:23.523: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1233 PodName:var-expansion-98f20532-cff6-4eb1-bd37-f1e7cb3e39b1 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 10:22:23.524: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:22:23.525: INFO: ExecWithOptions: Clientset creation
Jan 21 10:22:23.526: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/var-expansion-1233/pods/var-expansion-98f20532-cff6-4eb1-bd37-f1e7cb3e39b1/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Jan 21 10:22:23.774: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1233 PodName:var-expansion-98f20532-cff6-4eb1-bd37-f1e7cb3e39b1 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 10:22:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:22:23.783: INFO: ExecWithOptions: Clientset creation
Jan 21 10:22:23.783: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/var-expansion-1233/pods/var-expansion-98f20532-cff6-4eb1-bd37-f1e7cb3e39b1/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Jan 21 10:22:24.519: INFO: Successfully updated pod "var-expansion-98f20532-cff6-4eb1-bd37-f1e7cb3e39b1"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 21 10:22:24.526: INFO: Deleting pod "var-expansion-98f20532-cff6-4eb1-bd37-f1e7cb3e39b1" in namespace "var-expansion-1233"
Jan 21 10:22:24.542: INFO: Wait up to 5m0s for pod "var-expansion-98f20532-cff6-4eb1-bd37-f1e7cb3e39b1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:22:58.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1233" for this suite.

• [SLOW TEST:41.423 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":205,"skipped":4034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:22:58.629: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jan 21 10:22:58.853: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 21 10:23:58.911: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:23:58.921: INFO: Starting informer...
STEP: Starting pods...
Jan 21 10:23:59.172: INFO: Pod1 is running on conformance1. Tainting Node
Jan 21 10:24:05.430: INFO: Pod2 is running on conformance1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 21 10:24:12.416: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 21 10:24:32.535: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:24:32.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8930" for this suite.

• [SLOW TEST:94.011 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":206,"skipped":4094,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:24:32.643: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4788
STEP: creating service affinity-clusterip-transition in namespace services-4788
STEP: creating replication controller affinity-clusterip-transition in namespace services-4788
I0121 10:24:32.866114      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4788, replica count: 3
I0121 10:24:35.917935      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:24:38.918754      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:24:41.919482      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 10:24:41.932: INFO: Creating new exec pod
Jan 21 10:24:46.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-4788 exec execpod-affinityz7b9n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 21 10:24:47.410: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 21 10:24:47.410: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:24:47.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-4788 exec execpod-affinityz7b9n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.170.16 80'
Jan 21 10:24:47.912: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.170.16 80\nConnection to 10.10.170.16 80 port [tcp/http] succeeded!\n"
Jan 21 10:24:47.912: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:24:47.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-4788 exec execpod-affinityz7b9n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.170.16:80/ ; done'
Jan 21 10:24:49.126: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n"
Jan 21 10:24:49.126: INFO: stdout: "\naffinity-clusterip-transition-85782\naffinity-clusterip-transition-cxsmx\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-cxsmx\naffinity-clusterip-transition-85782\naffinity-clusterip-transition-85782\naffinity-clusterip-transition-cxsmx\naffinity-clusterip-transition-85782\naffinity-clusterip-transition-cxsmx\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-cxsmx\naffinity-clusterip-transition-85782\naffinity-clusterip-transition-cxsmx\naffinity-clusterip-transition-cxsmx\naffinity-clusterip-transition-cxsmx\naffinity-clusterip-transition-4rrwz"
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-85782
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-cxsmx
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-cxsmx
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-85782
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-85782
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-cxsmx
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-85782
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-cxsmx
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-cxsmx
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-85782
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-cxsmx
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-cxsmx
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-cxsmx
Jan 21 10:24:49.126: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-4788 exec execpod-affinityz7b9n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.170.16:80/ ; done'
Jan 21 10:24:49.851: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.170.16:80/\n"
Jan 21 10:24:49.851: INFO: stdout: "\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz\naffinity-clusterip-transition-4rrwz"
Jan 21 10:24:49.851: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.851: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.851: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.851: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.851: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.851: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.851: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Received response from host: affinity-clusterip-transition-4rrwz
Jan 21 10:24:49.852: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4788, will wait for the garbage collector to delete the pods
Jan 21 10:24:49.996: INFO: Deleting ReplicationController affinity-clusterip-transition took: 20.251676ms
Jan 21 10:24:50.397: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 400.39308ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:24:54.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4788" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:21.961 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":207,"skipped":4098,"failed":0}
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:24:54.605: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:24:54.744: INFO: The status of Pod server-envvars-71d40f55-2b7a-437e-91f3-58775ba87043 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:24:56.761: INFO: The status of Pod server-envvars-71d40f55-2b7a-437e-91f3-58775ba87043 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:24:58.769: INFO: The status of Pod server-envvars-71d40f55-2b7a-437e-91f3-58775ba87043 is Running (Ready = true)
Jan 21 10:24:58.853: INFO: Waiting up to 5m0s for pod "client-envvars-97d3f71f-a55f-4fff-91a7-66727fd969e2" in namespace "pods-3323" to be "Succeeded or Failed"
Jan 21 10:24:58.888: INFO: Pod "client-envvars-97d3f71f-a55f-4fff-91a7-66727fd969e2": Phase="Pending", Reason="", readiness=false. Elapsed: 35.03597ms
Jan 21 10:25:00.911: INFO: Pod "client-envvars-97d3f71f-a55f-4fff-91a7-66727fd969e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057400365s
Jan 21 10:25:02.929: INFO: Pod "client-envvars-97d3f71f-a55f-4fff-91a7-66727fd969e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076104179s
STEP: Saw pod success
Jan 21 10:25:02.930: INFO: Pod "client-envvars-97d3f71f-a55f-4fff-91a7-66727fd969e2" satisfied condition "Succeeded or Failed"
Jan 21 10:25:02.948: INFO: Trying to get logs from node conformance1 pod client-envvars-97d3f71f-a55f-4fff-91a7-66727fd969e2 container env3cont: <nil>
STEP: delete the pod
Jan 21 10:25:03.030: INFO: Waiting for pod client-envvars-97d3f71f-a55f-4fff-91a7-66727fd969e2 to disappear
Jan 21 10:25:03.038: INFO: Pod client-envvars-97d3f71f-a55f-4fff-91a7-66727fd969e2 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:25:03.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3323" for this suite.

• [SLOW TEST:8.452 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":208,"skipped":4098,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:25:03.059: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 21 10:25:03.236: INFO: Waiting up to 5m0s for pod "pod-599de675-3044-481e-9ba6-5c2c197d18bd" in namespace "emptydir-3492" to be "Succeeded or Failed"
Jan 21 10:25:03.304: INFO: Pod "pod-599de675-3044-481e-9ba6-5c2c197d18bd": Phase="Pending", Reason="", readiness=false. Elapsed: 68.900912ms
Jan 21 10:25:05.314: INFO: Pod "pod-599de675-3044-481e-9ba6-5c2c197d18bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078598724s
Jan 21 10:25:07.325: INFO: Pod "pod-599de675-3044-481e-9ba6-5c2c197d18bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089216937s
STEP: Saw pod success
Jan 21 10:25:07.325: INFO: Pod "pod-599de675-3044-481e-9ba6-5c2c197d18bd" satisfied condition "Succeeded or Failed"
Jan 21 10:25:07.335: INFO: Trying to get logs from node conformance1 pod pod-599de675-3044-481e-9ba6-5c2c197d18bd container test-container: <nil>
STEP: delete the pod
Jan 21 10:25:07.389: INFO: Waiting for pod pod-599de675-3044-481e-9ba6-5c2c197d18bd to disappear
Jan 21 10:25:07.397: INFO: Pod pod-599de675-3044-481e-9ba6-5c2c197d18bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:25:07.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3492" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":4104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:25:07.429: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 10:25:10.140: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 10:25:12.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 25, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 25, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 25, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 25, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:25:14.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 25, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 25, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 25, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 25, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 10:25:17.216: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:25:18.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8457" for this suite.
STEP: Destroying namespace "webhook-8457-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.018 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":210,"skipped":4156,"failed":0}
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:25:18.448: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 21 10:25:19.492: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:25:19.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2423" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":211,"skipped":4156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:25:19.700: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0121 10:25:20.849953      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 21 10:25:20.850: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:25:20.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6779" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":212,"skipped":4183,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:25:20.896: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5659
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 21 10:25:21.021: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 21 10:25:21.117: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:25:23.137: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:25:25.140: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:25:27.131: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 10:25:29.132: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 10:25:31.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 10:25:33.133: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 10:25:35.124: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 10:25:37.134: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 10:25:39.164: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 10:25:41.181: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 21 10:25:43.135: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 21 10:25:43.148: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 21 10:25:47.195: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 21 10:25:47.195: INFO: Breadth first check of 172.16.209.113 on host 10.10.1.86...
Jan 21 10:25:47.200: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.209.111:9080/dial?request=hostname&protocol=udp&host=172.16.209.113&port=8081&tries=1'] Namespace:pod-network-test-5659 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 10:25:47.200: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:25:47.201: INFO: ExecWithOptions: Clientset creation
Jan 21 10:25:47.201: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/pod-network-test-5659/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.209.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.209.113%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 21 10:25:47.499: INFO: Waiting for responses: map[]
Jan 21 10:25:47.499: INFO: reached 172.16.209.113 after 0/1 tries
Jan 21 10:25:47.499: INFO: Breadth first check of 172.16.106.148 on host 10.10.1.136...
Jan 21 10:25:47.507: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.209.111:9080/dial?request=hostname&protocol=udp&host=172.16.106.148&port=8081&tries=1'] Namespace:pod-network-test-5659 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 10:25:47.507: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 10:25:47.508: INFO: ExecWithOptions: Clientset creation
Jan 21 10:25:47.508: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/pod-network-test-5659/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.209.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.106.148%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 21 10:25:47.737: INFO: Waiting for responses: map[]
Jan 21 10:25:47.738: INFO: reached 172.16.106.148 after 0/1 tries
Jan 21 10:25:47.738: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:25:47.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5659" for this suite.

• [SLOW TEST:26.888 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":4187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:25:47.785: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 21 10:25:53.002: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:25:53.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8139" for this suite.

• [SLOW TEST:5.337 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":4225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:25:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:25:53.319: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 21 10:25:53.435: INFO: The status of Pod pod-exec-websocket-9a81e2d8-24e6-40ac-8b84-5819282f60b9 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:25:55.459: INFO: The status of Pod pod-exec-websocket-9a81e2d8-24e6-40ac-8b84-5819282f60b9 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:25:57.448: INFO: The status of Pod pod-exec-websocket-9a81e2d8-24e6-40ac-8b84-5819282f60b9 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:25:59.470: INFO: The status of Pod pod-exec-websocket-9a81e2d8-24e6-40ac-8b84-5819282f60b9 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:25:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5140" for this suite.

• [SLOW TEST:6.837 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":215,"skipped":4255,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:25:59.981: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 10:26:01.960: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 10:26:04.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 26, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:26:06.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 26, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 10:26:09.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:26:09.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2294" for this suite.
STEP: Destroying namespace "webhook-2294-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.614 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":216,"skipped":4272,"failed":0}
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:26:09.594: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:26:09.925: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349" in namespace "downward-api-5928" to be "Succeeded or Failed"
Jan 21 10:26:09.945: INFO: Pod "downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349": Phase="Pending", Reason="", readiness=false. Elapsed: 19.391111ms
Jan 21 10:26:11.987: INFO: Pod "downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061309861s
Jan 21 10:26:13.999: INFO: Pod "downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073906921s
Jan 21 10:26:16.026: INFO: Pod "downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.100413826s
STEP: Saw pod success
Jan 21 10:26:16.026: INFO: Pod "downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349" satisfied condition "Succeeded or Failed"
Jan 21 10:26:16.046: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349 container client-container: <nil>
STEP: delete the pod
Jan 21 10:26:16.115: INFO: Waiting for pod downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349 to disappear
Jan 21 10:26:16.125: INFO: Pod downwardapi-volume-6b86d565-f3f6-496f-9fa4-c2e9451d1349 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:26:16.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5928" for this suite.

• [SLOW TEST:6.557 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":4272,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:26:16.151: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:26:32.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9339" for this suite.
STEP: Destroying namespace "nsdeletetest-5042" for this suite.
Jan 21 10:26:32.582: INFO: Namespace nsdeletetest-5042 was already deleted
STEP: Destroying namespace "nsdeletetest-265" for this suite.

• [SLOW TEST:16.449 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":218,"skipped":4273,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:26:32.602: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:26:32.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3391 version'
Jan 21 10:26:32.850: INFO: stderr: ""
Jan 21 10:26:32.850: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:41:01Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:34:54Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:26:32.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3391" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":219,"skipped":4281,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:26:32.873: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6150
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6150
I0121 10:26:33.151832      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6150, replica count: 2
I0121 10:26:36.204126      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:26:39.207775      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 10:26:39.208: INFO: Creating new exec pod
Jan 21 10:26:44.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6150 exec execpod5xs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 21 10:26:45.040: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 21 10:26:45.040: INFO: stdout: "externalname-service-8l4dj"
Jan 21 10:26:45.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6150 exec execpod5xs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.164.225 80'
Jan 21 10:26:45.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.164.225 80\nConnection to 10.10.164.225 80 port [tcp/http] succeeded!\n"
Jan 21 10:26:45.544: INFO: stdout: "externalname-service-b54g9"
Jan 21 10:26:45.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6150 exec execpod5xs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.86 30669'
Jan 21 10:26:46.094: INFO: stderr: "+ nc -v -t -w 2 10.10.1.86 30669\n+ echo hostName\nConnection to 10.10.1.86 30669 port [tcp/*] succeeded!\n"
Jan 21 10:26:46.094: INFO: stdout: "externalname-service-8l4dj"
Jan 21 10:26:46.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6150 exec execpod5xs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.136 30669'
Jan 21 10:26:46.458: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.1.136 30669\nConnection to 10.10.1.136 30669 port [tcp/*] succeeded!\n"
Jan 21 10:26:46.458: INFO: stdout: ""
Jan 21 10:26:47.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6150 exec execpod5xs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.136 30669'
Jan 21 10:26:47.806: INFO: stderr: "+ nc -v -t -w 2 10.10.1.136 30669\nConnection to 10.10.1.136 30669 port [tcp/*] succeeded!\n+ echo hostName\n"
Jan 21 10:26:47.807: INFO: stdout: "externalname-service-8l4dj"
Jan 21 10:26:47.807: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:26:47.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6150" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:15.186 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":220,"skipped":4301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:26:48.061: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:26:48.296: INFO: Waiting up to 5m0s for pod "downwardapi-volume-581fe33f-6835-45af-bb37-11a183a752ee" in namespace "downward-api-8779" to be "Succeeded or Failed"
Jan 21 10:26:48.361: INFO: Pod "downwardapi-volume-581fe33f-6835-45af-bb37-11a183a752ee": Phase="Pending", Reason="", readiness=false. Elapsed: 65.62889ms
Jan 21 10:26:50.372: INFO: Pod "downwardapi-volume-581fe33f-6835-45af-bb37-11a183a752ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076048391s
Jan 21 10:26:52.392: INFO: Pod "downwardapi-volume-581fe33f-6835-45af-bb37-11a183a752ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096366191s
STEP: Saw pod success
Jan 21 10:26:52.393: INFO: Pod "downwardapi-volume-581fe33f-6835-45af-bb37-11a183a752ee" satisfied condition "Succeeded or Failed"
Jan 21 10:26:52.399: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-581fe33f-6835-45af-bb37-11a183a752ee container client-container: <nil>
STEP: delete the pod
Jan 21 10:26:52.488: INFO: Waiting for pod downwardapi-volume-581fe33f-6835-45af-bb37-11a183a752ee to disappear
Jan 21 10:26:52.496: INFO: Pod downwardapi-volume-581fe33f-6835-45af-bb37-11a183a752ee no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:26:52.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8779" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":4369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:26:52.540: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jan 21 10:27:23.096: INFO: 88 pods remaining
Jan 21 10:27:23.096: INFO: 84 pods has nil DeletionTimestamp
Jan 21 10:27:23.096: INFO: 
Jan 21 10:27:28.740: INFO: 71 pods remaining
Jan 21 10:27:28.740: INFO: 68 pods has nil DeletionTimestamp
Jan 21 10:27:28.740: INFO: 
Jan 21 10:27:32.523: INFO: 50 pods remaining
Jan 21 10:27:32.523: INFO: 50 pods has nil DeletionTimestamp
Jan 21 10:27:32.523: INFO: 
STEP: Gathering metrics
W0121 10:27:37.952294      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 21 10:27:37.952: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 21 10:27:37.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-268vr" in namespace "gc-1132"
Jan 21 10:27:38.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-29bfg" in namespace "gc-1132"
Jan 21 10:27:38.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bd45" in namespace "gc-1132"
Jan 21 10:27:38.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mcn9" in namespace "gc-1132"
Jan 21 10:27:39.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vfxs" in namespace "gc-1132"
Jan 21 10:27:39.451: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wxbg" in namespace "gc-1132"
Jan 21 10:27:39.923: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f9zt" in namespace "gc-1132"
Jan 21 10:27:40.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fx6f" in namespace "gc-1132"
Jan 21 10:27:40.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j7qw" in namespace "gc-1132"
Jan 21 10:27:40.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-586kr" in namespace "gc-1132"
Jan 21 10:27:41.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bmvj" in namespace "gc-1132"
Jan 21 10:27:41.447: INFO: Deleting pod "simpletest-rc-to-be-deleted-5q8p8" in namespace "gc-1132"
Jan 21 10:27:41.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qmfm" in namespace "gc-1132"
Jan 21 10:27:42.003: INFO: Deleting pod "simpletest-rc-to-be-deleted-62c4q" in namespace "gc-1132"
Jan 21 10:27:42.280: INFO: Deleting pod "simpletest-rc-to-be-deleted-6clnm" in namespace "gc-1132"
Jan 21 10:27:42.386: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rzw4" in namespace "gc-1132"
Jan 21 10:27:42.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hgpz" in namespace "gc-1132"
Jan 21 10:27:43.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-847ss" in namespace "gc-1132"
Jan 21 10:27:43.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-886wk" in namespace "gc-1132"
Jan 21 10:27:44.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mm9s" in namespace "gc-1132"
Jan 21 10:27:44.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bbgb" in namespace "gc-1132"
Jan 21 10:27:45.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-9z4xj" in namespace "gc-1132"
Jan 21 10:27:45.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc6qv" in namespace "gc-1132"
Jan 21 10:27:45.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9h7h" in namespace "gc-1132"
Jan 21 10:27:45.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-clj88" in namespace "gc-1132"
Jan 21 10:27:46.077: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8vzz" in namespace "gc-1132"
Jan 21 10:27:46.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-dd6vw" in namespace "gc-1132"
Jan 21 10:27:46.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-djgjk" in namespace "gc-1132"
Jan 21 10:27:47.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-djj58" in namespace "gc-1132"
Jan 21 10:27:47.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-dng9d" in namespace "gc-1132"
Jan 21 10:27:48.723: INFO: Deleting pod "simpletest-rc-to-be-deleted-fcvlz" in namespace "gc-1132"
Jan 21 10:27:49.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl2lv" in namespace "gc-1132"
Jan 21 10:27:49.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-fq7jh" in namespace "gc-1132"
Jan 21 10:27:50.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbt2b" in namespace "gc-1132"
Jan 21 10:27:50.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggvbz" in namespace "gc-1132"
Jan 21 10:27:51.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqq7z" in namespace "gc-1132"
Jan 21 10:27:52.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-gv492" in namespace "gc-1132"
Jan 21 10:27:54.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvdnj" in namespace "gc-1132"
Jan 21 10:27:55.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw9mc" in namespace "gc-1132"
Jan 21 10:27:55.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-gx8cd" in namespace "gc-1132"
Jan 21 10:27:55.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhkgt" in namespace "gc-1132"
Jan 21 10:27:56.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-hk5lb" in namespace "gc-1132"
Jan 21 10:27:56.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkm26" in namespace "gc-1132"
Jan 21 10:27:56.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzpdx" in namespace "gc-1132"
Jan 21 10:27:56.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzthj" in namespace "gc-1132"
Jan 21 10:27:56.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-jps86" in namespace "gc-1132"
Jan 21 10:27:57.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-kknd2" in namespace "gc-1132"
Jan 21 10:27:58.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-kw5tc" in namespace "gc-1132"
Jan 21 10:27:58.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-l59vl" in namespace "gc-1132"
Jan 21 10:27:59.081: INFO: Deleting pod "simpletest-rc-to-be-deleted-l7kmb" in namespace "gc-1132"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:28:00.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1132" for this suite.

• [SLOW TEST:68.676 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":222,"skipped":4412,"failed":0}
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:28:01.216: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-9a6c95e4-8447-45a9-bf46-8e1f320f295f in namespace container-probe-9216
Jan 21 10:29:08.996: INFO: Started pod liveness-9a6c95e4-8447-45a9-bf46-8e1f320f295f in namespace container-probe-9216
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 10:29:09.016: INFO: Initial restart count of pod liveness-9a6c95e4-8447-45a9-bf46-8e1f320f295f is 0
Jan 21 10:29:19.126: INFO: Restart count of pod container-probe-9216/liveness-9a6c95e4-8447-45a9-bf46-8e1f320f295f is now 1 (10.081488249s elapsed)
Jan 21 10:29:37.242: INFO: Restart count of pod container-probe-9216/liveness-9a6c95e4-8447-45a9-bf46-8e1f320f295f is now 2 (28.197625789s elapsed)
Jan 21 10:29:57.404: INFO: Restart count of pod container-probe-9216/liveness-9a6c95e4-8447-45a9-bf46-8e1f320f295f is now 3 (48.359830865s elapsed)
Jan 21 10:30:17.544: INFO: Restart count of pod container-probe-9216/liveness-9a6c95e4-8447-45a9-bf46-8e1f320f295f is now 4 (1m8.499478776s elapsed)
Jan 21 10:31:17.946: INFO: Restart count of pod container-probe-9216/liveness-9a6c95e4-8447-45a9-bf46-8e1f320f295f is now 5 (2m8.901490003s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:31:18.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9216" for this suite.

• [SLOW TEST:196.850 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":4412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:31:18.067: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Jan 21 10:31:19.078: INFO: created pod pod-service-account-defaultsa
Jan 21 10:31:19.079: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 21 10:31:19.107: INFO: created pod pod-service-account-mountsa
Jan 21 10:31:19.107: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 21 10:31:19.151: INFO: created pod pod-service-account-nomountsa
Jan 21 10:31:19.152: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 21 10:31:19.212: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 21 10:31:19.212: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 21 10:31:19.271: INFO: created pod pod-service-account-mountsa-mountspec
Jan 21 10:31:19.271: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 21 10:31:19.361: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 21 10:31:19.361: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 21 10:31:19.390: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 21 10:31:19.390: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 21 10:31:19.457: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 21 10:31:19.457: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 21 10:31:19.508: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 21 10:31:19.508: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:31:19.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4109" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":224,"skipped":4439,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:31:19.679: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-z2gk
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 10:31:19.951: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-z2gk" in namespace "subpath-5949" to be "Succeeded or Failed"
Jan 21 10:31:19.971: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Pending", Reason="", readiness=false. Elapsed: 20.562073ms
Jan 21 10:31:21.989: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037872281s
Jan 21 10:31:24.056: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104894821s
Jan 21 10:31:26.183: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232225854s
Jan 21 10:31:28.241: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.290438551s
Jan 21 10:31:30.401: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.450564805s
Jan 21 10:31:32.444: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Pending", Reason="", readiness=false. Elapsed: 12.492910334s
Jan 21 10:31:34.456: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 14.505528314s
Jan 21 10:31:36.489: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 16.538585384s
Jan 21 10:31:38.581: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 18.629890008s
Jan 21 10:31:40.598: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 20.647545281s
Jan 21 10:31:42.610: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 22.658836659s
Jan 21 10:31:44.621: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 24.670245405s
Jan 21 10:31:46.636: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 26.685357752s
Jan 21 10:31:48.674: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 28.72331092s
Jan 21 10:31:50.683: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Running", Reason="", readiness=true. Elapsed: 30.732325819s
Jan 21 10:31:52.692: INFO: Pod "pod-subpath-test-secret-z2gk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.741241052s
STEP: Saw pod success
Jan 21 10:31:52.692: INFO: Pod "pod-subpath-test-secret-z2gk" satisfied condition "Succeeded or Failed"
Jan 21 10:31:52.697: INFO: Trying to get logs from node conformance1 pod pod-subpath-test-secret-z2gk container test-container-subpath-secret-z2gk: <nil>
STEP: delete the pod
Jan 21 10:31:52.771: INFO: Waiting for pod pod-subpath-test-secret-z2gk to disappear
Jan 21 10:31:52.788: INFO: Pod pod-subpath-test-secret-z2gk no longer exists
STEP: Deleting pod pod-subpath-test-secret-z2gk
Jan 21 10:31:52.788: INFO: Deleting pod "pod-subpath-test-secret-z2gk" in namespace "subpath-5949"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:31:52.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5949" for this suite.

• [SLOW TEST:33.138 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":225,"skipped":4443,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:31:52.817: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:31:52.951: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan 21 10:32:03.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 create -f -'
Jan 21 10:32:05.179: INFO: stderr: ""
Jan 21 10:32:05.179: INFO: stdout: "e2e-test-crd-publish-openapi-2220-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 21 10:32:05.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 delete e2e-test-crd-publish-openapi-2220-crds test-foo'
Jan 21 10:32:05.466: INFO: stderr: ""
Jan 21 10:32:05.466: INFO: stdout: "e2e-test-crd-publish-openapi-2220-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 21 10:32:05.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 apply -f -'
Jan 21 10:32:06.128: INFO: stderr: ""
Jan 21 10:32:06.128: INFO: stdout: "e2e-test-crd-publish-openapi-2220-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 21 10:32:06.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 delete e2e-test-crd-publish-openapi-2220-crds test-foo'
Jan 21 10:32:06.279: INFO: stderr: ""
Jan 21 10:32:06.279: INFO: stdout: "e2e-test-crd-publish-openapi-2220-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Jan 21 10:32:06.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 create -f -'
Jan 21 10:32:06.765: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 21 10:32:06.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 create -f -'
Jan 21 10:32:07.245: INFO: rc: 1
Jan 21 10:32:07.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 apply -f -'
Jan 21 10:32:07.715: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan 21 10:32:07.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 create -f -'
Jan 21 10:32:08.174: INFO: rc: 1
Jan 21 10:32:08.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 --namespace=crd-publish-openapi-1479 apply -f -'
Jan 21 10:32:09.240: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 21 10:32:09.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 explain e2e-test-crd-publish-openapi-2220-crds'
Jan 21 10:32:09.841: INFO: stderr: ""
Jan 21 10:32:09.841: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2220-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 21 10:32:09.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 explain e2e-test-crd-publish-openapi-2220-crds.metadata'
Jan 21 10:32:10.419: INFO: stderr: ""
Jan 21 10:32:10.419: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2220-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 21 10:32:10.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 explain e2e-test-crd-publish-openapi-2220-crds.spec'
Jan 21 10:32:10.929: INFO: stderr: ""
Jan 21 10:32:10.929: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2220-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 21 10:32:10.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 explain e2e-test-crd-publish-openapi-2220-crds.spec.bars'
Jan 21 10:32:11.382: INFO: stderr: ""
Jan 21 10:32:11.382: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2220-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 21 10:32:11.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-1479 explain e2e-test-crd-publish-openapi-2220-crds.spec.bars2'
Jan 21 10:32:11.875: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:32:19.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1479" for this suite.

• [SLOW TEST:26.850 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":226,"skipped":4459,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:32:19.669: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jan 21 10:32:19.782: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 21 10:32:24.793: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:32:24.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1851" for this suite.

• [SLOW TEST:5.275 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":227,"skipped":4461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:32:24.945: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-deebea5f-8b92-48b2-bc7f-bdeb54d732d8
STEP: Creating a pod to test consume secrets
Jan 21 10:32:25.211: INFO: Waiting up to 5m0s for pod "pod-secrets-316e8ee1-985e-4010-b7eb-5bdebb0f0c80" in namespace "secrets-807" to be "Succeeded or Failed"
Jan 21 10:32:25.227: INFO: Pod "pod-secrets-316e8ee1-985e-4010-b7eb-5bdebb0f0c80": Phase="Pending", Reason="", readiness=false. Elapsed: 16.620711ms
Jan 21 10:32:27.241: INFO: Pod "pod-secrets-316e8ee1-985e-4010-b7eb-5bdebb0f0c80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030162037s
Jan 21 10:32:29.260: INFO: Pod "pod-secrets-316e8ee1-985e-4010-b7eb-5bdebb0f0c80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049285996s
STEP: Saw pod success
Jan 21 10:32:29.260: INFO: Pod "pod-secrets-316e8ee1-985e-4010-b7eb-5bdebb0f0c80" satisfied condition "Succeeded or Failed"
Jan 21 10:32:29.282: INFO: Trying to get logs from node conformance1 pod pod-secrets-316e8ee1-985e-4010-b7eb-5bdebb0f0c80 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 10:32:29.331: INFO: Waiting for pod pod-secrets-316e8ee1-985e-4010-b7eb-5bdebb0f0c80 to disappear
Jan 21 10:32:29.343: INFO: Pod pod-secrets-316e8ee1-985e-4010-b7eb-5bdebb0f0c80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:32:29.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-807" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":228,"skipped":4486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:32:29.387: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:32:29.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7503" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":229,"skipped":4512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:32:29.776: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 21 10:32:34.469: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5572 pod-service-account-246d210e-bbad-47f3-9427-1053bc05f5e7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 21 10:32:35.019: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5572 pod-service-account-246d210e-bbad-47f3-9427-1053bc05f5e7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 21 10:32:35.762: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5572 pod-service-account-246d210e-bbad-47f3-9427-1053bc05f5e7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:32:36.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5572" for this suite.

• [SLOW TEST:6.489 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":230,"skipped":4546,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:32:36.265: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 21 10:32:36.409: INFO: Waiting up to 5m0s for pod "pod-480f4ee8-eb26-45af-837b-33f83a353e1e" in namespace "emptydir-7576" to be "Succeeded or Failed"
Jan 21 10:32:36.419: INFO: Pod "pod-480f4ee8-eb26-45af-837b-33f83a353e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.253734ms
Jan 21 10:32:38.525: INFO: Pod "pod-480f4ee8-eb26-45af-837b-33f83a353e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115612785s
Jan 21 10:32:40.538: INFO: Pod "pod-480f4ee8-eb26-45af-837b-33f83a353e1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.129066864s
STEP: Saw pod success
Jan 21 10:32:40.539: INFO: Pod "pod-480f4ee8-eb26-45af-837b-33f83a353e1e" satisfied condition "Succeeded or Failed"
Jan 21 10:32:40.545: INFO: Trying to get logs from node conformance1 pod pod-480f4ee8-eb26-45af-837b-33f83a353e1e container test-container: <nil>
STEP: delete the pod
Jan 21 10:32:40.607: INFO: Waiting for pod pod-480f4ee8-eb26-45af-837b-33f83a353e1e to disappear
Jan 21 10:32:40.620: INFO: Pod pod-480f4ee8-eb26-45af-837b-33f83a353e1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:32:40.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7576" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":231,"skipped":4553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:32:40.690: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Jan 21 10:32:40.920: INFO: created test-podtemplate-1
Jan 21 10:32:40.932: INFO: created test-podtemplate-2
Jan 21 10:32:40.953: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 21 10:32:40.969: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 21 10:32:41.005: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:32:41.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5813" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":232,"skipped":4587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:32:41.045: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:32:41.184: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 21 10:32:41.209: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:41.210: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jan 21 10:32:41.336: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:41.338: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:42.348: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:42.348: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:43.352: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:43.352: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:44.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:44.355: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:45.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:32:45.348: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 21 10:32:45.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:32:45.399: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 21 10:32:46.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:46.413: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 21 10:32:46.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:46.462: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:47.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:47.471: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:48.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:48.498: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:49.521: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:49.522: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:50.468: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:50.468: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:51.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:51.484: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:52.470: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:52.470: INFO: Node dc-hg-2 is running 0 daemon pod, expected 1
Jan 21 10:32:53.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:32:53.475: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8431, will wait for the garbage collector to delete the pods
Jan 21 10:32:53.567: INFO: Deleting DaemonSet.extensions daemon-set took: 10.519569ms
Jan 21 10:32:53.768: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.350577ms
Jan 21 10:32:57.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:32:57.381: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 21 10:32:57.387: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26459"},"items":null}

Jan 21 10:32:57.395: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26459"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:32:57.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8431" for this suite.

• [SLOW TEST:16.411 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":233,"skipped":4623,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:32:57.461: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jan 21 10:32:57.660: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:32:59.675: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:33:01.671: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 21 10:33:01.728: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:33:03.743: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:33:05.736: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 21 10:33:05.825: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 21 10:33:05.833: INFO: Pod pod-with-poststart-http-hook still exists
Jan 21 10:33:07.835: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 21 10:33:07.865: INFO: Pod pod-with-poststart-http-hook still exists
Jan 21 10:33:09.834: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 21 10:33:09.849: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:33:09.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4237" for this suite.

• [SLOW TEST:12.419 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":4635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:33:09.880: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:33:09.990: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5203b94-f3ab-4656-a1e7-35bcd0a29b24" in namespace "projected-1808" to be "Succeeded or Failed"
Jan 21 10:33:10.001: INFO: Pod "downwardapi-volume-d5203b94-f3ab-4656-a1e7-35bcd0a29b24": Phase="Pending", Reason="", readiness=false. Elapsed: 11.165093ms
Jan 21 10:33:12.031: INFO: Pod "downwardapi-volume-d5203b94-f3ab-4656-a1e7-35bcd0a29b24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040819789s
Jan 21 10:33:14.053: INFO: Pod "downwardapi-volume-d5203b94-f3ab-4656-a1e7-35bcd0a29b24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062900525s
STEP: Saw pod success
Jan 21 10:33:14.062: INFO: Pod "downwardapi-volume-d5203b94-f3ab-4656-a1e7-35bcd0a29b24" satisfied condition "Succeeded or Failed"
Jan 21 10:33:14.082: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-d5203b94-f3ab-4656-a1e7-35bcd0a29b24 container client-container: <nil>
STEP: delete the pod
Jan 21 10:33:14.142: INFO: Waiting for pod downwardapi-volume-d5203b94-f3ab-4656-a1e7-35bcd0a29b24 to disappear
Jan 21 10:33:14.150: INFO: Pod downwardapi-volume-d5203b94-f3ab-4656-a1e7-35bcd0a29b24 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:33:14.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1808" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4664,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:33:14.177: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Jan 21 10:33:14.330: INFO: Found Service test-service-k6gm9 in namespace services-6358 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 21 10:33:14.330: INFO: Service test-service-k6gm9 created
STEP: Getting /status
Jan 21 10:33:14.349: INFO: Service test-service-k6gm9 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jan 21 10:33:14.380: INFO: observed Service test-service-k6gm9 in namespace services-6358 with annotations: map[] & LoadBalancer: {[]}
Jan 21 10:33:14.380: INFO: Found Service test-service-k6gm9 in namespace services-6358 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 21 10:33:14.380: INFO: Service test-service-k6gm9 has service status patched
STEP: updating the ServiceStatus
Jan 21 10:33:14.402: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jan 21 10:33:14.414: INFO: Observed Service test-service-k6gm9 in namespace services-6358 with annotations: map[] & Conditions: {[]}
Jan 21 10:33:14.415: INFO: Observed event: &Service{ObjectMeta:{test-service-k6gm9  services-6358  a4a70293-cd80-40df-97dd-ef461a302bf1 26554 0 2022-01-21 10:33:14 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-01-21 10:33:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-01-21 10:33:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.10.238.52,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.10.238.52],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 21 10:33:14.416: INFO: Found Service test-service-k6gm9 in namespace services-6358 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 21 10:33:14.416: INFO: Service test-service-k6gm9 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jan 21 10:33:14.467: INFO: observed Service test-service-k6gm9 in namespace services-6358 with labels: map[test-service-static:true]
Jan 21 10:33:14.468: INFO: observed Service test-service-k6gm9 in namespace services-6358 with labels: map[test-service-static:true]
Jan 21 10:33:14.469: INFO: observed Service test-service-k6gm9 in namespace services-6358 with labels: map[test-service-static:true]
Jan 21 10:33:14.470: INFO: Found Service test-service-k6gm9 in namespace services-6358 with labels: map[test-service:patched test-service-static:true]
Jan 21 10:33:14.470: INFO: Service test-service-k6gm9 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jan 21 10:33:14.537: INFO: Observed event: ADDED
Jan 21 10:33:14.538: INFO: Observed event: MODIFIED
Jan 21 10:33:14.540: INFO: Observed event: MODIFIED
Jan 21 10:33:14.540: INFO: Observed event: MODIFIED
Jan 21 10:33:14.540: INFO: Found Service test-service-k6gm9 in namespace services-6358 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 21 10:33:14.540: INFO: Service test-service-k6gm9 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:33:14.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6358" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":236,"skipped":4679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:33:14.624: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:33:14.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc" in namespace "projected-8448" to be "Succeeded or Failed"
Jan 21 10:33:14.880: INFO: Pod "downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc": Phase="Pending", Reason="", readiness=false. Elapsed: 110.675653ms
Jan 21 10:33:16.901: INFO: Pod "downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132279134s
Jan 21 10:33:18.912: INFO: Pod "downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.142680042s
Jan 21 10:33:20.930: INFO: Pod "downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.160905784s
STEP: Saw pod success
Jan 21 10:33:20.930: INFO: Pod "downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc" satisfied condition "Succeeded or Failed"
Jan 21 10:33:20.954: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc container client-container: <nil>
STEP: delete the pod
Jan 21 10:33:21.021: INFO: Waiting for pod downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc to disappear
Jan 21 10:33:21.028: INFO: Pod downwardapi-volume-c9266dfa-dd60-49ef-907e-82c2cbdbd4fc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:33:21.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8448" for this suite.

• [SLOW TEST:6.477 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4765,"failed":0}
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:33:21.102: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:33:21.326: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-573451d1-f1a8-42e3-a8d4-bf43c3dcc75f" in namespace "security-context-test-3713" to be "Succeeded or Failed"
Jan 21 10:33:21.359: INFO: Pod "alpine-nnp-false-573451d1-f1a8-42e3-a8d4-bf43c3dcc75f": Phase="Pending", Reason="", readiness=false. Elapsed: 33.212789ms
Jan 21 10:33:23.406: INFO: Pod "alpine-nnp-false-573451d1-f1a8-42e3-a8d4-bf43c3dcc75f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079931028s
Jan 21 10:33:25.420: INFO: Pod "alpine-nnp-false-573451d1-f1a8-42e3-a8d4-bf43c3dcc75f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.094551247s
Jan 21 10:33:25.420: INFO: Pod "alpine-nnp-false-573451d1-f1a8-42e3-a8d4-bf43c3dcc75f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:33:25.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3713" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":238,"skipped":4765,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:33:25.468: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-7090ad63-d5dc-4825-a1ea-d5a13a19f5b9
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:33:25.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8151" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":239,"skipped":4776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:33:25.647: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:35:01.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8265" for this suite.

• [SLOW TEST:96.243 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":240,"skipped":4803,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:35:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:35:30.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3975" for this suite.

• [SLOW TEST:28.513 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":241,"skipped":4805,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:35:30.401: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-e48444a2-54d4-4e5b-a9af-a4af285f75db
STEP: Creating a pod to test consume configMaps
Jan 21 10:35:30.492: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5" in namespace "configmap-4144" to be "Succeeded or Failed"
Jan 21 10:35:30.501: INFO: Pod "pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.761658ms
Jan 21 10:35:32.524: INFO: Pod "pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032056167s
Jan 21 10:35:34.560: INFO: Pod "pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068213232s
Jan 21 10:35:36.573: INFO: Pod "pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080480828s
STEP: Saw pod success
Jan 21 10:35:36.573: INFO: Pod "pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5" satisfied condition "Succeeded or Failed"
Jan 21 10:35:36.579: INFO: Trying to get logs from node conformance1 pod pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 10:35:36.658: INFO: Waiting for pod pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5 to disappear
Jan 21 10:35:36.664: INFO: Pod pod-configmaps-3e2e3262-9c8e-4a80-b25f-56cdea1a8af5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:35:36.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4144" for this suite.

• [SLOW TEST:6.290 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4815,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:35:36.692: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:41:00.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-818" for this suite.

• [SLOW TEST:324.187 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":243,"skipped":4825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:41:00.896: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-05c62bfe-6449-479d-a866-3a62491f5dc3
STEP: Creating a pod to test consume configMaps
Jan 21 10:41:01.158: INFO: Waiting up to 5m0s for pod "pod-configmaps-86417f6d-f16e-4fe6-b29b-bb659ae9636f" in namespace "configmap-8517" to be "Succeeded or Failed"
Jan 21 10:41:01.167: INFO: Pod "pod-configmaps-86417f6d-f16e-4fe6-b29b-bb659ae9636f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.857437ms
Jan 21 10:41:03.175: INFO: Pod "pod-configmaps-86417f6d-f16e-4fe6-b29b-bb659ae9636f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016950602s
Jan 21 10:41:05.192: INFO: Pod "pod-configmaps-86417f6d-f16e-4fe6-b29b-bb659ae9636f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033264009s
STEP: Saw pod success
Jan 21 10:41:05.207: INFO: Pod "pod-configmaps-86417f6d-f16e-4fe6-b29b-bb659ae9636f" satisfied condition "Succeeded or Failed"
Jan 21 10:41:05.226: INFO: Trying to get logs from node conformance1 pod pod-configmaps-86417f6d-f16e-4fe6-b29b-bb659ae9636f container agnhost-container: <nil>
STEP: delete the pod
Jan 21 10:41:05.327: INFO: Waiting for pod pod-configmaps-86417f6d-f16e-4fe6-b29b-bb659ae9636f to disappear
Jan 21 10:41:05.336: INFO: Pod pod-configmaps-86417f6d-f16e-4fe6-b29b-bb659ae9636f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:41:05.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8517" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":244,"skipped":4925,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:41:05.367: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:41:22.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4325" for this suite.

• [SLOW TEST:17.292 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":245,"skipped":4926,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:41:22.660: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 21 10:41:22.795: INFO: The status of Pod annotationupdate95101913-73bf-4765-93db-52a069f50565 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:41:24.881: INFO: The status of Pod annotationupdate95101913-73bf-4765-93db-52a069f50565 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:41:26.812: INFO: The status of Pod annotationupdate95101913-73bf-4765-93db-52a069f50565 is Running (Ready = true)
Jan 21 10:41:27.492: INFO: Successfully updated pod "annotationupdate95101913-73bf-4765-93db-52a069f50565"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:41:29.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1127" for this suite.

• [SLOW TEST:6.926 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":246,"skipped":4943,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:41:29.593: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:41:29.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3838" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":247,"skipped":4952,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:41:29.835: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7098
Jan 21 10:41:30.040: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:41:32.061: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:41:34.054: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 21 10:41:34.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7098 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 21 10:41:34.500: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 21 10:41:34.500: INFO: stdout: "iptables"
Jan 21 10:41:34.500: INFO: proxyMode: iptables
Jan 21 10:41:34.524: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 21 10:41:34.533: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7098
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7098
I0121 10:41:34.619501      18 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7098, replica count: 3
I0121 10:41:37.675432      18 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:41:40.676277      18 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:41:43.677675      18 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 10:41:43.711: INFO: Creating new exec pod
Jan 21 10:41:48.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7098 exec execpod-affinityp9z4r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 21 10:41:49.207: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 21 10:41:49.207: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:41:49.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7098 exec execpod-affinityp9z4r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.93.229 80'
Jan 21 10:41:49.664: INFO: stderr: "+ + nc -v -t -w 2 10.10.93.229 80\necho hostName\nConnection to 10.10.93.229 80 port [tcp/http] succeeded!\n"
Jan 21 10:41:49.664: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:41:49.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7098 exec execpod-affinityp9z4r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.86 31588'
Jan 21 10:41:50.214: INFO: stderr: "+ nc -v -t -w 2 10.10.1.86 31588\nConnection to 10.10.1.86 31588 port [tcp/*] succeeded!\n+ echo hostName\n"
Jan 21 10:41:50.214: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:41:50.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7098 exec execpod-affinityp9z4r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.136 31588'
Jan 21 10:41:50.596: INFO: stderr: "+ nc -v -t -w 2 10.10.1.136 31588\n+ echo hostName\nConnection to 10.10.1.136 31588 port [tcp/*] succeeded!\n"
Jan 21 10:41:50.596: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:41:50.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7098 exec execpod-affinityp9z4r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.86:31588/ ; done'
Jan 21 10:41:51.211: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n"
Jan 21 10:41:51.212: INFO: stdout: "\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n\naffinity-nodeport-timeout-t894n"
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Received response from host: affinity-nodeport-timeout-t894n
Jan 21 10:41:51.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7098 exec execpod-affinityp9z4r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.1.86:31588/'
Jan 21 10:41:51.624: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n"
Jan 21 10:41:51.624: INFO: stdout: "affinity-nodeport-timeout-t894n"
Jan 21 10:42:11.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-7098 exec execpod-affinityp9z4r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.1.86:31588/'
Jan 21 10:42:12.437: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.1.86:31588/\n"
Jan 21 10:42:12.438: INFO: stdout: "affinity-nodeport-timeout-pgcv2"
Jan 21 10:42:12.438: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7098, will wait for the garbage collector to delete the pods
Jan 21 10:42:12.613: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 34.803038ms
Jan 21 10:42:12.927: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 313.65707ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:42:17.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7098" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:47.753 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":248,"skipped":4956,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:42:17.590: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:42:17.695: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:42:19.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1405" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":249,"skipped":4966,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:42:19.376: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jan 21 10:42:19.809: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:42:21.824: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:42:23.830: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 21 10:42:23.950: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:42:25.967: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:42:27.967: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 21 10:42:28.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 10:42:28.053: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 10:42:30.054: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 10:42:30.071: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 10:42:32.054: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 10:42:32.068: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:42:32.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3047" for this suite.

• [SLOW TEST:12.722 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":4969,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:42:32.098: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8227
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jan 21 10:42:32.257: INFO: Found 0 stateful pods, waiting for 3
Jan 21 10:42:42.270: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 10:42:42.270: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 10:42:42.270: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 21 10:42:52.309: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 10:42:52.309: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 10:42:52.309: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 10:42:52.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8227 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 10:42:52.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 10:42:52.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 10:42:52.910: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 21 10:43:02.982: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 21 10:43:13.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8227 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 10:43:13.502: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 21 10:43:13.502: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 10:43:13.502: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jan 21 10:43:33.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8227 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 10:43:34.079: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 10:43:34.079: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 10:43:34.079: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 10:43:34.160: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 21 10:43:44.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-8227 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 10:43:44.901: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 21 10:43:44.901: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 10:43:44.901: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 21 10:44:04.984: INFO: Deleting all statefulset in ns statefulset-8227
Jan 21 10:44:04.993: INFO: Scaling statefulset ss2 to 0
Jan 21 10:44:15.068: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 10:44:15.074: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:44:15.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8227" for this suite.

• [SLOW TEST:103.051 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":251,"skipped":4979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:44:15.150: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-35483af5-2b1b-49b7-9f59-7b93045ec145 in namespace container-probe-137
Jan 21 10:44:19.351: INFO: Started pod liveness-35483af5-2b1b-49b7-9f59-7b93045ec145 in namespace container-probe-137
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 10:44:19.358: INFO: Initial restart count of pod liveness-35483af5-2b1b-49b7-9f59-7b93045ec145 is 0
Jan 21 10:44:37.474: INFO: Restart count of pod container-probe-137/liveness-35483af5-2b1b-49b7-9f59-7b93045ec145 is now 1 (18.115135473s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:44:37.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-137" for this suite.

• [SLOW TEST:22.397 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":5004,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:44:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:46:01.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5879" for this suite.

• [SLOW TEST:84.409 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":253,"skipped":5014,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:46:02.128: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:46:02.497: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7" in namespace "projected-1797" to be "Succeeded or Failed"
Jan 21 10:46:02.514: INFO: Pod "downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.955732ms
Jan 21 10:46:04.555: INFO: Pod "downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058671443s
Jan 21 10:46:06.696: INFO: Pod "downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.199009826s
Jan 21 10:46:08.960: INFO: Pod "downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.462838202s
Jan 21 10:46:11.000: INFO: Pod "downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.503157059s
Jan 21 10:46:13.032: INFO: Pod "downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.534974786s
STEP: Saw pod success
Jan 21 10:46:13.032: INFO: Pod "downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7" satisfied condition "Succeeded or Failed"
Jan 21 10:46:13.052: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7 container client-container: <nil>
STEP: delete the pod
Jan 21 10:46:13.328: INFO: Waiting for pod downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7 to disappear
Jan 21 10:46:13.368: INFO: Pod downwardapi-volume-f3be19a4-c3b3-4a4e-9fb4-266b404413b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:46:13.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1797" for this suite.

• [SLOW TEST:11.391 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":5022,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:46:13.523: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 21 10:46:13.926: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7390  a3fd3f6d-e47e-4e4a-b146-f55df7810e25 28111 0 2022-01-21 10:46:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-21 10:46:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 10:46:13.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7390  a3fd3f6d-e47e-4e4a-b146-f55df7810e25 28112 0 2022-01-21 10:46:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-21 10:46:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 21 10:46:14.051: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7390  a3fd3f6d-e47e-4e4a-b146-f55df7810e25 28113 0 2022-01-21 10:46:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-21 10:46:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 21 10:46:14.052: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7390  a3fd3f6d-e47e-4e4a-b146-f55df7810e25 28114 0 2022-01-21 10:46:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-21 10:46:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:46:14.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7390" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":255,"skipped":5031,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:46:14.130: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:46:14.749: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 21 10:46:14.961: INFO: The status of Pod pod-logs-websocket-aae2f43e-12dd-4a57-aaf0-ab3fad5461c0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:46:17.112: INFO: The status of Pod pod-logs-websocket-aae2f43e-12dd-4a57-aaf0-ab3fad5461c0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:46:19.095: INFO: The status of Pod pod-logs-websocket-aae2f43e-12dd-4a57-aaf0-ab3fad5461c0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:46:20.998: INFO: The status of Pod pod-logs-websocket-aae2f43e-12dd-4a57-aaf0-ab3fad5461c0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:46:23.000: INFO: The status of Pod pod-logs-websocket-aae2f43e-12dd-4a57-aaf0-ab3fad5461c0 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:46:24.975: INFO: The status of Pod pod-logs-websocket-aae2f43e-12dd-4a57-aaf0-ab3fad5461c0 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:46:25.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-857" for this suite.

• [SLOW TEST:10.945 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":5064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:46:25.077: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:47:25.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8411" for this suite.

• [SLOW TEST:60.362 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":5155,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:47:25.438: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-9774
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-9774
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9774
Jan 21 10:47:25.788: INFO: Found 0 stateful pods, waiting for 1
Jan 21 10:47:35.798: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 21 10:47:35.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-9774 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 10:47:36.586: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 10:47:36.586: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 10:47:36.586: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 10:47:36.631: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 21 10:47:46.647: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 10:47:46.647: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 10:47:46.735: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 21 10:47:46.735: INFO: ss-0  conformance1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  }]
Jan 21 10:47:46.735: INFO: 
Jan 21 10:47:46.735: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 21 10:47:47.788: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.959961855s
Jan 21 10:47:48.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.907354691s
Jan 21 10:47:49.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.890322577s
Jan 21 10:47:51.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.77992462s
Jan 21 10:47:52.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.678140413s
Jan 21 10:47:53.093: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.659591059s
Jan 21 10:47:54.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.602209623s
Jan 21 10:47:55.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.577582649s
Jan 21 10:47:56.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 549.030789ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9774
Jan 21 10:47:57.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-9774 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 10:47:57.682: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 21 10:47:57.682: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 10:47:57.682: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 21 10:47:57.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-9774 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 10:47:58.706: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 21 10:47:58.706: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 10:47:58.706: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 21 10:47:58.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-9774 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 21 10:47:59.539: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 21 10:47:59.539: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 21 10:47:59.539: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 21 10:47:59.574: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 10:47:59.574: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 10:47:59.574: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 21 10:47:59.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-9774 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 10:48:00.225: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 10:48:00.225: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 10:48:00.225: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 10:48:00.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-9774 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 10:48:00.703: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 10:48:00.703: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 10:48:00.703: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 10:48:00.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=statefulset-9774 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 21 10:48:01.655: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 21 10:48:01.655: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 21 10:48:01.655: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 21 10:48:01.655: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 10:48:01.678: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 21 10:48:11.717: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 10:48:11.717: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 10:48:11.717: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 10:48:11.898: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 21 10:48:11.898: INFO: ss-0  conformance1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  }]
Jan 21 10:48:11.898: INFO: ss-1  dc-hg-2       Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  }]
Jan 21 10:48:11.898: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  }]
Jan 21 10:48:11.898: INFO: 
Jan 21 10:48:11.898: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 10:48:12.946: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 21 10:48:12.947: INFO: ss-0  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  }]
Jan 21 10:48:12.947: INFO: ss-1  dc-hg-2       Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  }]
Jan 21 10:48:12.947: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  }]
Jan 21 10:48:12.947: INFO: 
Jan 21 10:48:12.947: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 10:48:13.958: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 21 10:48:13.958: INFO: ss-0  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  }]
Jan 21 10:48:13.958: INFO: ss-1  dc-hg-2       Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  }]
Jan 21 10:48:13.958: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  }]
Jan 21 10:48:13.958: INFO: 
Jan 21 10:48:13.958: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 10:48:15.015: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 21 10:48:15.015: INFO: ss-0  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  }]
Jan 21 10:48:15.015: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  }]
Jan 21 10:48:15.015: INFO: 
Jan 21 10:48:15.015: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 21 10:48:16.079: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 21 10:48:16.079: INFO: ss-0  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  }]
Jan 21 10:48:16.079: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:46 +0000 UTC  }]
Jan 21 10:48:16.079: INFO: 
Jan 21 10:48:16.079: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 21 10:48:17.125: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 21 10:48:17.125: INFO: ss-0  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:48:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 10:47:25 +0000 UTC  }]
Jan 21 10:48:17.125: INFO: 
Jan 21 10:48:17.125: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 21 10:48:18.151: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.620903558s
Jan 21 10:48:19.160: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.59499942s
Jan 21 10:48:20.175: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.585482515s
Jan 21 10:48:21.184: INFO: Verifying statefulset ss doesn't scale past 0 for another 571.944469ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9774
Jan 21 10:48:22.210: INFO: Scaling statefulset ss to 0
Jan 21 10:48:22.265: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 21 10:48:22.278: INFO: Deleting all statefulset in ns statefulset-9774
Jan 21 10:48:22.297: INFO: Scaling statefulset ss to 0
Jan 21 10:48:22.382: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 10:48:22.409: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:48:22.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9774" for this suite.

• [SLOW TEST:57.437 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":258,"skipped":5158,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:48:22.876: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Jan 21 10:48:23.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3609 api-versions'
Jan 21 10:48:24.212: INFO: stderr: ""
Jan 21 10:48:24.212: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nkyverno.io/v1\nkyverno.io/v1alpha1\nkyverno.io/v1alpha2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nwgpolicyk8s.io/v1alpha1\nwgpolicyk8s.io/v1alpha2\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:48:24.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3609" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":259,"skipped":5162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:48:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 21 10:48:24.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-6689 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 21 10:48:25.498: INFO: stderr: ""
Jan 21 10:48:25.504: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 21 10:48:25.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-6689 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 21 10:48:37.110: INFO: stderr: ""
Jan 21 10:48:37.110: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 21 10:48:37.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-6689 delete pods e2e-test-httpd-pod'
Jan 21 10:48:41.067: INFO: stderr: ""
Jan 21 10:48:41.067: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:48:41.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6689" for this suite.

• [SLOW TEST:16.595 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:926
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":260,"skipped":5215,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:48:41.147: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-f17dfc50-fbaa-4ca5-a52b-ac1fd8484714
STEP: Creating a pod to test consume secrets
Jan 21 10:48:41.406: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b" in namespace "projected-8815" to be "Succeeded or Failed"
Jan 21 10:48:41.509: INFO: Pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 102.28108ms
Jan 21 10:48:43.524: INFO: Pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.117857756s
Jan 21 10:48:45.538: INFO: Pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131389802s
Jan 21 10:48:47.620: INFO: Pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.21406972s
Jan 21 10:48:49.734: INFO: Pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.327312969s
Jan 21 10:48:51.748: INFO: Pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.341517571s
Jan 21 10:48:53.776: INFO: Pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.369358253s
STEP: Saw pod success
Jan 21 10:48:53.776: INFO: Pod "pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b" satisfied condition "Succeeded or Failed"
Jan 21 10:48:53.785: INFO: Trying to get logs from node conformance1 pod pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 10:48:53.898: INFO: Waiting for pod pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b to disappear
Jan 21 10:48:53.936: INFO: Pod pod-projected-secrets-f0bafa55-5ee0-43dd-a882-af5cdb149d8b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:48:53.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8815" for this suite.

• [SLOW TEST:12.906 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":5223,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:48:54.074: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:48:54.704: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:48:56.751: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:48:58.719: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:49:00.787: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:49:02.742: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:04.730: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:06.712: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:08.828: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:10.718: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:12.722: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:14.725: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:16.722: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:18.773: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:20.716: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:22.740: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = false)
Jan 21 10:49:24.718: INFO: The status of Pod test-webserver-ea9ae0c6-ba74-429a-9fc9-0b8bffc1ff47 is Running (Ready = true)
Jan 21 10:49:24.729: INFO: Container started at 2022-01-21 10:49:00 +0000 UTC, pod became ready at 2022-01-21 10:49:24 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:49:24.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5006" for this suite.

• [SLOW TEST:30.748 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":5231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:49:24.849: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 21 10:49:25.091: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 21 10:49:25.165: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 21 10:49:25.165: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jan 21 10:49:25.269: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 21 10:49:25.270: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 21 10:49:25.452: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 21 10:49:25.452: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 21 10:49:32.797: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:49:32.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3507" for this suite.

• [SLOW TEST:8.292 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":263,"skipped":5266,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:49:33.144: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 10:49:35.205: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 21 10:49:37.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:49:39.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:49:41.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:49:43.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 49, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 10:49:46.458: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:49:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7003" for this suite.
STEP: Destroying namespace "webhook-7003-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:27.124 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":264,"skipped":5275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:50:00.275: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 21 10:50:01.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:50:01.259: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:02.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:50:02.380: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:03.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:50:03.710: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:04.617: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:50:04.617: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:05.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:50:05.638: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:06.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:50:06.490: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:07.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:50:07.467: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:08.445: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:50:08.445: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:09.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:50:09.454: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:10.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:50:10.415: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:11.427: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:50:11.427: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:12.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:50:12.284: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:50:13.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 10:50:13.294: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Jan 21 10:50:13.557: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28843"},"items":null}

Jan 21 10:50:13.578: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28843"},"items":[{"metadata":{"name":"daemon-set-c44ct","generateName":"daemon-set-","namespace":"daemonsets-8826","uid":"0f33d8b5-ec29-4ede-af6b-aac5af7589c8","resourceVersion":"28841","creationTimestamp":"2022-01-21T10:50:01Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"098e4e2e176098953e00cae86af05fb29ec3542e25289e0928ea9f2f8ccea3c6","cni.projectcalico.org/podIP":"172.16.209.107/32","cni.projectcalico.org/podIPs":"172.16.209.107/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2f449aa9-28b3-42f9-8302-2bdce833da55","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-01-21T10:50:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f449aa9-28b3-42f9-8302-2bdce833da55\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-01-21T10:50:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-01-21T10:50:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.209.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zbxml","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zbxml","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"conformance1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["conformance1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-21T10:50:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-21T10:50:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-21T10:50:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-21T10:50:01Z"}],"hostIP":"10.10.1.86","podIP":"172.16.209.107","podIPs":[{"ip":"172.16.209.107"}],"startTime":"2022-01-21T10:50:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-01-21T10:50:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://6af8c4451d899663d49d27d9123dbd2fa64c0b08bf8fe36649df7f836113f81e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-sjd2l","generateName":"daemon-set-","namespace":"daemonsets-8826","uid":"09aeba60-0514-4520-996c-4c52be7f0f79","resourceVersion":"28825","creationTimestamp":"2022-01-21T10:50:01Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3f0b766d917341e28d357c684e32bd493a0d8a3c1edb7634b397d4bc38e22e57","cni.projectcalico.org/podIP":"172.16.106.135/32","cni.projectcalico.org/podIPs":"172.16.106.135/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2f449aa9-28b3-42f9-8302-2bdce833da55","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-01-21T10:50:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f449aa9-28b3-42f9-8302-2bdce833da55\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-01-21T10:50:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-01-21T10:50:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.106.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lpjs8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lpjs8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"dc-hg-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["dc-hg-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-21T10:50:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-21T10:50:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-21T10:50:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-21T10:50:01Z"}],"hostIP":"10.10.1.136","podIP":"172.16.106.135","podIPs":[{"ip":"172.16.106.135"}],"startTime":"2022-01-21T10:50:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-01-21T10:50:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://d0c1d17e8aa39e5f0581e8f50036564af8309e1cbbc40005985ac9148ea0bb90","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:50:13.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8826" for this suite.

• [SLOW TEST:13.517 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":265,"skipped":5310,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:50:13.801: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 10:50:17.704: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 21 10:50:19.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 50, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:50:21.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 50, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:50:23.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 50, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:50:25.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 50, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 50, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 10:50:28.967: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:50:30.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8083" for this suite.
STEP: Destroying namespace "webhook-8083-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.643 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":266,"skipped":5322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:50:31.444: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:50:32.347: INFO: The status of Pod pod-secrets-83411db8-8ddb-4b96-8af0-293615ff26f4 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:50:34.358: INFO: The status of Pod pod-secrets-83411db8-8ddb-4b96-8af0-293615ff26f4 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:50:36.390: INFO: The status of Pod pod-secrets-83411db8-8ddb-4b96-8af0-293615ff26f4 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:50:38.466: INFO: The status of Pod pod-secrets-83411db8-8ddb-4b96-8af0-293615ff26f4 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:50:40.423: INFO: The status of Pod pod-secrets-83411db8-8ddb-4b96-8af0-293615ff26f4 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 10:50:42.357: INFO: The status of Pod pod-secrets-83411db8-8ddb-4b96-8af0-293615ff26f4 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:50:42.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8994" for this suite.

• [SLOW TEST:11.163 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":267,"skipped":5354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:50:42.613: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-9272c275-e5f3-4103-a2fd-e2d7ac5492c9
STEP: Creating a pod to test consume configMaps
Jan 21 10:50:42.966: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790" in namespace "projected-3090" to be "Succeeded or Failed"
Jan 21 10:50:43.042: INFO: Pod "pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790": Phase="Pending", Reason="", readiness=false. Elapsed: 75.434265ms
Jan 21 10:50:45.099: INFO: Pod "pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132319588s
Jan 21 10:50:47.321: INFO: Pod "pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355135767s
Jan 21 10:50:49.461: INFO: Pod "pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790": Phase="Pending", Reason="", readiness=false. Elapsed: 6.494396952s
Jan 21 10:50:51.545: INFO: Pod "pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.578713888s
STEP: Saw pod success
Jan 21 10:50:51.545: INFO: Pod "pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790" satisfied condition "Succeeded or Failed"
Jan 21 10:50:51.667: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 10:50:51.911: INFO: Waiting for pod pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790 to disappear
Jan 21 10:50:51.975: INFO: Pod pod-projected-configmaps-64ba3c00-c97c-4165-be88-41981ce72790 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:50:51.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3090" for this suite.

• [SLOW TEST:9.700 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":5408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:50:52.421: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-56d47c27-e5c6-4347-b266-ca8cdf90d6fd
STEP: Creating a pod to test consume configMaps
Jan 21 10:50:53.546: INFO: Waiting up to 5m0s for pod "pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea" in namespace "configmap-2178" to be "Succeeded or Failed"
Jan 21 10:50:53.646: INFO: Pod "pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea": Phase="Pending", Reason="", readiness=false. Elapsed: 99.470624ms
Jan 21 10:50:55.666: INFO: Pod "pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120020464s
Jan 21 10:50:57.701: INFO: Pod "pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154620641s
Jan 21 10:50:59.760: INFO: Pod "pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.21421207s
STEP: Saw pod success
Jan 21 10:50:59.761: INFO: Pod "pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea" satisfied condition "Succeeded or Failed"
Jan 21 10:50:59.782: INFO: Trying to get logs from node conformance1 pod pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea container agnhost-container: <nil>
STEP: delete the pod
Jan 21 10:51:00.192: INFO: Waiting for pod pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea to disappear
Jan 21 10:51:00.210: INFO: Pod pod-configmaps-f13b1d00-78c5-4394-b7bf-f43877e87eea no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:51:00.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2178" for this suite.

• [SLOW TEST:7.858 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":269,"skipped":5433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:51:00.339: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-bd3afaf2-d13b-4772-9dbe-4d348f0a27e7 in namespace container-probe-5611
Jan 21 10:51:06.846: INFO: Started pod busybox-bd3afaf2-d13b-4772-9dbe-4d348f0a27e7 in namespace container-probe-5611
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 10:51:06.869: INFO: Initial restart count of pod busybox-bd3afaf2-d13b-4772-9dbe-4d348f0a27e7 is 0
Jan 21 10:51:53.500: INFO: Restart count of pod container-probe-5611/busybox-bd3afaf2-d13b-4772-9dbe-4d348f0a27e7 is now 1 (46.631320684s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:51:53.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5611" for this suite.

• [SLOW TEST:53.348 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":5528,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:51:53.706: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-nqjs
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 10:51:54.037: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nqjs" in namespace "subpath-3201" to be "Succeeded or Failed"
Jan 21 10:51:54.069: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Pending", Reason="", readiness=false. Elapsed: 31.276192ms
Jan 21 10:51:56.097: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059555519s
Jan 21 10:51:58.150: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.111963662s
Jan 21 10:52:00.312: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274681243s
Jan 21 10:52:02.329: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 8.291213381s
Jan 21 10:52:04.341: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 10.302945555s
Jan 21 10:52:06.372: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 12.334656914s
Jan 21 10:52:08.527: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 14.488840602s
Jan 21 10:52:10.540: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 16.501870429s
Jan 21 10:52:12.563: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 18.524900666s
Jan 21 10:52:14.577: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 20.538821526s
Jan 21 10:52:16.585: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 22.547530656s
Jan 21 10:52:18.687: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 24.649664112s
Jan 21 10:52:20.701: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Running", Reason="", readiness=true. Elapsed: 26.663339573s
Jan 21 10:52:22.739: INFO: Pod "pod-subpath-test-projected-nqjs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.701665565s
STEP: Saw pod success
Jan 21 10:52:22.740: INFO: Pod "pod-subpath-test-projected-nqjs" satisfied condition "Succeeded or Failed"
Jan 21 10:52:22.832: INFO: Trying to get logs from node conformance1 pod pod-subpath-test-projected-nqjs container test-container-subpath-projected-nqjs: <nil>
STEP: delete the pod
Jan 21 10:52:22.994: INFO: Waiting for pod pod-subpath-test-projected-nqjs to disappear
Jan 21 10:52:23.027: INFO: Pod pod-subpath-test-projected-nqjs no longer exists
STEP: Deleting pod pod-subpath-test-projected-nqjs
Jan 21 10:52:23.027: INFO: Deleting pod "pod-subpath-test-projected-nqjs" in namespace "subpath-3201"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:52:23.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3201" for this suite.

• [SLOW TEST:29.460 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":271,"skipped":5549,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:52:23.167: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9759
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9759
I0121 10:52:24.079721      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9759, replica count: 2
I0121 10:52:27.181072      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:52:30.187780      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:52:33.191759      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:52:36.194487      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 10:52:36.194: INFO: Creating new exec pod
Jan 21 10:52:45.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9759 exec execpod7kb55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 21 10:52:46.599: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 21 10:52:46.599: INFO: stdout: ""
Jan 21 10:52:47.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9759 exec execpod7kb55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 21 10:52:48.183: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 21 10:52:48.183: INFO: stdout: ""
Jan 21 10:52:48.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9759 exec execpod7kb55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 21 10:52:50.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 21 10:52:50.147: INFO: stdout: ""
Jan 21 10:52:50.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9759 exec execpod7kb55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 21 10:52:51.199: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 21 10:52:51.199: INFO: stdout: ""
Jan 21 10:52:51.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9759 exec execpod7kb55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 21 10:52:52.371: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 21 10:52:52.372: INFO: stdout: "externalname-service-pck55"
Jan 21 10:52:52.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9759 exec execpod7kb55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.172.163 80'
Jan 21 10:52:53.230: INFO: stderr: "+ nc -v -t -w 2 10.10.172.163 80\n+ echo hostName\nConnection to 10.10.172.163 80 port [tcp/http] succeeded!\n"
Jan 21 10:52:53.230: INFO: stdout: "externalname-service-pck55"
Jan 21 10:52:53.230: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:52:53.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9759" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:30.614 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":272,"skipped":5549,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:52:53.781: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 21 10:52:58.682: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:52:58.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4101" for this suite.

• [SLOW TEST:5.647 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":5557,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:52:59.428: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:53:18.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5863" for this suite.

• [SLOW TEST:18.987 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":274,"skipped":5588,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:53:18.422: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 21 10:53:19.019: INFO: Waiting up to 5m0s for pod "pod-0b652419-a945-48a5-99c5-8f37aab6c174" in namespace "emptydir-707" to be "Succeeded or Failed"
Jan 21 10:53:19.125: INFO: Pod "pod-0b652419-a945-48a5-99c5-8f37aab6c174": Phase="Pending", Reason="", readiness=false. Elapsed: 105.383151ms
Jan 21 10:53:21.194: INFO: Pod "pod-0b652419-a945-48a5-99c5-8f37aab6c174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174118641s
Jan 21 10:53:23.211: INFO: Pod "pod-0b652419-a945-48a5-99c5-8f37aab6c174": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191551909s
Jan 21 10:53:25.231: INFO: Pod "pod-0b652419-a945-48a5-99c5-8f37aab6c174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.211779168s
STEP: Saw pod success
Jan 21 10:53:25.232: INFO: Pod "pod-0b652419-a945-48a5-99c5-8f37aab6c174" satisfied condition "Succeeded or Failed"
Jan 21 10:53:25.271: INFO: Trying to get logs from node conformance1 pod pod-0b652419-a945-48a5-99c5-8f37aab6c174 container test-container: <nil>
STEP: delete the pod
Jan 21 10:53:25.500: INFO: Waiting for pod pod-0b652419-a945-48a5-99c5-8f37aab6c174 to disappear
Jan 21 10:53:25.526: INFO: Pod pod-0b652419-a945-48a5-99c5-8f37aab6c174 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:53:25.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-707" for this suite.

• [SLOW TEST:7.281 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":5611,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:53:25.705: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 10:53:27.959: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 10:53:30.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 53, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:53:32.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 53, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 10:53:34.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 10, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 10, 53, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 10:53:37.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 21 10:53:43.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=webhook-7833 attach --namespace=webhook-7833 to-be-attached-pod -i -c=container1'
Jan 21 10:53:43.733: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:53:43.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7833" for this suite.
STEP: Destroying namespace "webhook-7833-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.635 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":276,"skipped":5611,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:53:44.471: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Jan 21 10:53:45.636: INFO: Waiting up to 5m0s for pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a" in namespace "containers-9289" to be "Succeeded or Failed"
Jan 21 10:53:45.704: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 68.374161ms
Jan 21 10:53:47.782: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146578709s
Jan 21 10:53:49.970: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333975944s
Jan 21 10:53:52.197: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.56152966s
Jan 21 10:53:54.231: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.595505389s
Jan 21 10:53:56.455: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.819715104s
Jan 21 10:53:58.477: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.84174439s
Jan 21 10:54:00.564: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.928233216s
STEP: Saw pod success
Jan 21 10:54:00.564: INFO: Pod "client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a" satisfied condition "Succeeded or Failed"
Jan 21 10:54:00.628: INFO: Trying to get logs from node conformance1 pod client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a container agnhost-container: <nil>
STEP: delete the pod
Jan 21 10:54:00.811: INFO: Waiting for pod client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a to disappear
Jan 21 10:54:00.841: INFO: Pod client-containers-a12b93ed-7ea2-4f38-b92d-50838aecfc2a no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:54:00.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9289" for this suite.

• [SLOW TEST:16.493 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":277,"skipped":5621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:54:00.955: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6998
STEP: creating service affinity-nodeport-transition in namespace services-6998
STEP: creating replication controller affinity-nodeport-transition in namespace services-6998
I0121 10:54:01.546734      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6998, replica count: 3
I0121 10:54:04.701981      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:54:07.712681      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:54:10.714464      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:54:13.715591      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 10:54:16.716997      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 10:54:16.756: INFO: Creating new exec pod
Jan 21 10:54:23.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6998 exec execpod-affinityxhnhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 21 10:54:24.405: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 21 10:54:24.405: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:54:24.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6998 exec execpod-affinityxhnhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.127.45 80'
Jan 21 10:54:24.992: INFO: stderr: "+ nc -v -t -w 2 10.10.127.45 80\n+ echo hostName\nConnection to 10.10.127.45 80 port [tcp/http] succeeded!\n"
Jan 21 10:54:24.992: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:54:24.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6998 exec execpod-affinityxhnhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.86 30474'
Jan 21 10:54:25.671: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.1.86 30474\nConnection to 10.10.1.86 30474 port [tcp/*] succeeded!\n"
Jan 21 10:54:25.671: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:54:25.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6998 exec execpod-affinityxhnhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.1.136 30474'
Jan 21 10:54:26.427: INFO: stderr: "+ nc -v -t -w 2 10.10.1.136 30474\n+ echo hostName\nConnection to 10.10.1.136 30474 port [tcp/*] succeeded!\n"
Jan 21 10:54:26.427: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 10:54:26.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6998 exec execpod-affinityxhnhh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.86:30474/ ; done'
Jan 21 10:54:27.243: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n"
Jan 21 10:54:27.243: INFO: stdout: "\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-25w5p\naffinity-nodeport-transition-tzr6j\naffinity-nodeport-transition-tzr6j\naffinity-nodeport-transition-tzr6j\naffinity-nodeport-transition-tzr6j\naffinity-nodeport-transition-tzr6j\naffinity-nodeport-transition-25w5p\naffinity-nodeport-transition-25w5p\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-25w5p"
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-25w5p
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-tzr6j
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-tzr6j
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-tzr6j
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-tzr6j
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-tzr6j
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-25w5p
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-25w5p
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:27.243: INFO: Received response from host: affinity-nodeport-transition-25w5p
Jan 21 10:54:27.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-6998 exec execpod-affinityxhnhh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.86:30474/ ; done'
Jan 21 10:54:28.109: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:30474/\n"
Jan 21 10:54:28.109: INFO: stdout: "\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh\naffinity-nodeport-transition-zl4dh"
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Received response from host: affinity-nodeport-transition-zl4dh
Jan 21 10:54:28.109: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6998, will wait for the garbage collector to delete the pods
Jan 21 10:54:28.388: INFO: Deleting ReplicationController affinity-nodeport-transition took: 24.338652ms
Jan 21 10:54:29.491: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 1.103648259s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:54:38.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6998" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:37.670 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":278,"skipped":5653,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:54:38.624: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9245.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9245.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 10:54:49.796: INFO: DNS probes using dns-test-b9019c0e-511f-4771-8a54-eb894094ccd5 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9245.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9245.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 10:55:00.319: INFO: File wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 21 10:55:00.331: INFO: File jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 21 10:55:00.331: INFO: Lookups using dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 failed for: [wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local]

Jan 21 10:55:05.340: INFO: File wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 21 10:55:05.363: INFO: File jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 21 10:55:05.363: INFO: Lookups using dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 failed for: [wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local]

Jan 21 10:55:10.341: INFO: File wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 21 10:55:10.356: INFO: File jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 21 10:55:10.356: INFO: Lookups using dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 failed for: [wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local]

Jan 21 10:55:15.338: INFO: File wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 21 10:55:15.361: INFO: File jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 21 10:55:15.361: INFO: Lookups using dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 failed for: [wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local]

Jan 21 10:55:20.351: INFO: File wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local from pod  dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 contains '' instead of 'bar.example.com.'
Jan 21 10:55:20.362: INFO: Lookups using dns-9245/dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 failed for: [wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local]

Jan 21 10:55:25.381: INFO: DNS probes using dns-test-a0709803-693f-4507-b4db-b52dbb1d01d3 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9245.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9245.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9245.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9245.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 10:55:36.274: INFO: DNS probes using dns-test-241ddb5c-34bb-4b8f-8cc5-c30df259d98a succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:55:36.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9245" for this suite.

• [SLOW TEST:58.060 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":279,"skipped":5657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:55:36.687: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-9384
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9384
STEP: Deleting pre-stop pod
Jan 21 10:56:00.532: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:56:00.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9384" for this suite.

• [SLOW TEST:24.117 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":280,"skipped":5684,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:56:00.806: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jan 21 10:56:33.192: INFO: EndpointSlice for Service endpointslice-308/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:56:43.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-308" for this suite.

• [SLOW TEST:42.642 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":281,"skipped":5688,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:56:43.449: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 10:56:43.931: INFO: Create a RollingUpdate DaemonSet
Jan 21 10:56:43.958: INFO: Check that daemon pods launch on every node of the cluster
Jan 21 10:56:44.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:56:44.032: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:45.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:56:45.074: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:46.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:56:46.077: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:47.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:56:47.080: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:48.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:56:48.200: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:49.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:56:49.216: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:50.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:56:50.465: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:51.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:56:51.140: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:52.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:56:52.268: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:53.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 10:56:53.326: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 10:56:54.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 10:56:54.247: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 21 10:56:54.247: INFO: Update the DaemonSet to trigger a rollout
Jan 21 10:56:54.350: INFO: Updating DaemonSet daemon-set
Jan 21 10:57:03.642: INFO: Roll back the DaemonSet before rollout is complete
Jan 21 10:57:03.699: INFO: Updating DaemonSet daemon-set
Jan 21 10:57:03.699: INFO: Make sure DaemonSet rollback is complete
Jan 21 10:57:03.761: INFO: Wrong image for pod: daemon-set-9tfwx. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 21 10:57:03.761: INFO: Pod daemon-set-9tfwx is not available
Jan 21 10:57:12.061: INFO: Pod daemon-set-hsj7d is not available
Jan 21 10:57:12.953: INFO: Pod daemon-set-hsj7d is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3097, will wait for the garbage collector to delete the pods
Jan 21 10:57:13.098: INFO: Deleting DaemonSet.extensions daemon-set took: 18.898564ms
Jan 21 10:57:13.499: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.753716ms
Jan 21 10:57:21.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 10:57:21.431: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 21 10:57:21.451: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30200"},"items":null}

Jan 21 10:57:21.466: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30200"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:57:21.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3097" for this suite.

• [SLOW TEST:38.132 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":282,"skipped":5705,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:57:21.583: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jan 21 10:57:21.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 create -f -'
Jan 21 10:57:22.650: INFO: stderr: ""
Jan 21 10:57:22.650: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 10:57:22.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 21 10:57:23.354: INFO: stderr: ""
Jan 21 10:57:23.354: INFO: stdout: "update-demo-nautilus-v6lfc update-demo-nautilus-xbpnq "
Jan 21 10:57:23.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-v6lfc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 10:57:23.644: INFO: stderr: ""
Jan 21 10:57:23.644: INFO: stdout: ""
Jan 21 10:57:23.644: INFO: update-demo-nautilus-v6lfc is created but not running
Jan 21 10:57:28.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 21 10:57:29.384: INFO: stderr: ""
Jan 21 10:57:29.384: INFO: stdout: "update-demo-nautilus-v6lfc update-demo-nautilus-xbpnq "
Jan 21 10:57:29.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-v6lfc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 10:57:30.182: INFO: stderr: ""
Jan 21 10:57:30.182: INFO: stdout: "true"
Jan 21 10:57:30.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-v6lfc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 21 10:57:30.519: INFO: stderr: ""
Jan 21 10:57:30.520: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 21 10:57:30.520: INFO: validating pod update-demo-nautilus-v6lfc
Jan 21 10:57:30.559: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 10:57:30.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 10:57:30.559: INFO: update-demo-nautilus-v6lfc is verified up and running
Jan 21 10:57:30.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-xbpnq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 10:57:30.788: INFO: stderr: ""
Jan 21 10:57:30.788: INFO: stdout: "true"
Jan 21 10:57:30.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-xbpnq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 21 10:57:31.004: INFO: stderr: ""
Jan 21 10:57:31.004: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 21 10:57:31.004: INFO: validating pod update-demo-nautilus-xbpnq
Jan 21 10:57:31.020: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 10:57:31.020: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 10:57:31.020: INFO: update-demo-nautilus-xbpnq is verified up and running
STEP: scaling down the replication controller
Jan 21 10:57:31.036: INFO: scanned /root for discovery docs: <nil>
Jan 21 10:57:31.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 21 10:57:32.347: INFO: stderr: ""
Jan 21 10:57:32.347: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 10:57:32.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 21 10:57:32.732: INFO: stderr: ""
Jan 21 10:57:32.732: INFO: stdout: "update-demo-nautilus-v6lfc update-demo-nautilus-xbpnq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 21 10:57:37.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 21 10:57:38.005: INFO: stderr: ""
Jan 21 10:57:38.005: INFO: stdout: "update-demo-nautilus-v6lfc "
Jan 21 10:57:38.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-v6lfc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 10:57:39.136: INFO: stderr: ""
Jan 21 10:57:39.136: INFO: stdout: "true"
Jan 21 10:57:39.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-v6lfc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 21 10:57:39.710: INFO: stderr: ""
Jan 21 10:57:39.710: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 21 10:57:39.710: INFO: validating pod update-demo-nautilus-v6lfc
Jan 21 10:57:39.722: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 10:57:39.722: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 10:57:39.722: INFO: update-demo-nautilus-v6lfc is verified up and running
STEP: scaling up the replication controller
Jan 21 10:57:39.727: INFO: scanned /root for discovery docs: <nil>
Jan 21 10:57:39.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 21 10:57:41.244: INFO: stderr: ""
Jan 21 10:57:41.244: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 10:57:41.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 21 10:57:41.502: INFO: stderr: ""
Jan 21 10:57:41.502: INFO: stdout: "update-demo-nautilus-gf4bs update-demo-nautilus-v6lfc "
Jan 21 10:57:41.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-gf4bs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 10:57:42.031: INFO: stderr: ""
Jan 21 10:57:42.031: INFO: stdout: ""
Jan 21 10:57:42.031: INFO: update-demo-nautilus-gf4bs is created but not running
Jan 21 10:57:47.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 21 10:57:47.294: INFO: stderr: ""
Jan 21 10:57:47.294: INFO: stdout: "update-demo-nautilus-gf4bs update-demo-nautilus-v6lfc "
Jan 21 10:57:47.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-gf4bs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 10:57:47.569: INFO: stderr: ""
Jan 21 10:57:47.569: INFO: stdout: "true"
Jan 21 10:57:47.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-gf4bs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 21 10:57:47.932: INFO: stderr: ""
Jan 21 10:57:47.932: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 21 10:57:47.932: INFO: validating pod update-demo-nautilus-gf4bs
Jan 21 10:57:47.954: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 10:57:47.954: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 10:57:47.954: INFO: update-demo-nautilus-gf4bs is verified up and running
Jan 21 10:57:47.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-v6lfc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 21 10:57:48.358: INFO: stderr: ""
Jan 21 10:57:48.358: INFO: stdout: "true"
Jan 21 10:57:48.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods update-demo-nautilus-v6lfc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 21 10:57:49.547: INFO: stderr: ""
Jan 21 10:57:49.547: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 21 10:57:49.547: INFO: validating pod update-demo-nautilus-v6lfc
Jan 21 10:57:49.564: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 10:57:49.565: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 10:57:49.565: INFO: update-demo-nautilus-v6lfc is verified up and running
STEP: using delete to clean up resources
Jan 21 10:57:49.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 delete --grace-period=0 --force -f -'
Jan 21 10:57:49.961: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 10:57:49.961: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 21 10:57:49.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get rc,svc -l name=update-demo --no-headers'
Jan 21 10:57:50.663: INFO: stderr: "No resources found in kubectl-3883 namespace.\n"
Jan 21 10:57:50.663: INFO: stdout: ""
Jan 21 10:57:50.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-3883 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 21 10:57:51.217: INFO: stderr: ""
Jan 21 10:57:51.217: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:57:51.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3883" for this suite.

• [SLOW TEST:29.818 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":283,"skipped":5705,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:57:51.401: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 10:57:52.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb" in namespace "downward-api-3408" to be "Succeeded or Failed"
Jan 21 10:57:52.272: INFO: Pod "downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb": Phase="Pending", Reason="", readiness=false. Elapsed: 84.329286ms
Jan 21 10:57:54.349: INFO: Pod "downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1620403s
Jan 21 10:57:56.541: INFO: Pod "downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.353546087s
Jan 21 10:57:58.920: INFO: Pod "downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.732254685s
Jan 21 10:58:00.962: INFO: Pod "downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.774513175s
Jan 21 10:58:03.032: INFO: Pod "downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.844171602s
STEP: Saw pod success
Jan 21 10:58:03.032: INFO: Pod "downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb" satisfied condition "Succeeded or Failed"
Jan 21 10:58:03.056: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb container client-container: <nil>
STEP: delete the pod
Jan 21 10:58:03.249: INFO: Waiting for pod downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb to disappear
Jan 21 10:58:03.326: INFO: Pod downwardapi-volume-5fba12c5-804b-410d-8544-f93603d171bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 10:58:03.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3408" for this suite.

• [SLOW TEST:12.083 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":5708,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 10:58:03.743: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-16312978-9b03-4c63-8482-772b5df3225a in namespace container-probe-5161
Jan 21 10:58:08.604: INFO: Started pod busybox-16312978-9b03-4c63-8482-772b5df3225a in namespace container-probe-5161
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 10:58:08.847: INFO: Initial restart count of pod busybox-16312978-9b03-4c63-8482-772b5df3225a is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:02:09.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5161" for this suite.

• [SLOW TEST:245.498 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5723,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:02:09.247: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:02:22.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3213" for this suite.

• [SLOW TEST:13.454 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":286,"skipped":5723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:02:22.709: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 21 11:02:23.383: INFO: Pod name wrapped-volume-race-a6d81337-1697-4892-98b4-031e85017c42: Found 0 pods out of 5
Jan 21 11:02:28.471: INFO: Pod name wrapped-volume-race-a6d81337-1697-4892-98b4-031e85017c42: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a6d81337-1697-4892-98b4-031e85017c42 in namespace emptydir-wrapper-7413, will wait for the garbage collector to delete the pods
Jan 21 11:02:45.372: INFO: Deleting ReplicationController wrapped-volume-race-a6d81337-1697-4892-98b4-031e85017c42 took: 36.588383ms
Jan 21 11:02:46.175: INFO: Terminating ReplicationController wrapped-volume-race-a6d81337-1697-4892-98b4-031e85017c42 pods took: 802.939858ms
STEP: Creating RC which spawns configmap-volume pods
Jan 21 11:02:54.170: INFO: Pod name wrapped-volume-race-bf2e8fb0-c549-4d1f-b050-0d55dde69ea3: Found 0 pods out of 5
Jan 21 11:02:59.186: INFO: Pod name wrapped-volume-race-bf2e8fb0-c549-4d1f-b050-0d55dde69ea3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bf2e8fb0-c549-4d1f-b050-0d55dde69ea3 in namespace emptydir-wrapper-7413, will wait for the garbage collector to delete the pods
Jan 21 11:03:13.375: INFO: Deleting ReplicationController wrapped-volume-race-bf2e8fb0-c549-4d1f-b050-0d55dde69ea3 took: 18.764999ms
Jan 21 11:03:13.675: INFO: Terminating ReplicationController wrapped-volume-race-bf2e8fb0-c549-4d1f-b050-0d55dde69ea3 pods took: 300.438651ms
STEP: Creating RC which spawns configmap-volume pods
Jan 21 11:03:21.646: INFO: Pod name wrapped-volume-race-78c3ed17-f2d0-4c51-8c6d-b8e8ab918442: Found 0 pods out of 5
Jan 21 11:03:26.663: INFO: Pod name wrapped-volume-race-78c3ed17-f2d0-4c51-8c6d-b8e8ab918442: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-78c3ed17-f2d0-4c51-8c6d-b8e8ab918442 in namespace emptydir-wrapper-7413, will wait for the garbage collector to delete the pods
Jan 21 11:03:44.802: INFO: Deleting ReplicationController wrapped-volume-race-78c3ed17-f2d0-4c51-8c6d-b8e8ab918442 took: 12.660374ms
Jan 21 11:03:45.103: INFO: Terminating ReplicationController wrapped-volume-race-78c3ed17-f2d0-4c51-8c6d-b8e8ab918442 pods took: 300.833768ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:03:52.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7413" for this suite.

• [SLOW TEST:89.384 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":287,"skipped":5766,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:03:52.094: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 21 11:03:52.293: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 21 11:03:52.303: INFO: starting watch
STEP: patching
STEP: updating
Jan 21 11:03:52.359: INFO: waiting for watch events with expected annotations
Jan 21 11:03:52.360: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:03:52.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8236" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":288,"skipped":5773,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:03:52.442: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-55eae50f-8779-4a55-84b1-f2252dfa1f54
STEP: Creating a pod to test consume secrets
Jan 21 11:03:52.641: INFO: Waiting up to 5m0s for pod "pod-secrets-71d739d6-04cd-42a0-8e26-b5344af8443a" in namespace "secrets-9337" to be "Succeeded or Failed"
Jan 21 11:03:52.659: INFO: Pod "pod-secrets-71d739d6-04cd-42a0-8e26-b5344af8443a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.668184ms
Jan 21 11:03:54.674: INFO: Pod "pod-secrets-71d739d6-04cd-42a0-8e26-b5344af8443a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032419084s
Jan 21 11:03:56.685: INFO: Pod "pod-secrets-71d739d6-04cd-42a0-8e26-b5344af8443a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043395282s
STEP: Saw pod success
Jan 21 11:03:56.685: INFO: Pod "pod-secrets-71d739d6-04cd-42a0-8e26-b5344af8443a" satisfied condition "Succeeded or Failed"
Jan 21 11:03:56.692: INFO: Trying to get logs from node conformance1 pod pod-secrets-71d739d6-04cd-42a0-8e26-b5344af8443a container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 11:03:56.992: INFO: Waiting for pod pod-secrets-71d739d6-04cd-42a0-8e26-b5344af8443a to disappear
Jan 21 11:03:57.013: INFO: Pod pod-secrets-71d739d6-04cd-42a0-8e26-b5344af8443a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:03:57.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9337" for this suite.
STEP: Destroying namespace "secret-namespace-5220" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":5783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:03:57.061: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jan 21 11:03:57.447: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:03:59.491: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:04:01.484: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:04:03.472: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:04:05.458: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:04:07.461: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jan 21 11:04:07.500: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:04:09.520: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:04:11.516: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 21 11:04:11.524: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:11.524: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:11.525: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:11.526: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:11.741: INFO: Exec stderr: ""
Jan 21 11:04:11.741: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:11.741: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:11.742: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:11.742: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:11.959: INFO: Exec stderr: ""
Jan 21 11:04:11.959: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:11.959: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:11.961: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:11.961: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:12.163: INFO: Exec stderr: ""
Jan 21 11:04:12.163: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:12.163: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:12.164: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:12.164: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:12.413: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 21 11:04:12.414: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:12.414: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:12.428: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:12.428: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:12.619: INFO: Exec stderr: ""
Jan 21 11:04:12.619: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:12.620: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:12.621: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:12.793: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 21 11:04:12.793: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:12.793: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:12.794: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:12.794: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:12.993: INFO: Exec stderr: ""
Jan 21 11:04:12.993: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:12.993: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:12.999: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:13.000: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:13.196: INFO: Exec stderr: ""
Jan 21 11:04:13.196: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:13.196: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:13.197: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:13.198: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:13.410: INFO: Exec stderr: ""
Jan 21 11:04:13.410: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 21 11:04:13.410: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
Jan 21 11:04:13.415: INFO: ExecWithOptions: Clientset creation
Jan 21 11:04:13.416: INFO: ExecWithOptions: execute(POST https://10.10.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2637/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jan 21 11:04:13.609: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:04:13.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2637" for this suite.

• [SLOW TEST:16.575 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":290,"skipped":5806,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:04:13.639: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 21 11:04:13.800: INFO: The status of Pod pod-update-activedeadlineseconds-08646ba4-5620-43be-8d36-d7098154feca is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:04:15.844: INFO: The status of Pod pod-update-activedeadlineseconds-08646ba4-5620-43be-8d36-d7098154feca is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:04:17.810: INFO: The status of Pod pod-update-activedeadlineseconds-08646ba4-5620-43be-8d36-d7098154feca is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 21 11:04:18.371: INFO: Successfully updated pod "pod-update-activedeadlineseconds-08646ba4-5620-43be-8d36-d7098154feca"
Jan 21 11:04:18.381: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-08646ba4-5620-43be-8d36-d7098154feca" in namespace "pods-8813" to be "terminated due to deadline exceeded"
Jan 21 11:04:18.406: INFO: Pod "pod-update-activedeadlineseconds-08646ba4-5620-43be-8d36-d7098154feca": Phase="Running", Reason="", readiness=true. Elapsed: 24.687027ms
Jan 21 11:04:20.420: INFO: Pod "pod-update-activedeadlineseconds-08646ba4-5620-43be-8d36-d7098154feca": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 2.038761033s
Jan 21 11:04:20.421: INFO: Pod "pod-update-activedeadlineseconds-08646ba4-5620-43be-8d36-d7098154feca" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:04:20.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8813" for this suite.

• [SLOW TEST:6.832 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5812,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:04:20.477: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jan 21 11:04:20.760: INFO: observed Pod pod-test in namespace pods-9753 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 21 11:04:20.760: INFO: observed Pod pod-test in namespace pods-9753 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC  }]
Jan 21 11:04:20.803: INFO: observed Pod pod-test in namespace pods-9753 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC  }]
Jan 21 11:04:23.600: INFO: observed Pod pod-test in namespace pods-9753 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC  }]
Jan 21 11:04:24.211: INFO: Found Pod pod-test in namespace pods-9753 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-21 11:04:20 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jan 21 11:04:24.303: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jan 21 11:04:24.428: INFO: observed event type ADDED
Jan 21 11:04:24.428: INFO: observed event type MODIFIED
Jan 21 11:04:24.428: INFO: observed event type MODIFIED
Jan 21 11:04:24.429: INFO: observed event type MODIFIED
Jan 21 11:04:24.429: INFO: observed event type MODIFIED
Jan 21 11:04:24.430: INFO: observed event type MODIFIED
Jan 21 11:04:24.430: INFO: observed event type MODIFIED
Jan 21 11:04:24.431: INFO: observed event type MODIFIED
Jan 21 11:04:26.269: INFO: observed event type MODIFIED
Jan 21 11:04:26.898: INFO: observed event type MODIFIED
Jan 21 11:04:28.627: INFO: observed event type MODIFIED
Jan 21 11:04:28.711: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:04:28.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9753" for this suite.

• [SLOW TEST:8.409 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":292,"skipped":5828,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:04:28.886: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 21 11:04:29.101: INFO: Waiting up to 5m0s for pod "pod-c6120f6f-0ef2-4cc7-b2ff-7934a19b3ff4" in namespace "emptydir-6152" to be "Succeeded or Failed"
Jan 21 11:04:29.148: INFO: Pod "pod-c6120f6f-0ef2-4cc7-b2ff-7934a19b3ff4": Phase="Pending", Reason="", readiness=false. Elapsed: 47.211426ms
Jan 21 11:04:31.176: INFO: Pod "pod-c6120f6f-0ef2-4cc7-b2ff-7934a19b3ff4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075000222s
Jan 21 11:04:33.190: INFO: Pod "pod-c6120f6f-0ef2-4cc7-b2ff-7934a19b3ff4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089880408s
STEP: Saw pod success
Jan 21 11:04:33.191: INFO: Pod "pod-c6120f6f-0ef2-4cc7-b2ff-7934a19b3ff4" satisfied condition "Succeeded or Failed"
Jan 21 11:04:33.202: INFO: Trying to get logs from node conformance1 pod pod-c6120f6f-0ef2-4cc7-b2ff-7934a19b3ff4 container test-container: <nil>
STEP: delete the pod
Jan 21 11:04:33.247: INFO: Waiting for pod pod-c6120f6f-0ef2-4cc7-b2ff-7934a19b3ff4 to disappear
Jan 21 11:04:33.257: INFO: Pod pod-c6120f6f-0ef2-4cc7-b2ff-7934a19b3ff4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:04:33.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6152" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:04:33.291: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 21 11:04:33.380: INFO: PodSpec: initContainers in spec.initContainers
Jan 21 11:05:19.759: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-81c61763-b843-49f8-8b07-be78227ef5bb", GenerateName:"", Namespace:"init-container-5895", SelfLink:"", UID:"c2306c0e-550e-4860-a4e2-47d883dbcf49", ResourceVersion:"31853", Generation:0, CreationTimestamp:time.Date(2022, time.January, 21, 11, 4, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"380407595"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0680c612d128ff5f3232e2226309c0c1dfb8961561cc07944fe35703f338a5bf", "cni.projectcalico.org/podIP":"172.16.209.83/32", "cni.projectcalico.org/podIPs":"172.16.209.83/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.January, 21, 11, 4, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004192918), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.January, 21, 11, 4, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004192948), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.January, 21, 11, 4, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004192990), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-z7r9h", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0041c2300), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z7r9h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z7r9h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z7r9h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005eedf20), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"conformance1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003a47730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005eedfc0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005eedfe0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005eedfe8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005eedfec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004a13560), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.January, 21, 11, 4, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.January, 21, 11, 4, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.January, 21, 11, 4, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.January, 21, 11, 4, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.1.86", PodIP:"172.16.209.83", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.209.83"}}, StartTime:time.Date(2022, time.January, 21, 11, 4, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003a47810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003a47880)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"docker://4cb81e1cdd8f1b8c080c9cb829e5a5711e5ad5ee9a7e2f1794d4b65655f9b969", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041c2380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041c2360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc0086b406f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:05:19.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5895" for this suite.

• [SLOW TEST:46.504 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":294,"skipped":5871,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:05:19.818: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jan 21 11:05:19.963: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:05:21.988: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:05:23.980: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 21 11:05:25.034: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:05:25.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4019" for this suite.

• [SLOW TEST:5.530 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":295,"skipped":5885,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:05:25.329: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-07a75a12-5829-4f4e-a739-750d497dd906
STEP: Creating configMap with name cm-test-opt-upd-e9caaeaf-34c7-4610-b373-e03d4ca81c4f
STEP: Creating the pod
Jan 21 11:05:25.559: INFO: The status of Pod pod-configmaps-3a4898d6-45a2-4fe6-8f7d-2541022d1e95 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:05:27.575: INFO: The status of Pod pod-configmaps-3a4898d6-45a2-4fe6-8f7d-2541022d1e95 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:05:29.596: INFO: The status of Pod pod-configmaps-3a4898d6-45a2-4fe6-8f7d-2541022d1e95 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:05:31.627: INFO: The status of Pod pod-configmaps-3a4898d6-45a2-4fe6-8f7d-2541022d1e95 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:05:33.577: INFO: The status of Pod pod-configmaps-3a4898d6-45a2-4fe6-8f7d-2541022d1e95 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-07a75a12-5829-4f4e-a739-750d497dd906
STEP: Updating configmap cm-test-opt-upd-e9caaeaf-34c7-4610-b373-e03d4ca81c4f
STEP: Creating configMap with name cm-test-opt-create-ddaf0f9f-dfda-454b-a95b-6830d7eff2af
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:05:35.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6489" for this suite.

• [SLOW TEST:10.478 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5892,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:05:35.807: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9326
STEP: creating service affinity-clusterip in namespace services-9326
STEP: creating replication controller affinity-clusterip in namespace services-9326
I0121 11:05:35.954148      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9326, replica count: 3
I0121 11:05:39.005901      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 11:05:42.007658      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 11:05:45.008749      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 11:05:45.042: INFO: Creating new exec pod
Jan 21 11:05:50.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9326 exec execpod-affinityz55nl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 21 11:05:50.750: INFO: stderr: "+ + nc -v -t -w 2 affinity-clusterip 80\necho hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 21 11:05:50.750: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 11:05:50.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9326 exec execpod-affinityz55nl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.205.161 80'
Jan 21 11:05:51.298: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.205.161 80\nConnection to 10.10.205.161 80 port [tcp/http] succeeded!\n"
Jan 21 11:05:51.298: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 21 11:05:51.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=services-9326 exec execpod-affinityz55nl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.205.161:80/ ; done'
Jan 21 11:05:51.906: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.205.161:80/\n"
Jan 21 11:05:51.906: INFO: stdout: "\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc\naffinity-clusterip-k8nhc"
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Received response from host: affinity-clusterip-k8nhc
Jan 21 11:05:51.906: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9326, will wait for the garbage collector to delete the pods
Jan 21 11:05:52.108: INFO: Deleting ReplicationController affinity-clusterip took: 40.110801ms
Jan 21 11:05:52.411: INFO: Terminating ReplicationController affinity-clusterip pods took: 303.10248ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:05:56.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9326" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:20.699 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":297,"skipped":5894,"failed":0}
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:05:56.548: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 21 11:05:56.675: INFO: The status of Pod annotationupdate0869441a-a4e1-48f3-bd40-937d1219cd64 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:05:58.700: INFO: The status of Pod annotationupdate0869441a-a4e1-48f3-bd40-937d1219cd64 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:06:00.686: INFO: The status of Pod annotationupdate0869441a-a4e1-48f3-bd40-937d1219cd64 is Running (Ready = true)
Jan 21 11:06:01.236: INFO: Successfully updated pod "annotationupdate0869441a-a4e1-48f3-bd40-937d1219cd64"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:03.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5075" for this suite.

• [SLOW TEST:6.798 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":298,"skipped":5894,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:03.347: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 21 11:06:03.460: INFO: Waiting up to 5m0s for pod "pod-a0cc5546-2d6e-4c4b-8b88-327111dbea39" in namespace "emptydir-2286" to be "Succeeded or Failed"
Jan 21 11:06:03.471: INFO: Pod "pod-a0cc5546-2d6e-4c4b-8b88-327111dbea39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.88718ms
Jan 21 11:06:05.488: INFO: Pod "pod-a0cc5546-2d6e-4c4b-8b88-327111dbea39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027772527s
Jan 21 11:06:07.502: INFO: Pod "pod-a0cc5546-2d6e-4c4b-8b88-327111dbea39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042206286s
STEP: Saw pod success
Jan 21 11:06:07.502: INFO: Pod "pod-a0cc5546-2d6e-4c4b-8b88-327111dbea39" satisfied condition "Succeeded or Failed"
Jan 21 11:06:07.508: INFO: Trying to get logs from node conformance1 pod pod-a0cc5546-2d6e-4c4b-8b88-327111dbea39 container test-container: <nil>
STEP: delete the pod
Jan 21 11:06:07.562: INFO: Waiting for pod pod-a0cc5546-2d6e-4c4b-8b88-327111dbea39 to disappear
Jan 21 11:06:07.569: INFO: Pod pod-a0cc5546-2d6e-4c4b-8b88-327111dbea39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:07.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2286" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5908,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:07.610: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:06:07.733: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4d263d71-d622-475d-ab75-127bdada75e8" in namespace "security-context-test-2221" to be "Succeeded or Failed"
Jan 21 11:06:07.751: INFO: Pod "busybox-user-65534-4d263d71-d622-475d-ab75-127bdada75e8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.211391ms
Jan 21 11:06:09.772: INFO: Pod "busybox-user-65534-4d263d71-d622-475d-ab75-127bdada75e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039075963s
Jan 21 11:06:11.802: INFO: Pod "busybox-user-65534-4d263d71-d622-475d-ab75-127bdada75e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06902937s
Jan 21 11:06:13.828: INFO: Pod "busybox-user-65534-4d263d71-d622-475d-ab75-127bdada75e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095322325s
Jan 21 11:06:13.829: INFO: Pod "busybox-user-65534-4d263d71-d622-475d-ab75-127bdada75e8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:13.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2221" for this suite.

• [SLOW TEST:6.254 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:50
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5916,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:13.869: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 21 11:06:14.018: INFO: Waiting up to 5m0s for pod "downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d" in namespace "downward-api-1520" to be "Succeeded or Failed"
Jan 21 11:06:14.031: INFO: Pod "downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.130698ms
Jan 21 11:06:16.058: INFO: Pod "downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040273412s
Jan 21 11:06:18.066: INFO: Pod "downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047995495s
Jan 21 11:06:20.077: INFO: Pod "downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059232287s
STEP: Saw pod success
Jan 21 11:06:20.077: INFO: Pod "downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d" satisfied condition "Succeeded or Failed"
Jan 21 11:06:20.086: INFO: Trying to get logs from node conformance1 pod downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d container dapi-container: <nil>
STEP: delete the pod
Jan 21 11:06:20.148: INFO: Waiting for pod downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d to disappear
Jan 21 11:06:20.156: INFO: Pod downward-api-856d9fd7-bb90-4369-af58-7ee32c79963d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:20.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1520" for this suite.

• [SLOW TEST:6.313 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:20.183: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 21 11:06:20.351: INFO: Waiting up to 5m0s for pod "downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0" in namespace "downward-api-4273" to be "Succeeded or Failed"
Jan 21 11:06:20.411: INFO: Pod "downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 60.447446ms
Jan 21 11:06:22.450: INFO: Pod "downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099150265s
Jan 21 11:06:24.462: INFO: Pod "downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.111030745s
Jan 21 11:06:26.484: INFO: Pod "downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.132676365s
STEP: Saw pod success
Jan 21 11:06:26.487: INFO: Pod "downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0" satisfied condition "Succeeded or Failed"
Jan 21 11:06:26.500: INFO: Trying to get logs from node conformance1 pod downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0 container dapi-container: <nil>
STEP: delete the pod
Jan 21 11:06:26.574: INFO: Waiting for pod downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0 to disappear
Jan 21 11:06:26.581: INFO: Pod downward-api-ffd2c490-cec1-4581-9e13-1891de0e0ab0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:26.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4273" for this suite.

• [SLOW TEST:6.440 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5945,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:26.626: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:06:26.796: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 21 11:06:27.905: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:27.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3664" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":303,"skipped":5947,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:27.979: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 21 11:06:28.157: INFO: The status of Pod labelsupdate2bc83c1d-eb63-468c-8919-3e7f1cd61dae is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:06:30.166: INFO: The status of Pod labelsupdate2bc83c1d-eb63-468c-8919-3e7f1cd61dae is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:06:32.213: INFO: The status of Pod labelsupdate2bc83c1d-eb63-468c-8919-3e7f1cd61dae is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:06:34.208: INFO: The status of Pod labelsupdate2bc83c1d-eb63-468c-8919-3e7f1cd61dae is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:06:36.172: INFO: The status of Pod labelsupdate2bc83c1d-eb63-468c-8919-3e7f1cd61dae is Running (Ready = true)
Jan 21 11:06:36.734: INFO: Successfully updated pod "labelsupdate2bc83c1d-eb63-468c-8919-3e7f1cd61dae"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:38.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6334" for this suite.

• [SLOW TEST:10.858 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5952,"failed":0}
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:38.841: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Jan 21 11:06:38.936: INFO: Major version: 1
STEP: Confirm minor version
Jan 21 11:06:38.936: INFO: cleanMinorVersion: 23
Jan 21 11:06:38.936: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:38.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6014" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":305,"skipped":5952,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:38.968: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:06:39.086: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c915a40e-bb69-403d-8985-62de787b2e69" in namespace "security-context-test-3750" to be "Succeeded or Failed"
Jan 21 11:06:39.102: INFO: Pod "busybox-readonly-false-c915a40e-bb69-403d-8985-62de787b2e69": Phase="Pending", Reason="", readiness=false. Elapsed: 15.789933ms
Jan 21 11:06:41.138: INFO: Pod "busybox-readonly-false-c915a40e-bb69-403d-8985-62de787b2e69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051685813s
Jan 21 11:06:43.157: INFO: Pod "busybox-readonly-false-c915a40e-bb69-403d-8985-62de787b2e69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071266659s
Jan 21 11:06:43.157: INFO: Pod "busybox-readonly-false-c915a40e-bb69-403d-8985-62de787b2e69" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:43.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3750" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":5968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:43.185: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-9d39df0c-0b49-4446-ad23-b2d7ec06fd36
STEP: Creating a pod to test consume configMaps
Jan 21 11:06:43.396: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9" in namespace "projected-3747" to be "Succeeded or Failed"
Jan 21 11:06:43.444: INFO: Pod "pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9": Phase="Pending", Reason="", readiness=false. Elapsed: 41.023959ms
Jan 21 11:06:45.483: INFO: Pod "pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080341556s
Jan 21 11:06:47.521: INFO: Pod "pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118187262s
Jan 21 11:06:49.573: INFO: Pod "pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.170360483s
STEP: Saw pod success
Jan 21 11:06:49.573: INFO: Pod "pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9" satisfied condition "Succeeded or Failed"
Jan 21 11:06:49.636: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 11:06:49.729: INFO: Waiting for pod pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9 to disappear
Jan 21 11:06:49.739: INFO: Pod pod-projected-configmaps-8ea584fc-4759-4ced-89e4-ea123c6cf7a9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:49.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3747" for this suite.

• [SLOW TEST:6.581 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":307,"skipped":6001,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:49.778: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Jan 21 11:06:49.942: INFO: Waiting up to 5m0s for pod "client-containers-1740e1af-cf11-4419-a7a9-3280d517cf4b" in namespace "containers-214" to be "Succeeded or Failed"
Jan 21 11:06:49.960: INFO: Pod "client-containers-1740e1af-cf11-4419-a7a9-3280d517cf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.049328ms
Jan 21 11:06:51.999: INFO: Pod "client-containers-1740e1af-cf11-4419-a7a9-3280d517cf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05722146s
Jan 21 11:06:54.009: INFO: Pod "client-containers-1740e1af-cf11-4419-a7a9-3280d517cf4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066625389s
STEP: Saw pod success
Jan 21 11:06:54.009: INFO: Pod "client-containers-1740e1af-cf11-4419-a7a9-3280d517cf4b" satisfied condition "Succeeded or Failed"
Jan 21 11:06:54.015: INFO: Trying to get logs from node conformance1 pod client-containers-1740e1af-cf11-4419-a7a9-3280d517cf4b container agnhost-container: <nil>
STEP: delete the pod
Jan 21 11:06:54.099: INFO: Waiting for pod client-containers-1740e1af-cf11-4419-a7a9-3280d517cf4b to disappear
Jan 21 11:06:54.106: INFO: Pod client-containers-1740e1af-cf11-4419-a7a9-3280d517cf4b no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:06:54.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-214" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":6032,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:06:54.160: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 11:06:56.099: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 21 11:06:58.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 11, 6, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 6, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 11, 6, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 6, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 11:07:00.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 11, 6, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 6, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 11, 6, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 6, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 11:07:03.187: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:07:03.201: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3019-crds.webhook.example.com via the AdmissionRegistration API
Jan 21 11:07:03.793: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:06.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5725" for this suite.
STEP: Destroying namespace "webhook-5725-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.175 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":309,"skipped":6049,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:07.335: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Jan 21 11:07:07.887: INFO: Waiting up to 5m0s for pod "client-containers-288e7dd7-6c7a-43a3-9571-995b47b442e4" in namespace "containers-4876" to be "Succeeded or Failed"
Jan 21 11:07:07.968: INFO: Pod "client-containers-288e7dd7-6c7a-43a3-9571-995b47b442e4": Phase="Pending", Reason="", readiness=false. Elapsed: 80.172478ms
Jan 21 11:07:09.982: INFO: Pod "client-containers-288e7dd7-6c7a-43a3-9571-995b47b442e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094926267s
Jan 21 11:07:12.037: INFO: Pod "client-containers-288e7dd7-6c7a-43a3-9571-995b47b442e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.149052283s
STEP: Saw pod success
Jan 21 11:07:12.037: INFO: Pod "client-containers-288e7dd7-6c7a-43a3-9571-995b47b442e4" satisfied condition "Succeeded or Failed"
Jan 21 11:07:12.082: INFO: Trying to get logs from node conformance1 pod client-containers-288e7dd7-6c7a-43a3-9571-995b47b442e4 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 11:07:12.206: INFO: Waiting for pod client-containers-288e7dd7-6c7a-43a3-9571-995b47b442e4 to disappear
Jan 21 11:07:12.219: INFO: Pod client-containers-288e7dd7-6c7a-43a3-9571-995b47b442e4 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:12.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4876" for this suite.

• [SLOW TEST:5.030 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":6057,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:12.367: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 11:07:14.480: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 11:07:16.515: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 11, 7, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 7, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 11, 7, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 7, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 11:07:19.661: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:07:19.669: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:22.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6040" for this suite.
STEP: Destroying namespace "webhook-6040-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.971 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":311,"skipped":6065,"failed":0}
SSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:23.339: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 21 11:07:23.706: INFO: starting watch
STEP: patching
STEP: updating
Jan 21 11:07:23.746: INFO: waiting for watch events with expected annotations
Jan 21 11:07:23.746: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:23.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7757" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":312,"skipped":6068,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:23.849: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-7547d326-9eb4-4342-90c3-6d5b7cd63320
STEP: Creating a pod to test consume secrets
Jan 21 11:07:24.075: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186" in namespace "projected-510" to be "Succeeded or Failed"
Jan 21 11:07:24.094: INFO: Pod "pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186": Phase="Pending", Reason="", readiness=false. Elapsed: 18.419381ms
Jan 21 11:07:26.107: INFO: Pod "pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031838705s
Jan 21 11:07:28.175: INFO: Pod "pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098914425s
Jan 21 11:07:30.201: INFO: Pod "pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.12530605s
STEP: Saw pod success
Jan 21 11:07:30.201: INFO: Pod "pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186" satisfied condition "Succeeded or Failed"
Jan 21 11:07:30.245: INFO: Trying to get logs from node conformance1 pod pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 11:07:30.391: INFO: Waiting for pod pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186 to disappear
Jan 21 11:07:30.433: INFO: Pod pod-projected-secrets-934b2c68-3dee-49cc-8d27-fa6c6f689186 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:30.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-510" for this suite.

• [SLOW TEST:6.660 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":6074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:30.510: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6797.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6797.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6797.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6797.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 11:07:34.951: INFO: DNS probes using dns-6797/dns-test-7a15ef99-5803-4043-95a8-43e35d772b16 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:34.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6797" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":314,"skipped":6100,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:35.063: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Jan 21 11:07:35.232: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:07:37.257: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:07:39.251: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:07:41.259: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:42.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9140" for this suite.

• [SLOW TEST:7.269 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":315,"skipped":6102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:42.350: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 11:07:42.487: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01823749-36e9-4787-b263-e604e5f3be68" in namespace "downward-api-9421" to be "Succeeded or Failed"
Jan 21 11:07:42.498: INFO: Pod "downwardapi-volume-01823749-36e9-4787-b263-e604e5f3be68": Phase="Pending", Reason="", readiness=false. Elapsed: 10.588441ms
Jan 21 11:07:44.511: INFO: Pod "downwardapi-volume-01823749-36e9-4787-b263-e604e5f3be68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024430053s
Jan 21 11:07:46.527: INFO: Pod "downwardapi-volume-01823749-36e9-4787-b263-e604e5f3be68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040122916s
STEP: Saw pod success
Jan 21 11:07:46.527: INFO: Pod "downwardapi-volume-01823749-36e9-4787-b263-e604e5f3be68" satisfied condition "Succeeded or Failed"
Jan 21 11:07:46.536: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-01823749-36e9-4787-b263-e604e5f3be68 container client-container: <nil>
STEP: delete the pod
Jan 21 11:07:46.592: INFO: Waiting for pod downwardapi-volume-01823749-36e9-4787-b263-e604e5f3be68 to disappear
Jan 21 11:07:46.606: INFO: Pod downwardapi-volume-01823749-36e9-4787-b263-e604e5f3be68 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:46.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9421" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":6141,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:46.649: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 21 11:07:46.807: INFO: Waiting up to 5m0s for pod "pod-687c111c-e11b-4cd7-93d2-35ff19127290" in namespace "emptydir-4556" to be "Succeeded or Failed"
Jan 21 11:07:46.822: INFO: Pod "pod-687c111c-e11b-4cd7-93d2-35ff19127290": Phase="Pending", Reason="", readiness=false. Elapsed: 15.06332ms
Jan 21 11:07:49.012: INFO: Pod "pod-687c111c-e11b-4cd7-93d2-35ff19127290": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205478363s
Jan 21 11:07:51.050: INFO: Pod "pod-687c111c-e11b-4cd7-93d2-35ff19127290": Phase="Pending", Reason="", readiness=false. Elapsed: 4.243234994s
Jan 21 11:07:53.080: INFO: Pod "pod-687c111c-e11b-4cd7-93d2-35ff19127290": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.273680904s
STEP: Saw pod success
Jan 21 11:07:53.081: INFO: Pod "pod-687c111c-e11b-4cd7-93d2-35ff19127290" satisfied condition "Succeeded or Failed"
Jan 21 11:07:53.091: INFO: Trying to get logs from node conformance1 pod pod-687c111c-e11b-4cd7-93d2-35ff19127290 container test-container: <nil>
STEP: delete the pod
Jan 21 11:07:53.147: INFO: Waiting for pod pod-687c111c-e11b-4cd7-93d2-35ff19127290 to disappear
Jan 21 11:07:53.166: INFO: Pod pod-687c111c-e11b-4cd7-93d2-35ff19127290 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:53.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4556" for this suite.

• [SLOW TEST:6.545 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":317,"skipped":6149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:53.195: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 11:07:53.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da" in namespace "projected-4315" to be "Succeeded or Failed"
Jan 21 11:07:53.376: INFO: Pod "downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da": Phase="Pending", Reason="", readiness=false. Elapsed: 11.15978ms
Jan 21 11:07:55.393: INFO: Pod "downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027945698s
Jan 21 11:07:57.408: INFO: Pod "downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042595642s
Jan 21 11:07:59.445: INFO: Pod "downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079604825s
STEP: Saw pod success
Jan 21 11:07:59.445: INFO: Pod "downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da" satisfied condition "Succeeded or Failed"
Jan 21 11:07:59.451: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da container client-container: <nil>
STEP: delete the pod
Jan 21 11:07:59.540: INFO: Waiting for pod downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da to disappear
Jan 21 11:07:59.562: INFO: Pod downwardapi-volume-154082cf-2ac4-4ec0-aeca-4f44064079da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:07:59.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4315" for this suite.

• [SLOW TEST:6.431 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":6171,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:07:59.626: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-68c3fadc-3b8b-4b15-90c3-5d00bc0ae637
STEP: Creating a pod to test consume configMaps
Jan 21 11:07:59.809: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dbb85843-576f-4452-90d3-79dd93b34720" in namespace "projected-6501" to be "Succeeded or Failed"
Jan 21 11:07:59.880: INFO: Pod "pod-projected-configmaps-dbb85843-576f-4452-90d3-79dd93b34720": Phase="Pending", Reason="", readiness=false. Elapsed: 70.471681ms
Jan 21 11:08:01.905: INFO: Pod "pod-projected-configmaps-dbb85843-576f-4452-90d3-79dd93b34720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096103975s
Jan 21 11:08:03.924: INFO: Pod "pod-projected-configmaps-dbb85843-576f-4452-90d3-79dd93b34720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.115086411s
STEP: Saw pod success
Jan 21 11:08:03.924: INFO: Pod "pod-projected-configmaps-dbb85843-576f-4452-90d3-79dd93b34720" satisfied condition "Succeeded or Failed"
Jan 21 11:08:03.933: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-dbb85843-576f-4452-90d3-79dd93b34720 container agnhost-container: <nil>
STEP: delete the pod
Jan 21 11:08:04.007: INFO: Waiting for pod pod-projected-configmaps-dbb85843-576f-4452-90d3-79dd93b34720 to disappear
Jan 21 11:08:04.019: INFO: Pod pod-projected-configmaps-dbb85843-576f-4452-90d3-79dd93b34720 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:08:04.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6501" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":6185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:08:04.073: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:08:04.180: INFO: Creating pod...
Jan 21 11:08:04.235: INFO: Pod Quantity: 1 Status: Pending
Jan 21 11:08:05.244: INFO: Pod Quantity: 1 Status: Pending
Jan 21 11:08:06.246: INFO: Pod Quantity: 1 Status: Pending
Jan 21 11:08:07.252: INFO: Pod Quantity: 1 Status: Pending
Jan 21 11:08:08.245: INFO: Pod Quantity: 1 Status: Pending
Jan 21 11:08:09.271: INFO: Pod Status: Running
Jan 21 11:08:09.271: INFO: Creating service...
Jan 21 11:08:09.321: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/pods/agnhost/proxy/some/path/with/DELETE
Jan 21 11:08:09.376: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 21 11:08:09.376: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/pods/agnhost/proxy/some/path/with/GET
Jan 21 11:08:09.424: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 21 11:08:09.424: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/pods/agnhost/proxy/some/path/with/HEAD
Jan 21 11:08:09.462: INFO: http.Client request:HEAD | StatusCode:200
Jan 21 11:08:09.462: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 21 11:08:09.471: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 21 11:08:09.471: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/pods/agnhost/proxy/some/path/with/PATCH
Jan 21 11:08:09.491: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 21 11:08:09.491: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/pods/agnhost/proxy/some/path/with/POST
Jan 21 11:08:09.530: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 21 11:08:09.530: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/pods/agnhost/proxy/some/path/with/PUT
Jan 21 11:08:09.538: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 21 11:08:09.538: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/services/test-service/proxy/some/path/with/DELETE
Jan 21 11:08:09.555: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 21 11:08:09.556: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/services/test-service/proxy/some/path/with/GET
Jan 21 11:08:09.571: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 21 11:08:09.571: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/services/test-service/proxy/some/path/with/HEAD
Jan 21 11:08:09.609: INFO: http.Client request:HEAD | StatusCode:200
Jan 21 11:08:09.609: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/services/test-service/proxy/some/path/with/OPTIONS
Jan 21 11:08:09.636: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 21 11:08:09.636: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/services/test-service/proxy/some/path/with/PATCH
Jan 21 11:08:09.649: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 21 11:08:09.649: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/services/test-service/proxy/some/path/with/POST
Jan 21 11:08:09.669: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 21 11:08:09.669: INFO: Starting http.Client for https://10.10.0.1:443/api/v1/namespaces/proxy-9334/services/test-service/proxy/some/path/with/PUT
Jan 21 11:08:09.685: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:08:09.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9334" for this suite.

• [SLOW TEST:5.674 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":320,"skipped":6210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:08:09.747: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 21 11:08:09.976: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 21 11:09:10.042: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Jan 21 11:09:10.120: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 21 11:09:10.137: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 21 11:09:10.199: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 21 11:09:10.233: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:09:32.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1507" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:82.697 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":321,"skipped":6233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:09:32.446: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:09:32.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9402" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":322,"skipped":6263,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:09:32.735: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 21 11:09:32.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea1091f1-6699-4c94-864d-130c78d4063a" in namespace "downward-api-5885" to be "Succeeded or Failed"
Jan 21 11:09:32.906: INFO: Pod "downwardapi-volume-ea1091f1-6699-4c94-864d-130c78d4063a": Phase="Pending", Reason="", readiness=false. Elapsed: 58.474417ms
Jan 21 11:09:34.928: INFO: Pod "downwardapi-volume-ea1091f1-6699-4c94-864d-130c78d4063a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080528623s
Jan 21 11:09:36.943: INFO: Pod "downwardapi-volume-ea1091f1-6699-4c94-864d-130c78d4063a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.094809993s
STEP: Saw pod success
Jan 21 11:09:36.943: INFO: Pod "downwardapi-volume-ea1091f1-6699-4c94-864d-130c78d4063a" satisfied condition "Succeeded or Failed"
Jan 21 11:09:36.949: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-ea1091f1-6699-4c94-864d-130c78d4063a container client-container: <nil>
STEP: delete the pod
Jan 21 11:09:37.059: INFO: Waiting for pod downwardapi-volume-ea1091f1-6699-4c94-864d-130c78d4063a to disappear
Jan 21 11:09:37.075: INFO: Pod downwardapi-volume-ea1091f1-6699-4c94-864d-130c78d4063a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:09:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5885" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":6263,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:09:37.107: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 21 11:09:37.331: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 21 11:10:37.416: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Jan 21 11:10:37.482: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 21 11:10:37.562: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 21 11:10:37.646: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 21 11:10:37.667: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:10:51.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4925" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.877 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":324,"skipped":6282,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:10:51.984: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:10:56.173: INFO: Deleting pod "var-expansion-5c00c417-cf56-4434-8ce6-aa741bf33b28" in namespace "var-expansion-1017"
Jan 21 11:10:56.196: INFO: Wait up to 5m0s for pod "var-expansion-5c00c417-cf56-4434-8ce6-aa741bf33b28" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:02.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1017" for this suite.

• [SLOW TEST:10.286 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":325,"skipped":6292,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:02.271: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:11:02.456: INFO: The status of Pod busybox-readonly-fs655cf93a-24ee-42d5-9d84-8a8ca922761c is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:11:04.471: INFO: The status of Pod busybox-readonly-fs655cf93a-24ee-42d5-9d84-8a8ca922761c is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:11:06.477: INFO: The status of Pod busybox-readonly-fs655cf93a-24ee-42d5-9d84-8a8ca922761c is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:11:08.504: INFO: The status of Pod busybox-readonly-fs655cf93a-24ee-42d5-9d84-8a8ca922761c is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:08.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6139" for this suite.

• [SLOW TEST:6.362 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6297,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:08.640: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Jan 21 11:11:08.769: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-9019 proxy --unix-socket=/tmp/kubectl-proxy-unix2227183445/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:08.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9019" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":327,"skipped":6297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:08.892: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 21 11:11:14.250: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:14.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6682" for this suite.

• [SLOW TEST:5.575 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":328,"skipped":6342,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:14.468: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1331
STEP: creating the pod
Jan 21 11:11:14.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5836 create -f -'
Jan 21 11:11:17.591: INFO: stderr: ""
Jan 21 11:11:17.591: INFO: stdout: "pod/pause created\n"
Jan 21 11:11:17.591: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 21 11:11:17.591: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5836" to be "running and ready"
Jan 21 11:11:17.618: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 26.470071ms
Jan 21 11:11:19.638: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046488174s
Jan 21 11:11:21.651: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.059897027s
Jan 21 11:11:21.651: INFO: Pod "pause" satisfied condition "running and ready"
Jan 21 11:11:21.652: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 21 11:11:21.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5836 label pods pause testing-label=testing-label-value'
Jan 21 11:11:21.833: INFO: stderr: ""
Jan 21 11:11:21.833: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 21 11:11:21.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5836 get pod pause -L testing-label'
Jan 21 11:11:22.010: INFO: stderr: ""
Jan 21 11:11:22.010: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 21 11:11:22.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5836 label pods pause testing-label-'
Jan 21 11:11:22.278: INFO: stderr: ""
Jan 21 11:11:22.279: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 21 11:11:22.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5836 get pod pause -L testing-label'
Jan 21 11:11:22.439: INFO: stderr: ""
Jan 21 11:11:22.439: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1337
STEP: using delete to clean up resources
Jan 21 11:11:22.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5836 delete --grace-period=0 --force -f -'
Jan 21 11:11:22.625: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 11:11:22.625: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 21 11:11:22.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5836 get rc,svc -l name=pause --no-headers'
Jan 21 11:11:22.806: INFO: stderr: "No resources found in kubectl-5836 namespace.\n"
Jan 21 11:11:22.806: INFO: stdout: ""
Jan 21 11:11:22.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-5836 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 21 11:11:22.952: INFO: stderr: ""
Jan 21 11:11:22.952: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:22.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5836" for this suite.

• [SLOW TEST:8.507 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1329
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":329,"skipped":6343,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:22.976: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:11:23.318: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8f37ad87-aaca-4a29-9a85-9e6e456903a2", Controller:(*bool)(0xc002f33136), BlockOwnerDeletion:(*bool)(0xc002f33137)}}
Jan 21 11:11:23.463: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a0080d29-e5fd-456c-887b-223592a5ccc9", Controller:(*bool)(0xc004107456), BlockOwnerDeletion:(*bool)(0xc004107457)}}
Jan 21 11:11:23.496: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fda50deb-4255-4814-b41b-1a8f607796c9", Controller:(*bool)(0xc0041076b6), BlockOwnerDeletion:(*bool)(0xc0041076b7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:28.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4868" for this suite.

• [SLOW TEST:5.837 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":330,"skipped":6368,"failed":0}
S
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:28.812: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 21 11:11:29.124: INFO: Waiting up to 5m0s for pod "security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764" in namespace "security-context-3962" to be "Succeeded or Failed"
Jan 21 11:11:29.227: INFO: Pod "security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764": Phase="Pending", Reason="", readiness=false. Elapsed: 102.135051ms
Jan 21 11:11:31.249: INFO: Pod "security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123966914s
Jan 21 11:11:33.265: INFO: Pod "security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140057863s
Jan 21 11:11:35.279: INFO: Pod "security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.154431762s
STEP: Saw pod success
Jan 21 11:11:35.279: INFO: Pod "security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764" satisfied condition "Succeeded or Failed"
Jan 21 11:11:35.290: INFO: Trying to get logs from node conformance1 pod security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764 container test-container: <nil>
STEP: delete the pod
Jan 21 11:11:35.343: INFO: Waiting for pod security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764 to disappear
Jan 21 11:11:35.351: INFO: Pod security-context-bfdaf754-7ff9-4c4f-9f5a-07d4a144e764 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:35.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3962" for this suite.

• [SLOW TEST:6.562 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":331,"skipped":6369,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:35.375: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 21 11:11:35.533: INFO: The status of Pod labelsupdateea9e2fff-8087-4147-a754-df32e9fd3573 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:11:37.550: INFO: The status of Pod labelsupdateea9e2fff-8087-4147-a754-df32e9fd3573 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:11:39.556: INFO: The status of Pod labelsupdateea9e2fff-8087-4147-a754-df32e9fd3573 is Pending, waiting for it to be Running (with Ready = true)
Jan 21 11:11:41.549: INFO: The status of Pod labelsupdateea9e2fff-8087-4147-a754-df32e9fd3573 is Running (Ready = true)
Jan 21 11:11:42.127: INFO: Successfully updated pod "labelsupdateea9e2fff-8087-4147-a754-df32e9fd3573"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:46.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-198" for this suite.

• [SLOW TEST:10.854 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":6384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:46.235: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-4d2fb613-1bd9-458a-91eb-fadb083e2e79
STEP: Creating a pod to test consume configMaps
Jan 21 11:11:46.389: INFO: Waiting up to 5m0s for pod "pod-configmaps-16e98312-9ada-4f07-98ed-6b3350682bbc" in namespace "configmap-1320" to be "Succeeded or Failed"
Jan 21 11:11:46.400: INFO: Pod "pod-configmaps-16e98312-9ada-4f07-98ed-6b3350682bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.850651ms
Jan 21 11:11:48.429: INFO: Pod "pod-configmaps-16e98312-9ada-4f07-98ed-6b3350682bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040215609s
Jan 21 11:11:50.452: INFO: Pod "pod-configmaps-16e98312-9ada-4f07-98ed-6b3350682bbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062587269s
STEP: Saw pod success
Jan 21 11:11:50.452: INFO: Pod "pod-configmaps-16e98312-9ada-4f07-98ed-6b3350682bbc" satisfied condition "Succeeded or Failed"
Jan 21 11:11:50.461: INFO: Trying to get logs from node conformance1 pod pod-configmaps-16e98312-9ada-4f07-98ed-6b3350682bbc container agnhost-container: <nil>
STEP: delete the pod
Jan 21 11:11:50.514: INFO: Waiting for pod pod-configmaps-16e98312-9ada-4f07-98ed-6b3350682bbc to disappear
Jan 21 11:11:50.528: INFO: Pod pod-configmaps-16e98312-9ada-4f07-98ed-6b3350682bbc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:50.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1320" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":6417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:50.568: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Jan 21 11:11:50.748: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-2223 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:11:50.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2223" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":334,"skipped":6443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:11:51.012: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 21 11:11:51.147: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 21 11:11:51.182: INFO: Waiting for terminating namespaces to be deleted...
Jan 21 11:11:51.191: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
Jan 21 11:11:51.213: INFO: labelsupdateea9e2fff-8087-4147-a754-df32e9fd3573 from downward-api-198 started at 2022-01-21 11:11:35 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.221: INFO: 	Container client-container ready: true, restart count 0
Jan 21 11:11:51.221: INFO: calico-node-q458w from kube-system started at 2022-01-21 06:54:48 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.221: INFO: 	Container calico-node ready: true, restart count 0
Jan 21 11:11:51.221: INFO: nirmata-cni-installer-dqv99 from nirmata started at 2022-01-21 10:24:39 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.221: INFO: 	Container install-cni ready: true, restart count 1
Jan 21 11:11:51.221: INFO: sonobuoy from sonobuoy started at 2022-01-21 09:19:31 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.221: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 21 11:11:51.221: INFO: sonobuoy-e2e-job-9425c772f26a4979 from sonobuoy started at 2022-01-21 09:19:34 +0000 UTC (2 container statuses recorded)
Jan 21 11:11:51.222: INFO: 	Container e2e ready: true, restart count 0
Jan 21 11:11:51.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 11:11:51.222: INFO: sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-9kb9z from sonobuoy started at 2022-01-21 09:19:35 +0000 UTC (2 container statuses recorded)
Jan 21 11:11:51.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 11:11:51.222: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 21 11:11:51.222: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
Jan 21 11:11:51.236: INFO: haproxy-ingress-6968c464f7-jwf8j from ingress-haproxy started at 2022-01-21 09:59:40 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.237: INFO: 	Container haproxy-ingress ready: false, restart count 27
Jan 21 11:11:51.237: INFO: ingress-default-backend-6f6c8556b8-s69bg from ingress-haproxy started at 2022-01-21 10:24:05 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.237: INFO: 	Container ingress-default-backend ready: true, restart count 0
Jan 21 11:11:51.237: INFO: calico-kube-controllers-7bf9dd85d9-xd27g from kube-system started at 2022-01-21 06:55:04 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.237: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 21 11:11:51.237: INFO: calico-node-fg2tt from kube-system started at 2022-01-21 06:54:48 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.237: INFO: 	Container calico-node ready: true, restart count 0
Jan 21 11:11:51.237: INFO: coredns-6779677b89-ldbmt from kube-system started at 2022-01-21 06:55:08 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.237: INFO: 	Container coredns ready: true, restart count 0
Jan 21 11:11:51.237: INFO: coredns-6779677b89-mr5vk from kube-system started at 2022-01-21 06:55:07 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.237: INFO: 	Container coredns ready: true, restart count 0
Jan 21 11:11:51.238: INFO: metrics-server-646d5db6cc-nmv44 from kube-system started at 2022-01-21 06:55:09 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.238: INFO: 	Container metrics-server ready: true, restart count 0
Jan 21 11:11:51.238: INFO: nirmata-cni-installer-jq2k2 from nirmata started at 2022-01-21 06:57:57 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.238: INFO: 	Container install-cni ready: true, restart count 1
Jan 21 11:11:51.238: INFO: nirmata-kube-controller-666bcc6bbc-zss9n from nirmata started at 2022-01-21 06:55:04 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.238: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Jan 21 11:11:51.238: INFO: otel-agent-857cc46474-zg5xk from nirmata started at 2022-01-21 09:59:40 +0000 UTC (1 container statuses recorded)
Jan 21 11:11:51.238: INFO: 	Container otel-agent ready: true, restart count 0
Jan 21 11:11:51.238: INFO: sonobuoy-systemd-logs-daemon-set-444e58a75e1145ce-rw7d8 from sonobuoy started at 2022-01-21 09:19:34 +0000 UTC (2 container statuses recorded)
Jan 21 11:11:51.238: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 11:11:51.238: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bddcb689-2a63-45a7-b6e8-92b75991d9a4 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.10.1.86 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-bddcb689-2a63-45a7-b6e8-92b75991d9a4 off the node conformance1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bddcb689-2a63-45a7-b6e8-92b75991d9a4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:17:03.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5196" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:312.684 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":335,"skipped":6488,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:17:03.699: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:17:03.798: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 21 11:17:12.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-7841 --namespace=crd-publish-openapi-7841 create -f -'
Jan 21 11:17:14.669: INFO: stderr: ""
Jan 21 11:17:14.669: INFO: stdout: "e2e-test-crd-publish-openapi-5617-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 21 11:17:14.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-7841 --namespace=crd-publish-openapi-7841 delete e2e-test-crd-publish-openapi-5617-crds test-cr'
Jan 21 11:17:14.922: INFO: stderr: ""
Jan 21 11:17:14.922: INFO: stdout: "e2e-test-crd-publish-openapi-5617-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 21 11:17:14.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-7841 --namespace=crd-publish-openapi-7841 apply -f -'
Jan 21 11:17:15.498: INFO: stderr: ""
Jan 21 11:17:15.498: INFO: stdout: "e2e-test-crd-publish-openapi-5617-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 21 11:17:15.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-7841 --namespace=crd-publish-openapi-7841 delete e2e-test-crd-publish-openapi-5617-crds test-cr'
Jan 21 11:17:15.651: INFO: stderr: ""
Jan 21 11:17:15.651: INFO: stdout: "e2e-test-crd-publish-openapi-5617-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 21 11:17:15.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=crd-publish-openapi-7841 explain e2e-test-crd-publish-openapi-5617-crds'
Jan 21 11:17:16.336: INFO: stderr: ""
Jan 21 11:17:16.336: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5617-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:17:23.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7841" for this suite.

• [SLOW TEST:19.769 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":336,"skipped":6504,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:17:23.469: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-adeaeddb-4618-4f4b-a9a1-da29ab31a73a in namespace container-probe-7732
Jan 21 11:17:27.618: INFO: Started pod liveness-adeaeddb-4618-4f4b-a9a1-da29ab31a73a in namespace container-probe-7732
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 11:17:27.626: INFO: Initial restart count of pod liveness-adeaeddb-4618-4f4b-a9a1-da29ab31a73a is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:21:29.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7732" for this suite.

• [SLOW TEST:246.116 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":337,"skipped":6511,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:21:29.595: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:21:30.005: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 21 11:21:30.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 11:21:30.108: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:31.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 11:21:31.148: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:32.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 11:21:32.156: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:33.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 11:21:33.139: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:34.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 11:21:34.161: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:35.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 11:21:35.176: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 21 11:21:35.289: INFO: Wrong image for pod: daemon-set-f52gz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 21 11:21:35.289: INFO: Wrong image for pod: daemon-set-ncjbs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 21 11:21:36.314: INFO: Wrong image for pod: daemon-set-f52gz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 21 11:21:37.312: INFO: Wrong image for pod: daemon-set-f52gz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 21 11:21:38.336: INFO: Wrong image for pod: daemon-set-f52gz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 21 11:21:38.337: INFO: Pod daemon-set-n6l9b is not available
Jan 21 11:21:39.315: INFO: Wrong image for pod: daemon-set-f52gz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 21 11:21:39.316: INFO: Pod daemon-set-n6l9b is not available
Jan 21 11:21:40.310: INFO: Wrong image for pod: daemon-set-f52gz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 21 11:21:40.310: INFO: Pod daemon-set-n6l9b is not available
Jan 21 11:21:41.312: INFO: Wrong image for pod: daemon-set-f52gz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 21 11:21:41.312: INFO: Pod daemon-set-n6l9b is not available
Jan 21 11:21:44.310: INFO: Pod daemon-set-xkz84 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 21 11:21:44.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 11:21:44.339: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:45.359: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 11:21:45.359: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:46.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 11:21:46.399: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:47.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 21 11:21:47.360: INFO: Node conformance1 is running 0 daemon pod, expected 1
Jan 21 11:21:48.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 21 11:21:48.381: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9644, will wait for the garbage collector to delete the pods
Jan 21 11:21:48.656: INFO: Deleting DaemonSet.extensions daemon-set took: 48.028678ms
Jan 21 11:21:49.057: INFO: Terminating DaemonSet.extensions daemon-set pods took: 401.084713ms
Jan 21 11:21:51.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 21 11:21:51.467: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 21 11:21:51.472: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34350"},"items":null}

Jan 21 11:21:51.478: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34350"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:21:51.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9644" for this suite.

• [SLOW TEST:21.925 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":338,"skipped":6519,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:21:51.525: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 21 11:21:53.400: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 21 11:21:55.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 11, 21, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 21, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 11, 21, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 21, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 11:21:58.523: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:21:58.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1917" for this suite.
STEP: Destroying namespace "webhook-1917-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.405 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":339,"skipped":6525,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:21:58.930: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Jan 21 11:21:59.123: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 21 11:21:59.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 create -f -'
Jan 21 11:22:01.630: INFO: stderr: ""
Jan 21 11:22:01.630: INFO: stdout: "service/agnhost-replica created\n"
Jan 21 11:22:01.631: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 21 11:22:01.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 create -f -'
Jan 21 11:22:02.310: INFO: stderr: ""
Jan 21 11:22:02.311: INFO: stdout: "service/agnhost-primary created\n"
Jan 21 11:22:02.311: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 21 11:22:02.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 create -f -'
Jan 21 11:22:02.903: INFO: stderr: ""
Jan 21 11:22:02.903: INFO: stdout: "service/frontend created\n"
Jan 21 11:22:02.903: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 21 11:22:02.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 create -f -'
Jan 21 11:22:03.465: INFO: stderr: ""
Jan 21 11:22:03.465: INFO: stdout: "deployment.apps/frontend created\n"
Jan 21 11:22:03.465: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 21 11:22:03.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 create -f -'
Jan 21 11:22:04.889: INFO: stderr: ""
Jan 21 11:22:04.895: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 21 11:22:04.895: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 21 11:22:04.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 create -f -'
Jan 21 11:22:06.731: INFO: stderr: ""
Jan 21 11:22:06.731: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 21 11:22:06.731: INFO: Waiting for all frontend pods to be Running.
Jan 21 11:22:16.788: INFO: Waiting for frontend to serve content.
Jan 21 11:22:16.808: INFO: Trying to add a new entry to the guestbook.
Jan 21 11:22:16.828: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 21 11:22:16.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 delete --grace-period=0 --force -f -'
Jan 21 11:22:17.121: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 11:22:17.121: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 11:22:17.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 delete --grace-period=0 --force -f -'
Jan 21 11:22:17.493: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 11:22:17.493: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 11:22:17.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 delete --grace-period=0 --force -f -'
Jan 21 11:22:17.721: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 11:22:17.721: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 11:22:17.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 delete --grace-period=0 --force -f -'
Jan 21 11:22:17.890: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 11:22:17.890: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 11:22:17.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 delete --grace-period=0 --force -f -'
Jan 21 11:22:18.321: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 11:22:18.321: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 11:22:18.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-197368542 --namespace=kubectl-8901 delete --grace-period=0 --force -f -'
Jan 21 11:22:19.535: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 11:22:19.535: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:22:19.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8901" for this suite.

• [SLOW TEST:20.716 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":340,"skipped":6533,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:22:19.672: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 21 11:22:19.962: INFO: Waiting up to 5m0s for pod "pod-e456f897-6105-4315-bb86-58cec369ca50" in namespace "emptydir-4893" to be "Succeeded or Failed"
Jan 21 11:22:20.025: INFO: Pod "pod-e456f897-6105-4315-bb86-58cec369ca50": Phase="Pending", Reason="", readiness=false. Elapsed: 62.729419ms
Jan 21 11:22:22.051: INFO: Pod "pod-e456f897-6105-4315-bb86-58cec369ca50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089519311s
Jan 21 11:22:24.077: INFO: Pod "pod-e456f897-6105-4315-bb86-58cec369ca50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115262019s
Jan 21 11:22:26.099: INFO: Pod "pod-e456f897-6105-4315-bb86-58cec369ca50": Phase="Pending", Reason="", readiness=false. Elapsed: 6.136684723s
Jan 21 11:22:28.113: INFO: Pod "pod-e456f897-6105-4315-bb86-58cec369ca50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.151502715s
STEP: Saw pod success
Jan 21 11:22:28.113: INFO: Pod "pod-e456f897-6105-4315-bb86-58cec369ca50" satisfied condition "Succeeded or Failed"
Jan 21 11:22:28.120: INFO: Trying to get logs from node conformance1 pod pod-e456f897-6105-4315-bb86-58cec369ca50 container test-container: <nil>
STEP: delete the pod
Jan 21 11:22:28.206: INFO: Waiting for pod pod-e456f897-6105-4315-bb86-58cec369ca50 to disappear
Jan 21 11:22:28.216: INFO: Pod pod-e456f897-6105-4315-bb86-58cec369ca50 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:22:28.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4893" for this suite.

• [SLOW TEST:8.583 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":341,"skipped":6545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:22:28.274: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:22:44.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8423" for this suite.

• [SLOW TEST:16.632 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":342,"skipped":6576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:22:44.907: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Jan 21 11:22:45.033: INFO: Waiting up to 5m0s for pod "var-expansion-323b7b20-0d41-4b6d-8688-586fff35de48" in namespace "var-expansion-5482" to be "Succeeded or Failed"
Jan 21 11:22:45.062: INFO: Pod "var-expansion-323b7b20-0d41-4b6d-8688-586fff35de48": Phase="Pending", Reason="", readiness=false. Elapsed: 28.35944ms
Jan 21 11:22:47.080: INFO: Pod "var-expansion-323b7b20-0d41-4b6d-8688-586fff35de48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046541373s
Jan 21 11:22:49.096: INFO: Pod "var-expansion-323b7b20-0d41-4b6d-8688-586fff35de48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062361795s
STEP: Saw pod success
Jan 21 11:22:49.096: INFO: Pod "var-expansion-323b7b20-0d41-4b6d-8688-586fff35de48" satisfied condition "Succeeded or Failed"
Jan 21 11:22:49.108: INFO: Trying to get logs from node conformance1 pod var-expansion-323b7b20-0d41-4b6d-8688-586fff35de48 container dapi-container: <nil>
STEP: delete the pod
Jan 21 11:22:49.192: INFO: Waiting for pod var-expansion-323b7b20-0d41-4b6d-8688-586fff35de48 to disappear
Jan 21 11:22:49.204: INFO: Pod var-expansion-323b7b20-0d41-4b6d-8688-586fff35de48 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:22:49.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5482" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":343,"skipped":6618,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:22:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 21 11:22:51.473: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 21 11:22:53.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 21, 11, 22, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 22, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 21, 11, 22, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 21, 11, 22, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 21 11:22:56.533: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:22:56.549: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:23:00.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6828" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:11.485 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":344,"skipped":6618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:23:00.747: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:23:01.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6019" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6668,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 21 11:23:01.260: INFO: >>> kubeConfig: /tmp/kubeconfig-197368542
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 21 11:23:01.504: INFO: Creating ReplicaSet my-hostname-basic-d2c0cd12-93c6-4c94-9545-ad9de71e5c05
Jan 21 11:23:01.559: INFO: Pod name my-hostname-basic-d2c0cd12-93c6-4c94-9545-ad9de71e5c05: Found 0 pods out of 1
Jan 21 11:23:06.605: INFO: Pod name my-hostname-basic-d2c0cd12-93c6-4c94-9545-ad9de71e5c05: Found 1 pods out of 1
Jan 21 11:23:06.605: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d2c0cd12-93c6-4c94-9545-ad9de71e5c05" is running
Jan 21 11:23:06.624: INFO: Pod "my-hostname-basic-d2c0cd12-93c6-4c94-9545-ad9de71e5c05-zjh7z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-21 11:23:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-21 11:23:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-21 11:23:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-21 11:23:01 +0000 UTC Reason: Message:}])
Jan 21 11:23:06.624: INFO: Trying to dial the pod
Jan 21 11:23:11.650: INFO: Controller my-hostname-basic-d2c0cd12-93c6-4c94-9545-ad9de71e5c05: Got expected result from replica 1 [my-hostname-basic-d2c0cd12-93c6-4c94-9545-ad9de71e5c05-zjh7z]: "my-hostname-basic-d2c0cd12-93c6-4c94-9545-ad9de71e5c05-zjh7z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 21 11:23:11.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-473" for this suite.

• [SLOW TEST:10.411 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":346,"skipped":6689,"failed":0}
SSSSSSSJan 21 11:23:11.672: INFO: Running AfterSuite actions on all nodes
Jan 21 11:23:11.672: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Jan 21 11:23:11.672: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jan 21 11:23:11.672: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Jan 21 11:23:11.672: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 21 11:23:11.672: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 21 11:23:11.672: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 21 11:23:11.672: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jan 21 11:23:11.672: INFO: Running AfterSuite actions on node 1
Jan 21 11:23:11.672: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6696,"failed":0}

Ran 346 of 7042 Specs in 7405.218 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6696 Skipped
PASS

Ginkgo ran 1 suite in 2h3m32.574332993s
Test Suite Passed
