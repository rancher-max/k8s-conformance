I0317 08:58:34.968387      22 e2e.go:132] Starting e2e run "5d7c7e55-32d7-4c12-801b-31dacfb4b590" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1647507514 - Will randomize all specs
Will run 346 of 7042 specs

Mar 17 08:58:36.953: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 08:58:36.955: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 17 08:58:36.970: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 17 08:58:36.987: INFO: The status of Pod rke-coredns-addon-deploy-job-lj29w is Succeeded, skipping waiting
Mar 17 08:58:36.987: INFO: The status of Pod rke-metrics-addon-deploy-job-fpn9x is Succeeded, skipping waiting
Mar 17 08:58:36.987: INFO: The status of Pod rke-network-plugin-deploy-job-x6hnn is Succeeded, skipping waiting
Mar 17 08:58:36.987: INFO: 8 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 17 08:58:36.987: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Mar 17 08:58:36.987: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 17 08:58:36.991: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Mar 17 08:58:36.991: INFO: e2e test version: v1.23.4
Mar 17 08:58:36.992: INFO: kube-apiserver version: v1.23.4
Mar 17 08:58:36.992: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 08:58:36.997: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 08:58:36.998: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
W0317 08:58:37.036250      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Mar 17 08:58:37.036: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 08:58:37.045: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d150d66-1fcb-420a-8722-384f1f940941" in namespace "downward-api-9559" to be "Succeeded or Failed"
Mar 17 08:58:37.051: INFO: Pod "downwardapi-volume-8d150d66-1fcb-420a-8722-384f1f940941": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436664ms
Mar 17 08:58:39.062: INFO: Pod "downwardapi-volume-8d150d66-1fcb-420a-8722-384f1f940941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017744688s
Mar 17 08:58:41.071: INFO: Pod "downwardapi-volume-8d150d66-1fcb-420a-8722-384f1f940941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026143948s
STEP: Saw pod success
Mar 17 08:58:41.071: INFO: Pod "downwardapi-volume-8d150d66-1fcb-420a-8722-384f1f940941" satisfied condition "Succeeded or Failed"
Mar 17 08:58:41.073: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-8d150d66-1fcb-420a-8722-384f1f940941 container client-container: <nil>
STEP: delete the pod
Mar 17 08:58:41.101: INFO: Waiting for pod downwardapi-volume-8d150d66-1fcb-420a-8722-384f1f940941 to disappear
Mar 17 08:58:41.103: INFO: Pod downwardapi-volume-8d150d66-1fcb-420a-8722-384f1f940941 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 08:58:41.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9559" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":38,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 08:58:41.112: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 08:58:41.176: INFO: The status of Pod pod-secrets-860a7ee7-c332-48d9-b76c-f0194252a948 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 08:58:43.182: INFO: The status of Pod pod-secrets-860a7ee7-c332-48d9-b76c-f0194252a948 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 08:58:45.186: INFO: The status of Pod pod-secrets-860a7ee7-c332-48d9-b76c-f0194252a948 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 08:58:45.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2256" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":2,"skipped":56,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 08:58:45.220: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-sgwd
STEP: Creating a pod to test atomic-volume-subpath
Mar 17 08:58:45.282: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sgwd" in namespace "subpath-569" to be "Succeeded or Failed"
Mar 17 08:58:45.303: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Pending", Reason="", readiness=false. Elapsed: 21.258759ms
Mar 17 08:58:47.308: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025889525s
Mar 17 08:58:49.313: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 4.030865718s
Mar 17 08:58:51.321: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 6.039512976s
Mar 17 08:58:53.325: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 8.043579403s
Mar 17 08:58:55.332: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 10.0496016s
Mar 17 08:58:57.336: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 12.053902474s
Mar 17 08:58:59.343: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 14.061491718s
Mar 17 08:59:01.349: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 16.067054386s
Mar 17 08:59:03.355: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 18.073063948s
Mar 17 08:59:05.359: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Running", Reason="", readiness=true. Elapsed: 20.076943992s
Mar 17 08:59:07.365: INFO: Pod "pod-subpath-test-secret-sgwd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.083016991s
STEP: Saw pod success
Mar 17 08:59:07.365: INFO: Pod "pod-subpath-test-secret-sgwd" satisfied condition "Succeeded or Failed"
Mar 17 08:59:07.372: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-subpath-test-secret-sgwd container test-container-subpath-secret-sgwd: <nil>
STEP: delete the pod
Mar 17 08:59:07.393: INFO: Waiting for pod pod-subpath-test-secret-sgwd to disappear
Mar 17 08:59:07.396: INFO: Pod pod-subpath-test-secret-sgwd no longer exists
STEP: Deleting pod pod-subpath-test-secret-sgwd
Mar 17 08:59:07.396: INFO: Deleting pod "pod-subpath-test-secret-sgwd" in namespace "subpath-569"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 08:59:07.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-569" for this suite.

• [SLOW TEST:22.187 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":3,"skipped":59,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 08:59:07.408: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-020bc488-3b9b-4b60-95d8-1a5df07a33c9 in namespace container-probe-3583
Mar 17 08:59:09.506: INFO: Started pod test-webserver-020bc488-3b9b-4b60-95d8-1a5df07a33c9 in namespace container-probe-3583
STEP: checking the pod's current state and verifying that restartCount is present
Mar 17 08:59:09.509: INFO: Initial restart count of pod test-webserver-020bc488-3b9b-4b60-95d8-1a5df07a33c9 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:03:10.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3583" for this suite.

• [SLOW TEST:242.857 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":67,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:03:10.265: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:03:10.314: INFO: Creating ReplicaSet my-hostname-basic-01281bce-9e1b-4b33-8ff9-0014bbd49ffa
Mar 17 09:03:10.329: INFO: Pod name my-hostname-basic-01281bce-9e1b-4b33-8ff9-0014bbd49ffa: Found 0 pods out of 1
Mar 17 09:03:15.334: INFO: Pod name my-hostname-basic-01281bce-9e1b-4b33-8ff9-0014bbd49ffa: Found 1 pods out of 1
Mar 17 09:03:15.334: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-01281bce-9e1b-4b33-8ff9-0014bbd49ffa" is running
Mar 17 09:03:15.339: INFO: Pod "my-hostname-basic-01281bce-9e1b-4b33-8ff9-0014bbd49ffa-v4kh6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-17 09:03:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-17 09:03:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-17 09:03:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-17 09:03:10 +0000 UTC Reason: Message:}])
Mar 17 09:03:15.340: INFO: Trying to dial the pod
Mar 17 09:03:20.353: INFO: Controller my-hostname-basic-01281bce-9e1b-4b33-8ff9-0014bbd49ffa: Got expected result from replica 1 [my-hostname-basic-01281bce-9e1b-4b33-8ff9-0014bbd49ffa-v4kh6]: "my-hostname-basic-01281bce-9e1b-4b33-8ff9-0014bbd49ffa-v4kh6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:03:20.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9143" for this suite.

• [SLOW TEST:10.105 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":5,"skipped":69,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:03:20.371: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6177
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 17 09:03:20.419: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 17 09:03:20.469: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:03:22.476: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:24.479: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:26.473: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:28.473: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:30.473: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:32.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:34.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:36.475: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:38.477: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:03:40.480: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 17 09:03:40.487: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 17 09:03:40.505: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 17 09:03:42.530: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 17 09:03:42.530: INFO: Breadth first check of 10.42.0.158 on host 172.31.33.68...
Mar 17 09:03:42.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.166:9080/dial?request=hostname&protocol=http&host=10.42.0.158&port=8083&tries=1'] Namespace:pod-network-test-6177 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:03:42.533: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:03:42.533: INFO: ExecWithOptions: Clientset creation
Mar 17 09:03:42.533: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6177/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.0.158%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:03:42.627: INFO: Waiting for responses: map[]
Mar 17 09:03:42.627: INFO: reached 10.42.0.158 after 0/1 tries
Mar 17 09:03:42.627: INFO: Breadth first check of 10.42.2.165 on host 172.31.35.106...
Mar 17 09:03:42.631: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.166:9080/dial?request=hostname&protocol=http&host=10.42.2.165&port=8083&tries=1'] Namespace:pod-network-test-6177 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:03:42.631: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:03:42.631: INFO: ExecWithOptions: Clientset creation
Mar 17 09:03:42.631: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6177/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.2.165%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:03:42.720: INFO: Waiting for responses: map[]
Mar 17 09:03:42.720: INFO: reached 10.42.2.165 after 0/1 tries
Mar 17 09:03:42.720: INFO: Breadth first check of 10.42.1.212 on host 172.31.39.36...
Mar 17 09:03:42.723: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.166:9080/dial?request=hostname&protocol=http&host=10.42.1.212&port=8083&tries=1'] Namespace:pod-network-test-6177 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:03:42.723: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:03:42.724: INFO: ExecWithOptions: Clientset creation
Mar 17 09:03:42.724: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6177/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.1.212%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:03:42.821: INFO: Waiting for responses: map[]
Mar 17 09:03:42.821: INFO: reached 10.42.1.212 after 0/1 tries
Mar 17 09:03:42.822: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:03:42.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6177" for this suite.

• [SLOW TEST:22.463 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:03:42.834: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Mar 17 09:03:42.889: INFO: Major version: 1
STEP: Confirm minor version
Mar 17 09:03:42.889: INFO: cleanMinorVersion: 23
Mar 17 09:03:42.889: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:03:42.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4266" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":7,"skipped":167,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:03:42.906: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-6568bb44-4902-4e73-8484-9dba6e08743d
STEP: Creating configMap with name cm-test-opt-upd-59a3be27-4f21-40be-ac87-e695a814be81
STEP: Creating the pod
Mar 17 09:03:43.027: INFO: The status of Pod pod-configmaps-a6fa9b44-cc42-43bf-8084-7298d468bd98 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:03:45.032: INFO: The status of Pod pod-configmaps-a6fa9b44-cc42-43bf-8084-7298d468bd98 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-6568bb44-4902-4e73-8484-9dba6e08743d
STEP: Updating configmap cm-test-opt-upd-59a3be27-4f21-40be-ac87-e695a814be81
STEP: Creating configMap with name cm-test-opt-create-aa651b1a-e8be-4d46-82e9-2700c7c16c0e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:03:49.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8327" for this suite.

• [SLOW TEST:6.243 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":172,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:03:49.149: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar 17 09:03:51.224: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2268 PodName:var-expansion-db757ef3-587c-43d5-9a01-e849d922d259 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:03:51.224: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:03:51.224: INFO: ExecWithOptions: Clientset creation
Mar 17 09:03:51.224: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-2268/pods/var-expansion-db757ef3-587c-43d5-9a01-e849d922d259/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Mar 17 09:03:51.319: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2268 PodName:var-expansion-db757ef3-587c-43d5-9a01-e849d922d259 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:03:51.319: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:03:51.319: INFO: ExecWithOptions: Clientset creation
Mar 17 09:03:51.319: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-2268/pods/var-expansion-db757ef3-587c-43d5-9a01-e849d922d259/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Mar 17 09:03:51.924: INFO: Successfully updated pod "var-expansion-db757ef3-587c-43d5-9a01-e849d922d259"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar 17 09:03:51.930: INFO: Deleting pod "var-expansion-db757ef3-587c-43d5-9a01-e849d922d259" in namespace "var-expansion-2268"
Mar 17 09:03:51.935: INFO: Wait up to 5m0s for pod "var-expansion-db757ef3-587c-43d5-9a01-e849d922d259" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:04:25.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2268" for this suite.

• [SLOW TEST:36.803 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":9,"skipped":177,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:04:25.953: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:04:28.042: INFO: Deleting pod "var-expansion-5d1af9b5-b2ea-4775-8712-3f5f2d3650b1" in namespace "var-expansion-7814"
Mar 17 09:04:28.050: INFO: Wait up to 5m0s for pod "var-expansion-5d1af9b5-b2ea-4775-8712-3f5f2d3650b1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:04:32.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7814" for this suite.

• [SLOW TEST:6.114 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":10,"skipped":196,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:04:32.069: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:04:32.225: INFO: Create a RollingUpdate DaemonSet
Mar 17 09:04:32.232: INFO: Check that daemon pods launch on every node of the cluster
Mar 17 09:04:32.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 09:04:32.242: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 09:04:33.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 09:04:33.260: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 09:04:34.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 09:04:34.257: INFO: Node ip-172-31-39-36 is running 0 daemon pod, expected 1
Mar 17 09:04:35.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 09:04:35.252: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 17 09:04:35.252: INFO: Update the DaemonSet to trigger a rollout
Mar 17 09:04:35.266: INFO: Updating DaemonSet daemon-set
Mar 17 09:04:37.308: INFO: Roll back the DaemonSet before rollout is complete
Mar 17 09:04:37.355: INFO: Updating DaemonSet daemon-set
Mar 17 09:04:37.355: INFO: Make sure DaemonSet rollback is complete
Mar 17 09:04:37.372: INFO: Wrong image for pod: daemon-set-zlrm9. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar 17 09:04:37.372: INFO: Pod daemon-set-zlrm9 is not available
Mar 17 09:04:40.384: INFO: Pod daemon-set-4x425 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2727, will wait for the garbage collector to delete the pods
Mar 17 09:04:40.472: INFO: Deleting DaemonSet.extensions daemon-set took: 7.346518ms
Mar 17 09:04:40.573: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.8514ms
Mar 17 09:04:43.679: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 09:04:43.679: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 17 09:04:43.685: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"51303"},"items":null}

Mar 17 09:04:43.688: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"51303"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:04:43.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2727" for this suite.

• [SLOW TEST:11.644 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":11,"skipped":199,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:04:43.713: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-ea79f050-fe4f-4a9b-855e-b8fb88e8efce
STEP: Creating a pod to test consume secrets
Mar 17 09:04:43.822: INFO: Waiting up to 5m0s for pod "pod-secrets-b858a148-3faf-47ac-a0d7-8a99534d2b60" in namespace "secrets-4658" to be "Succeeded or Failed"
Mar 17 09:04:43.835: INFO: Pod "pod-secrets-b858a148-3faf-47ac-a0d7-8a99534d2b60": Phase="Pending", Reason="", readiness=false. Elapsed: 12.464905ms
Mar 17 09:04:45.841: INFO: Pod "pod-secrets-b858a148-3faf-47ac-a0d7-8a99534d2b60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019397024s
STEP: Saw pod success
Mar 17 09:04:45.841: INFO: Pod "pod-secrets-b858a148-3faf-47ac-a0d7-8a99534d2b60" satisfied condition "Succeeded or Failed"
Mar 17 09:04:45.845: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-secrets-b858a148-3faf-47ac-a0d7-8a99534d2b60 container secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:04:45.867: INFO: Waiting for pod pod-secrets-b858a148-3faf-47ac-a0d7-8a99534d2b60 to disappear
Mar 17 09:04:45.873: INFO: Pod pod-secrets-b858a148-3faf-47ac-a0d7-8a99534d2b60 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:04:45.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4658" for this suite.
STEP: Destroying namespace "secret-namespace-3058" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":204,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:04:45.899: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Mar 17 09:04:46.550: INFO: created pod pod-service-account-defaultsa
Mar 17 09:04:46.550: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 17 09:04:46.558: INFO: created pod pod-service-account-mountsa
Mar 17 09:04:46.558: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 17 09:04:46.566: INFO: created pod pod-service-account-nomountsa
Mar 17 09:04:46.566: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 17 09:04:46.579: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 17 09:04:46.579: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 17 09:04:46.600: INFO: created pod pod-service-account-mountsa-mountspec
Mar 17 09:04:46.600: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 17 09:04:46.611: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 17 09:04:46.611: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 17 09:04:46.640: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 17 09:04:46.640: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 17 09:04:46.655: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 17 09:04:46.655: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 17 09:04:46.662: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 17 09:04:46.662: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:04:46.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7969" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":13,"skipped":209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:04:46.710: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Mar 17 09:04:46.797: INFO: created test-podtemplate-1
Mar 17 09:04:46.805: INFO: created test-podtemplate-2
Mar 17 09:04:46.809: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar 17 09:04:46.813: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar 17 09:04:46.834: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:04:46.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2921" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":14,"skipped":240,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:04:46.851: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Mar 17 09:04:46.937: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:04:48.943: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:04:50.941: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:04:52.942: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar 17 09:04:52.960: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:04:54.968: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 17 09:04:54.995: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 17 09:04:55.000: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 17 09:04:57.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 17 09:04:57.003: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 17 09:04:59.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 17 09:04:59.007: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:04:59.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4999" for this suite.

• [SLOW TEST:12.172 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":15,"skipped":249,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:04:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-bq2z
STEP: Creating a pod to test atomic-volume-subpath
Mar 17 09:04:59.115: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bq2z" in namespace "subpath-292" to be "Succeeded or Failed"
Mar 17 09:04:59.118: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Pending", Reason="", readiness=false. Elapsed: 3.600125ms
Mar 17 09:05:01.122: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 2.007552644s
Mar 17 09:05:03.129: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 4.01435634s
Mar 17 09:05:05.133: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 6.018402847s
Mar 17 09:05:07.140: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 8.025687233s
Mar 17 09:05:09.146: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 10.031160872s
Mar 17 09:05:11.150: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 12.035728751s
Mar 17 09:05:13.155: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 14.040003709s
Mar 17 09:05:15.159: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 16.04431118s
Mar 17 09:05:17.164: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 18.049127786s
Mar 17 09:05:19.169: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Running", Reason="", readiness=true. Elapsed: 20.0538816s
Mar 17 09:05:21.173: INFO: Pod "pod-subpath-test-configmap-bq2z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.05808438s
STEP: Saw pod success
Mar 17 09:05:21.173: INFO: Pod "pod-subpath-test-configmap-bq2z" satisfied condition "Succeeded or Failed"
Mar 17 09:05:21.176: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-subpath-test-configmap-bq2z container test-container-subpath-configmap-bq2z: <nil>
STEP: delete the pod
Mar 17 09:05:21.199: INFO: Waiting for pod pod-subpath-test-configmap-bq2z to disappear
Mar 17 09:05:21.209: INFO: Pod pod-subpath-test-configmap-bq2z no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bq2z
Mar 17 09:05:21.209: INFO: Deleting pod "pod-subpath-test-configmap-bq2z" in namespace "subpath-292"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:05:21.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-292" for this suite.

• [SLOW TEST:22.204 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":16,"skipped":251,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:05:21.229: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Mar 17 09:05:21.286: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8650 proxy --unix-socket=/tmp/kubectl-proxy-unix3478873285/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:05:21.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8650" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":17,"skipped":378,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:05:21.334: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:05:21.386: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-47329e76-4090-4d20-9d68-941f65c98444" in namespace "security-context-test-7163" to be "Succeeded or Failed"
Mar 17 09:05:21.398: INFO: Pod "busybox-privileged-false-47329e76-4090-4d20-9d68-941f65c98444": Phase="Pending", Reason="", readiness=false. Elapsed: 11.905782ms
Mar 17 09:05:23.404: INFO: Pod "busybox-privileged-false-47329e76-4090-4d20-9d68-941f65c98444": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017298748s
Mar 17 09:05:23.404: INFO: Pod "busybox-privileged-false-47329e76-4090-4d20-9d68-941f65c98444" satisfied condition "Succeeded or Failed"
Mar 17 09:05:23.409: INFO: Got logs for pod "busybox-privileged-false-47329e76-4090-4d20-9d68-941f65c98444": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:05:23.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7163" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":380,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:05:23.416: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar 17 09:05:23.479: INFO: The status of Pod annotationupdate5c25bf2b-d395-46e4-9738-78a4f9dd6037 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:05:25.483: INFO: The status of Pod annotationupdate5c25bf2b-d395-46e4-9738-78a4f9dd6037 is Running (Ready = true)
Mar 17 09:05:26.019: INFO: Successfully updated pod "annotationupdate5c25bf2b-d395-46e4-9738-78a4f9dd6037"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:05:30.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1870" for this suite.

• [SLOW TEST:6.642 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":19,"skipped":383,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:05:30.059: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 17 09:05:30.128: INFO: The status of Pod pod-update-activedeadlineseconds-cc04b687-702b-4811-9794-7fa152951af3 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:05:32.137: INFO: The status of Pod pod-update-activedeadlineseconds-cc04b687-702b-4811-9794-7fa152951af3 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 17 09:05:32.659: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cc04b687-702b-4811-9794-7fa152951af3"
Mar 17 09:05:32.659: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cc04b687-702b-4811-9794-7fa152951af3" in namespace "pods-6454" to be "terminated due to deadline exceeded"
Mar 17 09:05:32.666: INFO: Pod "pod-update-activedeadlineseconds-cc04b687-702b-4811-9794-7fa152951af3": Phase="Running", Reason="", readiness=true. Elapsed: 7.27559ms
Mar 17 09:05:34.670: INFO: Pod "pod-update-activedeadlineseconds-cc04b687-702b-4811-9794-7fa152951af3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011627273s
Mar 17 09:05:36.674: INFO: Pod "pod-update-activedeadlineseconds-cc04b687-702b-4811-9794-7fa152951af3": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.014953883s
Mar 17 09:05:36.674: INFO: Pod "pod-update-activedeadlineseconds-cc04b687-702b-4811-9794-7fa152951af3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:05:36.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6454" for this suite.

• [SLOW TEST:6.624 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":20,"skipped":390,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:05:36.683: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Mar 17 09:05:56.911: INFO: EndpointSlice for Service endpointslice-3644/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:06.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3644" for this suite.

• [SLOW TEST:30.257 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":21,"skipped":392,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:06.941: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 09:06:07.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-663d35c8-ecb2-4fec-bfdf-359c1e88b4a8" in namespace "projected-1894" to be "Succeeded or Failed"
Mar 17 09:06:07.015: INFO: Pod "downwardapi-volume-663d35c8-ecb2-4fec-bfdf-359c1e88b4a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.312534ms
Mar 17 09:06:09.018: INFO: Pod "downwardapi-volume-663d35c8-ecb2-4fec-bfdf-359c1e88b4a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00994755s
STEP: Saw pod success
Mar 17 09:06:09.018: INFO: Pod "downwardapi-volume-663d35c8-ecb2-4fec-bfdf-359c1e88b4a8" satisfied condition "Succeeded or Failed"
Mar 17 09:06:09.021: INFO: Trying to get logs from node ip-172-31-39-36 pod downwardapi-volume-663d35c8-ecb2-4fec-bfdf-359c1e88b4a8 container client-container: <nil>
STEP: delete the pod
Mar 17 09:06:09.050: INFO: Waiting for pod downwardapi-volume-663d35c8-ecb2-4fec-bfdf-359c1e88b4a8 to disappear
Mar 17 09:06:09.055: INFO: Pod downwardapi-volume-663d35c8-ecb2-4fec-bfdf-359c1e88b4a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:09.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1894" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":409,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:09.066: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 17 09:06:09.135: INFO: Waiting up to 5m0s for pod "pod-4e10b986-f034-4e13-957f-d8bdf3fa83df" in namespace "emptydir-6370" to be "Succeeded or Failed"
Mar 17 09:06:09.142: INFO: Pod "pod-4e10b986-f034-4e13-957f-d8bdf3fa83df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498954ms
Mar 17 09:06:11.145: INFO: Pod "pod-4e10b986-f034-4e13-957f-d8bdf3fa83df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010256938s
STEP: Saw pod success
Mar 17 09:06:11.145: INFO: Pod "pod-4e10b986-f034-4e13-957f-d8bdf3fa83df" satisfied condition "Succeeded or Failed"
Mar 17 09:06:11.148: INFO: Trying to get logs from node ip-172-31-39-36 pod pod-4e10b986-f034-4e13-957f-d8bdf3fa83df container test-container: <nil>
STEP: delete the pod
Mar 17 09:06:11.165: INFO: Waiting for pod pod-4e10b986-f034-4e13-957f-d8bdf3fa83df to disappear
Mar 17 09:06:11.169: INFO: Pod pod-4e10b986-f034-4e13-957f-d8bdf3fa83df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:11.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6370" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":23,"skipped":412,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:11.179: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:17.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8154" for this suite.
STEP: Destroying namespace "nsdeletetest-9710" for this suite.
Mar 17 09:06:17.345: INFO: Namespace nsdeletetest-9710 was already deleted
STEP: Destroying namespace "nsdeletetest-1158" for this suite.

• [SLOW TEST:6.172 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":24,"skipped":423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:17.354: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:20.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-264" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":25,"skipped":455,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:20.099: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4743
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4743
STEP: Waiting until pod test-pod will start running in namespace statefulset-4743
STEP: Creating statefulset with conflicting port in namespace statefulset-4743
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4743
Mar 17 09:06:22.243: INFO: Observed stateful pod in namespace: statefulset-4743, name: ss-0, uid: 57f695c0-ebef-4d13-9d09-9465de1a666c, status phase: Pending. Waiting for statefulset controller to delete.
Mar 17 09:06:22.274: INFO: Observed stateful pod in namespace: statefulset-4743, name: ss-0, uid: 57f695c0-ebef-4d13-9d09-9465de1a666c, status phase: Failed. Waiting for statefulset controller to delete.
Mar 17 09:06:22.304: INFO: Observed stateful pod in namespace: statefulset-4743, name: ss-0, uid: 57f695c0-ebef-4d13-9d09-9465de1a666c, status phase: Failed. Waiting for statefulset controller to delete.
Mar 17 09:06:22.322: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4743
STEP: Removing pod with conflicting port in namespace statefulset-4743
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4743 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Mar 17 09:06:24.436: INFO: Deleting all statefulset in ns statefulset-4743
Mar 17 09:06:24.442: INFO: Scaling statefulset ss to 0
Mar 17 09:06:34.466: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 09:06:34.472: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:34.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4743" for this suite.

• [SLOW TEST:14.414 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":26,"skipped":466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:34.515: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:06:34.927: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 17 09:06:36.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.March, 17, 9, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 6, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 9, 6, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 6, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:06:39.954: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:40.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4965" for this suite.
STEP: Destroying namespace "webhook-4965-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.605 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":27,"skipped":488,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:40.120: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-aed86b1f-f055-4c56-b7dd-feeaa1f00f48
STEP: Creating a pod to test consume configMaps
Mar 17 09:06:40.176: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-978e3d79-b429-4ef5-a1f6-57622a7dc515" in namespace "projected-3518" to be "Succeeded or Failed"
Mar 17 09:06:40.184: INFO: Pod "pod-projected-configmaps-978e3d79-b429-4ef5-a1f6-57622a7dc515": Phase="Pending", Reason="", readiness=false. Elapsed: 8.315629ms
Mar 17 09:06:42.188: INFO: Pod "pod-projected-configmaps-978e3d79-b429-4ef5-a1f6-57622a7dc515": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011923557s
STEP: Saw pod success
Mar 17 09:06:42.188: INFO: Pod "pod-projected-configmaps-978e3d79-b429-4ef5-a1f6-57622a7dc515" satisfied condition "Succeeded or Failed"
Mar 17 09:06:42.190: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-configmaps-978e3d79-b429-4ef5-a1f6-57622a7dc515 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:06:42.210: INFO: Waiting for pod pod-projected-configmaps-978e3d79-b429-4ef5-a1f6-57622a7dc515 to disappear
Mar 17 09:06:42.216: INFO: Pod pod-projected-configmaps-978e3d79-b429-4ef5-a1f6-57622a7dc515 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:42.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3518" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":28,"skipped":506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:42.230: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Mar 17 09:06:42.348: INFO: pods: 0 < 3
Mar 17 09:06:44.353: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Mar 17 09:06:50.479: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:52.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-560" for this suite.

• [SLOW TEST:10.292 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":29,"skipped":534,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:52.523: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar 17 09:06:52.607: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar 17 09:06:52.627: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:06:52.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8393" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":30,"skipped":553,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:06:52.661: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-44f2483a-f790-4b88-94a7-6ab6bbe4ebc8 in namespace container-probe-4218
Mar 17 09:06:54.749: INFO: Started pod busybox-44f2483a-f790-4b88-94a7-6ab6bbe4ebc8 in namespace container-probe-4218
STEP: checking the pod's current state and verifying that restartCount is present
Mar 17 09:06:54.752: INFO: Initial restart count of pod busybox-44f2483a-f790-4b88-94a7-6ab6bbe4ebc8 is 0
Mar 17 09:07:44.895: INFO: Restart count of pod container-probe-4218/busybox-44f2483a-f790-4b88-94a7-6ab6bbe4ebc8 is now 1 (50.143423622s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:07:44.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4218" for this suite.

• [SLOW TEST:52.256 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":556,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:07:44.917: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 17 09:07:44.997: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 17 09:07:45.003: INFO: Waiting for terminating namespaces to be deleted...
Mar 17 09:07:45.010: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-33-68 before test
Mar 17 09:07:45.018: INFO: fleet-agent-68b989995b-gwpf4 from cattle-fleet-system started at 2022-03-17 06:28:25 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container fleet-agent ready: true, restart count 1
Mar 17 09:07:45.018: INFO: cattle-cluster-agent-5bdc96ddf-7rt46 from cattle-system started at 2022-03-17 06:28:37 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container cluster-register ready: true, restart count 0
Mar 17 09:07:45.018: INFO: cattle-cluster-agent-5bdc96ddf-dkrkg from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container cluster-register ready: true, restart count 12
Mar 17 09:07:45.018: INFO: cattle-node-agent-xb7rh from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container agent ready: true, restart count 0
Mar 17 09:07:45.018: INFO: kube-api-auth-htl6k from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container kube-api-auth ready: true, restart count 0
Mar 17 09:07:45.018: INFO: calico-kube-controllers-fc7fcb565-nwnrh from kube-system started at 2022-03-17 06:27:25 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container calico-kube-controllers ready: true, restart count 12
Mar 17 09:07:45.018: INFO: canal-whdbp from kube-system started at 2022-03-17 06:27:25 +0000 UTC (2 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 09:07:45.018: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 09:07:45.018: INFO: coredns-5cb46d7c6-wk9ms from kube-system started at 2022-03-17 06:27:30 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container coredns ready: true, restart count 0
Mar 17 09:07:45.018: INFO: metrics-server-5c4895ffbd-9bbrg from kube-system started at 2022-03-17 06:27:34 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container metrics-server ready: true, restart count 0
Mar 17 09:07:45.018: INFO: rke-coredns-addon-deploy-job-lj29w from kube-system started at 2022-03-17 08:23:13 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Mar 17 09:07:45.018: INFO: rke-metrics-addon-deploy-job-fpn9x from kube-system started at 2022-03-17 06:27:33 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Mar 17 09:07:45.018: INFO: rke-network-plugin-deploy-job-x6hnn from kube-system started at 2022-03-17 06:27:23 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Mar 17 09:07:45.018: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-ng79r from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 09:07:45.018: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 09:07:45.018: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 17 09:07:45.018: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-35-106 before test
Mar 17 09:07:45.024: INFO: cattle-node-agent-zbmd5 from cattle-system started at 2022-03-17 06:29:26 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.024: INFO: 	Container agent ready: true, restart count 0
Mar 17 09:07:45.024: INFO: canal-2xxhj from kube-system started at 2022-03-17 06:29:26 +0000 UTC (2 container statuses recorded)
Mar 17 09:07:45.024: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 09:07:45.024: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 09:07:45.024: INFO: coredns-5cb46d7c6-khnbz from kube-system started at 2022-03-17 08:53:25 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.024: INFO: 	Container coredns ready: true, restart count 0
Mar 17 09:07:45.024: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-n98xh from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 09:07:45.024: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 09:07:45.024: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 17 09:07:45.024: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-39-36 before test
Mar 17 09:07:45.032: INFO: cattle-node-agent-lnw2j from cattle-system started at 2022-03-17 06:28:37 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.032: INFO: 	Container agent ready: true, restart count 0
Mar 17 09:07:45.032: INFO: canal-t2wwt from kube-system started at 2022-03-17 06:28:37 +0000 UTC (2 container statuses recorded)
Mar 17 09:07:45.032: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 09:07:45.032: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 09:07:45.032: INFO: coredns-autoscaler-6cb44df646-flpwt from kube-system started at 2022-03-17 08:56:22 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.032: INFO: 	Container autoscaler ready: true, restart count 0
Mar 17 09:07:45.032: INFO: sonobuoy from sonobuoy started at 2022-03-17 08:58:32 +0000 UTC (1 container statuses recorded)
Mar 17 09:07:45.032: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 17 09:07:45.032: INFO: sonobuoy-e2e-job-3f99102f0d96453a from sonobuoy started at 2022-03-17 08:58:33 +0000 UTC (2 container statuses recorded)
Mar 17 09:07:45.032: INFO: 	Container e2e ready: true, restart count 0
Mar 17 09:07:45.032: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 09:07:45.032: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-st4gk from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 09:07:45.032: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 09:07:45.032: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node ip-172-31-33-68
STEP: verifying the node has the label node ip-172-31-35-106
STEP: verifying the node has the label node ip-172-31-39-36
Mar 17 09:07:45.114: INFO: Pod fleet-agent-68b989995b-gwpf4 requesting resource cpu=0m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod cattle-cluster-agent-5bdc96ddf-7rt46 requesting resource cpu=0m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod cattle-cluster-agent-5bdc96ddf-dkrkg requesting resource cpu=0m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod cattle-node-agent-lnw2j requesting resource cpu=0m on Node ip-172-31-39-36
Mar 17 09:07:45.114: INFO: Pod cattle-node-agent-xb7rh requesting resource cpu=0m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod cattle-node-agent-zbmd5 requesting resource cpu=0m on Node ip-172-31-35-106
Mar 17 09:07:45.114: INFO: Pod kube-api-auth-htl6k requesting resource cpu=0m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod calico-kube-controllers-fc7fcb565-nwnrh requesting resource cpu=0m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod canal-2xxhj requesting resource cpu=250m on Node ip-172-31-35-106
Mar 17 09:07:45.114: INFO: Pod canal-t2wwt requesting resource cpu=250m on Node ip-172-31-39-36
Mar 17 09:07:45.114: INFO: Pod canal-whdbp requesting resource cpu=250m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod coredns-5cb46d7c6-khnbz requesting resource cpu=100m on Node ip-172-31-35-106
Mar 17 09:07:45.114: INFO: Pod coredns-5cb46d7c6-wk9ms requesting resource cpu=100m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod coredns-autoscaler-6cb44df646-flpwt requesting resource cpu=20m on Node ip-172-31-39-36
Mar 17 09:07:45.114: INFO: Pod metrics-server-5c4895ffbd-9bbrg requesting resource cpu=100m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-39-36
Mar 17 09:07:45.114: INFO: Pod sonobuoy-e2e-job-3f99102f0d96453a requesting resource cpu=0m on Node ip-172-31-39-36
Mar 17 09:07:45.114: INFO: Pod sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-n98xh requesting resource cpu=0m on Node ip-172-31-35-106
Mar 17 09:07:45.114: INFO: Pod sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-ng79r requesting resource cpu=0m on Node ip-172-31-33-68
Mar 17 09:07:45.114: INFO: Pod sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-st4gk requesting resource cpu=0m on Node ip-172-31-39-36
STEP: Starting Pods to consume most of the cluster CPU.
Mar 17 09:07:45.114: INFO: Creating a pod which consumes cpu=2555m on Node ip-172-31-35-106
Mar 17 09:07:45.124: INFO: Creating a pod which consumes cpu=2611m on Node ip-172-31-39-36
Mar 17 09:07:45.132: INFO: Creating a pod which consumes cpu=2485m on Node ip-172-31-33-68
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27e33c75-0288-47ed-bdaa-21291588dcdb.16dd1fdb3c49ec9d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5804/filler-pod-27e33c75-0288-47ed-bdaa-21291588dcdb to ip-172-31-33-68]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27e33c75-0288-47ed-bdaa-21291588dcdb.16dd1fdb7c08bf7b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27e33c75-0288-47ed-bdaa-21291588dcdb.16dd1fdb7fcd5b58], Reason = [Created], Message = [Created container filler-pod-27e33c75-0288-47ed-bdaa-21291588dcdb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27e33c75-0288-47ed-bdaa-21291588dcdb.16dd1fdb86698c5b], Reason = [Started], Message = [Started container filler-pod-27e33c75-0288-47ed-bdaa-21291588dcdb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb6bf2ef-cf2b-4e8b-9bcf-592d2fd5b129.16dd1fdb3a7ecb0f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5804/filler-pod-bb6bf2ef-cf2b-4e8b-9bcf-592d2fd5b129 to ip-172-31-35-106]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb6bf2ef-cf2b-4e8b-9bcf-592d2fd5b129.16dd1fdb6b956c1b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb6bf2ef-cf2b-4e8b-9bcf-592d2fd5b129.16dd1fdb6eb42fb0], Reason = [Created], Message = [Created container filler-pod-bb6bf2ef-cf2b-4e8b-9bcf-592d2fd5b129]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb6bf2ef-cf2b-4e8b-9bcf-592d2fd5b129.16dd1fdb7464a4e8], Reason = [Started], Message = [Started container filler-pod-bb6bf2ef-cf2b-4e8b-9bcf-592d2fd5b129]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e27c69a1-db66-4432-8e7d-bab1ca8ec5ed.16dd1fdb3b0e5016], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5804/filler-pod-e27c69a1-db66-4432-8e7d-bab1ca8ec5ed to ip-172-31-39-36]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e27c69a1-db66-4432-8e7d-bab1ca8ec5ed.16dd1fdb71b21295], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e27c69a1-db66-4432-8e7d-bab1ca8ec5ed.16dd1fdb756a1245], Reason = [Created], Message = [Created container filler-pod-e27c69a1-db66-4432-8e7d-bab1ca8ec5ed]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e27c69a1-db66-4432-8e7d-bab1ca8ec5ed.16dd1fdb7b9812d2], Reason = [Started], Message = [Started container filler-pod-e27c69a1-db66-4432-8e7d-bab1ca8ec5ed]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16dd1fdbb4a9ebe5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-33-68
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-35-106
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-39-36
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:07:48.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5804" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":32,"skipped":560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:07:48.280: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:07:48.606: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:07:51.627: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:07:51.630: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8115-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:07:55.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-891" for this suite.
STEP: Destroying namespace "webhook-891-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.848 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":33,"skipped":643,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:07:55.130: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 17 09:07:55.216: INFO: Waiting up to 5m0s for pod "pod-56151e27-b540-46bf-b765-6b7c39dd4403" in namespace "emptydir-4120" to be "Succeeded or Failed"
Mar 17 09:07:55.230: INFO: Pod "pod-56151e27-b540-46bf-b765-6b7c39dd4403": Phase="Pending", Reason="", readiness=false. Elapsed: 13.404723ms
Mar 17 09:07:57.235: INFO: Pod "pod-56151e27-b540-46bf-b765-6b7c39dd4403": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018246743s
STEP: Saw pod success
Mar 17 09:07:57.235: INFO: Pod "pod-56151e27-b540-46bf-b765-6b7c39dd4403" satisfied condition "Succeeded or Failed"
Mar 17 09:07:57.238: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-56151e27-b540-46bf-b765-6b7c39dd4403 container test-container: <nil>
STEP: delete the pod
Mar 17 09:07:57.259: INFO: Waiting for pod pod-56151e27-b540-46bf-b765-6b7c39dd4403 to disappear
Mar 17 09:07:57.264: INFO: Pod pod-56151e27-b540-46bf-b765-6b7c39dd4403 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:07:57.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4120" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":661,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:07:57.283: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-b4t2
STEP: Creating a pod to test atomic-volume-subpath
Mar 17 09:07:57.339: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-b4t2" in namespace "subpath-3777" to be "Succeeded or Failed"
Mar 17 09:07:57.351: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.712326ms
Mar 17 09:07:59.358: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018992739s
Mar 17 09:08:01.362: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 4.022853058s
Mar 17 09:08:03.366: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 6.026380567s
Mar 17 09:08:05.371: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 8.032022451s
Mar 17 09:08:07.378: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 10.03885935s
Mar 17 09:08:09.388: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 12.048983035s
Mar 17 09:08:11.393: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 14.054088452s
Mar 17 09:08:13.401: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 16.061397754s
Mar 17 09:08:15.404: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 18.064932493s
Mar 17 09:08:17.415: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Running", Reason="", readiness=true. Elapsed: 20.075290006s
Mar 17 09:08:19.418: INFO: Pod "pod-subpath-test-configmap-b4t2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.079217773s
STEP: Saw pod success
Mar 17 09:08:19.419: INFO: Pod "pod-subpath-test-configmap-b4t2" satisfied condition "Succeeded or Failed"
Mar 17 09:08:19.423: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-subpath-test-configmap-b4t2 container test-container-subpath-configmap-b4t2: <nil>
STEP: delete the pod
Mar 17 09:08:19.448: INFO: Waiting for pod pod-subpath-test-configmap-b4t2 to disappear
Mar 17 09:08:19.456: INFO: Pod pod-subpath-test-configmap-b4t2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-b4t2
Mar 17 09:08:19.456: INFO: Deleting pod "pod-subpath-test-configmap-b4t2" in namespace "subpath-3777"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:08:19.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3777" for this suite.

• [SLOW TEST:22.199 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":35,"skipped":670,"failed":0}
SSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:08:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:08:25.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9753" for this suite.

• [SLOW TEST:6.143 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":36,"skipped":673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:08:25.628: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar 17 09:08:29.716: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6734 PodName:pod-sharedvolume-49d3c287-76e9-4d00-a017-1ca0788dbfa2 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:08:29.716: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:08:29.716: INFO: ExecWithOptions: Clientset creation
Mar 17 09:08:29.716: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/emptydir-6734/pods/pod-sharedvolume-49d3c287-76e9-4d00-a017-1ca0788dbfa2/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:08:29.824: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:08:29.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6734" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":37,"skipped":730,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:08:29.836: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:08:29.890: INFO: Creating deployment "webserver-deployment"
Mar 17 09:08:29.894: INFO: Waiting for observed generation 1
Mar 17 09:08:31.902: INFO: Waiting for all required pods to come up
Mar 17 09:08:31.910: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 17 09:08:33.938: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 17 09:08:33.954: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 17 09:08:33.974: INFO: Updating deployment webserver-deployment
Mar 17 09:08:33.974: INFO: Waiting for observed generation 2
Mar 17 09:08:35.990: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 17 09:08:35.994: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 17 09:08:35.995: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 17 09:08:36.006: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 17 09:08:36.006: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 17 09:08:36.009: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 17 09:08:36.013: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 17 09:08:36.013: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 17 09:08:36.033: INFO: Updating deployment webserver-deployment
Mar 17 09:08:36.033: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 17 09:08:36.054: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 17 09:08:36.072: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar 17 09:08:36.082: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3521  df24bd95-dcc1-4f28-9ab6-0bff906f7328 53623 3 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042546a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-17 09:08:32 +0000 UTC,LastTransitionTime:2022-03-17 09:08:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-03-17 09:08:34 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 17 09:08:36.112: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-3521  69cf061a-c401-44f5-a0f5-c22f93c0cfcf 53629 3 2022-03-17 09:08:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment df24bd95-dcc1-4f28-9ab6-0bff906f7328 0xc004254a77 0xc004254a78}] []  [{kube-controller-manager Update apps/v1 2022-03-17 09:08:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df24bd95-dcc1-4f28-9ab6-0bff906f7328\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004254b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 17 09:08:36.112: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 17 09:08:36.112: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-3521  ba6831dc-febf-487d-8f2f-136bb5a426cc 53625 3 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment df24bd95-dcc1-4f28-9ab6-0bff906f7328 0xc004254b77 0xc004254b78}] []  [{kube-controller-manager Update apps/v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df24bd95-dcc1-4f28-9ab6-0bff906f7328\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004254c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 17 09:08:36.141: INFO: Pod "webserver-deployment-566f96c878-49mh2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-49mh2 webserver-deployment-566f96c878- deployment-3521  2be6785a-fb1e-42fe-bada-48b1209b8db3 53646 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 69cf061a-c401-44f5-a0f5-c22f93c0cfcf 0xc003cc5cf7 0xc003cc5cf8}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cf061a-c401-44f5-a0f5-c22f93c0cfcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ld8fq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ld8fq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.141: INFO: Pod "webserver-deployment-566f96c878-79llc" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-79llc webserver-deployment-566f96c878- deployment-3521  27386672-1da9-4151-b77e-c31287353a7a 53626 0 2022-03-17 09:08:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:0cf685bff2fde53e2b7bb9366975f9e8545e0faeed50c6e512076c86e0322ac4 cni.projectcalico.org/podIP:10.42.0.170/32 cni.projectcalico.org/podIPs:10.42.0.170/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 69cf061a-c401-44f5-a0f5-c22f93c0cfcf 0xc003cc5e67 0xc003cc5e68}] []  [{Go-http-client Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cf061a-c401-44f5-a0f5-c22f93c0cfcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 09:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pph6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pph6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-68,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.68,PodIP:,StartTime:2022-03-17 09:08:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.141: INFO: Pod "webserver-deployment-566f96c878-8558t" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-8558t webserver-deployment-566f96c878- deployment-3521  f31b0947-21d5-4a95-87a4-f6d13b7f93a1 53619 0 2022-03-17 09:08:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:b0885ae952c95ba84daaeeb58b8aee2d197e3c5b8da71fb27f15c4b25c1e9326 cni.projectcalico.org/podIP:10.42.1.227/32 cni.projectcalico.org/podIPs:10.42.1.227/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 69cf061a-c401-44f5-a0f5-c22f93c0cfcf 0xc0043500a7 0xc0043500a8}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cf061a-c401-44f5-a0f5-c22f93c0cfcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 09:08:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 09:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k569g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k569g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.39.36,PodIP:10.42.1.227,StartTime:2022-03-17 09:08:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.142: INFO: Pod "webserver-deployment-566f96c878-g66dd" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-g66dd webserver-deployment-566f96c878- deployment-3521  bffd2474-0141-4f11-bc8c-ddde035ac83c 53645 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 69cf061a-c401-44f5-a0f5-c22f93c0cfcf 0xc0043502e0 0xc0043502e1}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cf061a-c401-44f5-a0f5-c22f93c0cfcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fj9bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fj9bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.142: INFO: Pod "webserver-deployment-566f96c878-jz975" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-jz975 webserver-deployment-566f96c878- deployment-3521  d98cc88c-c480-42aa-b0ac-90e48836a437 53598 0 2022-03-17 09:08:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:1d90cd0bb252929da35916b1ac155d43f8521daf7532326bf45351fef8101bce cni.projectcalico.org/podIP:10.42.2.195/32 cni.projectcalico.org/podIPs:10.42.2.195/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 69cf061a-c401-44f5-a0f5-c22f93c0cfcf 0xc004350447 0xc004350448}] []  [{Go-http-client Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cf061a-c401-44f5-a0f5-c22f93c0cfcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 09:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5942,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5942,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:,StartTime:2022-03-17 09:08:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.142: INFO: Pod "webserver-deployment-566f96c878-kjbxk" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-kjbxk webserver-deployment-566f96c878- deployment-3521  51a8c4bf-620f-4829-beff-108b177c6b0e 53622 0 2022-03-17 09:08:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:95220dea6cb3ad4cae688f1a2abb786aecd4e03b742b88369b4b10ae95af4b76 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 69cf061a-c401-44f5-a0f5-c22f93c0cfcf 0xc004350657 0xc004350658}] []  [{Go-http-client Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cf061a-c401-44f5-a0f5-c22f93c0cfcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xj82b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xj82b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.39.36,PodIP:,StartTime:2022-03-17 09:08:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.143: INFO: Pod "webserver-deployment-566f96c878-pt2jq" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-pt2jq webserver-deployment-566f96c878- deployment-3521  faee3a0d-0e30-4999-b397-cf860e69fe29 53596 0 2022-03-17 09:08:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:e40c79714f23ca22bdd8d37f693627c7bf003f52a0a2f7cdaf3ea5ab2dbdd238 cni.projectcalico.org/podIP:10.42.2.194/32 cni.projectcalico.org/podIPs:10.42.2.194/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 69cf061a-c401-44f5-a0f5-c22f93c0cfcf 0xc004350897 0xc004350898}] []  [{Go-http-client Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-03-17 09:08:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cf061a-c401-44f5-a0f5-c22f93c0cfcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 09:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfksv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfksv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:,StartTime:2022-03-17 09:08:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.143: INFO: Pod "webserver-deployment-566f96c878-qwldz" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-qwldz webserver-deployment-566f96c878- deployment-3521  0f89bfc2-3919-4d1a-a91f-6fad150bd2f7 53647 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 69cf061a-c401-44f5-a0f5-c22f93c0cfcf 0xc004350aa7 0xc004350aa8}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cf061a-c401-44f5-a0f5-c22f93c0cfcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dhp94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dhp94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-68,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.143: INFO: Pod "webserver-deployment-5d9fdcc779-5dxfh" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5dxfh webserver-deployment-5d9fdcc779- deployment-3521  223d5c80-ca41-4374-a85b-3fb24df55b07 53640 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004350c20 0xc004350c21}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zc5l4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zc5l4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.143: INFO: Pod "webserver-deployment-5d9fdcc779-9jjwt" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9jjwt webserver-deployment-5d9fdcc779- deployment-3521  8cf55b10-4380-43be-a550-292b1f564e8f 53643 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004350d57 0xc004350d58}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbms5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbms5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.144: INFO: Pod "webserver-deployment-5d9fdcc779-bdh9k" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-bdh9k webserver-deployment-5d9fdcc779- deployment-3521  8325b6da-f9f5-47b6-acc2-bdbb481e5537 53642 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004350e97 0xc004350e98}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mm9nc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mm9nc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.144: INFO: Pod "webserver-deployment-5d9fdcc779-c8hg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-c8hg7 webserver-deployment-5d9fdcc779- deployment-3521  7d25e391-6241-450c-a6c4-e73b3df6d067 53641 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004350fd7 0xc004350fd8}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcfc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcfc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.144: INFO: Pod "webserver-deployment-5d9fdcc779-c8js2" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-c8js2 webserver-deployment-5d9fdcc779- deployment-3521  9e538bb5-968b-46a4-a21d-775cc52785ff 53498 0 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:fdea1c6f567ff49f0fec62f3fd9d9e0269cf88be35c54cdfa10b3c375296c8f2 cni.projectcalico.org/podIP:10.42.1.223/32 cni.projectcalico.org/podIPs:10.42.1.223/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004351160 0xc004351161}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 09:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2l6qz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2l6qz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.39.36,PodIP:10.42.1.223,StartTime:2022-03-17 09:08:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:08:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://88112c9cdf72e44339a2fb69f1b317994d84840774876d6dc52c261d30b9e2a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.144: INFO: Pod "webserver-deployment-5d9fdcc779-dvph8" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-dvph8 webserver-deployment-5d9fdcc779- deployment-3521  d55ceb13-a982-4d29-8d19-fbf812872d27 53492 0 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:9139cc7ca8c25ff0f047e53235bd5b191bc006ddcac3bb531c531f3e5a737047 cni.projectcalico.org/podIP:10.42.1.225/32 cni.projectcalico.org/podIPs:10.42.1.225/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004351390 0xc004351391}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6nwqz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6nwqz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.39.36,PodIP:10.42.1.225,StartTime:2022-03-17 09:08:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:08:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://3051447b3093b5fdab4ddb4f23eb375e49a155b5b06dd140f27df56cadb4ed7d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.144: INFO: Pod "webserver-deployment-5d9fdcc779-gd8cw" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gd8cw webserver-deployment-5d9fdcc779- deployment-3521  f29fb034-fd14-447e-ade5-8a3591570d29 53501 0 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:c2fb51c52cff8efc39accbfbb0b9a73988233d44ac18ffe17fa1c039c0bfc9e5 cni.projectcalico.org/podIP:10.42.0.168/32 cni.projectcalico.org/podIPs:10.42.0.168/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc0043515c0 0xc0043515c1}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7h56l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7h56l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-68,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.68,PodIP:10.42.0.168,StartTime:2022-03-17 09:08:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:08:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://a7d43845f4be99380156e73636cc26d9d589a142f47aa55c438db6a72314b07e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.145: INFO: Pod "webserver-deployment-5d9fdcc779-gzlpw" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gzlpw webserver-deployment-5d9fdcc779- deployment-3521  f62501dd-905d-4bcb-8930-ed4bead19158 53502 0 2022-03-17 09:08:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:e9ccac4458caf1b4b40bfb4ee14a7e546fbcf5e2f0a31ad10b54aa296b94d78f cni.projectcalico.org/podIP:10.42.0.169/32 cni.projectcalico.org/podIPs:10.42.0.169/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc0043517f0 0xc0043517f1}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qhsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qhsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-68,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.68,PodIP:10.42.0.169,StartTime:2022-03-17 09:08:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:08:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://6078ce52d4183b31b78e35f811a69bb3d2f78daa665b00b8e350dc76330b5dc4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.145: INFO: Pod "webserver-deployment-5d9fdcc779-h8xwc" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-h8xwc webserver-deployment-5d9fdcc779- deployment-3521  773b5527-7edf-4780-be94-846bfde1d82e 53521 0 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:f0eeb116f5460c442ceeb587007247cd977c6b5a71ef17b85afdab2a4ea62528 cni.projectcalico.org/podIP:10.42.2.191/32 cni.projectcalico.org/podIPs:10.42.2.191/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004351a20 0xc004351a21}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 09:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 09:08:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-trxb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-trxb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:10.42.2.191,StartTime:2022-03-17 09:08:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:08:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://b3a2fd22b11df21064c31cc054e438f8c152161b76f00a18440e11820f601b9a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.145: INFO: Pod "webserver-deployment-5d9fdcc779-lj8g7" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-lj8g7 webserver-deployment-5d9fdcc779- deployment-3521  2be40dd2-d262-40a6-b2bb-1ba01cfecbc7 53512 0 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:6a2433a96cb405c3b43a6bfb1f6f6c8c0c8acbd53d062160c36675775b26e0ca cni.projectcalico.org/podIP:10.42.2.190/32 cni.projectcalico.org/podIPs:10.42.2.190/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004351c50 0xc004351c51}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 09:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 09:08:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xsfqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xsfqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:10.42.2.190,StartTime:2022-03-17 09:08:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:08:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://fbe59870141049b14f2c6eec61e5b9ca81083e1777c907ecabf5fd39c0a2bf2a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.145: INFO: Pod "webserver-deployment-5d9fdcc779-lsh69" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-lsh69 webserver-deployment-5d9fdcc779- deployment-3521  b1ff51d2-d394-4160-8f10-e2c448fd023c 53644 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004351e60 0xc004351e61}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dhlv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dhlv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.145: INFO: Pod "webserver-deployment-5d9fdcc779-p2cs8" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-p2cs8 webserver-deployment-5d9fdcc779- deployment-3521  ba7bbdcd-58e3-406d-aa24-4387d5967c53 53636 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004351f97 0xc004351f98}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kdnw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kdnw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.145: INFO: Pod "webserver-deployment-5d9fdcc779-plx7z" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-plx7z webserver-deployment-5d9fdcc779- deployment-3521  628d7cb8-2cb0-4cca-8354-999427c3ffb4 53494 0 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:488f0a78ca750546d6a0e5843b627174b721ba68376da35cb31f04d56bdb4919 cni.projectcalico.org/podIP:10.42.1.224/32 cni.projectcalico.org/podIPs:10.42.1.224/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004374120 0xc004374121}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jr8jg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jr8jg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.39.36,PodIP:10.42.1.224,StartTime:2022-03-17 09:08:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:08:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://dfd2466f6f3a9a78c45db2da00bd2c6e200845406669b8216320146345259dc6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.146: INFO: Pod "webserver-deployment-5d9fdcc779-r4vpd" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-r4vpd webserver-deployment-5d9fdcc779- deployment-3521  0e28f4e6-36a4-4a05-b5ce-14c2059cb5aa 53638 0 2022-03-17 09:08:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004374330 0xc004374331}] []  [{Go-http-client Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-03-17 09:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gpcsl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gpcsl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:,StartTime:2022-03-17 09:08:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 09:08:36.146: INFO: Pod "webserver-deployment-5d9fdcc779-ws9xw" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ws9xw webserver-deployment-5d9fdcc779- deployment-3521  244b20fb-0cea-44ca-9b15-010fa26fba2d 53506 0 2022-03-17 09:08:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:17205cb0f1cf13792eafeb281be15bb1e4623d570de451cff4038549b3a26e6b cni.projectcalico.org/podIP:10.42.0.167/32 cni.projectcalico.org/podIPs:10.42.0.167/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 ba6831dc-febf-487d-8f2f-136bb5a426cc 0xc004374517 0xc004374518}] []  [{kube-controller-manager Update v1 2022-03-17 09:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba6831dc-febf-487d-8f2f-136bb5a426cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 09:08:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5bb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5bb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-68,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:08:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.68,PodIP:10.42.0.167,StartTime:2022-03-17 09:08:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:08:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://76ff967fed0baf6fb25a425a1f2bdf6b9839c1bb7468746f04438ce25c52eda4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:08:36.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3521" for this suite.

• [SLOW TEST:6.351 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":38,"skipped":740,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:08:36.187: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:08:36.422: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 17 09:08:36.439: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 09:08:36.439: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 09:08:37.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 09:08:37.447: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 09:08:38.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 17 09:08:38.460: INFO: Node ip-172-31-35-106 is running 0 daemon pod, expected 1
Mar 17 09:08:39.449: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 17 09:08:39.449: INFO: Node ip-172-31-35-106 is running 0 daemon pod, expected 1
Mar 17 09:08:40.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 09:08:40.446: INFO: Node ip-172-31-35-106 is running 0 daemon pod, expected 1
Mar 17 09:08:41.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 09:08:41.484: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 17 09:08:41.610: INFO: Wrong image for pod: daemon-set-9rncz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:41.610: INFO: Wrong image for pod: daemon-set-h5tdh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:41.610: INFO: Wrong image for pod: daemon-set-mm45p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:42.647: INFO: Wrong image for pod: daemon-set-h5tdh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:42.647: INFO: Wrong image for pod: daemon-set-mm45p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:43.644: INFO: Wrong image for pod: daemon-set-h5tdh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:43.644: INFO: Wrong image for pod: daemon-set-mm45p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:44.643: INFO: Wrong image for pod: daemon-set-h5tdh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:44.644: INFO: Wrong image for pod: daemon-set-mm45p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:45.643: INFO: Pod daemon-set-bc4gc is not available
Mar 17 09:08:45.644: INFO: Wrong image for pod: daemon-set-h5tdh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:45.644: INFO: Wrong image for pod: daemon-set-mm45p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:46.644: INFO: Pod daemon-set-bc4gc is not available
Mar 17 09:08:46.645: INFO: Wrong image for pod: daemon-set-h5tdh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:46.645: INFO: Wrong image for pod: daemon-set-mm45p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:47.644: INFO: Wrong image for pod: daemon-set-h5tdh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:48.651: INFO: Pod daemon-set-bp7zq is not available
Mar 17 09:08:48.651: INFO: Wrong image for pod: daemon-set-h5tdh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 17 09:08:51.643: INFO: Pod daemon-set-84fxv is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 17 09:08:51.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 09:08:51.652: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 09:08:52.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 09:08:52.659: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 09:08:53.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 09:08:53.661: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2306, will wait for the garbage collector to delete the pods
Mar 17 09:08:53.735: INFO: Deleting DaemonSet.extensions daemon-set took: 5.274673ms
Mar 17 09:08:53.836: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.063268ms
Mar 17 09:08:56.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 09:08:56.141: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 17 09:08:56.143: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54120"},"items":null}

Mar 17 09:08:56.146: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54120"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:08:56.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2306" for this suite.

• [SLOW TEST:19.983 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":39,"skipped":741,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:08:56.171: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-450
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-450
I0317 09:08:56.321957      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-450, replica count: 2
I0317 09:08:59.372753      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 09:08:59.372: INFO: Creating new exec pod
Mar 17 09:09:02.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 17 09:09:02.786: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 17 09:09:02.786: INFO: stdout: ""
Mar 17 09:09:03.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 17 09:09:03.991: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 17 09:09:03.991: INFO: stdout: ""
Mar 17 09:09:04.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 17 09:09:04.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 17 09:09:04.979: INFO: stdout: ""
Mar 17 09:09:05.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 17 09:09:06.004: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 17 09:09:06.004: INFO: stdout: ""
Mar 17 09:09:06.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 17 09:09:06.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 17 09:09:06.962: INFO: stdout: "externalname-service-wt99f"
Mar 17 09:09:06.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.61.7 80'
Mar 17 09:09:07.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.61.7 80\nConnection to 10.43.61.7 80 port [tcp/http] succeeded!\n"
Mar 17 09:09:07.172: INFO: stdout: ""
Mar 17 09:09:08.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.61.7 80'
Mar 17 09:09:08.334: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.61.7 80\nConnection to 10.43.61.7 80 port [tcp/http] succeeded!\n"
Mar 17 09:09:08.334: INFO: stdout: "externalname-service-hzwdc"
Mar 17 09:09:08.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.33.68 31605'
Mar 17 09:09:08.506: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.33.68 31605\nConnection to 172.31.33.68 31605 port [tcp/*] succeeded!\n"
Mar 17 09:09:08.506: INFO: stdout: "externalname-service-wt99f"
Mar 17 09:09:08.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-450 exec execpod9hhb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.39.36 31605'
Mar 17 09:09:08.661: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.39.36 31605\nConnection to 172.31.39.36 31605 port [tcp/*] succeeded!\n"
Mar 17 09:09:08.661: INFO: stdout: "externalname-service-wt99f"
Mar 17 09:09:08.661: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:09:08.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-450" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.560 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":40,"skipped":741,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:09:08.730: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 17 09:09:08.839: INFO: Waiting up to 5m0s for pod "pod-58226ee3-a861-4171-af73-f5158ad0a7e0" in namespace "emptydir-3141" to be "Succeeded or Failed"
Mar 17 09:09:08.862: INFO: Pod "pod-58226ee3-a861-4171-af73-f5158ad0a7e0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.267926ms
Mar 17 09:09:10.866: INFO: Pod "pod-58226ee3-a861-4171-af73-f5158ad0a7e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026895172s
STEP: Saw pod success
Mar 17 09:09:10.866: INFO: Pod "pod-58226ee3-a861-4171-af73-f5158ad0a7e0" satisfied condition "Succeeded or Failed"
Mar 17 09:09:10.870: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-58226ee3-a861-4171-af73-f5158ad0a7e0 container test-container: <nil>
STEP: delete the pod
Mar 17 09:09:10.893: INFO: Waiting for pod pod-58226ee3-a861-4171-af73-f5158ad0a7e0 to disappear
Mar 17 09:09:10.897: INFO: Pod pod-58226ee3-a861-4171-af73-f5158ad0a7e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:09:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3141" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":41,"skipped":742,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:09:10.904: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-207cd190-294e-4cca-b394-2799b2322044
STEP: Creating a pod to test consume configMaps
Mar 17 09:09:10.966: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-41bc6d3d-901c-4713-a6e4-33913d946fed" in namespace "projected-1730" to be "Succeeded or Failed"
Mar 17 09:09:10.978: INFO: Pod "pod-projected-configmaps-41bc6d3d-901c-4713-a6e4-33913d946fed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.171676ms
Mar 17 09:09:12.982: INFO: Pod "pod-projected-configmaps-41bc6d3d-901c-4713-a6e4-33913d946fed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016416478s
STEP: Saw pod success
Mar 17 09:09:12.983: INFO: Pod "pod-projected-configmaps-41bc6d3d-901c-4713-a6e4-33913d946fed" satisfied condition "Succeeded or Failed"
Mar 17 09:09:12.985: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-configmaps-41bc6d3d-901c-4713-a6e4-33913d946fed container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:09:13.002: INFO: Waiting for pod pod-projected-configmaps-41bc6d3d-901c-4713-a6e4-33913d946fed to disappear
Mar 17 09:09:13.005: INFO: Pod pod-projected-configmaps-41bc6d3d-901c-4713-a6e4-33913d946fed no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:09:13.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1730" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":759,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:09:13.017: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:09:13.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5899" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:09:13.116: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-7fd31083-ca8d-495b-9f17-dd5e1db8d958 in namespace container-probe-3067
Mar 17 09:09:15.183: INFO: Started pod busybox-7fd31083-ca8d-495b-9f17-dd5e1db8d958 in namespace container-probe-3067
STEP: checking the pod's current state and verifying that restartCount is present
Mar 17 09:09:15.186: INFO: Initial restart count of pod busybox-7fd31083-ca8d-495b-9f17-dd5e1db8d958 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:13:15.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3067" for this suite.

• [SLOW TEST:242.689 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":840,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:13:15.805: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:13:15.877: INFO: created pod
Mar 17 09:13:15.877: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6307" to be "Succeeded or Failed"
Mar 17 09:13:15.880: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.124759ms
Mar 17 09:13:17.884: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006720554s
STEP: Saw pod success
Mar 17 09:13:17.884: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 17 09:13:47.884: INFO: polling logs
Mar 17 09:13:47.916: INFO: Pod logs: 
2022/03/17 09:13:16 OK: Got token
2022/03/17 09:13:16 validating with in-cluster discovery
2022/03/17 09:13:16 OK: got issuer https://3.21.53.65:6443
2022/03/17 09:13:16 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://3.21.53.65:6443", Subject:"system:serviceaccount:svcaccounts-6307:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1647508996, NotBefore:1647508396, IssuedAt:1647508396, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6307", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"fc212433-014a-41a1-9356-bc6d85b6b03f"}}}
2022/03/17 09:13:16 OK: Constructed OIDC provider for issuer https://3.21.53.65:6443
2022/03/17 09:13:16 OK: Validated signature on JWT
2022/03/17 09:13:16 OK: Got valid claims from token!
2022/03/17 09:13:16 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://3.21.53.65:6443", Subject:"system:serviceaccount:svcaccounts-6307:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1647508996, NotBefore:1647508396, IssuedAt:1647508396, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6307", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"fc212433-014a-41a1-9356-bc6d85b6b03f"}}}

Mar 17 09:13:47.916: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:13:47.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6307" for this suite.

• [SLOW TEST:32.137 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":45,"skipped":856,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:13:47.942: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:13:48.014: INFO: Got root ca configmap in namespace "svcaccounts-4094"
Mar 17 09:13:48.024: INFO: Deleted root ca configmap in namespace "svcaccounts-4094"
STEP: waiting for a new root ca configmap created
Mar 17 09:13:48.528: INFO: Recreated root ca configmap in namespace "svcaccounts-4094"
Mar 17 09:13:48.532: INFO: Updated root ca configmap in namespace "svcaccounts-4094"
STEP: waiting for the root ca configmap reconciled
Mar 17 09:13:49.036: INFO: Reconciled root ca configmap in namespace "svcaccounts-4094"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:13:49.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4094" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":46,"skipped":868,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:13:49.045: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-8291d9af-af05-42dc-9443-e24461d44f7d
STEP: Creating a pod to test consume secrets
Mar 17 09:13:49.122: INFO: Waiting up to 5m0s for pod "pod-secrets-0c2a323f-3bda-4173-bfe9-889a3d6f23cf" in namespace "secrets-939" to be "Succeeded or Failed"
Mar 17 09:13:49.129: INFO: Pod "pod-secrets-0c2a323f-3bda-4173-bfe9-889a3d6f23cf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.007283ms
Mar 17 09:13:51.133: INFO: Pod "pod-secrets-0c2a323f-3bda-4173-bfe9-889a3d6f23cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010540291s
STEP: Saw pod success
Mar 17 09:13:51.133: INFO: Pod "pod-secrets-0c2a323f-3bda-4173-bfe9-889a3d6f23cf" satisfied condition "Succeeded or Failed"
Mar 17 09:13:51.135: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-secrets-0c2a323f-3bda-4173-bfe9-889a3d6f23cf container secret-env-test: <nil>
STEP: delete the pod
Mar 17 09:13:51.148: INFO: Waiting for pod pod-secrets-0c2a323f-3bda-4173-bfe9-889a3d6f23cf to disappear
Mar 17 09:13:51.151: INFO: Pod pod-secrets-0c2a323f-3bda-4173-bfe9-889a3d6f23cf no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:13:51.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-939" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":877,"failed":0}
SSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:13:51.163: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Mar 17 09:13:51.219: INFO: Waiting up to 5m0s for pod "client-containers-ab0d15d3-9cec-4198-8e2d-9a11c91bb9ec" in namespace "containers-4866" to be "Succeeded or Failed"
Mar 17 09:13:51.222: INFO: Pod "client-containers-ab0d15d3-9cec-4198-8e2d-9a11c91bb9ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226677ms
Mar 17 09:13:53.225: INFO: Pod "client-containers-ab0d15d3-9cec-4198-8e2d-9a11c91bb9ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006535158s
STEP: Saw pod success
Mar 17 09:13:53.226: INFO: Pod "client-containers-ab0d15d3-9cec-4198-8e2d-9a11c91bb9ec" satisfied condition "Succeeded or Failed"
Mar 17 09:13:53.228: INFO: Trying to get logs from node ip-172-31-35-106 pod client-containers-ab0d15d3-9cec-4198-8e2d-9a11c91bb9ec container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:13:53.243: INFO: Waiting for pod client-containers-ab0d15d3-9cec-4198-8e2d-9a11c91bb9ec to disappear
Mar 17 09:13:53.245: INFO: Pod client-containers-ab0d15d3-9cec-4198-8e2d-9a11c91bb9ec no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:13:53.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4866" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":881,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:13:53.254: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-6476
STEP: creating replication controller nodeport-test in namespace services-6476
I0317 09:13:53.342367      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6476, replica count: 2
I0317 09:13:56.394046      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 09:13:56.394: INFO: Creating new exec pod
Mar 17 09:13:59.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6476 exec execpodq6df8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar 17 09:13:59.581: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 17 09:13:59.581: INFO: stdout: "nodeport-test-8mwmc"
Mar 17 09:13:59.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6476 exec execpodq6df8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.70.191 80'
Mar 17 09:13:59.767: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.43.70.191 80\nConnection to 10.43.70.191 80 port [tcp/http] succeeded!\n"
Mar 17 09:13:59.767: INFO: stdout: ""
Mar 17 09:14:00.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6476 exec execpodq6df8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.70.191 80'
Mar 17 09:14:00.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.70.191 80\nConnection to 10.43.70.191 80 port [tcp/http] succeeded!\n"
Mar 17 09:14:00.942: INFO: stdout: ""
Mar 17 09:14:01.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6476 exec execpodq6df8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.70.191 80'
Mar 17 09:14:01.921: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.70.191 80\nConnection to 10.43.70.191 80 port [tcp/http] succeeded!\n"
Mar 17 09:14:01.921: INFO: stdout: ""
Mar 17 09:14:02.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6476 exec execpodq6df8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.70.191 80'
Mar 17 09:14:02.970: INFO: stderr: "+ nc -v -t -w 2 10.43.70.191 80\nConnection to 10.43.70.191 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Mar 17 09:14:02.970: INFO: stdout: "nodeport-test-sspgd"
Mar 17 09:14:02.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6476 exec execpodq6df8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.33.68 31250'
Mar 17 09:14:03.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.33.68 31250\nConnection to 172.31.33.68 31250 port [tcp/*] succeeded!\n"
Mar 17 09:14:03.131: INFO: stdout: "nodeport-test-8mwmc"
Mar 17 09:14:03.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6476 exec execpodq6df8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.35.106 31250'
Mar 17 09:14:03.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.35.106 31250\nConnection to 172.31.35.106 31250 port [tcp/*] succeeded!\n"
Mar 17 09:14:03.269: INFO: stdout: "nodeport-test-8mwmc"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:14:03.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6476" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.023 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":49,"skipped":885,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:14:03.278: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-26b30de1-cf97-4029-8511-ed706023c41d
STEP: Creating a pod to test consume secrets
Mar 17 09:14:03.369: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-010be656-5de3-446a-8063-937719017ad4" in namespace "projected-2220" to be "Succeeded or Failed"
Mar 17 09:14:03.375: INFO: Pod "pod-projected-secrets-010be656-5de3-446a-8063-937719017ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.546416ms
Mar 17 09:14:05.379: INFO: Pod "pod-projected-secrets-010be656-5de3-446a-8063-937719017ad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010154348s
STEP: Saw pod success
Mar 17 09:14:05.379: INFO: Pod "pod-projected-secrets-010be656-5de3-446a-8063-937719017ad4" satisfied condition "Succeeded or Failed"
Mar 17 09:14:05.382: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-secrets-010be656-5de3-446a-8063-937719017ad4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:14:05.396: INFO: Waiting for pod pod-projected-secrets-010be656-5de3-446a-8063-937719017ad4 to disappear
Mar 17 09:14:05.401: INFO: Pod pod-projected-secrets-010be656-5de3-446a-8063-937719017ad4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:14:05.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2220" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":887,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:14:05.414: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-bd37aaa1-f7d1-4db7-98f8-15140ba8b02d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:14:09.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3086" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":51,"skipped":890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:14:09.530: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar 17 09:14:09.590: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:14:13.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4179" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":52,"skipped":913,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:14:13.555: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 09:14:13.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-651f22e1-01d8-4521-bfa8-2a5060ab5400" in namespace "downward-api-3207" to be "Succeeded or Failed"
Mar 17 09:14:13.612: INFO: Pod "downwardapi-volume-651f22e1-01d8-4521-bfa8-2a5060ab5400": Phase="Pending", Reason="", readiness=false. Elapsed: 8.68219ms
Mar 17 09:14:15.617: INFO: Pod "downwardapi-volume-651f22e1-01d8-4521-bfa8-2a5060ab5400": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013763133s
STEP: Saw pod success
Mar 17 09:14:15.617: INFO: Pod "downwardapi-volume-651f22e1-01d8-4521-bfa8-2a5060ab5400" satisfied condition "Succeeded or Failed"
Mar 17 09:14:15.620: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-651f22e1-01d8-4521-bfa8-2a5060ab5400 container client-container: <nil>
STEP: delete the pod
Mar 17 09:14:15.642: INFO: Waiting for pod downwardapi-volume-651f22e1-01d8-4521-bfa8-2a5060ab5400 to disappear
Mar 17 09:14:15.647: INFO: Pod downwardapi-volume-651f22e1-01d8-4521-bfa8-2a5060ab5400 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:14:15.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3207" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":914,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:14:15.668: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-8535ba2c-28a0-45fb-8866-fcd008205812
STEP: Creating a pod to test consume configMaps
Mar 17 09:14:15.745: INFO: Waiting up to 5m0s for pod "pod-configmaps-85255f3c-82e5-4e30-870e-a288bd9a3ed9" in namespace "configmap-4356" to be "Succeeded or Failed"
Mar 17 09:14:15.749: INFO: Pod "pod-configmaps-85255f3c-82e5-4e30-870e-a288bd9a3ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430287ms
Mar 17 09:14:17.754: INFO: Pod "pod-configmaps-85255f3c-82e5-4e30-870e-a288bd9a3ed9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009079327s
STEP: Saw pod success
Mar 17 09:14:17.754: INFO: Pod "pod-configmaps-85255f3c-82e5-4e30-870e-a288bd9a3ed9" satisfied condition "Succeeded or Failed"
Mar 17 09:14:17.757: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-85255f3c-82e5-4e30-870e-a288bd9a3ed9 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:14:17.774: INFO: Waiting for pod pod-configmaps-85255f3c-82e5-4e30-870e-a288bd9a3ed9 to disappear
Mar 17 09:14:17.777: INFO: Pod pod-configmaps-85255f3c-82e5-4e30-870e-a288bd9a3ed9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:14:17.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4356" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":963,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:14:17.783: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-zjh5
STEP: Creating a pod to test atomic-volume-subpath
Mar 17 09:14:17.865: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zjh5" in namespace "subpath-6071" to be "Succeeded or Failed"
Mar 17 09:14:17.883: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.554453ms
Mar 17 09:14:19.889: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.023450988s
Mar 17 09:14:21.892: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 4.026738007s
Mar 17 09:14:23.896: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 6.030311774s
Mar 17 09:14:25.907: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 8.041993338s
Mar 17 09:14:27.914: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 10.048430252s
Mar 17 09:14:29.918: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 12.052590248s
Mar 17 09:14:31.925: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 14.059931903s
Mar 17 09:14:33.930: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 16.065231575s
Mar 17 09:14:35.941: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 18.075405715s
Mar 17 09:14:37.948: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Running", Reason="", readiness=true. Elapsed: 20.082700358s
Mar 17 09:14:39.963: INFO: Pod "pod-subpath-test-downwardapi-zjh5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.097347099s
STEP: Saw pod success
Mar 17 09:14:39.963: INFO: Pod "pod-subpath-test-downwardapi-zjh5" satisfied condition "Succeeded or Failed"
Mar 17 09:14:39.968: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-subpath-test-downwardapi-zjh5 container test-container-subpath-downwardapi-zjh5: <nil>
STEP: delete the pod
Mar 17 09:14:39.985: INFO: Waiting for pod pod-subpath-test-downwardapi-zjh5 to disappear
Mar 17 09:14:39.988: INFO: Pod pod-subpath-test-downwardapi-zjh5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-zjh5
Mar 17 09:14:39.988: INFO: Deleting pod "pod-subpath-test-downwardapi-zjh5" in namespace "subpath-6071"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:14:39.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6071" for this suite.

• [SLOW TEST:22.217 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":55,"skipped":975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:14:40.001: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-08f4f3b9-e390-413f-90e8-e1c333a8d18c
STEP: Creating the pod
Mar 17 09:14:40.100: INFO: The status of Pod pod-projected-configmaps-06a7b8a7-7b5e-4602-a4b6-1e58116c1583 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:14:42.104: INFO: The status of Pod pod-projected-configmaps-06a7b8a7-7b5e-4602-a4b6-1e58116c1583 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-08f4f3b9-e390-413f-90e8-e1c333a8d18c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:14:44.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9724" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":1008,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:14:44.146: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6906
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 17 09:14:44.187: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 17 09:14:44.255: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:14:46.262: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:14:48.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:14:50.259: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:14:52.265: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:14:54.261: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:14:56.262: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:14:58.263: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:15:00.260: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:15:02.261: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:15:04.265: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 17 09:15:04.275: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 17 09:15:04.279: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 17 09:15:08.313: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 17 09:15:08.313: INFO: Going to poll 10.42.0.175 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 17 09:15:08.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.175:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6906 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:15:08.316: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:15:08.317: INFO: ExecWithOptions: Clientset creation
Mar 17 09:15:08.317: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6906/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.0.175%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:15:08.407: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 17 09:15:08.407: INFO: Going to poll 10.42.2.214 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 17 09:15:08.410: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.2.214:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6906 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:15:08.410: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:15:08.411: INFO: ExecWithOptions: Clientset creation
Mar 17 09:15:08.411: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6906/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.2.214%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:15:08.506: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 17 09:15:08.506: INFO: Going to poll 10.42.1.234 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 17 09:15:08.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.1.234:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6906 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:15:08.509: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:15:08.510: INFO: ExecWithOptions: Clientset creation
Mar 17 09:15:08.510: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6906/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.1.234%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:15:08.607: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:08.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6906" for this suite.

• [SLOW TEST:24.477 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":57,"skipped":1012,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:15:09.119: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:15:12.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:12.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9978" for this suite.
STEP: Destroying namespace "webhook-9978-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":58,"skipped":1032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:12.285: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:23.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6100" for this suite.

• [SLOW TEST:11.117 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":59,"skipped":1055,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:23.403: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:23.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3762" for this suite.
STEP: Destroying namespace "nspatchtest-37539be5-c649-4dc9-a36d-7bd41759817c-4072" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":60,"skipped":1067,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:23.557: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-4ae01308-8a78-422e-ad20-fd4685bd77d8
STEP: Creating a pod to test consume configMaps
Mar 17 09:15:23.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-74a239fd-a099-4c3f-94f4-14c700268c40" in namespace "configmap-2688" to be "Succeeded or Failed"
Mar 17 09:15:23.642: INFO: Pod "pod-configmaps-74a239fd-a099-4c3f-94f4-14c700268c40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.32028ms
Mar 17 09:15:25.647: INFO: Pod "pod-configmaps-74a239fd-a099-4c3f-94f4-14c700268c40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0088413s
STEP: Saw pod success
Mar 17 09:15:25.647: INFO: Pod "pod-configmaps-74a239fd-a099-4c3f-94f4-14c700268c40" satisfied condition "Succeeded or Failed"
Mar 17 09:15:25.649: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-74a239fd-a099-4c3f-94f4-14c700268c40 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:15:25.667: INFO: Waiting for pod pod-configmaps-74a239fd-a099-4c3f-94f4-14c700268c40 to disappear
Mar 17 09:15:25.671: INFO: Pod pod-configmaps-74a239fd-a099-4c3f-94f4-14c700268c40 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:25.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2688" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1067,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:25.683: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:25.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6509" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":62,"skipped":1081,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:25.740: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-aa9ed01d-bcec-4567-8e56-93e3586d76cb
STEP: Creating secret with name s-test-opt-upd-c0e1b273-c049-48e2-9e37-40ae81ac9638
STEP: Creating the pod
Mar 17 09:15:25.812: INFO: The status of Pod pod-projected-secrets-63f5fbd8-efa7-41c9-be51-c5bd964727b3 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:15:27.816: INFO: The status of Pod pod-projected-secrets-63f5fbd8-efa7-41c9-be51-c5bd964727b3 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-aa9ed01d-bcec-4567-8e56-93e3586d76cb
STEP: Updating secret s-test-opt-upd-c0e1b273-c049-48e2-9e37-40ae81ac9638
STEP: Creating secret with name s-test-opt-create-ec3fb0c5-1fa1-449a-a6c1-64c0b5409001
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:29.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1762" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1086,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:29.889: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 17 09:15:29.930: INFO: Waiting up to 5m0s for pod "pod-b207c69b-4c62-4abd-9504-49f52939a3a3" in namespace "emptydir-8615" to be "Succeeded or Failed"
Mar 17 09:15:29.939: INFO: Pod "pod-b207c69b-4c62-4abd-9504-49f52939a3a3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.722817ms
Mar 17 09:15:31.951: INFO: Pod "pod-b207c69b-4c62-4abd-9504-49f52939a3a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020874401s
STEP: Saw pod success
Mar 17 09:15:31.951: INFO: Pod "pod-b207c69b-4c62-4abd-9504-49f52939a3a3" satisfied condition "Succeeded or Failed"
Mar 17 09:15:31.955: INFO: Trying to get logs from node ip-172-31-39-36 pod pod-b207c69b-4c62-4abd-9504-49f52939a3a3 container test-container: <nil>
STEP: delete the pod
Mar 17 09:15:31.981: INFO: Waiting for pod pod-b207c69b-4c62-4abd-9504-49f52939a3a3 to disappear
Mar 17 09:15:31.987: INFO: Pod pod-b207c69b-4c62-4abd-9504-49f52939a3a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:31.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8615" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1087,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:31.994: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 17 09:15:32.075: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:15:35.017: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:48.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3351" for this suite.

• [SLOW TEST:16.327 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":65,"skipped":1090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:48.321: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Mar 17 09:15:48.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 create -f -'
Mar 17 09:15:49.082: INFO: stderr: ""
Mar 17 09:15:49.082: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 17 09:15:49.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 17 09:15:49.142: INFO: stderr: ""
Mar 17 09:15:49.142: INFO: stdout: "update-demo-nautilus-llgwm update-demo-nautilus-tclm5 "
Mar 17 09:15:49.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get pods update-demo-nautilus-llgwm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 09:15:49.202: INFO: stderr: ""
Mar 17 09:15:49.202: INFO: stdout: ""
Mar 17 09:15:49.202: INFO: update-demo-nautilus-llgwm is created but not running
Mar 17 09:15:54.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 17 09:15:54.288: INFO: stderr: ""
Mar 17 09:15:54.288: INFO: stdout: "update-demo-nautilus-llgwm update-demo-nautilus-tclm5 "
Mar 17 09:15:54.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get pods update-demo-nautilus-llgwm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 09:15:54.362: INFO: stderr: ""
Mar 17 09:15:54.362: INFO: stdout: "true"
Mar 17 09:15:54.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get pods update-demo-nautilus-llgwm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 17 09:15:54.437: INFO: stderr: ""
Mar 17 09:15:54.437: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 17 09:15:54.437: INFO: validating pod update-demo-nautilus-llgwm
Mar 17 09:15:54.440: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 17 09:15:54.440: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 17 09:15:54.440: INFO: update-demo-nautilus-llgwm is verified up and running
Mar 17 09:15:54.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get pods update-demo-nautilus-tclm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 09:15:54.507: INFO: stderr: ""
Mar 17 09:15:54.507: INFO: stdout: "true"
Mar 17 09:15:54.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get pods update-demo-nautilus-tclm5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 17 09:15:54.577: INFO: stderr: ""
Mar 17 09:15:54.577: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 17 09:15:54.577: INFO: validating pod update-demo-nautilus-tclm5
Mar 17 09:15:54.580: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 17 09:15:54.580: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 17 09:15:54.580: INFO: update-demo-nautilus-tclm5 is verified up and running
STEP: using delete to clean up resources
Mar 17 09:15:54.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 delete --grace-period=0 --force -f -'
Mar 17 09:15:54.665: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 09:15:54.665: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 17 09:15:54.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get rc,svc -l name=update-demo --no-headers'
Mar 17 09:15:54.775: INFO: stderr: "No resources found in kubectl-2829 namespace.\n"
Mar 17 09:15:54.775: INFO: stdout: ""
Mar 17 09:15:54.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2829 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 17 09:15:54.879: INFO: stderr: ""
Mar 17 09:15:54.879: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:15:54.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2829" for this suite.

• [SLOW TEST:6.570 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":66,"skipped":1122,"failed":0}
SSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:15:54.892: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar 17 09:15:54.942: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar 17 09:15:54.963: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 17 09:15:54.963: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 17 09:15:54.978: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 17 09:15:54.978: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 17 09:15:54.998: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 17 09:15:54.998: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 17 09:16:02.040: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:16:02.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-356" for this suite.

• [SLOW TEST:7.192 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":67,"skipped":1125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:16:02.085: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:16:02.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-770" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":68,"skipped":1149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:16:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 17 09:16:02.295: INFO: Waiting up to 5m0s for pod "downward-api-5edba175-e3ec-4458-9c94-6d02f7cdc0fb" in namespace "downward-api-7759" to be "Succeeded or Failed"
Mar 17 09:16:02.309: INFO: Pod "downward-api-5edba175-e3ec-4458-9c94-6d02f7cdc0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.859836ms
Mar 17 09:16:04.312: INFO: Pod "downward-api-5edba175-e3ec-4458-9c94-6d02f7cdc0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017629017s
Mar 17 09:16:06.317: INFO: Pod "downward-api-5edba175-e3ec-4458-9c94-6d02f7cdc0fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021821243s
STEP: Saw pod success
Mar 17 09:16:06.317: INFO: Pod "downward-api-5edba175-e3ec-4458-9c94-6d02f7cdc0fb" satisfied condition "Succeeded or Failed"
Mar 17 09:16:06.320: INFO: Trying to get logs from node ip-172-31-35-106 pod downward-api-5edba175-e3ec-4458-9c94-6d02f7cdc0fb container dapi-container: <nil>
STEP: delete the pod
Mar 17 09:16:06.341: INFO: Waiting for pod downward-api-5edba175-e3ec-4458-9c94-6d02f7cdc0fb to disappear
Mar 17 09:16:06.345: INFO: Pod downward-api-5edba175-e3ec-4458-9c94-6d02f7cdc0fb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:16:06.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7759" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:16:06.355: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 17 09:16:06.411: INFO: Waiting up to 5m0s for pod "downward-api-6fcb28ff-80e1-4241-8446-ec79e9740b07" in namespace "downward-api-3203" to be "Succeeded or Failed"
Mar 17 09:16:06.422: INFO: Pod "downward-api-6fcb28ff-80e1-4241-8446-ec79e9740b07": Phase="Pending", Reason="", readiness=false. Elapsed: 11.494812ms
Mar 17 09:16:08.428: INFO: Pod "downward-api-6fcb28ff-80e1-4241-8446-ec79e9740b07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017504574s
Mar 17 09:16:10.432: INFO: Pod "downward-api-6fcb28ff-80e1-4241-8446-ec79e9740b07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021610088s
STEP: Saw pod success
Mar 17 09:16:10.432: INFO: Pod "downward-api-6fcb28ff-80e1-4241-8446-ec79e9740b07" satisfied condition "Succeeded or Failed"
Mar 17 09:16:10.434: INFO: Trying to get logs from node ip-172-31-35-106 pod downward-api-6fcb28ff-80e1-4241-8446-ec79e9740b07 container dapi-container: <nil>
STEP: delete the pod
Mar 17 09:16:10.451: INFO: Waiting for pod downward-api-6fcb28ff-80e1-4241-8446-ec79e9740b07 to disappear
Mar 17 09:16:10.453: INFO: Pod downward-api-6fcb28ff-80e1-4241-8446-ec79e9740b07 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:16:10.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3203" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":70,"skipped":1221,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:16:10.465: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-d6cb2599-66e8-4452-af09-e259789bd3f8
STEP: Creating a pod to test consume secrets
Mar 17 09:16:10.535: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f85dfe01-7e86-46d6-81ee-03151c0fa24a" in namespace "projected-8539" to be "Succeeded or Failed"
Mar 17 09:16:10.586: INFO: Pod "pod-projected-secrets-f85dfe01-7e86-46d6-81ee-03151c0fa24a": Phase="Pending", Reason="", readiness=false. Elapsed: 50.417864ms
Mar 17 09:16:12.590: INFO: Pod "pod-projected-secrets-f85dfe01-7e86-46d6-81ee-03151c0fa24a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054633749s
STEP: Saw pod success
Mar 17 09:16:12.590: INFO: Pod "pod-projected-secrets-f85dfe01-7e86-46d6-81ee-03151c0fa24a" satisfied condition "Succeeded or Failed"
Mar 17 09:16:12.593: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-secrets-f85dfe01-7e86-46d6-81ee-03151c0fa24a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:16:12.620: INFO: Waiting for pod pod-projected-secrets-f85dfe01-7e86-46d6-81ee-03151c0fa24a to disappear
Mar 17 09:16:12.622: INFO: Pod pod-projected-secrets-f85dfe01-7e86-46d6-81ee-03151c0fa24a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:16:12.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8539" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1221,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:16:12.642: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 17 09:16:12.704: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 17 09:17:12.739: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Mar 17 09:17:12.762: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 17 09:17:12.772: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 17 09:17:12.805: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 17 09:17:12.817: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 17 09:17:12.850: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 17 09:17:12.869: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:21.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7196" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.538 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":72,"skipped":1261,"failed":0}
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:21.180: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:23.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-498" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":73,"skipped":1263,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:23.334: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 17 09:17:23.425: INFO: Waiting up to 5m0s for pod "pod-fc154728-f2ca-4f25-8c23-7ae8a5d43d99" in namespace "emptydir-694" to be "Succeeded or Failed"
Mar 17 09:17:23.429: INFO: Pod "pod-fc154728-f2ca-4f25-8c23-7ae8a5d43d99": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078361ms
Mar 17 09:17:25.432: INFO: Pod "pod-fc154728-f2ca-4f25-8c23-7ae8a5d43d99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007321182s
Mar 17 09:17:27.436: INFO: Pod "pod-fc154728-f2ca-4f25-8c23-7ae8a5d43d99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011385579s
STEP: Saw pod success
Mar 17 09:17:27.436: INFO: Pod "pod-fc154728-f2ca-4f25-8c23-7ae8a5d43d99" satisfied condition "Succeeded or Failed"
Mar 17 09:17:27.439: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-fc154728-f2ca-4f25-8c23-7ae8a5d43d99 container test-container: <nil>
STEP: delete the pod
Mar 17 09:17:27.460: INFO: Waiting for pod pod-fc154728-f2ca-4f25-8c23-7ae8a5d43d99 to disappear
Mar 17 09:17:27.467: INFO: Pod pod-fc154728-f2ca-4f25-8c23-7ae8a5d43d99 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:27.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-694" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:27.482: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-139a672f-14ca-40f7-93e8-68bca7b88347
STEP: Creating a pod to test consume secrets
Mar 17 09:17:27.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-334c528b-0b65-4a39-96ec-ffc9d740a589" in namespace "projected-1620" to be "Succeeded or Failed"
Mar 17 09:17:27.583: INFO: Pod "pod-projected-secrets-334c528b-0b65-4a39-96ec-ffc9d740a589": Phase="Pending", Reason="", readiness=false. Elapsed: 19.192047ms
Mar 17 09:17:29.587: INFO: Pod "pod-projected-secrets-334c528b-0b65-4a39-96ec-ffc9d740a589": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023116201s
STEP: Saw pod success
Mar 17 09:17:29.587: INFO: Pod "pod-projected-secrets-334c528b-0b65-4a39-96ec-ffc9d740a589" satisfied condition "Succeeded or Failed"
Mar 17 09:17:29.589: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-secrets-334c528b-0b65-4a39-96ec-ffc9d740a589 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:17:29.609: INFO: Waiting for pod pod-projected-secrets-334c528b-0b65-4a39-96ec-ffc9d740a589 to disappear
Mar 17 09:17:29.612: INFO: Pod pod-projected-secrets-334c528b-0b65-4a39-96ec-ffc9d740a589 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:29.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1620" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1312,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:29.625: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 17 09:17:29.718: INFO: The status of Pod pod-update-44fe0df9-061f-430a-ad9b-c59d2c4576c4 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:17:31.723: INFO: The status of Pod pod-update-44fe0df9-061f-430a-ad9b-c59d2c4576c4 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:17:33.722: INFO: The status of Pod pod-update-44fe0df9-061f-430a-ad9b-c59d2c4576c4 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 17 09:17:34.239: INFO: Successfully updated pod "pod-update-44fe0df9-061f-430a-ad9b-c59d2c4576c4"
STEP: verifying the updated pod is in kubernetes
Mar 17 09:17:34.247: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:34.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7046" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1323,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:34.255: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-e2a5b397-f21b-4f5c-be84-882531560a50
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:34.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-35" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":77,"skipped":1338,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:34.311: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:17:34.386: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 17 09:17:38.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-131 --namespace=crd-publish-openapi-131 create -f -'
Mar 17 09:17:39.366: INFO: stderr: ""
Mar 17 09:17:39.366: INFO: stdout: "e2e-test-crd-publish-openapi-4109-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 17 09:17:39.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-131 --namespace=crd-publish-openapi-131 delete e2e-test-crd-publish-openapi-4109-crds test-cr'
Mar 17 09:17:39.426: INFO: stderr: ""
Mar 17 09:17:39.426: INFO: stdout: "e2e-test-crd-publish-openapi-4109-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 17 09:17:39.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-131 --namespace=crd-publish-openapi-131 apply -f -'
Mar 17 09:17:39.601: INFO: stderr: ""
Mar 17 09:17:39.601: INFO: stdout: "e2e-test-crd-publish-openapi-4109-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 17 09:17:39.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-131 --namespace=crd-publish-openapi-131 delete e2e-test-crd-publish-openapi-4109-crds test-cr'
Mar 17 09:17:39.664: INFO: stderr: ""
Mar 17 09:17:39.664: INFO: stdout: "e2e-test-crd-publish-openapi-4109-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 17 09:17:39.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-131 explain e2e-test-crd-publish-openapi-4109-crds'
Mar 17 09:17:39.836: INFO: stderr: ""
Mar 17 09:17:39.836: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4109-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:43.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-131" for this suite.

• [SLOW TEST:9.432 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":78,"skipped":1342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:43.743: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:46.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1971" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":79,"skipped":1385,"failed":0}
SSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:46.601: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-9788
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9788
STEP: Deleting pre-stop pod
Mar 17 09:17:55.686: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:55.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9788" for this suite.

• [SLOW TEST:9.104 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":80,"skipped":1388,"failed":0}
S
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:55.706: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:17:55.749: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 17 09:17:55.777: INFO: The status of Pod pod-logs-websocket-9ec1c54d-90fe-4182-9ae9-26cd8e6748a4 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:17:57.781: INFO: The status of Pod pod-logs-websocket-9ec1c54d-90fe-4182-9ae9-26cd8e6748a4 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:57.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8370" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1389,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:57.816: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:17:57.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4585" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":82,"skipped":1397,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:17:57.915: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 09:17:57.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6a08be0-4f30-4f26-a1fe-41ea9b3c8472" in namespace "projected-529" to be "Succeeded or Failed"
Mar 17 09:17:57.967: INFO: Pod "downwardapi-volume-c6a08be0-4f30-4f26-a1fe-41ea9b3c8472": Phase="Pending", Reason="", readiness=false. Elapsed: 6.114744ms
Mar 17 09:17:59.971: INFO: Pod "downwardapi-volume-c6a08be0-4f30-4f26-a1fe-41ea9b3c8472": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010153588s
STEP: Saw pod success
Mar 17 09:17:59.971: INFO: Pod "downwardapi-volume-c6a08be0-4f30-4f26-a1fe-41ea9b3c8472" satisfied condition "Succeeded or Failed"
Mar 17 09:17:59.976: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-c6a08be0-4f30-4f26-a1fe-41ea9b3c8472 container client-container: <nil>
STEP: delete the pod
Mar 17 09:18:00.030: INFO: Waiting for pod downwardapi-volume-c6a08be0-4f30-4f26-a1fe-41ea9b3c8472 to disappear
Mar 17 09:18:00.041: INFO: Pod downwardapi-volume-c6a08be0-4f30-4f26-a1fe-41ea9b3c8472 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:18:00.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-529" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1405,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:18:00.050: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 17 09:18:00.140: INFO: Waiting up to 5m0s for pod "pod-99a3567a-54e5-4993-ae02-9dafcd25391d" in namespace "emptydir-9635" to be "Succeeded or Failed"
Mar 17 09:18:00.151: INFO: Pod "pod-99a3567a-54e5-4993-ae02-9dafcd25391d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.678111ms
Mar 17 09:18:02.157: INFO: Pod "pod-99a3567a-54e5-4993-ae02-9dafcd25391d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016824544s
STEP: Saw pod success
Mar 17 09:18:02.157: INFO: Pod "pod-99a3567a-54e5-4993-ae02-9dafcd25391d" satisfied condition "Succeeded or Failed"
Mar 17 09:18:02.159: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-99a3567a-54e5-4993-ae02-9dafcd25391d container test-container: <nil>
STEP: delete the pod
Mar 17 09:18:02.172: INFO: Waiting for pod pod-99a3567a-54e5-4993-ae02-9dafcd25391d to disappear
Mar 17 09:18:02.178: INFO: Pod pod-99a3567a-54e5-4993-ae02-9dafcd25391d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:18:02.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9635" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1421,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:18:02.186: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6942
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-6942
Mar 17 09:18:02.247: INFO: Found 0 stateful pods, waiting for 1
Mar 17 09:18:12.253: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Mar 17 09:18:12.277: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Mar 17 09:18:12.290: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Mar 17 09:18:12.293: INFO: Observed &StatefulSet event: ADDED
Mar 17 09:18:12.293: INFO: Found Statefulset ss in namespace statefulset-6942 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 17 09:18:12.293: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Mar 17 09:18:12.293: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 17 09:18:12.301: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Mar 17 09:18:12.303: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Mar 17 09:18:12.303: INFO: Deleting all statefulset in ns statefulset-6942
Mar 17 09:18:12.305: INFO: Scaling statefulset ss to 0
Mar 17 09:18:22.336: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 09:18:22.339: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:18:22.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6942" for this suite.

• [SLOW TEST:20.189 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":85,"skipped":1435,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:18:22.374: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 17 09:18:22.481: INFO: Waiting up to 5m0s for pod "pod-70aaa037-bfba-4b9d-851c-47114c9f1db6" in namespace "emptydir-6177" to be "Succeeded or Failed"
Mar 17 09:18:22.487: INFO: Pod "pod-70aaa037-bfba-4b9d-851c-47114c9f1db6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122535ms
Mar 17 09:18:24.493: INFO: Pod "pod-70aaa037-bfba-4b9d-851c-47114c9f1db6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012126268s
Mar 17 09:18:26.499: INFO: Pod "pod-70aaa037-bfba-4b9d-851c-47114c9f1db6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018251565s
STEP: Saw pod success
Mar 17 09:18:26.499: INFO: Pod "pod-70aaa037-bfba-4b9d-851c-47114c9f1db6" satisfied condition "Succeeded or Failed"
Mar 17 09:18:26.503: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-70aaa037-bfba-4b9d-851c-47114c9f1db6 container test-container: <nil>
STEP: delete the pod
Mar 17 09:18:26.531: INFO: Waiting for pod pod-70aaa037-bfba-4b9d-851c-47114c9f1db6 to disappear
Mar 17 09:18:26.535: INFO: Pod pod-70aaa037-bfba-4b9d-851c-47114c9f1db6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:18:26.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6177" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1435,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:18:26.552: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Mar 17 09:18:26.646: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:18:28.654: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:18:30.656: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Mar 17 09:18:30.677: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:18:32.684: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 17 09:18:32.687: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:32.687: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:32.688: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:32.688: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:32.770: INFO: Exec stderr: ""
Mar 17 09:18:32.770: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:32.771: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:32.771: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:32.855: INFO: Exec stderr: ""
Mar 17 09:18:32.855: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:32.855: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:32.856: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:32.856: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:32.963: INFO: Exec stderr: ""
Mar 17 09:18:32.963: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:32.963: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:32.964: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:32.964: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:33.061: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 17 09:18:33.061: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:33.061: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:33.062: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:33.062: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:33.154: INFO: Exec stderr: ""
Mar 17 09:18:33.154: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:33.154: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:33.155: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:33.155: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:33.310: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 17 09:18:33.310: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:33.310: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:33.311: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:33.311: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:33.396: INFO: Exec stderr: ""
Mar 17 09:18:33.396: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:33.396: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:33.397: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:33.397: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:33.476: INFO: Exec stderr: ""
Mar 17 09:18:33.476: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:33.476: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:33.478: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:33.478: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:33.572: INFO: Exec stderr: ""
Mar 17 09:18:33.572: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-780 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:18:33.572: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:18:33.573: INFO: ExecWithOptions: Clientset creation
Mar 17 09:18:33.573: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-780/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:18:33.676: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:18:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-780" for this suite.

• [SLOW TEST:7.134 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1446,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:18:33.687: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:18:33.871: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:18:35.878: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:37.876: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:39.876: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:41.878: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:43.874: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:45.876: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:47.880: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:49.879: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:51.877: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = false)
Mar 17 09:18:53.875: INFO: The status of Pod test-webserver-554cd2a7-0289-4f0f-9cde-34e6c482dd8b is Running (Ready = true)
Mar 17 09:18:53.877: INFO: Container started at 2022-03-17 09:18:34 +0000 UTC, pod became ready at 2022-03-17 09:18:53 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:18:53.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-914" for this suite.

• [SLOW TEST:20.204 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1453,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:18:53.891: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 09:18:56.016: INFO: DNS probes using dns-1241/dns-test-f4c381e4-dfcc-45c2-96b8-79f6d6221631 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:18:56.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1241" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":89,"skipped":1463,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:18:56.051: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7859
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7859
STEP: creating replication controller externalsvc in namespace services-7859
I0317 09:18:56.187746      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7859, replica count: 2
I0317 09:18:59.245585      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 17 09:18:59.269: INFO: Creating new exec pod
Mar 17 09:19:01.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-7859 exec execpodfmfmm -- /bin/sh -x -c nslookup clusterip-service.services-7859.svc.cluster.local'
Mar 17 09:19:01.504: INFO: stderr: "+ nslookup clusterip-service.services-7859.svc.cluster.local\n"
Mar 17 09:19:01.504: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nclusterip-service.services-7859.svc.cluster.local\tcanonical name = externalsvc.services-7859.svc.cluster.local.\nName:\texternalsvc.services-7859.svc.cluster.local\nAddress: 10.43.13.16\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7859, will wait for the garbage collector to delete the pods
Mar 17 09:19:01.562: INFO: Deleting ReplicationController externalsvc took: 4.839823ms
Mar 17 09:19:01.662: INFO: Terminating ReplicationController externalsvc pods took: 100.219243ms
Mar 17 09:19:03.392: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:19:03.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7859" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.380 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":90,"skipped":1476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:19:03.431: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-940d991f-9330-4840-8465-83d2bc1645f3
STEP: Creating a pod to test consume configMaps
Mar 17 09:19:03.528: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9102f60a-6558-479f-b99d-6bd72fcab775" in namespace "projected-6299" to be "Succeeded or Failed"
Mar 17 09:19:03.538: INFO: Pod "pod-projected-configmaps-9102f60a-6558-479f-b99d-6bd72fcab775": Phase="Pending", Reason="", readiness=false. Elapsed: 9.948581ms
Mar 17 09:19:05.542: INFO: Pod "pod-projected-configmaps-9102f60a-6558-479f-b99d-6bd72fcab775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013905423s
STEP: Saw pod success
Mar 17 09:19:05.542: INFO: Pod "pod-projected-configmaps-9102f60a-6558-479f-b99d-6bd72fcab775" satisfied condition "Succeeded or Failed"
Mar 17 09:19:05.544: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-configmaps-9102f60a-6558-479f-b99d-6bd72fcab775 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:19:05.561: INFO: Waiting for pod pod-projected-configmaps-9102f60a-6558-479f-b99d-6bd72fcab775 to disappear
Mar 17 09:19:05.563: INFO: Pod pod-projected-configmaps-9102f60a-6558-479f-b99d-6bd72fcab775 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:19:05.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6299" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:19:05.571: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 17 09:19:05.653: INFO: Waiting up to 5m0s for pod "pod-f6cc884a-1d96-4f6c-932f-0a653afa3d9e" in namespace "emptydir-5209" to be "Succeeded or Failed"
Mar 17 09:19:05.674: INFO: Pod "pod-f6cc884a-1d96-4f6c-932f-0a653afa3d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.048079ms
Mar 17 09:19:07.678: INFO: Pod "pod-f6cc884a-1d96-4f6c-932f-0a653afa3d9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025024234s
STEP: Saw pod success
Mar 17 09:19:07.678: INFO: Pod "pod-f6cc884a-1d96-4f6c-932f-0a653afa3d9e" satisfied condition "Succeeded or Failed"
Mar 17 09:19:07.681: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-f6cc884a-1d96-4f6c-932f-0a653afa3d9e container test-container: <nil>
STEP: delete the pod
Mar 17 09:19:07.694: INFO: Waiting for pod pod-f6cc884a-1d96-4f6c-932f-0a653afa3d9e to disappear
Mar 17 09:19:07.699: INFO: Pod pod-f6cc884a-1d96-4f6c-932f-0a653afa3d9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:19:07.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5209" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":92,"skipped":1542,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:19:07.710: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5534
STEP: creating service affinity-clusterip in namespace services-5534
STEP: creating replication controller affinity-clusterip in namespace services-5534
I0317 09:19:07.779533      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5534, replica count: 3
I0317 09:19:10.831749      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 09:19:10.841: INFO: Creating new exec pod
Mar 17 09:19:13.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5534 exec execpod-affinitytdgff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar 17 09:19:14.056: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 17 09:19:14.056: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:19:14.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5534 exec execpod-affinitytdgff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.69.110 80'
Mar 17 09:19:14.231: INFO: stderr: "+ nc -v -t -w 2 10.43.69.110 80\n+ echo hostName\nConnection to 10.43.69.110 80 port [tcp/http] succeeded!\n"
Mar 17 09:19:14.231: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:19:14.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5534 exec execpod-affinitytdgff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.69.110:80/ ; done'
Mar 17 09:19:14.513: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.69.110:80/\n"
Mar 17 09:19:14.513: INFO: stdout: "\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c\naffinity-clusterip-p425c"
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Received response from host: affinity-clusterip-p425c
Mar 17 09:19:14.513: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5534, will wait for the garbage collector to delete the pods
Mar 17 09:19:14.597: INFO: Deleting ReplicationController affinity-clusterip took: 7.676489ms
Mar 17 09:19:14.698: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.928723ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:19:16.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5534" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.117 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":93,"skipped":1542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:19:16.828: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:19:16.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8418" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":94,"skipped":1573,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:19:16.948: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:19:16.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-384" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":95,"skipped":1576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:19:16.990: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:19:17.033: INFO: Endpoints addresses: [172.31.33.68] , ports: [6443]
Mar 17 09:19:17.033: INFO: EndpointSlices addresses: [172.31.33.68] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:19:17.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6924" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":96,"skipped":1622,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:19:17.058: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4290
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:19:17.113: INFO: Found 0 stateful pods, waiting for 1
Mar 17 09:19:27.120: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Mar 17 09:19:27.143: INFO: Found 1 stateful pods, waiting for 2
Mar 17 09:19:37.148: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 09:19:37.148: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Mar 17 09:19:37.168: INFO: Deleting all statefulset in ns statefulset-4290
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:19:37.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4290" for this suite.

• [SLOW TEST:20.131 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":97,"skipped":1644,"failed":0}
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:19:37.189: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar 17 09:19:37.273: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 17 09:20:37.318: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:20:37.321: INFO: Starting informer...
STEP: Starting pods...
Mar 17 09:20:37.544: INFO: Pod1 is running on ip-172-31-35-106. Tainting Node
Mar 17 09:20:39.765: INFO: Pod2 is running on ip-172-31-35-106. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 17 09:20:45.496: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 17 09:21:05.652: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:21:05.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8883" for this suite.

• [SLOW TEST:88.487 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":98,"skipped":1644,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:21:05.676: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Mar 17 09:21:05.733: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 17 09:21:10.737: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Mar 17 09:21:10.740: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:21:10.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9990" for this suite.

• [SLOW TEST:5.089 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":99,"skipped":1647,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:21:10.765: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 17 09:21:10.856: INFO: Waiting up to 5m0s for pod "downward-api-3f963917-1670-4ab4-9983-398393f89cd1" in namespace "downward-api-5838" to be "Succeeded or Failed"
Mar 17 09:21:10.863: INFO: Pod "downward-api-3f963917-1670-4ab4-9983-398393f89cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.188999ms
Mar 17 09:21:12.867: INFO: Pod "downward-api-3f963917-1670-4ab4-9983-398393f89cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010342806s
Mar 17 09:21:14.871: INFO: Pod "downward-api-3f963917-1670-4ab4-9983-398393f89cd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013888307s
STEP: Saw pod success
Mar 17 09:21:14.871: INFO: Pod "downward-api-3f963917-1670-4ab4-9983-398393f89cd1" satisfied condition "Succeeded or Failed"
Mar 17 09:21:14.875: INFO: Trying to get logs from node ip-172-31-35-106 pod downward-api-3f963917-1670-4ab4-9983-398393f89cd1 container dapi-container: <nil>
STEP: delete the pod
Mar 17 09:21:14.914: INFO: Waiting for pod downward-api-3f963917-1670-4ab4-9983-398393f89cd1 to disappear
Mar 17 09:21:14.918: INFO: Pod downward-api-3f963917-1670-4ab4-9983-398393f89cd1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:21:14.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5838" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":1668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:21:14.930: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 17 09:21:15.271: INFO: Pod name wrapped-volume-race-0ce2c208-0540-4daa-8cdc-38761a719954: Found 0 pods out of 5
Mar 17 09:21:20.281: INFO: Pod name wrapped-volume-race-0ce2c208-0540-4daa-8cdc-38761a719954: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0ce2c208-0540-4daa-8cdc-38761a719954 in namespace emptydir-wrapper-5629, will wait for the garbage collector to delete the pods
Mar 17 09:21:30.356: INFO: Deleting ReplicationController wrapped-volume-race-0ce2c208-0540-4daa-8cdc-38761a719954 took: 5.438931ms
Mar 17 09:21:30.457: INFO: Terminating ReplicationController wrapped-volume-race-0ce2c208-0540-4daa-8cdc-38761a719954 pods took: 100.845533ms
STEP: Creating RC which spawns configmap-volume pods
Mar 17 09:21:34.678: INFO: Pod name wrapped-volume-race-86a1edc4-95e7-498f-b7d2-46b63b70f931: Found 0 pods out of 5
Mar 17 09:21:39.686: INFO: Pod name wrapped-volume-race-86a1edc4-95e7-498f-b7d2-46b63b70f931: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-86a1edc4-95e7-498f-b7d2-46b63b70f931 in namespace emptydir-wrapper-5629, will wait for the garbage collector to delete the pods
Mar 17 09:21:51.774: INFO: Deleting ReplicationController wrapped-volume-race-86a1edc4-95e7-498f-b7d2-46b63b70f931 took: 7.087457ms
Mar 17 09:21:51.874: INFO: Terminating ReplicationController wrapped-volume-race-86a1edc4-95e7-498f-b7d2-46b63b70f931 pods took: 100.498986ms
STEP: Creating RC which spawns configmap-volume pods
Mar 17 09:21:54.994: INFO: Pod name wrapped-volume-race-911c52b9-dae2-4ee6-8443-1b76628e26e4: Found 0 pods out of 5
Mar 17 09:22:00.011: INFO: Pod name wrapped-volume-race-911c52b9-dae2-4ee6-8443-1b76628e26e4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-911c52b9-dae2-4ee6-8443-1b76628e26e4 in namespace emptydir-wrapper-5629, will wait for the garbage collector to delete the pods
Mar 17 09:22:10.100: INFO: Deleting ReplicationController wrapped-volume-race-911c52b9-dae2-4ee6-8443-1b76628e26e4 took: 6.015393ms
Mar 17 09:22:10.201: INFO: Terminating ReplicationController wrapped-volume-race-911c52b9-dae2-4ee6-8443-1b76628e26e4 pods took: 101.005517ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:22:14.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5629" for this suite.

• [SLOW TEST:59.431 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":101,"skipped":1737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:22:14.361: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:22:14.784: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 17 09:22:16.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.March, 17, 9, 22, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 22, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 9, 22, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 22, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:22:19.814: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:22:19.818: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7691-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:22:23.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2920" for this suite.
STEP: Destroying namespace "webhook-2920-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.242 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":102,"skipped":1767,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:22:23.604: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:22:39.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3342" for this suite.

• [SLOW TEST:16.213 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":103,"skipped":1774,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:22:39.818: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 17 09:22:41.907: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:22:41.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3215" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:22:41.948: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 17 09:22:42.026: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 17 09:22:42.038: INFO: Waiting for terminating namespaces to be deleted...
Mar 17 09:22:42.044: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-33-68 before test
Mar 17 09:22:42.060: INFO: fleet-agent-68b989995b-gwpf4 from cattle-fleet-system started at 2022-03-17 06:28:25 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container fleet-agent ready: true, restart count 1
Mar 17 09:22:42.061: INFO: cattle-cluster-agent-5bdc96ddf-7rt46 from cattle-system started at 2022-03-17 06:28:37 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container cluster-register ready: true, restart count 0
Mar 17 09:22:42.061: INFO: cattle-cluster-agent-5bdc96ddf-dkrkg from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container cluster-register ready: true, restart count 12
Mar 17 09:22:42.061: INFO: cattle-node-agent-xb7rh from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container agent ready: true, restart count 0
Mar 17 09:22:42.061: INFO: kube-api-auth-htl6k from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container kube-api-auth ready: true, restart count 0
Mar 17 09:22:42.061: INFO: calico-kube-controllers-fc7fcb565-nwnrh from kube-system started at 2022-03-17 06:27:25 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container calico-kube-controllers ready: true, restart count 12
Mar 17 09:22:42.061: INFO: canal-whdbp from kube-system started at 2022-03-17 06:27:25 +0000 UTC (2 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 09:22:42.061: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 09:22:42.061: INFO: coredns-5cb46d7c6-wk9ms from kube-system started at 2022-03-17 06:27:30 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container coredns ready: true, restart count 0
Mar 17 09:22:42.061: INFO: metrics-server-5c4895ffbd-9bbrg from kube-system started at 2022-03-17 06:27:34 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container metrics-server ready: true, restart count 0
Mar 17 09:22:42.061: INFO: rke-coredns-addon-deploy-job-lj29w from kube-system started at 2022-03-17 08:23:13 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Mar 17 09:22:42.061: INFO: rke-metrics-addon-deploy-job-fpn9x from kube-system started at 2022-03-17 06:27:33 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Mar 17 09:22:42.061: INFO: rke-network-plugin-deploy-job-x6hnn from kube-system started at 2022-03-17 06:27:23 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Mar 17 09:22:42.061: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-ng79r from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 09:22:42.061: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 09:22:42.061: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 17 09:22:42.061: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-35-106 before test
Mar 17 09:22:42.073: INFO: cattle-node-agent-zbmd5 from cattle-system started at 2022-03-17 06:29:26 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.073: INFO: 	Container agent ready: true, restart count 0
Mar 17 09:22:42.073: INFO: canal-2xxhj from kube-system started at 2022-03-17 06:29:26 +0000 UTC (2 container statuses recorded)
Mar 17 09:22:42.073: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 09:22:42.073: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 09:22:42.073: INFO: coredns-5cb46d7c6-khnbz from kube-system started at 2022-03-17 08:53:25 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.073: INFO: 	Container coredns ready: true, restart count 0
Mar 17 09:22:42.073: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-n98xh from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 09:22:42.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 09:22:42.073: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 17 09:22:42.073: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-39-36 before test
Mar 17 09:22:42.080: INFO: cattle-node-agent-lnw2j from cattle-system started at 2022-03-17 06:28:37 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.080: INFO: 	Container agent ready: true, restart count 0
Mar 17 09:22:42.080: INFO: canal-t2wwt from kube-system started at 2022-03-17 06:28:37 +0000 UTC (2 container statuses recorded)
Mar 17 09:22:42.080: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 09:22:42.080: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 09:22:42.080: INFO: coredns-autoscaler-6cb44df646-flpwt from kube-system started at 2022-03-17 08:56:22 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.080: INFO: 	Container autoscaler ready: true, restart count 0
Mar 17 09:22:42.080: INFO: sonobuoy from sonobuoy started at 2022-03-17 08:58:32 +0000 UTC (1 container statuses recorded)
Mar 17 09:22:42.080: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 17 09:22:42.080: INFO: sonobuoy-e2e-job-3f99102f0d96453a from sonobuoy started at 2022-03-17 08:58:33 +0000 UTC (2 container statuses recorded)
Mar 17 09:22:42.080: INFO: 	Container e2e ready: true, restart count 0
Mar 17 09:22:42.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 09:22:42.080: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-st4gk from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 09:22:42.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 09:22:42.080: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16dd20ac1261f404], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:22:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4815" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":105,"skipped":1859,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:22:43.115: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 09:22:43.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ccf32ae-4c30-44ab-85a3-503f870e3f79" in namespace "downward-api-5026" to be "Succeeded or Failed"
Mar 17 09:22:43.162: INFO: Pod "downwardapi-volume-1ccf32ae-4c30-44ab-85a3-503f870e3f79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.613074ms
Mar 17 09:22:45.166: INFO: Pod "downwardapi-volume-1ccf32ae-4c30-44ab-85a3-503f870e3f79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008757353s
STEP: Saw pod success
Mar 17 09:22:45.166: INFO: Pod "downwardapi-volume-1ccf32ae-4c30-44ab-85a3-503f870e3f79" satisfied condition "Succeeded or Failed"
Mar 17 09:22:45.169: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-1ccf32ae-4c30-44ab-85a3-503f870e3f79 container client-container: <nil>
STEP: delete the pod
Mar 17 09:22:45.194: INFO: Waiting for pod downwardapi-volume-1ccf32ae-4c30-44ab-85a3-503f870e3f79 to disappear
Mar 17 09:22:45.198: INFO: Pod downwardapi-volume-1ccf32ae-4c30-44ab-85a3-503f870e3f79 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:22:45.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5026" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":106,"skipped":1867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:22:45.206: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Mar 17 09:22:45.275: INFO: Waiting up to 5m0s for pod "client-containers-006c689d-b7e6-4667-afd6-ba9c7788e569" in namespace "containers-2960" to be "Succeeded or Failed"
Mar 17 09:22:45.279: INFO: Pod "client-containers-006c689d-b7e6-4667-afd6-ba9c7788e569": Phase="Pending", Reason="", readiness=false. Elapsed: 4.523134ms
Mar 17 09:22:47.284: INFO: Pod "client-containers-006c689d-b7e6-4667-afd6-ba9c7788e569": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009146414s
STEP: Saw pod success
Mar 17 09:22:47.284: INFO: Pod "client-containers-006c689d-b7e6-4667-afd6-ba9c7788e569" satisfied condition "Succeeded or Failed"
Mar 17 09:22:47.291: INFO: Trying to get logs from node ip-172-31-35-106 pod client-containers-006c689d-b7e6-4667-afd6-ba9c7788e569 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:22:47.326: INFO: Waiting for pod client-containers-006c689d-b7e6-4667-afd6-ba9c7788e569 to disappear
Mar 17 09:22:47.329: INFO: Pod client-containers-006c689d-b7e6-4667-afd6-ba9c7788e569 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:22:47.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2960" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":1902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:22:47.346: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-9861
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9861 to expose endpoints map[]
Mar 17 09:22:47.487: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 17 09:22:48.496: INFO: successfully validated that service multi-endpoint-test in namespace services-9861 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9861
Mar 17 09:22:48.513: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:22:50.519: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9861 to expose endpoints map[pod1:[100]]
Mar 17 09:22:50.535: INFO: successfully validated that service multi-endpoint-test in namespace services-9861 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9861
Mar 17 09:22:50.559: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:22:52.565: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9861 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 17 09:22:52.585: INFO: successfully validated that service multi-endpoint-test in namespace services-9861 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Mar 17 09:22:52.585: INFO: Creating new exec pod
Mar 17 09:22:55.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-9861 exec execpod8qxbn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar 17 09:22:55.773: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 17 09:22:55.773: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:22:55.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-9861 exec execpod8qxbn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.229.212 80'
Mar 17 09:22:55.995: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.229.212 80\nConnection to 10.43.229.212 80 port [tcp/http] succeeded!\n"
Mar 17 09:22:55.995: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:22:55.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-9861 exec execpod8qxbn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar 17 09:22:56.200: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 17 09:22:56.200: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:22:56.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-9861 exec execpod8qxbn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.229.212 81'
Mar 17 09:22:56.353: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.229.212 81\nConnection to 10.43.229.212 81 port [tcp/*] succeeded!\n"
Mar 17 09:22:56.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9861
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9861 to expose endpoints map[pod2:[101]]
Mar 17 09:22:56.417: INFO: successfully validated that service multi-endpoint-test in namespace services-9861 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9861
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9861 to expose endpoints map[]
Mar 17 09:22:56.487: INFO: successfully validated that service multi-endpoint-test in namespace services-9861 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:22:56.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9861" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.192 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":108,"skipped":1942,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:22:56.538: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar 17 09:22:56.603: INFO: observed Pod pod-test in namespace pods-6323 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 17 09:22:56.604: INFO: observed Pod pod-test in namespace pods-6323 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC  }]
Mar 17 09:22:56.624: INFO: observed Pod pod-test in namespace pods-6323 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC  }]
Mar 17 09:22:57.334: INFO: observed Pod pod-test in namespace pods-6323 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC  }]
Mar 17 09:22:58.440: INFO: Found Pod pod-test in namespace pods-6323 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 09:22:56 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Mar 17 09:22:58.455: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar 17 09:22:58.505: INFO: observed event type ADDED
Mar 17 09:22:58.505: INFO: observed event type MODIFIED
Mar 17 09:22:58.505: INFO: observed event type MODIFIED
Mar 17 09:22:58.505: INFO: observed event type MODIFIED
Mar 17 09:22:58.505: INFO: observed event type MODIFIED
Mar 17 09:22:58.506: INFO: observed event type MODIFIED
Mar 17 09:22:58.506: INFO: observed event type MODIFIED
Mar 17 09:22:58.506: INFO: observed event type MODIFIED
Mar 17 09:23:00.500: INFO: observed event type MODIFIED
Mar 17 09:23:00.670: INFO: observed event type MODIFIED
Mar 17 09:23:01.517: INFO: observed event type MODIFIED
Mar 17 09:23:01.525: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:23:01.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6323" for this suite.

• [SLOW TEST:5.023 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":109,"skipped":1949,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:23:01.562: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Mar 17 09:23:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:23:19.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4337" for this suite.

• [SLOW TEST:18.390 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":110,"skipped":1971,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:23:19.953: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Mar 17 09:23:20.007: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 17 09:23:20.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 create -f -'
Mar 17 09:23:20.665: INFO: stderr: ""
Mar 17 09:23:20.666: INFO: stdout: "service/agnhost-replica created\n"
Mar 17 09:23:20.666: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 17 09:23:20.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 create -f -'
Mar 17 09:23:20.867: INFO: stderr: ""
Mar 17 09:23:20.867: INFO: stdout: "service/agnhost-primary created\n"
Mar 17 09:23:20.867: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 17 09:23:20.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 create -f -'
Mar 17 09:23:21.025: INFO: stderr: ""
Mar 17 09:23:21.025: INFO: stdout: "service/frontend created\n"
Mar 17 09:23:21.025: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 17 09:23:21.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 create -f -'
Mar 17 09:23:21.193: INFO: stderr: ""
Mar 17 09:23:21.193: INFO: stdout: "deployment.apps/frontend created\n"
Mar 17 09:23:21.193: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 17 09:23:21.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 create -f -'
Mar 17 09:23:21.381: INFO: stderr: ""
Mar 17 09:23:21.381: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 17 09:23:21.381: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 17 09:23:21.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 create -f -'
Mar 17 09:23:21.552: INFO: stderr: ""
Mar 17 09:23:21.552: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar 17 09:23:21.552: INFO: Waiting for all frontend pods to be Running.
Mar 17 09:23:26.602: INFO: Waiting for frontend to serve content.
Mar 17 09:23:26.610: INFO: Trying to add a new entry to the guestbook.
Mar 17 09:23:26.619: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 17 09:23:26.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 delete --grace-period=0 --force -f -'
Mar 17 09:23:26.714: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 09:23:26.714: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar 17 09:23:26.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 delete --grace-period=0 --force -f -'
Mar 17 09:23:26.825: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 09:23:26.825: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 17 09:23:26.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 delete --grace-period=0 --force -f -'
Mar 17 09:23:26.916: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 09:23:26.916: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 17 09:23:26.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 delete --grace-period=0 --force -f -'
Mar 17 09:23:26.986: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 09:23:26.986: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 17 09:23:26.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 delete --grace-period=0 --force -f -'
Mar 17 09:23:27.080: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 09:23:27.080: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 17 09:23:27.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7047 delete --grace-period=0 --force -f -'
Mar 17 09:23:27.162: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 09:23:27.162: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:23:27.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7047" for this suite.

• [SLOW TEST:7.229 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":111,"skipped":1989,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:23:27.182: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1331
STEP: creating the pod
Mar 17 09:23:27.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9053 create -f -'
Mar 17 09:23:27.550: INFO: stderr: ""
Mar 17 09:23:27.550: INFO: stdout: "pod/pause created\n"
Mar 17 09:23:27.550: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 17 09:23:27.550: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9053" to be "running and ready"
Mar 17 09:23:27.558: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027231ms
Mar 17 09:23:29.563: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.013224001s
Mar 17 09:23:29.564: INFO: Pod "pause" satisfied condition "running and ready"
Mar 17 09:23:29.564: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 17 09:23:29.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9053 label pods pause testing-label=testing-label-value'
Mar 17 09:23:29.633: INFO: stderr: ""
Mar 17 09:23:29.634: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 17 09:23:29.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9053 get pod pause -L testing-label'
Mar 17 09:23:29.709: INFO: stderr: ""
Mar 17 09:23:29.709: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 17 09:23:29.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9053 label pods pause testing-label-'
Mar 17 09:23:29.781: INFO: stderr: ""
Mar 17 09:23:29.781: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 17 09:23:29.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9053 get pod pause -L testing-label'
Mar 17 09:23:29.844: INFO: stderr: ""
Mar 17 09:23:29.845: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1337
STEP: using delete to clean up resources
Mar 17 09:23:29.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9053 delete --grace-period=0 --force -f -'
Mar 17 09:23:29.913: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 09:23:29.913: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 17 09:23:29.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9053 get rc,svc -l name=pause --no-headers'
Mar 17 09:23:29.989: INFO: stderr: "No resources found in kubectl-9053 namespace.\n"
Mar 17 09:23:29.989: INFO: stdout: ""
Mar 17 09:23:29.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9053 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 17 09:23:30.062: INFO: stderr: ""
Mar 17 09:23:30.062: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:23:30.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9053" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":112,"skipped":2001,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:23:30.079: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6588
Mar 17 09:23:30.241: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:23:32.252: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 17 09:23:32.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6588 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 17 09:23:32.491: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 17 09:23:32.491: INFO: stdout: "iptables"
Mar 17 09:23:32.491: INFO: proxyMode: iptables
Mar 17 09:23:32.521: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 17 09:23:32.528: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6588
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6588
I0317 09:23:32.629224      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6588, replica count: 3
I0317 09:23:35.680973      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 09:23:35.692: INFO: Creating new exec pod
Mar 17 09:23:38.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6588 exec execpod-affinitytmhtk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar 17 09:23:38.900: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 17 09:23:38.900: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:23:38.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6588 exec execpod-affinitytmhtk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.1.66 80'
Mar 17 09:23:39.073: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.1.66 80\nConnection to 10.43.1.66 80 port [tcp/http] succeeded!\n"
Mar 17 09:23:39.073: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:23:39.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6588 exec execpod-affinitytmhtk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.39.36 31192'
Mar 17 09:23:39.230: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.39.36 31192\nConnection to 172.31.39.36 31192 port [tcp/*] succeeded!\n"
Mar 17 09:23:39.230: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:23:39.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6588 exec execpod-affinitytmhtk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.35.106 31192'
Mar 17 09:23:39.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.35.106 31192\nConnection to 172.31.35.106 31192 port [tcp/*] succeeded!\n"
Mar 17 09:23:39.397: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:23:39.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6588 exec execpod-affinitytmhtk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.33.68:31192/ ; done'
Mar 17 09:23:39.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n"
Mar 17 09:23:39.637: INFO: stdout: "\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h\naffinity-nodeport-timeout-bph6h"
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Received response from host: affinity-nodeport-timeout-bph6h
Mar 17 09:23:39.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6588 exec execpod-affinitytmhtk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.33.68:31192/'
Mar 17 09:23:39.809: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n"
Mar 17 09:23:39.809: INFO: stdout: "affinity-nodeport-timeout-bph6h"
Mar 17 09:23:59.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6588 exec execpod-affinitytmhtk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.33.68:31192/'
Mar 17 09:24:00.040: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.33.68:31192/\n"
Mar 17 09:24:00.040: INFO: stdout: "affinity-nodeport-timeout-jdm45"
Mar 17 09:24:00.040: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6588, will wait for the garbage collector to delete the pods
Mar 17 09:24:00.111: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.950742ms
Mar 17 09:24:00.212: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.977675ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:24:02.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6588" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:32.266 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":113,"skipped":2016,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:24:02.346: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-9a85815f-750a-4b07-8e96-72405f1cc7b5
STEP: Creating a pod to test consume configMaps
Mar 17 09:24:02.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ce987a2-19ca-40ee-b15e-c8f735cd687e" in namespace "configmap-5733" to be "Succeeded or Failed"
Mar 17 09:24:02.416: INFO: Pod "pod-configmaps-6ce987a2-19ca-40ee-b15e-c8f735cd687e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.332021ms
Mar 17 09:24:04.421: INFO: Pod "pod-configmaps-6ce987a2-19ca-40ee-b15e-c8f735cd687e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008724882s
STEP: Saw pod success
Mar 17 09:24:04.421: INFO: Pod "pod-configmaps-6ce987a2-19ca-40ee-b15e-c8f735cd687e" satisfied condition "Succeeded or Failed"
Mar 17 09:24:04.423: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-6ce987a2-19ca-40ee-b15e-c8f735cd687e container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:24:04.438: INFO: Waiting for pod pod-configmaps-6ce987a2-19ca-40ee-b15e-c8f735cd687e to disappear
Mar 17 09:24:04.441: INFO: Pod pod-configmaps-6ce987a2-19ca-40ee-b15e-c8f735cd687e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:24:04.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5733" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2018,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:24:04.453: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 17 09:24:06.538: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:24:06.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1353" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":115,"skipped":2026,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:24:06.559: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2814 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2814;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2814 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2814;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2814.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2814.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2814.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2814.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2814.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2814.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2814.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2814.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2814.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2814.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2814.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2814.svc;check="$$(dig +notcp +noall +answer +search 191.90.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.90.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.90.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.90.191_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2814 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2814;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2814 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2814;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2814.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2814.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2814.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2814.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2814.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2814.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2814.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2814.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2814.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2814.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2814.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2814.svc;check="$$(dig +notcp +noall +answer +search 191.90.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.90.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.90.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.90.191_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 09:24:10.675: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.679: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.681: INFO: Unable to read wheezy_udp@dns-test-service.dns-2814 from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.684: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2814 from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.686: INFO: Unable to read wheezy_udp@dns-test-service.dns-2814.svc from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.688: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2814.svc from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.705: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.710: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.713: INFO: Unable to read jessie_udp@dns-test-service.dns-2814 from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.715: INFO: Unable to read jessie_tcp@dns-test-service.dns-2814 from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.717: INFO: Unable to read jessie_udp@dns-test-service.dns-2814.svc from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-2814.svc from pod dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190: the server could not find the requested resource (get pods dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190)
Mar 17 09:24:10.732: INFO: Lookups using dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2814 wheezy_tcp@dns-test-service.dns-2814 wheezy_udp@dns-test-service.dns-2814.svc wheezy_tcp@dns-test-service.dns-2814.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2814 jessie_tcp@dns-test-service.dns-2814 jessie_udp@dns-test-service.dns-2814.svc jessie_tcp@dns-test-service.dns-2814.svc]

Mar 17 09:24:15.802: INFO: DNS probes using dns-2814/dns-test-6abe8ef0-d8b4-44c0-bb69-f2471bc3a190 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:24:15.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2814" for this suite.

• [SLOW TEST:9.350 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":116,"skipped":2033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:24:15.911: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:24:16.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6980" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":117,"skipped":2081,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:24:16.073: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:24:16.329: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:24:17.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6828" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":118,"skipped":2092,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:24:17.046: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-da070df9-94fe-4583-879b-1bb3cbea64eb
STEP: Creating secret with name s-test-opt-upd-2472cc23-e3ee-45a3-ab20-e1b6a24117ef
STEP: Creating the pod
Mar 17 09:24:17.175: INFO: The status of Pod pod-secrets-43b3cce1-4681-4a25-879f-74bda1ac5881 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:24:19.179: INFO: The status of Pod pod-secrets-43b3cce1-4681-4a25-879f-74bda1ac5881 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:24:21.190: INFO: The status of Pod pod-secrets-43b3cce1-4681-4a25-879f-74bda1ac5881 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-da070df9-94fe-4583-879b-1bb3cbea64eb
STEP: Updating secret s-test-opt-upd-2472cc23-e3ee-45a3-ab20-e1b6a24117ef
STEP: Creating secret with name s-test-opt-create-0e1cbe10-345a-401d-ab7d-fb5c925ee4c0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:25:45.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8012" for this suite.

• [SLOW TEST:88.669 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":2107,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:25:45.716: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 17 09:25:45.797: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3419  ac67b894-bd7f-49b2-9a32-4ebbf2eff70a 61703 0 2022-03-17 09:25:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-03-17 09:25:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:25:45.797: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3419  ac67b894-bd7f-49b2-9a32-4ebbf2eff70a 61704 0 2022-03-17 09:25:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-03-17 09:25:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:25:45.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3419" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":120,"skipped":2110,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:25:45.805: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 09:25:45.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45e4a8c0-5e7a-4342-852c-0d336733080c" in namespace "projected-4136" to be "Succeeded or Failed"
Mar 17 09:25:45.914: INFO: Pod "downwardapi-volume-45e4a8c0-5e7a-4342-852c-0d336733080c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.978171ms
Mar 17 09:25:47.919: INFO: Pod "downwardapi-volume-45e4a8c0-5e7a-4342-852c-0d336733080c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026662865s
STEP: Saw pod success
Mar 17 09:25:47.919: INFO: Pod "downwardapi-volume-45e4a8c0-5e7a-4342-852c-0d336733080c" satisfied condition "Succeeded or Failed"
Mar 17 09:25:47.922: INFO: Trying to get logs from node ip-172-31-39-36 pod downwardapi-volume-45e4a8c0-5e7a-4342-852c-0d336733080c container client-container: <nil>
STEP: delete the pod
Mar 17 09:25:47.958: INFO: Waiting for pod downwardapi-volume-45e4a8c0-5e7a-4342-852c-0d336733080c to disappear
Mar 17 09:25:47.967: INFO: Pod downwardapi-volume-45e4a8c0-5e7a-4342-852c-0d336733080c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:25:47.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4136" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2118,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:25:47.977: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-zgr2
STEP: Creating a pod to test atomic-volume-subpath
Mar 17 09:25:48.059: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zgr2" in namespace "subpath-6993" to be "Succeeded or Failed"
Mar 17 09:25:48.073: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.143809ms
Mar 17 09:25:50.076: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016679752s
Mar 17 09:25:52.080: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 4.021293818s
Mar 17 09:25:54.087: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 6.027990369s
Mar 17 09:25:56.095: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 8.036317645s
Mar 17 09:25:58.104: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 10.045024202s
Mar 17 09:26:00.108: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 12.049548422s
Mar 17 09:26:02.113: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 14.05383794s
Mar 17 09:26:04.118: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 16.058658274s
Mar 17 09:26:06.123: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 18.064436964s
Mar 17 09:26:08.129: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Running", Reason="", readiness=true. Elapsed: 20.069908471s
Mar 17 09:26:10.139: INFO: Pod "pod-subpath-test-projected-zgr2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.07989253s
STEP: Saw pod success
Mar 17 09:26:10.139: INFO: Pod "pod-subpath-test-projected-zgr2" satisfied condition "Succeeded or Failed"
Mar 17 09:26:10.141: INFO: Trying to get logs from node ip-172-31-39-36 pod pod-subpath-test-projected-zgr2 container test-container-subpath-projected-zgr2: <nil>
STEP: delete the pod
Mar 17 09:26:10.159: INFO: Waiting for pod pod-subpath-test-projected-zgr2 to disappear
Mar 17 09:26:10.162: INFO: Pod pod-subpath-test-projected-zgr2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-zgr2
Mar 17 09:26:10.162: INFO: Deleting pod "pod-subpath-test-projected-zgr2" in namespace "subpath-6993"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:10.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6993" for this suite.

• [SLOW TEST:22.194 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":122,"skipped":2124,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:10.171: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:10.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5665" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":123,"skipped":2135,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:10.278: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:14.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6606" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":124,"skipped":2140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:14.376: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-e55a24c5-61fc-41ac-be66-b7d33dd909f5
STEP: Creating a pod to test consume secrets
Mar 17 09:26:14.496: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9af462ad-5dc8-4604-a98e-f5440f3000fb" in namespace "projected-6078" to be "Succeeded or Failed"
Mar 17 09:26:14.506: INFO: Pod "pod-projected-secrets-9af462ad-5dc8-4604-a98e-f5440f3000fb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.329443ms
Mar 17 09:26:16.511: INFO: Pod "pod-projected-secrets-9af462ad-5dc8-4604-a98e-f5440f3000fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014425245s
STEP: Saw pod success
Mar 17 09:26:16.511: INFO: Pod "pod-projected-secrets-9af462ad-5dc8-4604-a98e-f5440f3000fb" satisfied condition "Succeeded or Failed"
Mar 17 09:26:16.513: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-secrets-9af462ad-5dc8-4604-a98e-f5440f3000fb container secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:26:16.528: INFO: Waiting for pod pod-projected-secrets-9af462ad-5dc8-4604-a98e-f5440f3000fb to disappear
Mar 17 09:26:16.531: INFO: Pod pod-projected-secrets-9af462ad-5dc8-4604-a98e-f5440f3000fb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:16.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6078" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2174,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:16.540: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 09:26:16.589: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24defdab-0dcd-4f97-b1b4-ed9c3a73ede1" in namespace "downward-api-9462" to be "Succeeded or Failed"
Mar 17 09:26:16.600: INFO: Pod "downwardapi-volume-24defdab-0dcd-4f97-b1b4-ed9c3a73ede1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.189748ms
Mar 17 09:26:18.604: INFO: Pod "downwardapi-volume-24defdab-0dcd-4f97-b1b4-ed9c3a73ede1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015099073s
STEP: Saw pod success
Mar 17 09:26:18.604: INFO: Pod "downwardapi-volume-24defdab-0dcd-4f97-b1b4-ed9c3a73ede1" satisfied condition "Succeeded or Failed"
Mar 17 09:26:18.606: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-24defdab-0dcd-4f97-b1b4-ed9c3a73ede1 container client-container: <nil>
STEP: delete the pod
Mar 17 09:26:18.625: INFO: Waiting for pod downwardapi-volume-24defdab-0dcd-4f97-b1b4-ed9c3a73ede1 to disappear
Mar 17 09:26:18.630: INFO: Pod downwardapi-volume-24defdab-0dcd-4f97-b1b4-ed9c3a73ede1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:18.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9462" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":126,"skipped":2182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:18.645: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Mar 17 09:26:18.701: INFO: Waiting up to 5m0s for pod "var-expansion-d6b5a22c-1deb-4013-afde-9439100bc141" in namespace "var-expansion-9854" to be "Succeeded or Failed"
Mar 17 09:26:18.709: INFO: Pod "var-expansion-d6b5a22c-1deb-4013-afde-9439100bc141": Phase="Pending", Reason="", readiness=false. Elapsed: 8.438093ms
Mar 17 09:26:20.713: INFO: Pod "var-expansion-d6b5a22c-1deb-4013-afde-9439100bc141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01283379s
STEP: Saw pod success
Mar 17 09:26:20.714: INFO: Pod "var-expansion-d6b5a22c-1deb-4013-afde-9439100bc141" satisfied condition "Succeeded or Failed"
Mar 17 09:26:20.716: INFO: Trying to get logs from node ip-172-31-35-106 pod var-expansion-d6b5a22c-1deb-4013-afde-9439100bc141 container dapi-container: <nil>
STEP: delete the pod
Mar 17 09:26:20.731: INFO: Waiting for pod var-expansion-d6b5a22c-1deb-4013-afde-9439100bc141 to disappear
Mar 17 09:26:20.735: INFO: Pod var-expansion-d6b5a22c-1deb-4013-afde-9439100bc141 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:20.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9854" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2219,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:20.746: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:20.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5079" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":128,"skipped":2235,"failed":0}
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:20.852: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Mar 17 09:26:20.888: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Mar 17 09:26:21.580: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 17 09:26:23.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.March, 17, 9, 26, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 26, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 9, 26, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 26, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 17 09:26:25.774: INFO: Waited 127.928299ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Mar 17 09:26:25.844: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:26.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3534" for this suite.

• [SLOW TEST:5.754 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":129,"skipped":2237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:26.607: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:26:28.669: INFO: Deleting pod "var-expansion-f4f73a0b-ae5f-442e-bdd7-759eb1cf33fd" in namespace "var-expansion-4028"
Mar 17 09:26:28.673: INFO: Wait up to 5m0s for pod "var-expansion-f4f73a0b-ae5f-442e-bdd7-759eb1cf33fd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:32.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4028" for this suite.

• [SLOW TEST:6.089 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":130,"skipped":2278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:32.698: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:32.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5707" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":131,"skipped":2339,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:32.791: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Mar 17 09:26:32.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-9646 api-versions'
Mar 17 09:26:32.927: INFO: stderr: ""
Mar 17 09:26:32.927: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncatalog.cattle.io/v1\ncertificates.k8s.io/v1\ncluster.cattle.io/v3\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmanagement.cattle.io/v3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nui.cattle.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:32.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9646" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":132,"skipped":2352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:32.937: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 17 09:26:33.007: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 17 09:26:33.011: INFO: starting watch
STEP: patching
STEP: updating
Mar 17 09:26:33.025: INFO: waiting for watch events with expected annotations
Mar 17 09:26:33.025: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:33.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9787" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":133,"skipped":2386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:33.066: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 17 09:26:35.144: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:26:35.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2160" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2425,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:26:35.178: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:32:01.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9294" for this suite.

• [SLOW TEST:326.104 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":135,"skipped":2426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:32:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 17 09:32:01.424: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 17 09:33:01.505: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Mar 17 09:33:01.556: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 17 09:33:01.582: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 17 09:33:01.670: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 17 09:33:01.691: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 17 09:33:01.761: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 17 09:33:01.796: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:33:15.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1499" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.690 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":136,"skipped":2469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:33:15.973: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4757
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Mar 17 09:33:16.140: INFO: Found 0 stateful pods, waiting for 3
Mar 17 09:33:26.145: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 09:33:26.145: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 09:33:26.145: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Mar 17 09:33:26.172: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 17 09:33:36.211: INFO: Updating stateful set ss2
Mar 17 09:33:36.218: INFO: Waiting for Pod statefulset-4757/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Mar 17 09:33:46.388: INFO: Found 2 stateful pods, waiting for 3
Mar 17 09:33:56.394: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 09:33:56.394: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 09:33:56.394: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 17 09:33:56.423: INFO: Updating stateful set ss2
Mar 17 09:33:56.431: INFO: Waiting for Pod statefulset-4757/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Mar 17 09:34:06.462: INFO: Updating stateful set ss2
Mar 17 09:34:06.489: INFO: Waiting for StatefulSet statefulset-4757/ss2 to complete update
Mar 17 09:34:06.490: INFO: Waiting for Pod statefulset-4757/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Mar 17 09:34:16.499: INFO: Deleting all statefulset in ns statefulset-4757
Mar 17 09:34:16.502: INFO: Scaling statefulset ss2 to 0
Mar 17 09:34:26.524: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 09:34:26.526: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:34:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4757" for this suite.

• [SLOW TEST:70.589 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":137,"skipped":2544,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:34:26.562: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 17 09:34:26.641: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:34:31.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3486" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2562,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:34:31.507: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-5e4588d6-28e3-4bf7-8b00-c50c91f8ad3c
STEP: Creating a pod to test consume secrets
Mar 17 09:34:31.590: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-47150e0a-b74f-4bd7-b808-d4d2205e97b8" in namespace "projected-8826" to be "Succeeded or Failed"
Mar 17 09:34:31.601: INFO: Pod "pod-projected-secrets-47150e0a-b74f-4bd7-b808-d4d2205e97b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.803983ms
Mar 17 09:34:33.605: INFO: Pod "pod-projected-secrets-47150e0a-b74f-4bd7-b808-d4d2205e97b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015153025s
STEP: Saw pod success
Mar 17 09:34:33.606: INFO: Pod "pod-projected-secrets-47150e0a-b74f-4bd7-b808-d4d2205e97b8" satisfied condition "Succeeded or Failed"
Mar 17 09:34:33.608: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-secrets-47150e0a-b74f-4bd7-b808-d4d2205e97b8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:34:33.651: INFO: Waiting for pod pod-projected-secrets-47150e0a-b74f-4bd7-b808-d4d2205e97b8 to disappear
Mar 17 09:34:33.659: INFO: Pod pod-projected-secrets-47150e0a-b74f-4bd7-b808-d4d2205e97b8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:34:33.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8826" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":139,"skipped":2563,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:34:33.672: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-647512cf-f33a-49b0-8911-87e55d52b22c
STEP: Creating a pod to test consume secrets
Mar 17 09:34:33.798: INFO: Waiting up to 5m0s for pod "pod-secrets-9c4df1ab-88d9-4e78-97d8-bda5a946ca6e" in namespace "secrets-9177" to be "Succeeded or Failed"
Mar 17 09:34:33.809: INFO: Pod "pod-secrets-9c4df1ab-88d9-4e78-97d8-bda5a946ca6e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.7707ms
Mar 17 09:34:35.820: INFO: Pod "pod-secrets-9c4df1ab-88d9-4e78-97d8-bda5a946ca6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022049814s
STEP: Saw pod success
Mar 17 09:34:35.820: INFO: Pod "pod-secrets-9c4df1ab-88d9-4e78-97d8-bda5a946ca6e" satisfied condition "Succeeded or Failed"
Mar 17 09:34:35.825: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-secrets-9c4df1ab-88d9-4e78-97d8-bda5a946ca6e container secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:34:35.846: INFO: Waiting for pod pod-secrets-9c4df1ab-88d9-4e78-97d8-bda5a946ca6e to disappear
Mar 17 09:34:35.848: INFO: Pod pod-secrets-9c4df1ab-88d9-4e78-97d8-bda5a946ca6e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:34:35.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9177" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2568,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:34:35.866: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 17 09:34:35.949: INFO: Waiting up to 5m0s for pod "pod-bad253be-e398-47f2-9db1-6c7caea3885c" in namespace "emptydir-3800" to be "Succeeded or Failed"
Mar 17 09:34:35.979: INFO: Pod "pod-bad253be-e398-47f2-9db1-6c7caea3885c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.06314ms
Mar 17 09:34:37.984: INFO: Pod "pod-bad253be-e398-47f2-9db1-6c7caea3885c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034634473s
STEP: Saw pod success
Mar 17 09:34:37.984: INFO: Pod "pod-bad253be-e398-47f2-9db1-6c7caea3885c" satisfied condition "Succeeded or Failed"
Mar 17 09:34:37.987: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-bad253be-e398-47f2-9db1-6c7caea3885c container test-container: <nil>
STEP: delete the pod
Mar 17 09:34:38.010: INFO: Waiting for pod pod-bad253be-e398-47f2-9db1-6c7caea3885c to disappear
Mar 17 09:34:38.017: INFO: Pod pod-bad253be-e398-47f2-9db1-6c7caea3885c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:34:38.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3800" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2589,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:34:38.028: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 17 09:34:38.088: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 17 09:34:50.496: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:34:53.349: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:05.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6197" for this suite.

• [SLOW TEST:27.910 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":142,"skipped":2589,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:05.939: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-d77270a5-1c42-41a8-88b5-8938525d1771
STEP: Creating secret with name secret-projected-all-test-volume-84c39e99-e4c9-41fa-be33-8634b18606cf
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 17 09:35:06.065: INFO: Waiting up to 5m0s for pod "projected-volume-8ddcf3de-43c8-4407-867e-0e38cfe7e268" in namespace "projected-6852" to be "Succeeded or Failed"
Mar 17 09:35:06.080: INFO: Pod "projected-volume-8ddcf3de-43c8-4407-867e-0e38cfe7e268": Phase="Pending", Reason="", readiness=false. Elapsed: 14.928461ms
Mar 17 09:35:08.083: INFO: Pod "projected-volume-8ddcf3de-43c8-4407-867e-0e38cfe7e268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018685363s
STEP: Saw pod success
Mar 17 09:35:08.083: INFO: Pod "projected-volume-8ddcf3de-43c8-4407-867e-0e38cfe7e268" satisfied condition "Succeeded or Failed"
Mar 17 09:35:08.086: INFO: Trying to get logs from node ip-172-31-35-106 pod projected-volume-8ddcf3de-43c8-4407-867e-0e38cfe7e268 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 17 09:35:08.113: INFO: Waiting for pod projected-volume-8ddcf3de-43c8-4407-867e-0e38cfe7e268 to disappear
Mar 17 09:35:08.117: INFO: Pod projected-volume-8ddcf3de-43c8-4407-867e-0e38cfe7e268 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:08.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6852" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2595,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:08.134: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7507
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7507
I0317 09:35:08.336531      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7507, replica count: 2
I0317 09:35:11.387101      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 09:35:11.387: INFO: Creating new exec pod
Mar 17 09:35:14.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-7507 exec execpoddnm5b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 17 09:35:14.793: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 17 09:35:14.793: INFO: stdout: "externalname-service-xk5jq"
Mar 17 09:35:14.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-7507 exec execpoddnm5b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.192.27 80'
Mar 17 09:35:14.974: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.192.27 80\nConnection to 10.43.192.27 80 port [tcp/http] succeeded!\n"
Mar 17 09:35:14.974: INFO: stdout: ""
Mar 17 09:35:15.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-7507 exec execpoddnm5b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.192.27 80'
Mar 17 09:35:16.143: INFO: stderr: "+ + ncecho -v -t hostName -w\n 2 10.43.192.27 80\nConnection to 10.43.192.27 80 port [tcp/http] succeeded!\n"
Mar 17 09:35:16.143: INFO: stdout: ""
Mar 17 09:35:16.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-7507 exec execpoddnm5b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.192.27 80'
Mar 17 09:35:17.130: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.192.27 80\nConnection to 10.43.192.27 80 port [tcp/http] succeeded!\n"
Mar 17 09:35:17.130: INFO: stdout: ""
Mar 17 09:35:17.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-7507 exec execpoddnm5b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.192.27 80'
Mar 17 09:35:18.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.192.27 80\nConnection to 10.43.192.27 80 port [tcp/http] succeeded!\n"
Mar 17 09:35:18.153: INFO: stdout: "externalname-service-xk5jq"
Mar 17 09:35:18.153: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:18.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7507" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.065 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":144,"skipped":2601,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:18.199: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Mar 17 09:35:18.299: INFO: Creating simple deployment test-deployment-qvczk
Mar 17 09:35:18.321: INFO: deployment "test-deployment-qvczk" doesn't have the required revision set
STEP: Getting /status
Mar 17 09:35:20.338: INFO: Deployment test-deployment-qvczk has Conditions: [{Available True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qvczk-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Mar 17 09:35:20.348: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 9, 35, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 35, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 9, 35, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 35, 18, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-qvczk-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Mar 17 09:35:20.350: INFO: Observed &Deployment event: ADDED
Mar 17 09:35:20.350: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qvczk-764bc7c4b7"}
Mar 17 09:35:20.350: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.350: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qvczk-764bc7c4b7"}
Mar 17 09:35:20.350: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 17 09:35:20.350: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.350: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 17 09:35:20.350: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qvczk-764bc7c4b7" is progressing.}
Mar 17 09:35:20.351: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.352: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 17 09:35:20.352: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qvczk-764bc7c4b7" has successfully progressed.}
Mar 17 09:35:20.352: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.352: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 17 09:35:20.352: INFO: Observed Deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qvczk-764bc7c4b7" has successfully progressed.}
Mar 17 09:35:20.352: INFO: Found Deployment test-deployment-qvczk in namespace deployment-5522 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 17 09:35:20.352: INFO: Deployment test-deployment-qvczk has an updated status
STEP: patching the Statefulset Status
Mar 17 09:35:20.352: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 17 09:35:20.358: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Mar 17 09:35:20.359: INFO: Observed &Deployment event: ADDED
Mar 17 09:35:20.359: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qvczk-764bc7c4b7"}
Mar 17 09:35:20.359: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.359: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qvczk-764bc7c4b7"}
Mar 17 09:35:20.360: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 17 09:35:20.360: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.360: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 17 09:35:20.360: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:18 +0000 UTC 2022-03-17 09:35:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qvczk-764bc7c4b7" is progressing.}
Mar 17 09:35:20.360: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.360: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 17 09:35:20.360: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qvczk-764bc7c4b7" has successfully progressed.}
Mar 17 09:35:20.360: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.360: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 17 09:35:20.360: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-17 09:35:20 +0000 UTC 2022-03-17 09:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qvczk-764bc7c4b7" has successfully progressed.}
Mar 17 09:35:20.360: INFO: Observed deployment test-deployment-qvczk in namespace deployment-5522 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 17 09:35:20.360: INFO: Observed &Deployment event: MODIFIED
Mar 17 09:35:20.360: INFO: Found deployment test-deployment-qvczk in namespace deployment-5522 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 17 09:35:20.360: INFO: Deployment test-deployment-qvczk has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar 17 09:35:20.363: INFO: Deployment "test-deployment-qvczk":
&Deployment{ObjectMeta:{test-deployment-qvczk  deployment-5522  32d6abaf-f1f5-4023-895c-54bec55fb7d6 64525 1 2022-03-17 09:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-03-17 09:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-03-17 09:35:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-03-17 09:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c28648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 17 09:35:20.367: INFO: New ReplicaSet "test-deployment-qvczk-764bc7c4b7" of Deployment "test-deployment-qvczk":
&ReplicaSet{ObjectMeta:{test-deployment-qvczk-764bc7c4b7  deployment-5522  6ca63e6e-b161-4394-9245-1a70a4074788 64520 1 2022-03-17 09:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-qvczk 32d6abaf-f1f5-4023-895c-54bec55fb7d6 0xc006c28a00 0xc006c28a01}] []  [{kube-controller-manager Update apps/v1 2022-03-17 09:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32d6abaf-f1f5-4023-895c-54bec55fb7d6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 09:35:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c28ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 17 09:35:20.370: INFO: Pod "test-deployment-qvczk-764bc7c4b7-xhxdf" is available:
&Pod{ObjectMeta:{test-deployment-qvczk-764bc7c4b7-xhxdf test-deployment-qvczk-764bc7c4b7- deployment-5522  7290afd3-5274-43ee-a8a0-0ebc777ad648 64519 0 2022-03-17 09:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/containerID:413803d74e4b94ad5a9a52179969bce2ce65f25dfa6af9c017abdace8adb8a0a cni.projectcalico.org/podIP:10.42.2.45/32 cni.projectcalico.org/podIPs:10.42.2.45/32] [{apps/v1 ReplicaSet test-deployment-qvczk-764bc7c4b7 6ca63e6e-b161-4394-9245-1a70a4074788 0xc006bfd350 0xc006bfd351}] []  [{kube-controller-manager Update v1 2022-03-17 09:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ca63e6e-b161-4394-9245-1a70a4074788\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 09:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 09:35:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4w4dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4w4dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:35:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:35:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 09:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:10.42.2.45,StartTime:2022-03-17 09:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 09:35:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://c549f6cd939e3e01e5563762910815ee2195abb113fcfe75753668409484ad86,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:20.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5522" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":145,"skipped":2613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:20.379: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:35:21.168: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:35:24.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 17 09:35:26.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=webhook-9627 attach --namespace=webhook-9627 to-be-attached-pod -i -c=container1'
Mar 17 09:35:26.316: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:26.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9627" for this suite.
STEP: Destroying namespace "webhook-9627-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.029 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":146,"skipped":2677,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:26.409: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 17 09:35:26.478: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-774  f69d336c-19c7-460a-be7f-a6daf1ceaae0 64678 0 2022-03-17 09:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-17 09:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:35:26.478: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-774  f69d336c-19c7-460a-be7f-a6daf1ceaae0 64680 0 2022-03-17 09:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-17 09:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 17 09:35:26.487: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-774  f69d336c-19c7-460a-be7f-a6daf1ceaae0 64681 0 2022-03-17 09:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-17 09:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:35:26.487: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-774  f69d336c-19c7-460a-be7f-a6daf1ceaae0 64682 0 2022-03-17 09:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-17 09:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:26.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-774" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":147,"skipped":2727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:26.494: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:35:26.557: INFO: The status of Pod busybox-scheduling-ee6f3e41-5eb2-4e72-adfd-2d7a87d4ad2d is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:35:28.560: INFO: The status of Pod busybox-scheduling-ee6f3e41-5eb2-4e72-adfd-2d7a87d4ad2d is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:28.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7557" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2751,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:28.581: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 17 09:35:28.671: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 09:35:28.671: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 09:35:29.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 09:35:29.683: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 09:35:30.678: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 09:35:30.678: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Mar 17 09:35:30.703: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"64762"},"items":null}

Mar 17 09:35:30.706: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"64762"},"items":[{"metadata":{"name":"daemon-set-68bjq","generateName":"daemon-set-","namespace":"daemonsets-4598","uid":"2d2ec3d7-eab1-40c1-b459-6f8bc0722b2f","resourceVersion":"64750","creationTimestamp":"2022-03-17T09:35:28Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4c02de710b299cb38f786c0ffce8169b442437ed64786fb29d9cebf98c5986a0","cni.projectcalico.org/podIP":"10.42.1.8/32","cni.projectcalico.org/podIPs":"10.42.1.8/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"80f43dd1-be0c-465d-9eac-68c410cf4d06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80f43dd1-be0c-465d-9eac-68c410cf4d06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jn8tt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jn8tt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-39-36","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-39-36"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:28Z"}],"hostIP":"172.31.39.36","podIP":"10.42.1.8","podIPs":[{"ip":"10.42.1.8"}],"startTime":"2022-03-17T09:35:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-03-17T09:35:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://189a8699de520741992b37a6267300d1fb3b84c184818ab7599f9ce43ec5c087","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-c5ds9","generateName":"daemon-set-","namespace":"daemonsets-4598","uid":"9687d20c-5a8f-447f-9afb-4112754935b5","resourceVersion":"64760","creationTimestamp":"2022-03-17T09:35:28Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fec71755ba6abb82a2f9fa4a1146788b1b4f0f50f1f3b8397efea4ab90cdb748","cni.projectcalico.org/podIP":"10.42.2.48/32","cni.projectcalico.org/podIPs":"10.42.2.48/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"80f43dd1-be0c-465d-9eac-68c410cf4d06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80f43dd1-be0c-465d-9eac-68c410cf4d06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7lsbf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7lsbf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-35-106","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-35-106"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:28Z"}],"hostIP":"172.31.35.106","podIP":"10.42.2.48","podIPs":[{"ip":"10.42.2.48"}],"startTime":"2022-03-17T09:35:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-03-17T09:35:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://add846bf545edf67fcc5652c26f4f62d2aeed7363b55003f36f93b130566530b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-s427n","generateName":"daemon-set-","namespace":"daemonsets-4598","uid":"a7470819-5b43-460b-b613-63e55345b06f","resourceVersion":"64758","creationTimestamp":"2022-03-17T09:35:28Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"516853b1e3c48d969265ce47fa3ef5c37d4e874b4fdf79656f6b1f368aceb011","cni.projectcalico.org/podIP":"10.42.0.189/32","cni.projectcalico.org/podIPs":"10.42.0.189/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"80f43dd1-be0c-465d-9eac-68c410cf4d06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80f43dd1-be0c-465d-9eac-68c410cf4d06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-03-17T09:35:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jxjth","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jxjth","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-33-68","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-33-68"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-17T09:35:28Z"}],"hostIP":"172.31.33.68","podIP":"10.42.0.189","podIPs":[{"ip":"10.42.0.189"}],"startTime":"2022-03-17T09:35:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-03-17T09:35:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://fa77216e2bdf519184dfffe55648b1ed423229147f18b081d442262aa677b457","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:30.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4598" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":149,"skipped":2768,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:30.739: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Mar 17 09:35:30.791: INFO: The status of Pod pod-hostip-cf002cf4-afae-4bc0-877e-eb1ba8fea6a9 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:35:32.802: INFO: The status of Pod pod-hostip-cf002cf4-afae-4bc0-877e-eb1ba8fea6a9 is Running (Ready = true)
Mar 17 09:35:32.809: INFO: Pod pod-hostip-cf002cf4-afae-4bc0-877e-eb1ba8fea6a9 has hostIP: 172.31.39.36
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:32.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6669" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2779,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:32.818: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:35:33.538: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:35:36.564: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:35:36.568: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9764-crds.webhook.example.com via the AdmissionRegistration API
Mar 17 09:35:37.129: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:39.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1267" for this suite.
STEP: Destroying namespace "webhook-1267-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.151 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":151,"skipped":2782,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:39.970: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4524.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4524.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4524.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4524.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 09:35:44.118: INFO: DNS probes using dns-4524/dns-test-0d09f5a3-56f9-49e8-9128-2d25f7e09487 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:44.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4524" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":152,"skipped":2782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:44.162: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:35:44.893: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:35:47.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:47.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2895" for this suite.
STEP: Destroying namespace "webhook-2895-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":153,"skipped":2804,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:48.063: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Mar 17 09:35:48.138: INFO: Waiting up to 5m0s for pod "var-expansion-04ddd2a9-bf76-418d-824e-0660255dfea7" in namespace "var-expansion-8291" to be "Succeeded or Failed"
Mar 17 09:35:48.147: INFO: Pod "var-expansion-04ddd2a9-bf76-418d-824e-0660255dfea7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.18089ms
Mar 17 09:35:50.151: INFO: Pod "var-expansion-04ddd2a9-bf76-418d-824e-0660255dfea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013036151s
STEP: Saw pod success
Mar 17 09:35:50.151: INFO: Pod "var-expansion-04ddd2a9-bf76-418d-824e-0660255dfea7" satisfied condition "Succeeded or Failed"
Mar 17 09:35:50.153: INFO: Trying to get logs from node ip-172-31-35-106 pod var-expansion-04ddd2a9-bf76-418d-824e-0660255dfea7 container dapi-container: <nil>
STEP: delete the pod
Mar 17 09:35:50.193: INFO: Waiting for pod var-expansion-04ddd2a9-bf76-418d-824e-0660255dfea7 to disappear
Mar 17 09:35:50.196: INFO: Pod var-expansion-04ddd2a9-bf76-418d-824e-0660255dfea7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:50.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8291" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":154,"skipped":2808,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:50.202: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:35:50.242: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:35:56.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2680" for this suite.

• [SLOW TEST:6.624 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":155,"skipped":2811,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:35:56.826: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:35:57.424: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:36:00.441: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:00.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9725" for this suite.
STEP: Destroying namespace "webhook-9725-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":156,"skipped":2830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:00.685: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:36:00.791: INFO: The status of Pod busybox-readonly-fs91912bc1-428a-47cc-9c8b-e7c6cd6d12d5 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:36:02.795: INFO: The status of Pod busybox-readonly-fs91912bc1-428a-47cc-9c8b-e7c6cd6d12d5 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:02.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3372" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2886,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:02.813: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:36:03.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:36:06.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:18.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9129" for this suite.
STEP: Destroying namespace "webhook-9129-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.750 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":158,"skipped":2900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:18.566: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar 17 09:36:18.661: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:21.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7552" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":159,"skipped":2922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:21.546: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:34.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1425" for this suite.
STEP: Destroying namespace "nsdeletetest-9167" for this suite.
Mar 17 09:36:34.815: INFO: Namespace nsdeletetest-9167 was already deleted
STEP: Destroying namespace "nsdeletetest-4479" for this suite.

• [SLOW TEST:13.275 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":160,"skipped":2975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:34.822: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:36:35.208: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:36:38.228: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:38.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6008" for this suite.
STEP: Destroying namespace "webhook-6008-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":161,"skipped":3027,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:38.370: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar 17 09:36:38.457: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:38.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9523" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":162,"skipped":3029,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:38.497: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar 17 09:36:38.556: INFO: The status of Pod labelsupdate60cd7479-b524-40af-8492-07079b9a5899 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:36:40.560: INFO: The status of Pod labelsupdate60cd7479-b524-40af-8492-07079b9a5899 is Running (Ready = true)
Mar 17 09:36:41.082: INFO: Successfully updated pod "labelsupdate60cd7479-b524-40af-8492-07079b9a5899"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:43.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2967" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":3034,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:43.109: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:36:43.540: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:36:46.559: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:36:46.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7880" for this suite.
STEP: Destroying namespace "webhook-7880-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":164,"skipped":3037,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:36:46.765: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Mar 17 09:37:00.864: INFO: 67 pods remaining
Mar 17 09:37:00.864: INFO: 67 pods has nil DeletionTimestamp
Mar 17 09:37:00.864: INFO: 
STEP: Gathering metrics
Mar 17 09:37:05.872: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0317 09:37:05.872798      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 17 09:37:05.872: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cxwn" in namespace "gc-6060"
Mar 17 09:37:05.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qnch" in namespace "gc-6060"
Mar 17 09:37:05.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-42lqx" in namespace "gc-6060"
Mar 17 09:37:05.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-44rdq" in namespace "gc-6060"
Mar 17 09:37:05.974: INFO: Deleting pod "simpletest-rc-to-be-deleted-45qss" in namespace "gc-6060"
Mar 17 09:37:05.983: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gls7" in namespace "gc-6060"
Mar 17 09:37:06.043: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jgc9" in namespace "gc-6060"
Mar 17 09:37:06.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-4k5qb" in namespace "gc-6060"
Mar 17 09:37:06.134: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lx6n" in namespace "gc-6060"
Mar 17 09:37:06.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fk7j" in namespace "gc-6060"
Mar 17 09:37:06.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-5md98" in namespace "gc-6060"
Mar 17 09:37:06.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cb6c" in namespace "gc-6060"
Mar 17 09:37:06.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wg4q" in namespace "gc-6060"
Mar 17 09:37:06.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wm76" in namespace "gc-6060"
Mar 17 09:37:06.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fz9l" in namespace "gc-6060"
Mar 17 09:37:06.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hzz5" in namespace "gc-6060"
Mar 17 09:37:06.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jr6r" in namespace "gc-6060"
Mar 17 09:37:06.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gvzw" in namespace "gc-6060"
Mar 17 09:37:06.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sx4d" in namespace "gc-6060"
Mar 17 09:37:06.587: INFO: Deleting pod "simpletest-rc-to-be-deleted-97n6q" in namespace "gc-6060"
Mar 17 09:37:06.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-9n8rw" in namespace "gc-6060"
Mar 17 09:37:06.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-9npr9" in namespace "gc-6060"
Mar 17 09:37:06.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc4sp" in namespace "gc-6060"
Mar 17 09:37:06.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjkxn" in namespace "gc-6060"
Mar 17 09:37:06.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqt8x" in namespace "gc-6060"
Mar 17 09:37:06.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-brt4g" in namespace "gc-6060"
Mar 17 09:37:06.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvs29" in namespace "gc-6060"
Mar 17 09:37:06.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5nrq" in namespace "gc-6060"
Mar 17 09:37:06.990: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch464" in namespace "gc-6060"
Mar 17 09:37:07.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-chf2s" in namespace "gc-6060"
Mar 17 09:37:07.044: INFO: Deleting pod "simpletest-rc-to-be-deleted-chxrr" in namespace "gc-6060"
Mar 17 09:37:07.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl5q8" in namespace "gc-6060"
Mar 17 09:37:07.111: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwpfn" in namespace "gc-6060"
Mar 17 09:37:07.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgxmc" in namespace "gc-6060"
Mar 17 09:37:07.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlnx9" in namespace "gc-6060"
Mar 17 09:37:07.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvd4l" in namespace "gc-6060"
Mar 17 09:37:07.281: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8l2b" in namespace "gc-6060"
Mar 17 09:37:07.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-fcgtd" in namespace "gc-6060"
Mar 17 09:37:07.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmjqr" in namespace "gc-6060"
Mar 17 09:37:07.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-fr7sq" in namespace "gc-6060"
Mar 17 09:37:07.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-g26z9" in namespace "gc-6060"
Mar 17 09:37:07.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5cxs" in namespace "gc-6060"
Mar 17 09:37:07.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-g64xq" in namespace "gc-6060"
Mar 17 09:37:07.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-g66xd" in namespace "gc-6060"
Mar 17 09:37:07.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-g67h2" in namespace "gc-6060"
Mar 17 09:37:07.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfk42" in namespace "gc-6060"
Mar 17 09:37:07.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4htk" in namespace "gc-6060"
Mar 17 09:37:07.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjb6l" in namespace "gc-6060"
Mar 17 09:37:07.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-htf6r" in namespace "gc-6060"
Mar 17 09:37:07.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjq9t" in namespace "gc-6060"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:07.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6060" for this suite.

• [SLOW TEST:21.193 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":165,"skipped":3042,"failed":0}
SSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:07.959: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 17 09:37:08.201: INFO: starting watch
STEP: patching
STEP: updating
Mar 17 09:37:08.223: INFO: waiting for watch events with expected annotations
Mar 17 09:37:08.223: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:08.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1650" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":166,"skipped":3050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:08.346: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:08.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7492" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":167,"skipped":3092,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:08.579: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:37:08.707: INFO: The status of Pod busybox-host-aliasesf11dfd70-0fa6-475a-b823-bbf7996e56b1 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:37:10.714: INFO: The status of Pod busybox-host-aliasesf11dfd70-0fa6-475a-b823-bbf7996e56b1 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:37:12.713: INFO: The status of Pod busybox-host-aliasesf11dfd70-0fa6-475a-b823-bbf7996e56b1 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:37:14.721: INFO: The status of Pod busybox-host-aliasesf11dfd70-0fa6-475a-b823-bbf7996e56b1 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:37:16.714: INFO: The status of Pod busybox-host-aliasesf11dfd70-0fa6-475a-b823-bbf7996e56b1 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:16.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6251" for this suite.

• [SLOW TEST:8.150 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":3112,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:16.729: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar 17 09:37:16.771: INFO: The status of Pod labelsupdate523621d3-3442-4ea7-b47b-374f9a7cb853 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:37:18.776: INFO: The status of Pod labelsupdate523621d3-3442-4ea7-b47b-374f9a7cb853 is Running (Ready = true)
Mar 17 09:37:19.300: INFO: Successfully updated pod "labelsupdate523621d3-3442-4ea7-b47b-374f9a7cb853"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:21.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8829" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":169,"skipped":3117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:21.332: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar 17 09:37:21.391: INFO: Waiting up to 5m0s for pod "security-context-f9deb766-edc6-475a-88f2-018e23aa4923" in namespace "security-context-4638" to be "Succeeded or Failed"
Mar 17 09:37:21.398: INFO: Pod "security-context-f9deb766-edc6-475a-88f2-018e23aa4923": Phase="Pending", Reason="", readiness=false. Elapsed: 6.705838ms
Mar 17 09:37:23.402: INFO: Pod "security-context-f9deb766-edc6-475a-88f2-018e23aa4923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010790511s
Mar 17 09:37:25.407: INFO: Pod "security-context-f9deb766-edc6-475a-88f2-018e23aa4923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015799084s
STEP: Saw pod success
Mar 17 09:37:25.407: INFO: Pod "security-context-f9deb766-edc6-475a-88f2-018e23aa4923" satisfied condition "Succeeded or Failed"
Mar 17 09:37:25.409: INFO: Trying to get logs from node ip-172-31-35-106 pod security-context-f9deb766-edc6-475a-88f2-018e23aa4923 container test-container: <nil>
STEP: delete the pod
Mar 17 09:37:25.426: INFO: Waiting for pod security-context-f9deb766-edc6-475a-88f2-018e23aa4923 to disappear
Mar 17 09:37:25.429: INFO: Pod security-context-f9deb766-edc6-475a-88f2-018e23aa4923 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:25.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4638" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":170,"skipped":3164,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:25.439: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:25.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-882" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":171,"skipped":3174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:25.550: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 17 09:37:27.646: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:27.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7964" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":172,"skipped":3209,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:27.679: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:35.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1651" for this suite.

• [SLOW TEST:8.060 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":173,"skipped":3220,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:35.739: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:37:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:38.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4137" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":174,"skipped":3229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:38.974: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 09:37:39.021: INFO: Waiting up to 5m0s for pod "downwardapi-volume-679d7ceb-a301-49e8-ab68-75ef48a1f9ef" in namespace "downward-api-7595" to be "Succeeded or Failed"
Mar 17 09:37:39.026: INFO: Pod "downwardapi-volume-679d7ceb-a301-49e8-ab68-75ef48a1f9ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.456786ms
Mar 17 09:37:41.029: INFO: Pod "downwardapi-volume-679d7ceb-a301-49e8-ab68-75ef48a1f9ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007625199s
STEP: Saw pod success
Mar 17 09:37:41.029: INFO: Pod "downwardapi-volume-679d7ceb-a301-49e8-ab68-75ef48a1f9ef" satisfied condition "Succeeded or Failed"
Mar 17 09:37:41.031: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-679d7ceb-a301-49e8-ab68-75ef48a1f9ef container client-container: <nil>
STEP: delete the pod
Mar 17 09:37:41.044: INFO: Waiting for pod downwardapi-volume-679d7ceb-a301-49e8-ab68-75ef48a1f9ef to disappear
Mar 17 09:37:41.046: INFO: Pod downwardapi-volume-679d7ceb-a301-49e8-ab68-75ef48a1f9ef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:41.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7595" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3295,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:41.056: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 17 09:37:41.116: INFO: Waiting up to 5m0s for pod "pod-59973a6c-725d-4b7b-ab3e-77f78eb086a8" in namespace "emptydir-3335" to be "Succeeded or Failed"
Mar 17 09:37:41.121: INFO: Pod "pod-59973a6c-725d-4b7b-ab3e-77f78eb086a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33885ms
Mar 17 09:37:43.125: INFO: Pod "pod-59973a6c-725d-4b7b-ab3e-77f78eb086a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008414597s
STEP: Saw pod success
Mar 17 09:37:43.125: INFO: Pod "pod-59973a6c-725d-4b7b-ab3e-77f78eb086a8" satisfied condition "Succeeded or Failed"
Mar 17 09:37:43.128: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-59973a6c-725d-4b7b-ab3e-77f78eb086a8 container test-container: <nil>
STEP: delete the pod
Mar 17 09:37:43.145: INFO: Waiting for pod pod-59973a6c-725d-4b7b-ab3e-77f78eb086a8 to disappear
Mar 17 09:37:43.150: INFO: Pod pod-59973a6c-725d-4b7b-ab3e-77f78eb086a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:43.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3335" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":176,"skipped":3315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:43.166: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:37:43.264: INFO: Waiting up to 5m0s for pod "busybox-user-65534-747b27f6-6d3d-43c7-b58b-981aa6d23394" in namespace "security-context-test-101" to be "Succeeded or Failed"
Mar 17 09:37:43.278: INFO: Pod "busybox-user-65534-747b27f6-6d3d-43c7-b58b-981aa6d23394": Phase="Pending", Reason="", readiness=false. Elapsed: 13.472439ms
Mar 17 09:37:45.283: INFO: Pod "busybox-user-65534-747b27f6-6d3d-43c7-b58b-981aa6d23394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018922841s
Mar 17 09:37:45.283: INFO: Pod "busybox-user-65534-747b27f6-6d3d-43c7-b58b-981aa6d23394" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:37:45.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-101" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3417,"failed":0}
SSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:37:45.293: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:38:09.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3546" for this suite.

• [SLOW TEST:24.309 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":178,"skipped":3421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:38:09.604: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Mar 17 09:38:09.681: INFO: Waiting up to 5m0s for pod "client-containers-2e56f366-5ae6-4a33-a3a3-e122d05739b6" in namespace "containers-8512" to be "Succeeded or Failed"
Mar 17 09:38:09.694: INFO: Pod "client-containers-2e56f366-5ae6-4a33-a3a3-e122d05739b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.674117ms
Mar 17 09:38:11.699: INFO: Pod "client-containers-2e56f366-5ae6-4a33-a3a3-e122d05739b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017146707s
STEP: Saw pod success
Mar 17 09:38:11.699: INFO: Pod "client-containers-2e56f366-5ae6-4a33-a3a3-e122d05739b6" satisfied condition "Succeeded or Failed"
Mar 17 09:38:11.701: INFO: Trying to get logs from node ip-172-31-35-106 pod client-containers-2e56f366-5ae6-4a33-a3a3-e122d05739b6 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:38:11.719: INFO: Waiting for pod client-containers-2e56f366-5ae6-4a33-a3a3-e122d05739b6 to disappear
Mar 17 09:38:11.723: INFO: Pod client-containers-2e56f366-5ae6-4a33-a3a3-e122d05739b6 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:38:11.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8512" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3450,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:38:11.729: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-858.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-858.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-858.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-858.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 09:38:15.799: INFO: DNS probes using dns-test-3e8bba32-c0f6-4e25-9f2c-fc7bb6b32f19 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-858.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-858.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-858.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-858.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 09:38:17.849: INFO: File wheezy_udp@dns-test-service-3.dns-858.svc.cluster.local from pod  dns-858/dns-test-1457e9dc-92f6-43c9-a262-252e93316578 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 17 09:38:17.853: INFO: File jessie_udp@dns-test-service-3.dns-858.svc.cluster.local from pod  dns-858/dns-test-1457e9dc-92f6-43c9-a262-252e93316578 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 17 09:38:17.853: INFO: Lookups using dns-858/dns-test-1457e9dc-92f6-43c9-a262-252e93316578 failed for: [wheezy_udp@dns-test-service-3.dns-858.svc.cluster.local jessie_udp@dns-test-service-3.dns-858.svc.cluster.local]

Mar 17 09:38:22.861: INFO: DNS probes using dns-test-1457e9dc-92f6-43c9-a262-252e93316578 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-858.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-858.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-858.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-858.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 09:38:24.924: INFO: DNS probes using dns-test-0dcdb3b9-3d85-4147-ab7a-6133e0f611b7 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:38:24.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-858" for this suite.

• [SLOW TEST:13.241 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":180,"skipped":3455,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:38:24.971: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 17 09:38:25.091: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 17 09:39:25.128: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:39:25.131: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:39:25.186: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar 17 09:39:25.190: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:39:25.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2650" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:39:25.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3738" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.396 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":181,"skipped":3465,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:39:25.367: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-24d7f0e8-77d6-48b1-b1f2-83e2016d6453
STEP: Creating a pod to test consume secrets
Mar 17 09:39:25.472: INFO: Waiting up to 5m0s for pod "pod-secrets-3ac0ac91-a613-40a3-96ed-77d4b93fb7fc" in namespace "secrets-1821" to be "Succeeded or Failed"
Mar 17 09:39:25.482: INFO: Pod "pod-secrets-3ac0ac91-a613-40a3-96ed-77d4b93fb7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.804625ms
Mar 17 09:39:27.486: INFO: Pod "pod-secrets-3ac0ac91-a613-40a3-96ed-77d4b93fb7fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013522148s
STEP: Saw pod success
Mar 17 09:39:27.486: INFO: Pod "pod-secrets-3ac0ac91-a613-40a3-96ed-77d4b93fb7fc" satisfied condition "Succeeded or Failed"
Mar 17 09:39:27.491: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-secrets-3ac0ac91-a613-40a3-96ed-77d4b93fb7fc container secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:39:27.507: INFO: Waiting for pod pod-secrets-3ac0ac91-a613-40a3-96ed-77d4b93fb7fc to disappear
Mar 17 09:39:27.511: INFO: Pod pod-secrets-3ac0ac91-a613-40a3-96ed-77d4b93fb7fc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:39:27.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1821" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:39:27.518: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:39:27.867: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:39:30.906: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:39:30.909: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:39:34.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-133" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.754 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":183,"skipped":3530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:39:34.272: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5073
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Mar 17 09:39:34.423: INFO: Found 0 stateful pods, waiting for 3
Mar 17 09:39:44.431: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 09:39:44.431: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 09:39:44.431: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 09:39:44.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-5073 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 09:39:44.643: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 09:39:44.643: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 09:39:44.643: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Mar 17 09:39:54.686: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 17 09:40:04.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-5073 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 09:40:04.864: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 17 09:40:04.864: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 09:40:04.864: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 09:40:14.893: INFO: Waiting for StatefulSet statefulset-5073/ss2 to complete update
STEP: Rolling back to a previous revision
Mar 17 09:40:24.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-5073 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 09:40:25.067: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 09:40:25.067: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 09:40:25.067: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 09:40:35.100: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 17 09:40:45.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-5073 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 09:40:45.295: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 17 09:40:45.295: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 09:40:45.295: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 09:40:55.318: INFO: Waiting for StatefulSet statefulset-5073/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Mar 17 09:41:05.335: INFO: Deleting all statefulset in ns statefulset-5073
Mar 17 09:41:05.338: INFO: Scaling statefulset ss2 to 0
Mar 17 09:41:15.361: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 09:41:15.364: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:41:15.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5073" for this suite.

• [SLOW TEST:101.134 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":184,"skipped":3567,"failed":0}
SSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:41:15.406: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 17 09:41:16.169: INFO: starting watch
STEP: patching
STEP: updating
Mar 17 09:41:16.176: INFO: waiting for watch events with expected annotations
Mar 17 09:41:16.176: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:41:16.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3667" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":185,"skipped":3570,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:41:16.226: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:46:16.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-124" for this suite.

• [SLOW TEST:300.091 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":186,"skipped":3590,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:46:16.319: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Mar 17 09:46:16.380: INFO: Waiting up to 5m0s for pod "var-expansion-2901bec4-d7dd-4fcb-8bc6-e63a5bf7cd1f" in namespace "var-expansion-7419" to be "Succeeded or Failed"
Mar 17 09:46:16.399: INFO: Pod "var-expansion-2901bec4-d7dd-4fcb-8bc6-e63a5bf7cd1f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.543304ms
Mar 17 09:46:18.406: INFO: Pod "var-expansion-2901bec4-d7dd-4fcb-8bc6-e63a5bf7cd1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025937308s
STEP: Saw pod success
Mar 17 09:46:18.406: INFO: Pod "var-expansion-2901bec4-d7dd-4fcb-8bc6-e63a5bf7cd1f" satisfied condition "Succeeded or Failed"
Mar 17 09:46:18.409: INFO: Trying to get logs from node ip-172-31-35-106 pod var-expansion-2901bec4-d7dd-4fcb-8bc6-e63a5bf7cd1f container dapi-container: <nil>
STEP: delete the pod
Mar 17 09:46:18.445: INFO: Waiting for pod var-expansion-2901bec4-d7dd-4fcb-8bc6-e63a5bf7cd1f to disappear
Mar 17 09:46:18.450: INFO: Pod var-expansion-2901bec4-d7dd-4fcb-8bc6-e63a5bf7cd1f no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:46:18.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7419" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:46:18.470: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Mar 17 09:46:18.578: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 17 09:46:23.587: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:46:23.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8398" for this suite.

• [SLOW TEST:5.171 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":188,"skipped":3631,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:46:23.641: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:46:23.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-5799 version'
Mar 17 09:46:23.778: INFO: stderr: ""
Mar 17 09:46:23.778: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.4\", GitCommit:\"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T12:38:05Z\", GoVersion:\"go1.17.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.4\", GitCommit:\"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T12:32:02Z\", GoVersion:\"go1.17.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:46:23.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5799" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":189,"skipped":3652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:46:23.794: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Mar 17 09:46:23.918: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:46:25.923: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:46:27.921: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.33.68 on the node which pod1 resides and expect scheduled
Mar 17 09:46:27.933: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:46:29.938: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:46:31.938: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.33.68 but use UDP protocol on the node which pod2 resides
Mar 17 09:46:31.952: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:46:33.957: INFO: The status of Pod pod3 is Running (Ready = true)
Mar 17 09:46:33.967: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:46:35.973: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Mar 17 09:46:35.976: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.33.68 http://127.0.0.1:54323/hostname] Namespace:hostport-7711 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:46:35.976: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:46:35.977: INFO: ExecWithOptions: Clientset creation
Mar 17 09:46:35.977: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-7711/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.33.68+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.33.68, port: 54323
Mar 17 09:46:36.155: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.33.68:54323/hostname] Namespace:hostport-7711 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:46:36.155: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:46:36.156: INFO: ExecWithOptions: Clientset creation
Mar 17 09:46:36.156: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-7711/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.33.68%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.33.68, port: 54323 UDP
Mar 17 09:46:36.300: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.33.68 54323] Namespace:hostport-7711 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:46:36.300: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:46:36.301: INFO: ExecWithOptions: Clientset creation
Mar 17 09:46:36.301: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-7711/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+172.31.33.68+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:46:41.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7711" for this suite.

• [SLOW TEST:17.629 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":190,"skipped":3741,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:46:41.423: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:46:41.929: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 17 09:46:41.930: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 17 09:46:41.930: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 17 09:46:41.930: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 17 09:46:41.930: INFO: Checking APIGroup: apps
Mar 17 09:46:41.931: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 17 09:46:41.931: INFO: Versions found [{apps/v1 v1}]
Mar 17 09:46:41.931: INFO: apps/v1 matches apps/v1
Mar 17 09:46:41.931: INFO: Checking APIGroup: events.k8s.io
Mar 17 09:46:41.932: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 17 09:46:41.932: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar 17 09:46:41.932: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 17 09:46:41.932: INFO: Checking APIGroup: authentication.k8s.io
Mar 17 09:46:41.932: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 17 09:46:41.932: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 17 09:46:41.932: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 17 09:46:41.932: INFO: Checking APIGroup: authorization.k8s.io
Mar 17 09:46:41.933: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 17 09:46:41.933: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 17 09:46:41.933: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 17 09:46:41.933: INFO: Checking APIGroup: autoscaling
Mar 17 09:46:41.934: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 17 09:46:41.934: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar 17 09:46:41.934: INFO: autoscaling/v2 matches autoscaling/v2
Mar 17 09:46:41.934: INFO: Checking APIGroup: batch
Mar 17 09:46:41.935: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 17 09:46:41.935: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar 17 09:46:41.935: INFO: batch/v1 matches batch/v1
Mar 17 09:46:41.935: INFO: Checking APIGroup: certificates.k8s.io
Mar 17 09:46:41.936: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 17 09:46:41.936: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 17 09:46:41.936: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 17 09:46:41.936: INFO: Checking APIGroup: networking.k8s.io
Mar 17 09:46:41.937: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 17 09:46:41.937: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 17 09:46:41.937: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 17 09:46:41.937: INFO: Checking APIGroup: policy
Mar 17 09:46:41.939: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 17 09:46:41.939: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Mar 17 09:46:41.939: INFO: policy/v1 matches policy/v1
Mar 17 09:46:41.939: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 17 09:46:41.940: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 17 09:46:41.941: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 17 09:46:41.941: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 17 09:46:41.941: INFO: Checking APIGroup: storage.k8s.io
Mar 17 09:46:41.942: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 17 09:46:41.942: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 17 09:46:41.942: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 17 09:46:41.942: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 17 09:46:41.943: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 17 09:46:41.943: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 17 09:46:41.943: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 17 09:46:41.943: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 17 09:46:41.944: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 17 09:46:41.944: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 17 09:46:41.944: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 17 09:46:41.944: INFO: Checking APIGroup: scheduling.k8s.io
Mar 17 09:46:41.945: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 17 09:46:41.945: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 17 09:46:41.945: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 17 09:46:41.945: INFO: Checking APIGroup: coordination.k8s.io
Mar 17 09:46:41.946: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 17 09:46:41.946: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 17 09:46:41.946: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 17 09:46:41.947: INFO: Checking APIGroup: node.k8s.io
Mar 17 09:46:41.947: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 17 09:46:41.947: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar 17 09:46:41.947: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 17 09:46:41.948: INFO: Checking APIGroup: discovery.k8s.io
Mar 17 09:46:41.948: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 17 09:46:41.948: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Mar 17 09:46:41.948: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 17 09:46:41.948: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 17 09:46:41.949: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar 17 09:46:41.949: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar 17 09:46:41.949: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar 17 09:46:41.949: INFO: Checking APIGroup: catalog.cattle.io
Mar 17 09:46:41.950: INFO: PreferredVersion.GroupVersion: catalog.cattle.io/v1
Mar 17 09:46:41.950: INFO: Versions found [{catalog.cattle.io/v1 v1}]
Mar 17 09:46:41.950: INFO: catalog.cattle.io/v1 matches catalog.cattle.io/v1
Mar 17 09:46:41.950: INFO: Checking APIGroup: crd.projectcalico.org
Mar 17 09:46:41.950: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar 17 09:46:41.951: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar 17 09:46:41.951: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar 17 09:46:41.951: INFO: Checking APIGroup: ui.cattle.io
Mar 17 09:46:41.951: INFO: PreferredVersion.GroupVersion: ui.cattle.io/v1
Mar 17 09:46:41.951: INFO: Versions found [{ui.cattle.io/v1 v1}]
Mar 17 09:46:41.951: INFO: ui.cattle.io/v1 matches ui.cattle.io/v1
Mar 17 09:46:41.951: INFO: Checking APIGroup: cluster.cattle.io
Mar 17 09:46:41.952: INFO: PreferredVersion.GroupVersion: cluster.cattle.io/v3
Mar 17 09:46:41.952: INFO: Versions found [{cluster.cattle.io/v3 v3}]
Mar 17 09:46:41.952: INFO: cluster.cattle.io/v3 matches cluster.cattle.io/v3
Mar 17 09:46:41.952: INFO: Checking APIGroup: management.cattle.io
Mar 17 09:46:41.953: INFO: PreferredVersion.GroupVersion: management.cattle.io/v3
Mar 17 09:46:41.953: INFO: Versions found [{management.cattle.io/v3 v3}]
Mar 17 09:46:41.953: INFO: management.cattle.io/v3 matches management.cattle.io/v3
Mar 17 09:46:41.953: INFO: Checking APIGroup: metrics.k8s.io
Mar 17 09:46:41.953: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 17 09:46:41.953: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 17 09:46:41.953: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:46:41.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-644" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":191,"skipped":3743,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:46:41.967: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-04282162-c542-48b6-aad3-18d454ee1de9
STEP: Creating the pod
Mar 17 09:46:42.073: INFO: The status of Pod pod-configmaps-db72b0bb-a688-47cb-85bb-8d79e62e7ab6 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:46:44.078: INFO: The status of Pod pod-configmaps-db72b0bb-a688-47cb-85bb-8d79e62e7ab6 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-04282162-c542-48b6-aad3-18d454ee1de9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:46:46.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7212" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3751,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:46:46.127: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-147
STEP: creating service affinity-nodeport-transition in namespace services-147
STEP: creating replication controller affinity-nodeport-transition in namespace services-147
I0317 09:46:46.213126      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-147, replica count: 3
I0317 09:46:49.264745      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 09:46:49.281: INFO: Creating new exec pod
Mar 17 09:46:52.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-147 exec execpod-affinityt6nrt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar 17 09:46:52.684: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 17 09:46:52.684: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:46:52.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-147 exec execpod-affinityt6nrt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.132.167 80'
Mar 17 09:46:52.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.132.167 80\nConnection to 10.43.132.167 80 port [tcp/http] succeeded!\n"
Mar 17 09:46:52.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:46:52.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-147 exec execpod-affinityt6nrt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.33.68 32567'
Mar 17 09:46:53.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.33.68 32567\nConnection to 172.31.33.68 32567 port [tcp/*] succeeded!\n"
Mar 17 09:46:53.100: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:46:53.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-147 exec execpod-affinityt6nrt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.39.36 32567'
Mar 17 09:46:53.289: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 172.31.39.36 32567\nConnection to 172.31.39.36 32567 port [tcp/*] succeeded!\n"
Mar 17 09:46:53.289: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:46:53.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-147 exec execpod-affinityt6nrt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.33.68:32567/ ; done'
Mar 17 09:46:53.664: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n"
Mar 17 09:46:53.664: INFO: stdout: "\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-88fk6\naffinity-nodeport-transition-dm9xb\naffinity-nodeport-transition-88fk6\naffinity-nodeport-transition-88fk6\naffinity-nodeport-transition-dm9xb\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-dm9xb\naffinity-nodeport-transition-88fk6\naffinity-nodeport-transition-88fk6\naffinity-nodeport-transition-dm9xb\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-88fk6\naffinity-nodeport-transition-88fk6\naffinity-nodeport-transition-twhb9"
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-88fk6
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-dm9xb
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-88fk6
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-88fk6
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-dm9xb
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-dm9xb
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-88fk6
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-88fk6
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-dm9xb
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-88fk6
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-88fk6
Mar 17 09:46:53.664: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-147 exec execpod-affinityt6nrt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.33.68:32567/ ; done'
Mar 17 09:46:53.964: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:32567/\n"
Mar 17 09:46:53.964: INFO: stdout: "\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9\naffinity-nodeport-transition-twhb9"
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Received response from host: affinity-nodeport-transition-twhb9
Mar 17 09:46:53.964: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-147, will wait for the garbage collector to delete the pods
Mar 17 09:46:54.043: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.803554ms
Mar 17 09:46:54.143: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.607112ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:46:56.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-147" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.350 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":193,"skipped":3758,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:46:56.478: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 17 09:46:56.527: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 17 09:47:56.574: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:47:56.586: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar 17 09:47:58.680: INFO: found a healthy node: ip-172-31-35-106
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:48:04.760: INFO: pods created so far: [1 1 1]
Mar 17 09:48:04.760: INFO: length of pods created so far: 3
Mar 17 09:48:06.772: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:48:13.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9765" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:48:13.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6154" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:77.497 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":194,"skipped":3771,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:48:13.975: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-5dec43d4-dc53-4f8f-8343-6360838f528c
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:48:14.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4986" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":195,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:48:14.037: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 17 09:48:14.131: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 17 09:48:14.136: INFO: starting watch
STEP: patching
STEP: updating
Mar 17 09:48:14.153: INFO: waiting for watch events with expected annotations
Mar 17 09:48:14.153: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:48:14.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3821" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":196,"skipped":3843,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:48:14.181: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Mar 17 09:48:14.251: INFO: Found Service test-service-rcwpq in namespace services-9172 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 17 09:48:14.251: INFO: Service test-service-rcwpq created
STEP: Getting /status
Mar 17 09:48:14.260: INFO: Service test-service-rcwpq has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Mar 17 09:48:14.269: INFO: observed Service test-service-rcwpq in namespace services-9172 with annotations: map[] & LoadBalancer: {[]}
Mar 17 09:48:14.269: INFO: Found Service test-service-rcwpq in namespace services-9172 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 17 09:48:14.269: INFO: Service test-service-rcwpq has service status patched
STEP: updating the ServiceStatus
Mar 17 09:48:14.283: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Mar 17 09:48:14.288: INFO: Observed Service test-service-rcwpq in namespace services-9172 with annotations: map[] & Conditions: {[]}
Mar 17 09:48:14.289: INFO: Observed event: &Service{ObjectMeta:{test-service-rcwpq  services-9172  8fae0bc8-e4a1-4d4f-aa1d-c95d3bbe0817 71260 0 2022-03-17 09:48:14 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-03-17 09:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-03-17 09:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.177.136,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.177.136],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 17 09:48:14.289: INFO: Found Service test-service-rcwpq in namespace services-9172 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 17 09:48:14.289: INFO: Service test-service-rcwpq has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Mar 17 09:48:14.311: INFO: observed Service test-service-rcwpq in namespace services-9172 with labels: map[test-service-static:true]
Mar 17 09:48:14.311: INFO: observed Service test-service-rcwpq in namespace services-9172 with labels: map[test-service-static:true]
Mar 17 09:48:14.311: INFO: observed Service test-service-rcwpq in namespace services-9172 with labels: map[test-service-static:true]
Mar 17 09:48:14.311: INFO: Found Service test-service-rcwpq in namespace services-9172 with labels: map[test-service:patched test-service-static:true]
Mar 17 09:48:14.312: INFO: Service test-service-rcwpq patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Mar 17 09:48:14.331: INFO: Observed event: ADDED
Mar 17 09:48:14.331: INFO: Observed event: MODIFIED
Mar 17 09:48:14.331: INFO: Observed event: MODIFIED
Mar 17 09:48:14.331: INFO: Observed event: MODIFIED
Mar 17 09:48:14.331: INFO: Found Service test-service-rcwpq in namespace services-9172 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 17 09:48:14.331: INFO: Service test-service-rcwpq deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:48:14.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9172" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":197,"skipped":3861,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:48:14.341: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0317 09:48:54.568788      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 17 09:48:54.568: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 17 09:48:54.568: INFO: Deleting pod "simpletest.rc-22vn9" in namespace "gc-6852"
Mar 17 09:48:54.577: INFO: Deleting pod "simpletest.rc-26fpt" in namespace "gc-6852"
Mar 17 09:48:54.584: INFO: Deleting pod "simpletest.rc-28xnd" in namespace "gc-6852"
Mar 17 09:48:54.601: INFO: Deleting pod "simpletest.rc-42v6s" in namespace "gc-6852"
Mar 17 09:48:54.615: INFO: Deleting pod "simpletest.rc-444cd" in namespace "gc-6852"
Mar 17 09:48:54.630: INFO: Deleting pod "simpletest.rc-4d99z" in namespace "gc-6852"
Mar 17 09:48:54.648: INFO: Deleting pod "simpletest.rc-4h7xq" in namespace "gc-6852"
Mar 17 09:48:54.669: INFO: Deleting pod "simpletest.rc-4hm2k" in namespace "gc-6852"
Mar 17 09:48:54.692: INFO: Deleting pod "simpletest.rc-4hmv2" in namespace "gc-6852"
Mar 17 09:48:54.769: INFO: Deleting pod "simpletest.rc-4k9rk" in namespace "gc-6852"
Mar 17 09:48:54.840: INFO: Deleting pod "simpletest.rc-4s4v7" in namespace "gc-6852"
Mar 17 09:48:54.891: INFO: Deleting pod "simpletest.rc-4zjth" in namespace "gc-6852"
Mar 17 09:48:55.002: INFO: Deleting pod "simpletest.rc-5b2rm" in namespace "gc-6852"
Mar 17 09:48:55.051: INFO: Deleting pod "simpletest.rc-5cpl8" in namespace "gc-6852"
Mar 17 09:48:55.075: INFO: Deleting pod "simpletest.rc-5wjhm" in namespace "gc-6852"
Mar 17 09:48:55.089: INFO: Deleting pod "simpletest.rc-67sgp" in namespace "gc-6852"
Mar 17 09:48:55.101: INFO: Deleting pod "simpletest.rc-69rrh" in namespace "gc-6852"
Mar 17 09:48:55.121: INFO: Deleting pod "simpletest.rc-6vnjj" in namespace "gc-6852"
Mar 17 09:48:55.154: INFO: Deleting pod "simpletest.rc-6xxzx" in namespace "gc-6852"
Mar 17 09:48:55.199: INFO: Deleting pod "simpletest.rc-6z78t" in namespace "gc-6852"
Mar 17 09:48:55.211: INFO: Deleting pod "simpletest.rc-72tt4" in namespace "gc-6852"
Mar 17 09:48:55.234: INFO: Deleting pod "simpletest.rc-76fxl" in namespace "gc-6852"
Mar 17 09:48:55.272: INFO: Deleting pod "simpletest.rc-7b669" in namespace "gc-6852"
Mar 17 09:48:55.320: INFO: Deleting pod "simpletest.rc-7ns5z" in namespace "gc-6852"
Mar 17 09:48:55.369: INFO: Deleting pod "simpletest.rc-7tlvw" in namespace "gc-6852"
Mar 17 09:48:55.392: INFO: Deleting pod "simpletest.rc-8dcgt" in namespace "gc-6852"
Mar 17 09:48:55.430: INFO: Deleting pod "simpletest.rc-8sfd5" in namespace "gc-6852"
Mar 17 09:48:55.482: INFO: Deleting pod "simpletest.rc-8vjvs" in namespace "gc-6852"
Mar 17 09:48:55.518: INFO: Deleting pod "simpletest.rc-98j6v" in namespace "gc-6852"
Mar 17 09:48:55.577: INFO: Deleting pod "simpletest.rc-9wdvr" in namespace "gc-6852"
Mar 17 09:48:55.653: INFO: Deleting pod "simpletest.rc-bnrvl" in namespace "gc-6852"
Mar 17 09:48:55.698: INFO: Deleting pod "simpletest.rc-c4648" in namespace "gc-6852"
Mar 17 09:48:55.720: INFO: Deleting pod "simpletest.rc-ccpzd" in namespace "gc-6852"
Mar 17 09:48:55.740: INFO: Deleting pod "simpletest.rc-cd7tn" in namespace "gc-6852"
Mar 17 09:48:55.775: INFO: Deleting pod "simpletest.rc-cmdcr" in namespace "gc-6852"
Mar 17 09:48:55.863: INFO: Deleting pod "simpletest.rc-cmxt2" in namespace "gc-6852"
Mar 17 09:48:55.902: INFO: Deleting pod "simpletest.rc-dh2rf" in namespace "gc-6852"
Mar 17 09:48:55.966: INFO: Deleting pod "simpletest.rc-dk65q" in namespace "gc-6852"
Mar 17 09:48:55.994: INFO: Deleting pod "simpletest.rc-dtccr" in namespace "gc-6852"
Mar 17 09:48:56.035: INFO: Deleting pod "simpletest.rc-f7xmz" in namespace "gc-6852"
Mar 17 09:48:56.073: INFO: Deleting pod "simpletest.rc-fhcmk" in namespace "gc-6852"
Mar 17 09:48:56.090: INFO: Deleting pod "simpletest.rc-fk4kc" in namespace "gc-6852"
Mar 17 09:48:56.126: INFO: Deleting pod "simpletest.rc-fq27p" in namespace "gc-6852"
Mar 17 09:48:56.230: INFO: Deleting pod "simpletest.rc-ghwgh" in namespace "gc-6852"
Mar 17 09:48:56.262: INFO: Deleting pod "simpletest.rc-gpmpz" in namespace "gc-6852"
Mar 17 09:48:56.295: INFO: Deleting pod "simpletest.rc-h4276" in namespace "gc-6852"
Mar 17 09:48:56.325: INFO: Deleting pod "simpletest.rc-hdh59" in namespace "gc-6852"
Mar 17 09:48:56.361: INFO: Deleting pod "simpletest.rc-hjkf8" in namespace "gc-6852"
Mar 17 09:48:56.447: INFO: Deleting pod "simpletest.rc-j6fns" in namespace "gc-6852"
Mar 17 09:48:56.509: INFO: Deleting pod "simpletest.rc-jdp7d" in namespace "gc-6852"
Mar 17 09:48:56.537: INFO: Deleting pod "simpletest.rc-jmv45" in namespace "gc-6852"
Mar 17 09:48:56.624: INFO: Deleting pod "simpletest.rc-jvgf8" in namespace "gc-6852"
Mar 17 09:48:56.706: INFO: Deleting pod "simpletest.rc-kqcf9" in namespace "gc-6852"
Mar 17 09:48:56.749: INFO: Deleting pod "simpletest.rc-kwnv4" in namespace "gc-6852"
Mar 17 09:48:56.777: INFO: Deleting pod "simpletest.rc-lntm4" in namespace "gc-6852"
Mar 17 09:48:56.824: INFO: Deleting pod "simpletest.rc-lqpcz" in namespace "gc-6852"
Mar 17 09:48:56.862: INFO: Deleting pod "simpletest.rc-ls52m" in namespace "gc-6852"
Mar 17 09:48:56.878: INFO: Deleting pod "simpletest.rc-lv985" in namespace "gc-6852"
Mar 17 09:48:56.909: INFO: Deleting pod "simpletest.rc-lzlr8" in namespace "gc-6852"
Mar 17 09:48:56.954: INFO: Deleting pod "simpletest.rc-m26z9" in namespace "gc-6852"
Mar 17 09:48:57.005: INFO: Deleting pod "simpletest.rc-m9b9d" in namespace "gc-6852"
Mar 17 09:48:57.026: INFO: Deleting pod "simpletest.rc-msm97" in namespace "gc-6852"
Mar 17 09:48:57.056: INFO: Deleting pod "simpletest.rc-n9rfk" in namespace "gc-6852"
Mar 17 09:48:57.110: INFO: Deleting pod "simpletest.rc-nf25q" in namespace "gc-6852"
Mar 17 09:48:57.149: INFO: Deleting pod "simpletest.rc-nl7l2" in namespace "gc-6852"
Mar 17 09:48:57.174: INFO: Deleting pod "simpletest.rc-nmz6z" in namespace "gc-6852"
Mar 17 09:48:57.206: INFO: Deleting pod "simpletest.rc-p22d4" in namespace "gc-6852"
Mar 17 09:48:57.225: INFO: Deleting pod "simpletest.rc-pdfsb" in namespace "gc-6852"
Mar 17 09:48:57.294: INFO: Deleting pod "simpletest.rc-ph2xx" in namespace "gc-6852"
Mar 17 09:48:57.338: INFO: Deleting pod "simpletest.rc-pt9rf" in namespace "gc-6852"
Mar 17 09:48:57.382: INFO: Deleting pod "simpletest.rc-pvlkb" in namespace "gc-6852"
Mar 17 09:48:57.403: INFO: Deleting pod "simpletest.rc-q8skg" in namespace "gc-6852"
Mar 17 09:48:57.442: INFO: Deleting pod "simpletest.rc-qc52p" in namespace "gc-6852"
Mar 17 09:48:57.461: INFO: Deleting pod "simpletest.rc-qj8hm" in namespace "gc-6852"
Mar 17 09:48:57.485: INFO: Deleting pod "simpletest.rc-qktmv" in namespace "gc-6852"
Mar 17 09:48:57.503: INFO: Deleting pod "simpletest.rc-qpsrg" in namespace "gc-6852"
Mar 17 09:48:57.556: INFO: Deleting pod "simpletest.rc-qzd2t" in namespace "gc-6852"
Mar 17 09:48:57.597: INFO: Deleting pod "simpletest.rc-rpssh" in namespace "gc-6852"
Mar 17 09:48:57.620: INFO: Deleting pod "simpletest.rc-rt9lm" in namespace "gc-6852"
Mar 17 09:48:57.724: INFO: Deleting pod "simpletest.rc-rxdsg" in namespace "gc-6852"
Mar 17 09:48:57.810: INFO: Deleting pod "simpletest.rc-scw2f" in namespace "gc-6852"
Mar 17 09:48:57.863: INFO: Deleting pod "simpletest.rc-sjtrj" in namespace "gc-6852"
Mar 17 09:48:57.910: INFO: Deleting pod "simpletest.rc-sttgp" in namespace "gc-6852"
Mar 17 09:48:57.966: INFO: Deleting pod "simpletest.rc-sx7mm" in namespace "gc-6852"
Mar 17 09:48:57.995: INFO: Deleting pod "simpletest.rc-tqqzr" in namespace "gc-6852"
Mar 17 09:48:58.042: INFO: Deleting pod "simpletest.rc-trrqn" in namespace "gc-6852"
Mar 17 09:48:58.066: INFO: Deleting pod "simpletest.rc-txzk4" in namespace "gc-6852"
Mar 17 09:48:58.146: INFO: Deleting pod "simpletest.rc-vlgjt" in namespace "gc-6852"
Mar 17 09:48:58.243: INFO: Deleting pod "simpletest.rc-vls6r" in namespace "gc-6852"
Mar 17 09:48:58.270: INFO: Deleting pod "simpletest.rc-w26fq" in namespace "gc-6852"
Mar 17 09:48:58.301: INFO: Deleting pod "simpletest.rc-wh4lq" in namespace "gc-6852"
Mar 17 09:48:58.322: INFO: Deleting pod "simpletest.rc-wp6xz" in namespace "gc-6852"
Mar 17 09:48:58.380: INFO: Deleting pod "simpletest.rc-x49jl" in namespace "gc-6852"
Mar 17 09:48:58.416: INFO: Deleting pod "simpletest.rc-xl5wl" in namespace "gc-6852"
Mar 17 09:48:58.452: INFO: Deleting pod "simpletest.rc-xmm6b" in namespace "gc-6852"
Mar 17 09:48:58.511: INFO: Deleting pod "simpletest.rc-xqwdr" in namespace "gc-6852"
Mar 17 09:48:58.572: INFO: Deleting pod "simpletest.rc-z95nr" in namespace "gc-6852"
Mar 17 09:48:58.616: INFO: Deleting pod "simpletest.rc-zh5xn" in namespace "gc-6852"
Mar 17 09:48:58.643: INFO: Deleting pod "simpletest.rc-zkkmg" in namespace "gc-6852"
Mar 17 09:48:58.669: INFO: Deleting pod "simpletest.rc-zx9hl" in namespace "gc-6852"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:48:58.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6852" for this suite.

• [SLOW TEST:44.382 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":198,"skipped":3907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:48:58.723: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:49:14.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6991" for this suite.

• [SLOW TEST:16.200 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":199,"skipped":3937,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:49:14.923: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a collection of services
Mar 17 09:49:14.966: INFO: Creating e2e-svc-a-xx55k
Mar 17 09:49:14.982: INFO: Creating e2e-svc-b-b8v7w
Mar 17 09:49:15.007: INFO: Creating e2e-svc-c-98krm
STEP: deleting service collection
Mar 17 09:49:15.062: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:49:15.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9704" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":200,"skipped":3940,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:49:15.076: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-3c359494-322d-4331-b1e7-7470ee3b79a2
STEP: Creating a pod to test consume secrets
Mar 17 09:49:15.215: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-911f28ab-c263-4385-9e99-685cdcdf347b" in namespace "projected-8145" to be "Succeeded or Failed"
Mar 17 09:49:15.222: INFO: Pod "pod-projected-secrets-911f28ab-c263-4385-9e99-685cdcdf347b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.817905ms
Mar 17 09:49:17.230: INFO: Pod "pod-projected-secrets-911f28ab-c263-4385-9e99-685cdcdf347b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014333442s
Mar 17 09:49:19.235: INFO: Pod "pod-projected-secrets-911f28ab-c263-4385-9e99-685cdcdf347b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019292245s
STEP: Saw pod success
Mar 17 09:49:19.235: INFO: Pod "pod-projected-secrets-911f28ab-c263-4385-9e99-685cdcdf347b" satisfied condition "Succeeded or Failed"
Mar 17 09:49:19.238: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-secrets-911f28ab-c263-4385-9e99-685cdcdf347b container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 17 09:49:19.277: INFO: Waiting for pod pod-projected-secrets-911f28ab-c263-4385-9e99-685cdcdf347b to disappear
Mar 17 09:49:19.281: INFO: Pod pod-projected-secrets-911f28ab-c263-4385-9e99-685cdcdf347b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:49:19.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8145" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3958,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:49:19.302: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Mar 17 09:49:21.484: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:49:23.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-217" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":202,"skipped":3983,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:49:23.530: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 17 09:49:23.574: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4900  bc4e0edd-092b-466f-9730-2f8f28fea17d 73498 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:49:23.574: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4900  bc4e0edd-092b-466f-9730-2f8f28fea17d 73498 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 17 09:49:23.582: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4900  bc4e0edd-092b-466f-9730-2f8f28fea17d 73500 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:49:23.582: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4900  bc4e0edd-092b-466f-9730-2f8f28fea17d 73500 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 17 09:49:23.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4900  bc4e0edd-092b-466f-9730-2f8f28fea17d 73502 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:49:23.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4900  bc4e0edd-092b-466f-9730-2f8f28fea17d 73502 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 17 09:49:23.597: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4900  bc4e0edd-092b-466f-9730-2f8f28fea17d 73503 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:49:23.597: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4900  bc4e0edd-092b-466f-9730-2f8f28fea17d 73503 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 17 09:49:23.601: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4900  b94815c5-3aa8-477f-93a0-9f4b7541611a 73504 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:49:23.601: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4900  b94815c5-3aa8-477f-93a0-9f4b7541611a 73504 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 17 09:49:33.612: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4900  b94815c5-3aa8-477f-93a0-9f4b7541611a 73558 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:49:33.612: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4900  b94815c5-3aa8-477f-93a0-9f4b7541611a 73558 0 2022-03-17 09:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-17 09:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:49:43.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4900" for this suite.

• [SLOW TEST:20.095 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":203,"skipped":3991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:49:43.626: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:49:43.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3191" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":204,"skipped":4032,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:49:43.718: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Mar 17 09:49:50.529: INFO: 80 pods remaining
Mar 17 09:49:50.529: INFO: 80 pods has nil DeletionTimestamp
Mar 17 09:49:50.529: INFO: 
Mar 17 09:49:51.190: INFO: 75 pods remaining
Mar 17 09:49:51.191: INFO: 75 pods has nil DeletionTimestamp
Mar 17 09:49:51.191: INFO: 
Mar 17 09:49:52.078: INFO: 60 pods remaining
Mar 17 09:49:52.079: INFO: 59 pods has nil DeletionTimestamp
Mar 17 09:49:52.079: INFO: 
Mar 17 09:49:53.094: INFO: 47 pods remaining
Mar 17 09:49:53.094: INFO: 47 pods has nil DeletionTimestamp
Mar 17 09:49:53.094: INFO: 
Mar 17 09:49:54.039: INFO: 35 pods remaining
Mar 17 09:49:54.039: INFO: 35 pods has nil DeletionTimestamp
Mar 17 09:49:54.039: INFO: 
Mar 17 09:49:55.035: INFO: 19 pods remaining
Mar 17 09:49:55.035: INFO: 19 pods has nil DeletionTimestamp
Mar 17 09:49:55.036: INFO: 
Mar 17 09:49:56.059: INFO: 4 pods remaining
Mar 17 09:49:56.059: INFO: 3 pods has nil DeletionTimestamp
Mar 17 09:49:56.059: INFO: 
STEP: Gathering metrics
W0317 09:49:57.011896      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 17 09:49:57.012: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:49:57.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9876" for this suite.

• [SLOW TEST:13.319 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":205,"skipped":4040,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:49:57.037: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:49:57.433: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 17 09:49:59.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.March, 17, 9, 49, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 49, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 9, 49, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 9, 49, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:50:02.461: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:50:02.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7856" for this suite.
STEP: Destroying namespace "webhook-7856-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.610 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":206,"skipped":4046,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:50:02.647: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:50:13.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2422" for this suite.

• [SLOW TEST:11.342 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":207,"skipped":4057,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:50:13.990: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:50:14.084: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 17 09:50:17.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-9047 --namespace=crd-publish-openapi-9047 create -f -'
Mar 17 09:50:18.741: INFO: stderr: ""
Mar 17 09:50:18.741: INFO: stdout: "e2e-test-crd-publish-openapi-4770-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 17 09:50:18.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-9047 --namespace=crd-publish-openapi-9047 delete e2e-test-crd-publish-openapi-4770-crds test-cr'
Mar 17 09:50:18.803: INFO: stderr: ""
Mar 17 09:50:18.804: INFO: stdout: "e2e-test-crd-publish-openapi-4770-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 17 09:50:18.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-9047 --namespace=crd-publish-openapi-9047 apply -f -'
Mar 17 09:50:19.025: INFO: stderr: ""
Mar 17 09:50:19.025: INFO: stdout: "e2e-test-crd-publish-openapi-4770-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 17 09:50:19.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-9047 --namespace=crd-publish-openapi-9047 delete e2e-test-crd-publish-openapi-4770-crds test-cr'
Mar 17 09:50:19.090: INFO: stderr: ""
Mar 17 09:50:19.090: INFO: stdout: "e2e-test-crd-publish-openapi-4770-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 17 09:50:19.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-9047 explain e2e-test-crd-publish-openapi-4770-crds'
Mar 17 09:50:19.298: INFO: stderr: ""
Mar 17 09:50:19.298: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4770-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:50:21.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9047" for this suite.

• [SLOW TEST:7.728 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":208,"skipped":4057,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:50:21.719: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar 17 09:50:21.773: INFO: The status of Pod annotationupdateca9f56c2-3880-49d5-955e-fdcf6e400d88 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:50:23.778: INFO: The status of Pod annotationupdateca9f56c2-3880-49d5-955e-fdcf6e400d88 is Running (Ready = true)
Mar 17 09:50:24.306: INFO: Successfully updated pod "annotationupdateca9f56c2-3880-49d5-955e-fdcf6e400d88"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:50:28.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3818" for this suite.

• [SLOW TEST:6.636 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":4083,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:50:28.355: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 17 09:50:28.443: INFO: Waiting up to 5m0s for pod "downward-api-adc1df5e-019a-4a03-8921-bf6f0534026a" in namespace "downward-api-7647" to be "Succeeded or Failed"
Mar 17 09:50:28.487: INFO: Pod "downward-api-adc1df5e-019a-4a03-8921-bf6f0534026a": Phase="Pending", Reason="", readiness=false. Elapsed: 44.400863ms
Mar 17 09:50:30.491: INFO: Pod "downward-api-adc1df5e-019a-4a03-8921-bf6f0534026a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048096525s
STEP: Saw pod success
Mar 17 09:50:30.491: INFO: Pod "downward-api-adc1df5e-019a-4a03-8921-bf6f0534026a" satisfied condition "Succeeded or Failed"
Mar 17 09:50:30.495: INFO: Trying to get logs from node ip-172-31-35-106 pod downward-api-adc1df5e-019a-4a03-8921-bf6f0534026a container dapi-container: <nil>
STEP: delete the pod
Mar 17 09:50:30.515: INFO: Waiting for pod downward-api-adc1df5e-019a-4a03-8921-bf6f0534026a to disappear
Mar 17 09:50:30.519: INFO: Pod downward-api-adc1df5e-019a-4a03-8921-bf6f0534026a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:50:30.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7647" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":210,"skipped":4086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:50:30.540: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:50:30.635: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Creating first CR 
Mar 17 09:50:33.647: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-17T09:50:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-17T09:50:33Z]] name:name1 resourceVersion:75132 uid:a2975e4f-efbf-4c1a-9aca-17e86fef081e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 17 09:50:43.658: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-17T09:50:43Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-17T09:50:43Z]] name:name2 resourceVersion:75175 uid:6f280d0e-5849-459e-8ffb-83002ca33645] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 17 09:50:53.664: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-17T09:50:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-17T09:50:53Z]] name:name1 resourceVersion:75200 uid:a2975e4f-efbf-4c1a-9aca-17e86fef081e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 17 09:51:03.673: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-17T09:50:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-17T09:51:03Z]] name:name2 resourceVersion:75225 uid:6f280d0e-5849-459e-8ffb-83002ca33645] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 17 09:51:13.682: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-17T09:50:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-17T09:50:53Z]] name:name1 resourceVersion:75250 uid:a2975e4f-efbf-4c1a-9aca-17e86fef081e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 17 09:51:23.690: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-17T09:50:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-17T09:51:03Z]] name:name2 resourceVersion:75275 uid:6f280d0e-5849-459e-8ffb-83002ca33645] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:51:34.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6317" for this suite.

• [SLOW TEST:63.733 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":211,"skipped":4117,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:51:34.273: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:51:34.364: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 17 09:51:38.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 create -f -'
Mar 17 09:51:39.487: INFO: stderr: ""
Mar 17 09:51:39.487: INFO: stdout: "e2e-test-crd-publish-openapi-3353-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 17 09:51:39.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 delete e2e-test-crd-publish-openapi-3353-crds test-foo'
Mar 17 09:51:39.554: INFO: stderr: ""
Mar 17 09:51:39.554: INFO: stdout: "e2e-test-crd-publish-openapi-3353-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 17 09:51:39.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 apply -f -'
Mar 17 09:51:39.744: INFO: stderr: ""
Mar 17 09:51:39.744: INFO: stdout: "e2e-test-crd-publish-openapi-3353-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 17 09:51:39.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 delete e2e-test-crd-publish-openapi-3353-crds test-foo'
Mar 17 09:51:39.808: INFO: stderr: ""
Mar 17 09:51:39.808: INFO: stdout: "e2e-test-crd-publish-openapi-3353-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Mar 17 09:51:39.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 create -f -'
Mar 17 09:51:39.987: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 17 09:51:39.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 create -f -'
Mar 17 09:51:40.170: INFO: rc: 1
Mar 17 09:51:40.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 apply -f -'
Mar 17 09:51:40.382: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 17 09:51:40.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 create -f -'
Mar 17 09:51:40.560: INFO: rc: 1
Mar 17 09:51:40.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 --namespace=crd-publish-openapi-286 apply -f -'
Mar 17 09:51:40.720: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 17 09:51:40.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 explain e2e-test-crd-publish-openapi-3353-crds'
Mar 17 09:51:40.874: INFO: stderr: ""
Mar 17 09:51:40.874: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3353-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 17 09:51:40.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 explain e2e-test-crd-publish-openapi-3353-crds.metadata'
Mar 17 09:51:41.038: INFO: stderr: ""
Mar 17 09:51:41.038: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3353-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 17 09:51:41.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 explain e2e-test-crd-publish-openapi-3353-crds.spec'
Mar 17 09:51:41.192: INFO: stderr: ""
Mar 17 09:51:41.193: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3353-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 17 09:51:41.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 explain e2e-test-crd-publish-openapi-3353-crds.spec.bars'
Mar 17 09:51:41.348: INFO: stderr: ""
Mar 17 09:51:41.348: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3353-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 17 09:51:41.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-286 explain e2e-test-crd-publish-openapi-3353-crds.spec.bars2'
Mar 17 09:51:41.519: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:51:43.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-286" for this suite.

• [SLOW TEST:9.709 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":212,"skipped":4117,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:51:43.982: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
STEP: creating an pod
Mar 17 09:51:44.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2589 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 17 09:51:44.079: INFO: stderr: ""
Mar 17 09:51:44.079: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Mar 17 09:51:44.079: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 17 09:51:44.079: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2589" to be "running and ready, or succeeded"
Mar 17 09:51:44.086: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.011489ms
Mar 17 09:51:46.091: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011296358s
Mar 17 09:51:46.091: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 17 09:51:46.091: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 17 09:51:46.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2589 logs logs-generator logs-generator'
Mar 17 09:51:46.163: INFO: stderr: ""
Mar 17 09:51:46.163: INFO: stdout: "I0317 09:51:45.017916       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/fmnp 590\nI0317 09:51:45.218086       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/8wb2 210\nI0317 09:51:45.418791       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/7p9z 232\nI0317 09:51:45.618059       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/7d2 439\nI0317 09:51:45.818450       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/q4ms 476\nI0317 09:51:46.018784       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/zlk 587\n"
STEP: limiting log lines
Mar 17 09:51:46.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2589 logs logs-generator logs-generator --tail=1'
Mar 17 09:51:46.231: INFO: stderr: ""
Mar 17 09:51:46.231: INFO: stdout: "I0317 09:51:46.217997       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/2zqh 547\n"
Mar 17 09:51:46.231: INFO: got output "I0317 09:51:46.217997       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/2zqh 547\n"
STEP: limiting log bytes
Mar 17 09:51:46.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2589 logs logs-generator logs-generator --limit-bytes=1'
Mar 17 09:51:46.300: INFO: stderr: ""
Mar 17 09:51:46.300: INFO: stdout: "I"
Mar 17 09:51:46.300: INFO: got output "I"
STEP: exposing timestamps
Mar 17 09:51:46.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2589 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 17 09:51:46.382: INFO: stderr: ""
Mar 17 09:51:46.382: INFO: stdout: "2022-03-17T09:51:46.218112830Z I0317 09:51:46.217997       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/2zqh 547\n"
Mar 17 09:51:46.382: INFO: got output "2022-03-17T09:51:46.218112830Z I0317 09:51:46.217997       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/2zqh 547\n"
STEP: restricting to a time range
Mar 17 09:51:48.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2589 logs logs-generator logs-generator --since=1s'
Mar 17 09:51:48.951: INFO: stderr: ""
Mar 17 09:51:48.951: INFO: stdout: "I0317 09:51:48.018255       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/gwb7 405\nI0317 09:51:48.218604       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/h6js 211\nI0317 09:51:48.418821       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/79fz 297\nI0317 09:51:48.618210       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/hfz 484\nI0317 09:51:48.818605       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/n9t 202\n"
Mar 17 09:51:48.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2589 logs logs-generator logs-generator --since=24h'
Mar 17 09:51:49.028: INFO: stderr: ""
Mar 17 09:51:49.028: INFO: stdout: "I0317 09:51:45.017916       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/fmnp 590\nI0317 09:51:45.218086       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/8wb2 210\nI0317 09:51:45.418791       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/7p9z 232\nI0317 09:51:45.618059       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/7d2 439\nI0317 09:51:45.818450       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/q4ms 476\nI0317 09:51:46.018784       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/zlk 587\nI0317 09:51:46.217997       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/2zqh 547\nI0317 09:51:46.418326       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/lvk 565\nI0317 09:51:46.618657       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/8vxd 458\nI0317 09:51:46.817935       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/t7wg 339\nI0317 09:51:47.018377       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/tsc 393\nI0317 09:51:47.218738       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/qng 232\nI0317 09:51:47.418032       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/z85 479\nI0317 09:51:47.618456       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/d7qh 361\nI0317 09:51:47.818768       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/zqn 570\nI0317 09:51:48.018255       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/gwb7 405\nI0317 09:51:48.218604       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/h6js 211\nI0317 09:51:48.418821       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/79fz 297\nI0317 09:51:48.618210       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/hfz 484\nI0317 09:51:48.818605       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/n9t 202\nI0317 09:51:49.019574       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/bx2 366\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
Mar 17 09:51:49.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-2589 delete pod logs-generator'
Mar 17 09:51:50.224: INFO: stderr: ""
Mar 17 09:51:50.224: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:51:50.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2589" for this suite.

• [SLOW TEST:6.254 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1406
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":213,"skipped":4130,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:51:50.236: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6471
Mar 17 09:51:50.311: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:51:52.319: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 17 09:51:52.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6471 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 17 09:51:52.496: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 17 09:51:52.496: INFO: stdout: "iptables"
Mar 17 09:51:52.496: INFO: proxyMode: iptables
Mar 17 09:51:52.505: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 17 09:51:52.509: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6471
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6471
I0317 09:51:52.532366      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6471, replica count: 3
I0317 09:51:55.583428      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 09:51:55.596: INFO: Creating new exec pod
Mar 17 09:51:58.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6471 exec execpod-affinitytm65v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar 17 09:51:58.799: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 17 09:51:58.799: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:51:58.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6471 exec execpod-affinitytm65v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.99.45 80'
Mar 17 09:51:58.977: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.99.45 80\nConnection to 10.43.99.45 80 port [tcp/http] succeeded!\n"
Mar 17 09:51:58.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:51:58.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6471 exec execpod-affinitytm65v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.99.45:80/ ; done'
Mar 17 09:51:59.258: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n"
Mar 17 09:51:59.258: INFO: stdout: "\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb\naffinity-clusterip-timeout-bx6wb"
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Received response from host: affinity-clusterip-timeout-bx6wb
Mar 17 09:51:59.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6471 exec execpod-affinitytm65v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.43.99.45:80/'
Mar 17 09:51:59.423: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n"
Mar 17 09:51:59.423: INFO: stdout: "affinity-clusterip-timeout-bx6wb"
Mar 17 09:52:19.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6471 exec execpod-affinitytm65v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.43.99.45:80/'
Mar 17 09:52:19.596: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.43.99.45:80/\n"
Mar 17 09:52:19.596: INFO: stdout: "affinity-clusterip-timeout-cmqx4"
Mar 17 09:52:19.596: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6471, will wait for the garbage collector to delete the pods
Mar 17 09:52:19.670: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 3.888361ms
Mar 17 09:52:19.771: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.853924ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:52:22.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6471" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:31.976 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":214,"skipped":4132,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:52:22.212: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-2072/configmap-test-9fd59fa8-aaf2-48bf-a623-83e26c8cf502
STEP: Creating a pod to test consume configMaps
Mar 17 09:52:22.271: INFO: Waiting up to 5m0s for pod "pod-configmaps-c553c9ed-f3ea-4073-9aaf-707d5d5f35ac" in namespace "configmap-2072" to be "Succeeded or Failed"
Mar 17 09:52:22.278: INFO: Pod "pod-configmaps-c553c9ed-f3ea-4073-9aaf-707d5d5f35ac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.722395ms
Mar 17 09:52:24.282: INFO: Pod "pod-configmaps-c553c9ed-f3ea-4073-9aaf-707d5d5f35ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011954954s
Mar 17 09:52:26.287: INFO: Pod "pod-configmaps-c553c9ed-f3ea-4073-9aaf-707d5d5f35ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016349058s
STEP: Saw pod success
Mar 17 09:52:26.287: INFO: Pod "pod-configmaps-c553c9ed-f3ea-4073-9aaf-707d5d5f35ac" satisfied condition "Succeeded or Failed"
Mar 17 09:52:26.289: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-c553c9ed-f3ea-4073-9aaf-707d5d5f35ac container env-test: <nil>
STEP: delete the pod
Mar 17 09:52:26.304: INFO: Waiting for pod pod-configmaps-c553c9ed-f3ea-4073-9aaf-707d5d5f35ac to disappear
Mar 17 09:52:26.308: INFO: Pod pod-configmaps-c553c9ed-f3ea-4073-9aaf-707d5d5f35ac no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:52:26.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2072" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":215,"skipped":4140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:52:26.324: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-6c552a13-f316-4705-8d72-978e6af34e38
STEP: Creating a pod to test consume configMaps
Mar 17 09:52:26.396: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-62e2bf93-947b-4186-ba87-01e08f812738" in namespace "projected-390" to be "Succeeded or Failed"
Mar 17 09:52:26.403: INFO: Pod "pod-projected-configmaps-62e2bf93-947b-4186-ba87-01e08f812738": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913334ms
Mar 17 09:52:28.407: INFO: Pod "pod-projected-configmaps-62e2bf93-947b-4186-ba87-01e08f812738": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01130306s
Mar 17 09:52:30.413: INFO: Pod "pod-projected-configmaps-62e2bf93-947b-4186-ba87-01e08f812738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017323253s
STEP: Saw pod success
Mar 17 09:52:30.414: INFO: Pod "pod-projected-configmaps-62e2bf93-947b-4186-ba87-01e08f812738" satisfied condition "Succeeded or Failed"
Mar 17 09:52:30.416: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-configmaps-62e2bf93-947b-4186-ba87-01e08f812738 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 17 09:52:30.445: INFO: Waiting for pod pod-projected-configmaps-62e2bf93-947b-4186-ba87-01e08f812738 to disappear
Mar 17 09:52:30.452: INFO: Pod pod-projected-configmaps-62e2bf93-947b-4186-ba87-01e08f812738 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:52:30.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-390" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":4169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:52:30.481: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6469
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 17 09:52:30.541: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 17 09:52:30.596: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:52:32.599: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:34.601: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:36.603: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:38.603: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:40.600: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:42.602: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:44.601: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:46.601: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:48.600: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 09:52:50.602: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 17 09:52:50.607: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 17 09:52:52.611: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 17 09:52:52.615: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 17 09:52:54.646: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 17 09:52:54.646: INFO: Going to poll 10.42.0.29 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 17 09:52:54.650: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.29 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:52:54.650: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:52:54.651: INFO: ExecWithOptions: Clientset creation
Mar 17 09:52:54.651: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.0.29+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:52:55.753: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 17 09:52:55.753: INFO: Going to poll 10.42.2.197 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 17 09:52:55.758: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.2.197 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:52:55.758: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:52:55.759: INFO: ExecWithOptions: Clientset creation
Mar 17 09:52:55.759: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.2.197+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:52:56.859: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 17 09:52:56.859: INFO: Going to poll 10.42.1.113 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 17 09:52:56.865: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.1.113 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 09:52:56.865: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 09:52:56.865: INFO: ExecWithOptions: Clientset creation
Mar 17 09:52:56.865: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.1.113+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Mar 17 09:52:57.962: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:52:57.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6469" for this suite.

• [SLOW TEST:27.496 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":4196,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:52:57.977: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1537
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar 17 09:52:58.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-6229 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Mar 17 09:52:58.174: INFO: stderr: ""
Mar 17 09:52:58.174: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
Mar 17 09:52:58.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-6229 delete pods e2e-test-httpd-pod'
Mar 17 09:53:00.996: INFO: stderr: ""
Mar 17 09:53:00.996: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:53:00.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6229" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":218,"skipped":4214,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:53:01.009: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Mar 17 09:55:01.617: INFO: Successfully updated pod "var-expansion-944844d4-4690-4e57-94b8-ea651aadc2aa"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar 17 09:55:03.630: INFO: Deleting pod "var-expansion-944844d4-4690-4e57-94b8-ea651aadc2aa" in namespace "var-expansion-1678"
Mar 17 09:55:03.635: INFO: Wait up to 5m0s for pod "var-expansion-944844d4-4690-4e57-94b8-ea651aadc2aa" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:55:35.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1678" for this suite.

• [SLOW TEST:154.672 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":219,"skipped":4218,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:55:35.682: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3932.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3932.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 82.95.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.95.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.95.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.95.82_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3932.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3932.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 82.95.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.95.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.95.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.95.82_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 09:55:37.795: INFO: Unable to read wheezy_udp@dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:37.798: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:37.801: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:37.804: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:37.815: INFO: Unable to read jessie_udp@dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:37.818: INFO: Unable to read jessie_tcp@dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:37.821: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:37.823: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:37.833: INFO: Lookups using dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af failed for: [wheezy_udp@dns-test-service.dns-3932.svc.cluster.local wheezy_tcp@dns-test-service.dns-3932.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local jessie_udp@dns-test-service.dns-3932.svc.cluster.local jessie_tcp@dns-test-service.dns-3932.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local]

Mar 17 09:55:42.842: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:42.845: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:42.847: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local from pod dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af: the server could not find the requested resource (get pods dns-test-e53a265a-1722-492f-a59b-f6b33dff37af)
Mar 17 09:55:42.891: INFO: Lookups using dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af failed for: [wheezy_tcp@dns-test-service.dns-3932.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3932.svc.cluster.local]

Mar 17 09:55:47.889: INFO: DNS probes using dns-3932/dns-test-e53a265a-1722-492f-a59b-f6b33dff37af succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:55:48.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3932" for this suite.

• [SLOW TEST:12.332 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":220,"skipped":4232,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:55:48.014: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 17 09:55:48.143: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5098  bfbfc544-76b0-4d44-b153-7c646a98e69a 76515 0 2022-03-17 09:55:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-17 09:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:55:48.144: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5098  bfbfc544-76b0-4d44-b153-7c646a98e69a 76516 0 2022-03-17 09:55:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-17 09:55:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:55:48.144: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5098  bfbfc544-76b0-4d44-b153-7c646a98e69a 76517 0 2022-03-17 09:55:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-17 09:55:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 17 09:55:58.167: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5098  bfbfc544-76b0-4d44-b153-7c646a98e69a 76565 0 2022-03-17 09:55:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-17 09:55:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:55:58.168: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5098  bfbfc544-76b0-4d44-b153-7c646a98e69a 76566 0 2022-03-17 09:55:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-17 09:55:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 17 09:55:58.168: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5098  bfbfc544-76b0-4d44-b153-7c646a98e69a 76567 0 2022-03-17 09:55:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-17 09:55:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:55:58.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5098" for this suite.

• [SLOW TEST:10.161 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":221,"skipped":4238,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:55:58.176: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-22c206d3-81ee-4da3-b41f-c3a21f3fe583
Mar 17 09:55:58.234: INFO: Pod name my-hostname-basic-22c206d3-81ee-4da3-b41f-c3a21f3fe583: Found 0 pods out of 1
Mar 17 09:56:03.244: INFO: Pod name my-hostname-basic-22c206d3-81ee-4da3-b41f-c3a21f3fe583: Found 1 pods out of 1
Mar 17 09:56:03.244: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-22c206d3-81ee-4da3-b41f-c3a21f3fe583" are running
Mar 17 09:56:03.249: INFO: Pod "my-hostname-basic-22c206d3-81ee-4da3-b41f-c3a21f3fe583-nsmm6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-17 09:55:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-17 09:56:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-17 09:56:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-17 09:55:58 +0000 UTC Reason: Message:}])
Mar 17 09:56:03.249: INFO: Trying to dial the pod
Mar 17 09:56:08.266: INFO: Controller my-hostname-basic-22c206d3-81ee-4da3-b41f-c3a21f3fe583: Got expected result from replica 1 [my-hostname-basic-22c206d3-81ee-4da3-b41f-c3a21f3fe583-nsmm6]: "my-hostname-basic-22c206d3-81ee-4da3-b41f-c3a21f3fe583-nsmm6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:56:08.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2892" for this suite.

• [SLOW TEST:10.102 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":222,"skipped":4250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:56:08.278: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 09:56:08.691: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 09:56:11.709: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:56:11.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2209" for this suite.
STEP: Destroying namespace "webhook-2209-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":223,"skipped":4292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:56:11.811: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 09:56:12.002: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 17 09:56:12.019: INFO: The status of Pod pod-exec-websocket-3166146f-6d86-4915-8e20-4fd8f647fd15 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:56:14.024: INFO: The status of Pod pod-exec-websocket-3166146f-6d86-4915-8e20-4fd8f647fd15 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:56:14.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2530" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":224,"skipped":4317,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:56:14.130: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Mar 17 09:56:14.214: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 17 09:56:16.218: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 17 09:56:17.238: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:56:18.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1824" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":225,"skipped":4325,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:56:18.268: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 17 09:56:18.311: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 17 09:56:23.318: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:56:24.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4292" for this suite.

• [SLOW TEST:6.097 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":226,"skipped":4327,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:56:24.366: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6041
STEP: creating service affinity-nodeport in namespace services-6041
STEP: creating replication controller affinity-nodeport in namespace services-6041
I0317 09:56:24.425269      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6041, replica count: 3
I0317 09:56:27.476075      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 09:56:27.486: INFO: Creating new exec pod
Mar 17 09:56:30.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6041 exec execpod-affinitytzshm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar 17 09:56:30.659: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 17 09:56:30.659: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:56:30.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6041 exec execpod-affinitytzshm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.235.192 80'
Mar 17 09:56:30.808: INFO: stderr: "+ + nc -v -t -w 2 10.43.235.192 80\necho hostName\nConnection to 10.43.235.192 80 port [tcp/http] succeeded!\n"
Mar 17 09:56:30.808: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:56:30.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6041 exec execpod-affinitytzshm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.39.36 30991'
Mar 17 09:56:30.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.39.36 30991\nConnection to 172.31.39.36 30991 port [tcp/*] succeeded!\n"
Mar 17 09:56:30.962: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:56:30.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6041 exec execpod-affinitytzshm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.33.68 30991'
Mar 17 09:56:31.118: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.33.68 30991\nConnection to 172.31.33.68 30991 port [tcp/*] succeeded!\n"
Mar 17 09:56:31.118: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 09:56:31.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-6041 exec execpod-affinitytzshm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.33.68:30991/ ; done'
Mar 17 09:56:31.333: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.33.68:30991/\n"
Mar 17 09:56:31.333: INFO: stdout: "\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk\naffinity-nodeport-mnsdk"
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Received response from host: affinity-nodeport-mnsdk
Mar 17 09:56:31.333: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6041, will wait for the garbage collector to delete the pods
Mar 17 09:56:31.412: INFO: Deleting ReplicationController affinity-nodeport took: 5.663682ms
Mar 17 09:56:31.513: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.078591ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:56:33.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6041" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.477 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":227,"skipped":4328,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:56:33.843: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-a7098e62-2420-4de8-8e28-c57cbebd60b0
STEP: Creating a pod to test consume configMaps
Mar 17 09:56:33.891: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a4a767e-844f-42b9-9c8f-81e08089be99" in namespace "configmap-5622" to be "Succeeded or Failed"
Mar 17 09:56:33.895: INFO: Pod "pod-configmaps-0a4a767e-844f-42b9-9c8f-81e08089be99": Phase="Pending", Reason="", readiness=false. Elapsed: 3.410058ms
Mar 17 09:56:35.906: INFO: Pod "pod-configmaps-0a4a767e-844f-42b9-9c8f-81e08089be99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014154132s
STEP: Saw pod success
Mar 17 09:56:35.906: INFO: Pod "pod-configmaps-0a4a767e-844f-42b9-9c8f-81e08089be99" satisfied condition "Succeeded or Failed"
Mar 17 09:56:35.910: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-0a4a767e-844f-42b9-9c8f-81e08089be99 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 09:56:35.943: INFO: Waiting for pod pod-configmaps-0a4a767e-844f-42b9-9c8f-81e08089be99 to disappear
Mar 17 09:56:35.947: INFO: Pod pod-configmaps-0a4a767e-844f-42b9-9c8f-81e08089be99 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:56:35.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5622" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":228,"skipped":4344,"failed":0}

------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:56:35.986: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-19735b74-0765-47f3-8f86-26a83cb7694d in namespace container-probe-8952
Mar 17 09:56:38.148: INFO: Started pod liveness-19735b74-0765-47f3-8f86-26a83cb7694d in namespace container-probe-8952
STEP: checking the pod's current state and verifying that restartCount is present
Mar 17 09:56:38.150: INFO: Initial restart count of pod liveness-19735b74-0765-47f3-8f86-26a83cb7694d is 0
Mar 17 09:56:58.200: INFO: Restart count of pod container-probe-8952/liveness-19735b74-0765-47f3-8f86-26a83cb7694d is now 1 (20.050097617s elapsed)
Mar 17 09:57:18.254: INFO: Restart count of pod container-probe-8952/liveness-19735b74-0765-47f3-8f86-26a83cb7694d is now 2 (40.103233234s elapsed)
Mar 17 09:57:38.307: INFO: Restart count of pod container-probe-8952/liveness-19735b74-0765-47f3-8f86-26a83cb7694d is now 3 (1m0.156339944s elapsed)
Mar 17 09:57:58.358: INFO: Restart count of pod container-probe-8952/liveness-19735b74-0765-47f3-8f86-26a83cb7694d is now 4 (1m20.207719258s elapsed)
Mar 17 09:59:10.565: INFO: Restart count of pod container-probe-8952/liveness-19735b74-0765-47f3-8f86-26a83cb7694d is now 5 (2m32.414276391s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:59:10.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8952" for this suite.

• [SLOW TEST:154.620 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4344,"failed":0}
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:59:10.607: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7696, will wait for the garbage collector to delete the pods
Mar 17 09:59:12.744: INFO: Deleting Job.batch foo took: 4.393065ms
Mar 17 09:59:12.845: INFO: Terminating Job.batch foo pods took: 101.115037ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:59:45.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7696" for this suite.

• [SLOW TEST:34.454 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":230,"skipped":4344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:59:45.062: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:59:52.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9559" for this suite.

• [SLOW TEST:7.086 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":231,"skipped":4399,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:59:52.149: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 17 09:59:52.233: INFO: Waiting up to 5m0s for pod "pod-6832c581-5dc2-4fe7-a47d-d075c0d650bd" in namespace "emptydir-9278" to be "Succeeded or Failed"
Mar 17 09:59:52.259: INFO: Pod "pod-6832c581-5dc2-4fe7-a47d-d075c0d650bd": Phase="Pending", Reason="", readiness=false. Elapsed: 25.3871ms
Mar 17 09:59:54.262: INFO: Pod "pod-6832c581-5dc2-4fe7-a47d-d075c0d650bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029068167s
Mar 17 09:59:56.266: INFO: Pod "pod-6832c581-5dc2-4fe7-a47d-d075c0d650bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033065262s
STEP: Saw pod success
Mar 17 09:59:56.266: INFO: Pod "pod-6832c581-5dc2-4fe7-a47d-d075c0d650bd" satisfied condition "Succeeded or Failed"
Mar 17 09:59:56.271: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-6832c581-5dc2-4fe7-a47d-d075c0d650bd container test-container: <nil>
STEP: delete the pod
Mar 17 09:59:56.311: INFO: Waiting for pod pod-6832c581-5dc2-4fe7-a47d-d075c0d650bd to disappear
Mar 17 09:59:56.319: INFO: Pod pod-6832c581-5dc2-4fe7-a47d-d075c0d650bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 09:59:56.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9278" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4411,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 09:59:56.334: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1571
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar 17 09:59:56.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-4699 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 17 09:59:56.505: INFO: stderr: ""
Mar 17 09:59:56.505: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 17 10:00:01.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-4699 get pod e2e-test-httpd-pod -o json'
Mar 17 10:00:01.668: INFO: stderr: ""
Mar 17 10:00:01.668: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"da6b0b18330a31fb6ce12fc427a979a15c2d397f23e0c2f382f5d1d6a5b970b8\",\n            \"cni.projectcalico.org/podIP\": \"10.42.2.215/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.42.2.215/32\"\n        },\n        \"creationTimestamp\": \"2022-03-17T09:59:56Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4699\",\n        \"resourceVersion\": \"77826\",\n        \"uid\": \"33e8613d-1197-4322-9007-d0a45d1dae62\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-55zbh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-35-106\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-55zbh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-17T09:59:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-17T09:59:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-17T09:59:58Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-17T09:59:56Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2253e30aceb7f27d0e28a3f7e7f5e7089d6654b2a2025119ab6b510eff52c23f\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-03-17T09:59:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.35.106\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.2.215\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.42.2.215\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-03-17T09:59:56Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 17 10:00:01.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-4699 replace -f -'
Mar 17 10:00:02.386: INFO: stderr: ""
Mar 17 10:00:02.386: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1575
Mar 17 10:00:02.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-4699 delete pods e2e-test-httpd-pod'
Mar 17 10:00:04.869: INFO: stderr: ""
Mar 17 10:00:04.869: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:00:04.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4699" for this suite.

• [SLOW TEST:8.566 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":233,"skipped":4411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:00:04.901: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:00:05.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5455" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":234,"skipped":4453,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:00:05.042: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:00:05.109: INFO: The status of Pod server-envvars-688ca437-ccf2-4a5b-b58b-f03401847bf2 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:00:07.113: INFO: The status of Pod server-envvars-688ca437-ccf2-4a5b-b58b-f03401847bf2 is Running (Ready = true)
Mar 17 10:00:07.137: INFO: Waiting up to 5m0s for pod "client-envvars-319c78f4-93c6-44e4-9ab2-cb05a9aa364a" in namespace "pods-7967" to be "Succeeded or Failed"
Mar 17 10:00:07.146: INFO: Pod "client-envvars-319c78f4-93c6-44e4-9ab2-cb05a9aa364a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.109589ms
Mar 17 10:00:09.159: INFO: Pod "client-envvars-319c78f4-93c6-44e4-9ab2-cb05a9aa364a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022359274s
STEP: Saw pod success
Mar 17 10:00:09.160: INFO: Pod "client-envvars-319c78f4-93c6-44e4-9ab2-cb05a9aa364a" satisfied condition "Succeeded or Failed"
Mar 17 10:00:09.164: INFO: Trying to get logs from node ip-172-31-35-106 pod client-envvars-319c78f4-93c6-44e4-9ab2-cb05a9aa364a container env3cont: <nil>
STEP: delete the pod
Mar 17 10:00:09.212: INFO: Waiting for pod client-envvars-319c78f4-93c6-44e4-9ab2-cb05a9aa364a to disappear
Mar 17 10:00:09.223: INFO: Pod client-envvars-319c78f4-93c6-44e4-9ab2-cb05a9aa364a no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:00:09.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7967" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4461,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:00:09.260: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:00:09.491: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 17 10:00:11.532: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar 17 10:00:11.558: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1263  135d5b11-7fef-4bae-9fdb-be5c0e08ad91 77976 1 2022-03-17 10:00:11 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-03-17 10:00:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00725b858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 17 10:00:11.561: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar 17 10:00:11.561: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 17 10:00:11.562: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1263  c1f7b6e1-0ae6-4cc0-92f6-d58b18442e16 77977 1 2022-03-17 10:00:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 135d5b11-7fef-4bae-9fdb-be5c0e08ad91 0xc002fc3237 0xc002fc3238}] []  [{e2e.test Update apps/v1 2022-03-17 10:00:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:00:11 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"135d5b11-7fef-4bae-9fdb-be5c0e08ad91\"}":{}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:00:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002fc32f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:00:11.577: INFO: Pod "test-cleanup-controller-74zzn" is available:
&Pod{ObjectMeta:{test-cleanup-controller-74zzn test-cleanup-controller- deployment-1263  ab96f48d-3139-4c8b-8a26-42cc48893b3f 77974 0 2022-03-17 10:00:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:ecae4407515996a7ea95dd2723461bd049af1bbfc815aff90f78c20b6bcf2e54 cni.projectcalico.org/podIP:10.42.2.218/32 cni.projectcalico.org/podIPs:10.42.2.218/32] [{apps/v1 ReplicaSet test-cleanup-controller c1f7b6e1-0ae6-4cc0-92f6-d58b18442e16 0xc002fc3627 0xc002fc3628}] []  [{kube-controller-manager Update v1 2022-03-17 10:00:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1f7b6e1-0ae6-4cc0-92f6-d58b18442e16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 10:00:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 10:00:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttkdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttkdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:00:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:00:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:00:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:00:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:10.42.2.218,StartTime:2022-03-17 10:00:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 10:00:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://4730b2516ddcfd7a4660077dd3a205be7142dd605d026d500ac47e3995321308,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:00:11.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1263" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":236,"skipped":4465,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:00:11.599: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 17 10:00:11.674: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 17 10:00:11.682: INFO: starting watch
STEP: patching
STEP: updating
Mar 17 10:00:11.705: INFO: waiting for watch events with expected annotations
Mar 17 10:00:11.705: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:00:11.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2024" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":237,"skipped":4486,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:00:11.754: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:00:22.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1609" for this suite.

• [SLOW TEST:11.144 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":238,"skipped":4487,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:00:22.898: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-5765
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5765 to expose endpoints map[]
Mar 17 10:00:23.002: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar 17 10:00:24.017: INFO: successfully validated that service endpoint-test2 in namespace services-5765 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5765
Mar 17 10:00:24.031: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:00:26.037: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5765 to expose endpoints map[pod1:[80]]
Mar 17 10:00:26.055: INFO: successfully validated that service endpoint-test2 in namespace services-5765 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Mar 17 10:00:26.055: INFO: Creating new exec pod
Mar 17 10:00:29.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5765 exec execpodngg64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 17 10:00:29.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 17 10:00:29.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 10:00:29.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5765 exec execpodngg64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.129.9 80'
Mar 17 10:00:29.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.129.9 80\nConnection to 10.43.129.9 80 port [tcp/http] succeeded!\n"
Mar 17 10:00:29.500: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-5765
Mar 17 10:00:29.518: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:00:31.526: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5765 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 17 10:00:31.544: INFO: successfully validated that service endpoint-test2 in namespace services-5765 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Mar 17 10:00:32.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5765 exec execpodngg64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 17 10:00:32.720: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 17 10:00:32.720: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 10:00:32.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5765 exec execpodngg64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.129.9 80'
Mar 17 10:00:32.899: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.129.9 80\nConnection to 10.43.129.9 80 port [tcp/http] succeeded!\n"
Mar 17 10:00:32.899: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5765
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5765 to expose endpoints map[pod2:[80]]
Mar 17 10:00:33.950: INFO: successfully validated that service endpoint-test2 in namespace services-5765 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Mar 17 10:00:34.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5765 exec execpodngg64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 17 10:00:35.135: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 17 10:00:35.135: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 10:00:35.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5765 exec execpodngg64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.129.9 80'
Mar 17 10:00:35.299: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.129.9 80\nConnection to 10.43.129.9 80 port [tcp/http] succeeded!\n"
Mar 17 10:00:35.299: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-5765
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5765 to expose endpoints map[]
Mar 17 10:00:36.336: INFO: successfully validated that service endpoint-test2 in namespace services-5765 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:00:36.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5765" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.491 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":239,"skipped":4493,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:00:36.389: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 17 10:00:39.039: INFO: Successfully updated pod "adopt-release-hvksc"
STEP: Checking that the Job readopts the Pod
Mar 17 10:00:39.039: INFO: Waiting up to 15m0s for pod "adopt-release-hvksc" in namespace "job-7242" to be "adopted"
Mar 17 10:00:39.043: INFO: Pod "adopt-release-hvksc": Phase="Running", Reason="", readiness=true. Elapsed: 4.472284ms
Mar 17 10:00:41.049: INFO: Pod "adopt-release-hvksc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009776629s
Mar 17 10:00:41.049: INFO: Pod "adopt-release-hvksc" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 17 10:00:41.577: INFO: Successfully updated pod "adopt-release-hvksc"
STEP: Checking that the Job releases the Pod
Mar 17 10:00:41.577: INFO: Waiting up to 15m0s for pod "adopt-release-hvksc" in namespace "job-7242" to be "released"
Mar 17 10:00:41.584: INFO: Pod "adopt-release-hvksc": Phase="Running", Reason="", readiness=true. Elapsed: 7.030556ms
Mar 17 10:00:43.589: INFO: Pod "adopt-release-hvksc": Phase="Running", Reason="", readiness=true. Elapsed: 2.011156819s
Mar 17 10:00:43.589: INFO: Pod "adopt-release-hvksc" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:00:43.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7242" for this suite.

• [SLOW TEST:7.208 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":240,"skipped":4495,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:00:43.597: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Mar 17 10:00:43.655: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:02.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6320" for this suite.

• [SLOW TEST:18.844 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":241,"skipped":4498,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:02.442: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 17 10:01:02.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:01:02.539: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:01:03.556: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:01:03.556: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:01:04.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 10:01:04.550: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 17 10:01:04.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 10:01:04.590: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6902, will wait for the garbage collector to delete the pods
Mar 17 10:01:04.721: INFO: Deleting DaemonSet.extensions daemon-set took: 7.031722ms
Mar 17 10:01:04.822: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.99319ms
Mar 17 10:01:07.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:01:07.626: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 17 10:01:07.628: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"78498"},"items":null}

Mar 17 10:01:07.632: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"78498"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:07.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6902" for this suite.

• [SLOW TEST:5.214 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":242,"skipped":4510,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:07.657: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:07.708: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-4093
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:11.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1571" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:11.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4093" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":243,"skipped":4521,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:11.876: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:14.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2126" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":244,"skipped":4538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:14.014: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5971.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5971.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5971.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5971.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5971.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5971.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5971.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5971.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 10:01:16.123: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local from pod dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287: the server could not find the requested resource (get pods dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287)
Mar 17 10:01:16.125: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local from pod dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287: the server could not find the requested resource (get pods dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287)
Mar 17 10:01:16.129: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5971.svc.cluster.local from pod dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287: the server could not find the requested resource (get pods dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287)
Mar 17 10:01:16.133: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5971.svc.cluster.local from pod dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287: the server could not find the requested resource (get pods dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287)
Mar 17 10:01:16.136: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local from pod dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287: the server could not find the requested resource (get pods dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287)
Mar 17 10:01:16.140: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local from pod dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287: the server could not find the requested resource (get pods dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287)
Mar 17 10:01:16.143: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5971.svc.cluster.local from pod dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287: the server could not find the requested resource (get pods dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287)
Mar 17 10:01:16.147: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5971.svc.cluster.local from pod dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287: the server could not find the requested resource (get pods dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287)
Mar 17 10:01:16.147: INFO: Lookups using dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5971.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5971.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local jessie_udp@dns-test-service-2.dns-5971.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5971.svc.cluster.local]

Mar 17 10:01:21.170: INFO: DNS probes using dns-5971/dns-test-50dc44d0-ffe0-4b9a-a2bd-b3567c3b2287 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:21.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5971" for this suite.

• [SLOW TEST:7.209 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":245,"skipped":4580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:21.225: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:01:21.265: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:22.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8350" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":246,"skipped":4637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:22.338: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 17 10:01:22.472: INFO: Waiting up to 5m0s for pod "pod-578f252e-f426-45dc-a634-12fca47fc39d" in namespace "emptydir-882" to be "Succeeded or Failed"
Mar 17 10:01:22.484: INFO: Pod "pod-578f252e-f426-45dc-a634-12fca47fc39d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.472936ms
Mar 17 10:01:24.491: INFO: Pod "pod-578f252e-f426-45dc-a634-12fca47fc39d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018134455s
STEP: Saw pod success
Mar 17 10:01:24.491: INFO: Pod "pod-578f252e-f426-45dc-a634-12fca47fc39d" satisfied condition "Succeeded or Failed"
Mar 17 10:01:24.493: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-578f252e-f426-45dc-a634-12fca47fc39d container test-container: <nil>
STEP: delete the pod
Mar 17 10:01:24.508: INFO: Waiting for pod pod-578f252e-f426-45dc-a634-12fca47fc39d to disappear
Mar 17 10:01:24.514: INFO: Pod pod-578f252e-f426-45dc-a634-12fca47fc39d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:24.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-882" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":4668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:24.524: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-651/secret-test-a209b670-2a5b-4378-b6e3-df7f23f2658f
STEP: Creating a pod to test consume secrets
Mar 17 10:01:24.572: INFO: Waiting up to 5m0s for pod "pod-configmaps-acb39790-c200-4bdd-974f-6067c7bc5d25" in namespace "secrets-651" to be "Succeeded or Failed"
Mar 17 10:01:24.584: INFO: Pod "pod-configmaps-acb39790-c200-4bdd-974f-6067c7bc5d25": Phase="Pending", Reason="", readiness=false. Elapsed: 11.777589ms
Mar 17 10:01:26.588: INFO: Pod "pod-configmaps-acb39790-c200-4bdd-974f-6067c7bc5d25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016304053s
STEP: Saw pod success
Mar 17 10:01:26.588: INFO: Pod "pod-configmaps-acb39790-c200-4bdd-974f-6067c7bc5d25" satisfied condition "Succeeded or Failed"
Mar 17 10:01:26.594: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-acb39790-c200-4bdd-974f-6067c7bc5d25 container env-test: <nil>
STEP: delete the pod
Mar 17 10:01:26.619: INFO: Waiting for pod pod-configmaps-acb39790-c200-4bdd-974f-6067c7bc5d25 to disappear
Mar 17 10:01:26.625: INFO: Pod pod-configmaps-acb39790-c200-4bdd-974f-6067c7bc5d25 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:26.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-651" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4730,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:26.635: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:01:26.705: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 17 10:01:31.718: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 17 10:01:31.718: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 17 10:01:33.723: INFO: Creating deployment "test-rollover-deployment"
Mar 17 10:01:33.731: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 17 10:01:35.737: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 17 10:01:35.746: INFO: Ensure that both replica sets have 1 created replica
Mar 17 10:01:35.752: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 17 10:01:35.774: INFO: Updating deployment test-rollover-deployment
Mar 17 10:01:35.774: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 17 10:01:37.802: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 17 10:01:37.809: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 17 10:01:37.815: INFO: all replica sets need to contain the pod-template-hash label
Mar 17 10:01:37.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 17 10:01:39.824: INFO: all replica sets need to contain the pod-template-hash label
Mar 17 10:01:39.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 17 10:01:41.826: INFO: all replica sets need to contain the pod-template-hash label
Mar 17 10:01:41.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 17 10:01:43.830: INFO: all replica sets need to contain the pod-template-hash label
Mar 17 10:01:43.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 17 10:01:45.823: INFO: all replica sets need to contain the pod-template-hash label
Mar 17 10:01:45.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 17 10:01:47.822: INFO: all replica sets need to contain the pod-template-hash label
Mar 17 10:01:47.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.March, 17, 10, 1, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.March, 17, 10, 1, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 17 10:01:49.826: INFO: 
Mar 17 10:01:49.826: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar 17 10:01:49.833: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8817  fcc812eb-b37d-4549-b9e2-e9ac8cbda40b 78984 2 2022-03-17 10:01:33 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-17 10:01:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053de898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-17 10:01:33 +0000 UTC,LastTransitionTime:2022-03-17 10:01:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-03-17 10:01:48 +0000 UTC,LastTransitionTime:2022-03-17 10:01:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 17 10:01:49.837: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-8817  986afc3b-3a28-4477-9f40-b2803eefab2b 78974 2 2022-03-17 10:01:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fcc812eb-b37d-4549-b9e2-e9ac8cbda40b 0xc005303f57 0xc005303f58}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:01:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcc812eb-b37d-4549-b9e2-e9ac8cbda40b\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:01:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00540c018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:01:49.837: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 17 10:01:49.837: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8817  5c1e4321-a161-43cd-b7ad-3b599683dd5e 78982 2 2022-03-17 10:01:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fcc812eb-b37d-4549-b9e2-e9ac8cbda40b 0xc005303dc7 0xc005303dc8}] []  [{e2e.test Update apps/v1 2022-03-17 10:01:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcc812eb-b37d-4549-b9e2-e9ac8cbda40b\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:01:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005303ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:01:49.837: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-8817  2d8e904b-79ab-46d3-b286-c84165da59e2 78932 2 2022-03-17 10:01:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fcc812eb-b37d-4549-b9e2-e9ac8cbda40b 0xc00540c097 0xc00540c098}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcc812eb-b37d-4549-b9e2-e9ac8cbda40b\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:01:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00540c178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:01:49.844: INFO: Pod "test-rollover-deployment-668b7f667d-89df8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-89df8 test-rollover-deployment-668b7f667d- deployment-8817  10fc9466-c1f3-464b-b583-290d8d26ad60 78946 0 2022-03-17 10:01:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[cni.projectcalico.org/containerID:d270884d8a6eb2dcb24bcbb75a7b3e437f07a407039743296a7dd5bdf2ad74cb cni.projectcalico.org/podIP:10.42.2.231/32 cni.projectcalico.org/podIPs:10.42.2.231/32] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d 986afc3b-3a28-4477-9f40-b2803eefab2b 0xc0053ded97 0xc0053ded98}] []  [{kube-controller-manager Update v1 2022-03-17 10:01:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"986afc3b-3a28-4477-9f40-b2803eefab2b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 10:01:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 10:01:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwf57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwf57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:01:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:01:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:01:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:01:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:10.42.2.231,StartTime:2022-03-17 10:01:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 10:01:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:docker://55a7abaf38e26ecd541f078d56ac4031a12035e14557ae3612a362aa9447a826,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:01:49.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8817" for this suite.

• [SLOW TEST:23.220 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":249,"skipped":4751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:01:49.855: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:03.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7038" for this suite.

• [SLOW TEST:13.199 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":250,"skipped":4788,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:03.054: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-2e7e7fbb-ab30-4726-a973-6853d40c0058
STEP: Creating a pod to test consume secrets
Mar 17 10:02:03.177: INFO: Waiting up to 5m0s for pod "pod-secrets-7c241e7d-9b4a-4b98-afe0-803e4049d25e" in namespace "secrets-413" to be "Succeeded or Failed"
Mar 17 10:02:03.181: INFO: Pod "pod-secrets-7c241e7d-9b4a-4b98-afe0-803e4049d25e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02275ms
Mar 17 10:02:05.199: INFO: Pod "pod-secrets-7c241e7d-9b4a-4b98-afe0-803e4049d25e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022177348s
STEP: Saw pod success
Mar 17 10:02:05.199: INFO: Pod "pod-secrets-7c241e7d-9b4a-4b98-afe0-803e4049d25e" satisfied condition "Succeeded or Failed"
Mar 17 10:02:05.216: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-secrets-7c241e7d-9b4a-4b98-afe0-803e4049d25e container secret-volume-test: <nil>
STEP: delete the pod
Mar 17 10:02:05.258: INFO: Waiting for pod pod-secrets-7c241e7d-9b4a-4b98-afe0-803e4049d25e to disappear
Mar 17 10:02:05.265: INFO: Pod pod-secrets-7c241e7d-9b4a-4b98-afe0-803e4049d25e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:05.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-413" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":251,"skipped":4795,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:05.286: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1826
STEP: creating service affinity-clusterip-transition in namespace services-1826
STEP: creating replication controller affinity-clusterip-transition in namespace services-1826
I0317 10:02:05.489579      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1826, replica count: 3
I0317 10:02:08.542577      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 10:02:08.550: INFO: Creating new exec pod
Mar 17 10:02:11.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-1826 exec execpod-affinitypq6jl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar 17 10:02:11.920: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 17 10:02:11.920: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 10:02:11.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-1826 exec execpod-affinitypq6jl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.182.54 80'
Mar 17 10:02:12.079: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.182.54 80\nConnection to 10.43.182.54 80 port [tcp/http] succeeded!\n"
Mar 17 10:02:12.079: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 17 10:02:12.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-1826 exec execpod-affinitypq6jl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.182.54:80/ ; done'
Mar 17 10:02:12.310: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n"
Mar 17 10:02:12.310: INFO: stdout: "\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-xbftk\naffinity-clusterip-transition-xbftk\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-xbftk\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-b8sjl\naffinity-clusterip-transition-b8sjl\naffinity-clusterip-transition-b8sjl\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-xbftk\naffinity-clusterip-transition-xbftk\naffinity-clusterip-transition-xbftk"
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-xbftk
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-xbftk
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-xbftk
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-b8sjl
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-b8sjl
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-b8sjl
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-xbftk
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-xbftk
Mar 17 10:02:12.310: INFO: Received response from host: affinity-clusterip-transition-xbftk
Mar 17 10:02:12.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-1826 exec execpod-affinitypq6jl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.182.54:80/ ; done'
Mar 17 10:02:12.549: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.182.54:80/\n"
Mar 17 10:02:12.549: INFO: stdout: "\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6\naffinity-clusterip-transition-wl5m6"
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Received response from host: affinity-clusterip-transition-wl5m6
Mar 17 10:02:12.549: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1826, will wait for the garbage collector to delete the pods
Mar 17 10:02:12.624: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.020284ms
Mar 17 10:02:12.724: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.529663ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:15.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1826" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.873 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":252,"skipped":4802,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:15.159: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 17 10:02:15.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:02:15.337: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:02:16.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:02:16.345: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:02:17.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 10:02:17.345: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Mar 17 10:02:17.351: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Mar 17 10:02:17.361: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Mar 17 10:02:17.364: INFO: Observed &DaemonSet event: ADDED
Mar 17 10:02:17.364: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.364: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.364: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.365: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.365: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.365: INFO: Found daemon set daemon-set in namespace daemonsets-119 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 17 10:02:17.365: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Mar 17 10:02:17.378: INFO: Observed &DaemonSet event: ADDED
Mar 17 10:02:17.378: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.378: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.379: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.379: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.379: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.379: INFO: Observed daemon set daemon-set in namespace daemonsets-119 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 17 10:02:17.379: INFO: Observed &DaemonSet event: MODIFIED
Mar 17 10:02:17.379: INFO: Found daemon set daemon-set in namespace daemonsets-119 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 17 10:02:17.379: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-119, will wait for the garbage collector to delete the pods
Mar 17 10:02:17.444: INFO: Deleting DaemonSet.extensions daemon-set took: 8.309515ms
Mar 17 10:02:17.545: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.801542ms
Mar 17 10:02:20.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:02:20.349: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 17 10:02:20.359: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"79369"},"items":null}

Mar 17 10:02:20.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"79370"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:20.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-119" for this suite.

• [SLOW TEST:5.253 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":253,"skipped":4840,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:20.417: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:02:20.470: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 17 10:02:22.502: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:23.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-669" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":254,"skipped":4862,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar 17 10:02:23.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7584 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 17 10:02:23.622: INFO: stderr: ""
Mar 17 10:02:23.622: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar 17 10:02:23.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7584 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar 17 10:02:23.859: INFO: stderr: ""
Mar 17 10:02:23.859: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar 17 10:02:23.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7584 delete pods e2e-test-httpd-pod'
Mar 17 10:02:25.914: INFO: stderr: ""
Mar 17 10:02:25.914: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:25.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7584" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":255,"skipped":4880,"failed":0}

------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:25.927: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar 17 10:02:26.014: INFO: Waiting up to 5m0s for pod "security-context-49d8a419-7581-41cf-a838-953731db83dc" in namespace "security-context-6403" to be "Succeeded or Failed"
Mar 17 10:02:26.027: INFO: Pod "security-context-49d8a419-7581-41cf-a838-953731db83dc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476741ms
Mar 17 10:02:28.037: INFO: Pod "security-context-49d8a419-7581-41cf-a838-953731db83dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022475291s
STEP: Saw pod success
Mar 17 10:02:28.037: INFO: Pod "security-context-49d8a419-7581-41cf-a838-953731db83dc" satisfied condition "Succeeded or Failed"
Mar 17 10:02:28.042: INFO: Trying to get logs from node ip-172-31-35-106 pod security-context-49d8a419-7581-41cf-a838-953731db83dc container test-container: <nil>
STEP: delete the pod
Mar 17 10:02:28.070: INFO: Waiting for pod security-context-49d8a419-7581-41cf-a838-953731db83dc to disappear
Mar 17 10:02:28.074: INFO: Pod security-context-49d8a419-7581-41cf-a838-953731db83dc no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:28.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6403" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":256,"skipped":4880,"failed":0}

------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:28.082: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7594.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7594.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7594.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7594.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 17 10:02:30.173: INFO: DNS probes using dns-7594/dns-test-a59f3f53-2a40-4ee9-8f55-5a4824b4fecc succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:30.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7594" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":257,"skipped":4880,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:30.208: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-2f9cd5db-03fe-4746-89c0-a49ee29cdfa6
STEP: Creating a pod to test consume configMaps
Mar 17 10:02:30.303: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8983e1ea-559f-4865-8a1b-b4e41b9df6e6" in namespace "projected-2172" to be "Succeeded or Failed"
Mar 17 10:02:30.311: INFO: Pod "pod-projected-configmaps-8983e1ea-559f-4865-8a1b-b4e41b9df6e6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.141965ms
Mar 17 10:02:32.317: INFO: Pod "pod-projected-configmaps-8983e1ea-559f-4865-8a1b-b4e41b9df6e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013894096s
STEP: Saw pod success
Mar 17 10:02:32.317: INFO: Pod "pod-projected-configmaps-8983e1ea-559f-4865-8a1b-b4e41b9df6e6" satisfied condition "Succeeded or Failed"
Mar 17 10:02:32.320: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-configmaps-8983e1ea-559f-4865-8a1b-b4e41b9df6e6 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 10:02:32.366: INFO: Waiting for pod pod-projected-configmaps-8983e1ea-559f-4865-8a1b-b4e41b9df6e6 to disappear
Mar 17 10:02:32.380: INFO: Pod pod-projected-configmaps-8983e1ea-559f-4865-8a1b-b4e41b9df6e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:32.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2172" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4883,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:32.395: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:49.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2778" for this suite.

• [SLOW TEST:17.147 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":259,"skipped":4896,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:49.543: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:02:49.596: INFO: Creating deployment "test-recreate-deployment"
Mar 17 10:02:49.611: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 17 10:02:49.635: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 17 10:02:51.643: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 17 10:02:51.646: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 17 10:02:51.657: INFO: Updating deployment test-recreate-deployment
Mar 17 10:02:51.657: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar 17 10:02:51.830: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4955  a834a656-6212-4c71-b023-759e59b7238c 79789 2 2022-03-17 10:02:49 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-17 10:02:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:02:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b62838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-03-17 10:02:51 +0000 UTC,LastTransitionTime:2022-03-17 10:02:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-03-17 10:02:51 +0000 UTC,LastTransitionTime:2022-03-17 10:02:49 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 17 10:02:51.833: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-4955  1ef78032-dfff-4370-8ef3-71f6357d16dd 79786 1 2022-03-17 10:02:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a834a656-6212-4c71-b023-759e59b7238c 0xc0059d0397 0xc0059d0398}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:02:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a834a656-6212-4c71-b023-759e59b7238c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:02:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059d0438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:02:51.833: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 17 10:02:51.834: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-4955  bb2eb01d-fae4-4f93-bf09-d09ef4d2a949 79777 2 2022-03-17 10:02:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a834a656-6212-4c71-b023-759e59b7238c 0xc0059d04a7 0xc0059d04a8}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:02:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a834a656-6212-4c71-b023-759e59b7238c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:02:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059d0598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:02:51.837: INFO: Pod "test-recreate-deployment-5b99bd5487-fr9ds" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-fr9ds test-recreate-deployment-5b99bd5487- deployment-4955  e348fd81-4211-4903-a3ff-525a4260d816 79788 0 2022-03-17 10:02:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 1ef78032-dfff-4370-8ef3-71f6357d16dd 0xc005b62c77 0xc005b62c78}] []  [{Go-http-client Update v1 2022-03-17 10:02:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-03-17 10:02:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ef78032-dfff-4370-8ef3-71f6357d16dd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9msr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9msr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:02:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:02:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:02:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:02:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:,StartTime:2022-03-17 10:02:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:51.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4955" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":260,"skipped":4912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:51.862: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-6d04c36d-1c7f-45bc-be52-b63e8d389f0a
STEP: Creating a pod to test consume configMaps
Mar 17 10:02:51.951: INFO: Waiting up to 5m0s for pod "pod-configmaps-56148940-7bde-4679-add2-c6014ddbd86a" in namespace "configmap-7997" to be "Succeeded or Failed"
Mar 17 10:02:51.971: INFO: Pod "pod-configmaps-56148940-7bde-4679-add2-c6014ddbd86a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.109667ms
Mar 17 10:02:53.974: INFO: Pod "pod-configmaps-56148940-7bde-4679-add2-c6014ddbd86a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022905306s
STEP: Saw pod success
Mar 17 10:02:53.974: INFO: Pod "pod-configmaps-56148940-7bde-4679-add2-c6014ddbd86a" satisfied condition "Succeeded or Failed"
Mar 17 10:02:53.977: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-56148940-7bde-4679-add2-c6014ddbd86a container agnhost-container: <nil>
STEP: delete the pod
Mar 17 10:02:53.997: INFO: Waiting for pod pod-configmaps-56148940-7bde-4679-add2-c6014ddbd86a to disappear
Mar 17 10:02:54.004: INFO: Pod pod-configmaps-56148940-7bde-4679-add2-c6014ddbd86a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:02:54.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7997" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":4974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:02:54.013: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Mar 17 10:02:54.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 create -f -'
Mar 17 10:02:54.312: INFO: stderr: ""
Mar 17 10:02:54.312: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 17 10:02:54.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 17 10:02:54.405: INFO: stderr: ""
Mar 17 10:02:54.405: INFO: stdout: "update-demo-nautilus-2jjvf update-demo-nautilus-gk4ml "
Mar 17 10:02:54.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-2jjvf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 10:02:54.461: INFO: stderr: ""
Mar 17 10:02:54.461: INFO: stdout: ""
Mar 17 10:02:54.461: INFO: update-demo-nautilus-2jjvf is created but not running
Mar 17 10:02:59.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 17 10:02:59.531: INFO: stderr: ""
Mar 17 10:02:59.531: INFO: stdout: "update-demo-nautilus-2jjvf update-demo-nautilus-gk4ml "
Mar 17 10:02:59.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-2jjvf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 10:02:59.599: INFO: stderr: ""
Mar 17 10:02:59.599: INFO: stdout: "true"
Mar 17 10:02:59.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-2jjvf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 17 10:02:59.653: INFO: stderr: ""
Mar 17 10:02:59.653: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 17 10:02:59.653: INFO: validating pod update-demo-nautilus-2jjvf
Mar 17 10:02:59.658: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 17 10:02:59.658: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 17 10:02:59.658: INFO: update-demo-nautilus-2jjvf is verified up and running
Mar 17 10:02:59.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-gk4ml -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 10:02:59.724: INFO: stderr: ""
Mar 17 10:02:59.724: INFO: stdout: "true"
Mar 17 10:02:59.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-gk4ml -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 17 10:02:59.786: INFO: stderr: ""
Mar 17 10:02:59.786: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 17 10:02:59.786: INFO: validating pod update-demo-nautilus-gk4ml
Mar 17 10:02:59.790: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 17 10:02:59.790: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 17 10:02:59.790: INFO: update-demo-nautilus-gk4ml is verified up and running
STEP: scaling down the replication controller
Mar 17 10:02:59.791: INFO: scanned /root for discovery docs: <nil>
Mar 17 10:02:59.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 17 10:03:00.880: INFO: stderr: ""
Mar 17 10:03:00.880: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 17 10:03:00.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 17 10:03:00.937: INFO: stderr: ""
Mar 17 10:03:00.937: INFO: stdout: "update-demo-nautilus-gk4ml "
Mar 17 10:03:00.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-gk4ml -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 10:03:00.996: INFO: stderr: ""
Mar 17 10:03:00.996: INFO: stdout: "true"
Mar 17 10:03:00.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-gk4ml -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 17 10:03:01.055: INFO: stderr: ""
Mar 17 10:03:01.056: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 17 10:03:01.056: INFO: validating pod update-demo-nautilus-gk4ml
Mar 17 10:03:01.059: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 17 10:03:01.059: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 17 10:03:01.059: INFO: update-demo-nautilus-gk4ml is verified up and running
STEP: scaling up the replication controller
Mar 17 10:03:01.060: INFO: scanned /root for discovery docs: <nil>
Mar 17 10:03:01.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 17 10:03:02.156: INFO: stderr: ""
Mar 17 10:03:02.156: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 17 10:03:02.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 17 10:03:02.234: INFO: stderr: ""
Mar 17 10:03:02.234: INFO: stdout: "update-demo-nautilus-gk4ml update-demo-nautilus-gl67k "
Mar 17 10:03:02.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-gk4ml -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 10:03:02.301: INFO: stderr: ""
Mar 17 10:03:02.301: INFO: stdout: "true"
Mar 17 10:03:02.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-gk4ml -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 17 10:03:02.408: INFO: stderr: ""
Mar 17 10:03:02.408: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 17 10:03:02.408: INFO: validating pod update-demo-nautilus-gk4ml
Mar 17 10:03:02.419: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 17 10:03:02.419: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 17 10:03:02.419: INFO: update-demo-nautilus-gk4ml is verified up and running
Mar 17 10:03:02.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-gl67k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 17 10:03:02.518: INFO: stderr: ""
Mar 17 10:03:02.518: INFO: stdout: "true"
Mar 17 10:03:02.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods update-demo-nautilus-gl67k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 17 10:03:02.609: INFO: stderr: ""
Mar 17 10:03:02.609: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 17 10:03:02.609: INFO: validating pod update-demo-nautilus-gl67k
Mar 17 10:03:02.613: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 17 10:03:02.613: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 17 10:03:02.613: INFO: update-demo-nautilus-gl67k is verified up and running
STEP: using delete to clean up resources
Mar 17 10:03:02.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 delete --grace-period=0 --force -f -'
Mar 17 10:03:02.686: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 17 10:03:02.686: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 17 10:03:02.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get rc,svc -l name=update-demo --no-headers'
Mar 17 10:03:02.800: INFO: stderr: "No resources found in kubectl-8210 namespace.\n"
Mar 17 10:03:02.800: INFO: stdout: ""
Mar 17 10:03:02.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-8210 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 17 10:03:02.892: INFO: stderr: ""
Mar 17 10:03:02.892: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:02.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8210" for this suite.

• [SLOW TEST:8.890 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":262,"skipped":5004,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:02.903: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:03:02.952: INFO: Waiting up to 5m0s for pod "downwardapi-volume-631201fc-d3fc-4a82-89b2-51f30a43e8c1" in namespace "projected-1279" to be "Succeeded or Failed"
Mar 17 10:03:02.959: INFO: Pod "downwardapi-volume-631201fc-d3fc-4a82-89b2-51f30a43e8c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.951133ms
Mar 17 10:03:04.963: INFO: Pod "downwardapi-volume-631201fc-d3fc-4a82-89b2-51f30a43e8c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010909432s
STEP: Saw pod success
Mar 17 10:03:04.963: INFO: Pod "downwardapi-volume-631201fc-d3fc-4a82-89b2-51f30a43e8c1" satisfied condition "Succeeded or Failed"
Mar 17 10:03:04.965: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-631201fc-d3fc-4a82-89b2-51f30a43e8c1 container client-container: <nil>
STEP: delete the pod
Mar 17 10:03:04.987: INFO: Waiting for pod downwardapi-volume-631201fc-d3fc-4a82-89b2-51f30a43e8c1 to disappear
Mar 17 10:03:04.993: INFO: Pod downwardapi-volume-631201fc-d3fc-4a82-89b2-51f30a43e8c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:04.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1279" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":5026,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:05.013: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-263dd269-e2ae-46c9-8f07-b4859f279a11
STEP: Creating configMap with name cm-test-opt-upd-84631fc8-071d-4fe5-bfe9-44aae8e4ebeb
STEP: Creating the pod
Mar 17 10:03:05.108: INFO: The status of Pod pod-projected-configmaps-62706212-4562-4822-a0bc-15fc349709e4 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:03:07.113: INFO: The status of Pod pod-projected-configmaps-62706212-4562-4822-a0bc-15fc349709e4 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-263dd269-e2ae-46c9-8f07-b4859f279a11
STEP: Updating configmap cm-test-opt-upd-84631fc8-071d-4fe5-bfe9-44aae8e4ebeb
STEP: Creating configMap with name cm-test-opt-create-a277a271-e64e-4bc7-b5ba-c210982e2905
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:09.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1228" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":5069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:09.185: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:03:09.231: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f6ec3b85-4516-4d94-84e4-6656664b519c" in namespace "security-context-test-5190" to be "Succeeded or Failed"
Mar 17 10:03:09.236: INFO: Pod "alpine-nnp-false-f6ec3b85-4516-4d94-84e4-6656664b519c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650671ms
Mar 17 10:03:11.240: INFO: Pod "alpine-nnp-false-f6ec3b85-4516-4d94-84e4-6656664b519c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009289652s
Mar 17 10:03:13.244: INFO: Pod "alpine-nnp-false-f6ec3b85-4516-4d94-84e4-6656664b519c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013578798s
Mar 17 10:03:13.244: INFO: Pod "alpine-nnp-false-f6ec3b85-4516-4d94-84e4-6656664b519c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:13.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5190" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":5115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:13.271: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:03:13.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a38821a-c3ca-4321-b8ef-4117e77d404f" in namespace "projected-5016" to be "Succeeded or Failed"
Mar 17 10:03:13.351: INFO: Pod "downwardapi-volume-4a38821a-c3ca-4321-b8ef-4117e77d404f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.306178ms
Mar 17 10:03:15.358: INFO: Pod "downwardapi-volume-4a38821a-c3ca-4321-b8ef-4117e77d404f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021468395s
STEP: Saw pod success
Mar 17 10:03:15.358: INFO: Pod "downwardapi-volume-4a38821a-c3ca-4321-b8ef-4117e77d404f" satisfied condition "Succeeded or Failed"
Mar 17 10:03:15.362: INFO: Trying to get logs from node ip-172-31-39-36 pod downwardapi-volume-4a38821a-c3ca-4321-b8ef-4117e77d404f container client-container: <nil>
STEP: delete the pod
Mar 17 10:03:15.381: INFO: Waiting for pod downwardapi-volume-4a38821a-c3ca-4321-b8ef-4117e77d404f to disappear
Mar 17 10:03:15.387: INFO: Pod downwardapi-volume-4a38821a-c3ca-4321-b8ef-4117e77d404f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:15.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5016" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":266,"skipped":5186,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:15.395: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Mar 17 10:03:15.469: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 17 10:03:20.476: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Mar 17 10:03:20.479: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Mar 17 10:03:20.494: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Mar 17 10:03:20.496: INFO: Observed &ReplicaSet event: ADDED
Mar 17 10:03:20.496: INFO: Observed &ReplicaSet event: MODIFIED
Mar 17 10:03:20.496: INFO: Observed &ReplicaSet event: MODIFIED
Mar 17 10:03:20.497: INFO: Observed &ReplicaSet event: MODIFIED
Mar 17 10:03:20.497: INFO: Found replicaset test-rs in namespace replicaset-5265 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 17 10:03:20.497: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Mar 17 10:03:20.497: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 17 10:03:20.507: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Mar 17 10:03:20.522: INFO: Observed &ReplicaSet event: ADDED
Mar 17 10:03:20.523: INFO: Observed &ReplicaSet event: MODIFIED
Mar 17 10:03:20.523: INFO: Observed &ReplicaSet event: MODIFIED
Mar 17 10:03:20.524: INFO: Observed &ReplicaSet event: MODIFIED
Mar 17 10:03:20.524: INFO: Observed replicaset test-rs in namespace replicaset-5265 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 17 10:03:20.524: INFO: Observed &ReplicaSet event: MODIFIED
Mar 17 10:03:20.524: INFO: Found replicaset test-rs in namespace replicaset-5265 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 17 10:03:20.524: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:20.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5265" for this suite.

• [SLOW TEST:5.148 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":267,"skipped":5200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:20.545: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:03:20.636: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 17 10:03:20.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:20.646: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Mar 17 10:03:20.697: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:20.698: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:03:21.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:21.709: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:03:22.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 17 10:03:22.702: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 17 10:03:22.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 17 10:03:22.762: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar 17 10:03:23.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:23.767: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 17 10:03:23.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:23.783: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:03:24.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:24.789: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:03:25.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:25.790: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:03:26.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:26.788: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:03:27.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 17 10:03:27.786: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7271, will wait for the garbage collector to delete the pods
Mar 17 10:03:27.848: INFO: Deleting DaemonSet.extensions daemon-set took: 4.800475ms
Mar 17 10:03:27.949: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.393178ms
Mar 17 10:03:30.953: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:03:30.953: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 17 10:03:30.956: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"80335"},"items":null}

Mar 17 10:03:30.959: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"80335"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:30.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7271" for this suite.

• [SLOW TEST:10.462 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":268,"skipped":5225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:31.011: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:31.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9452" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":269,"skipped":5279,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:31.118: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Mar 17 10:03:31.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-930 cluster-info'
Mar 17 10:03:31.256: INFO: stderr: ""
Mar 17 10:03:31.256: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:03:31.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-930" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":270,"skipped":5279,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:03:31.270: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:04:31.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8711" for this suite.

• [SLOW TEST:60.102 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":5389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:04:31.372: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 17 10:04:33.958: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1530 pod-service-account-101cfa2f-f263-4832-9f1b-0715774da4af -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 17 10:04:34.132: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1530 pod-service-account-101cfa2f-f263-4832-9f1b-0715774da4af -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 17 10:04:34.289: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1530 pod-service-account-101cfa2f-f263-4832-9f1b-0715774da4af -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:04:34.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1530" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":272,"skipped":5414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:04:34.454: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-5178/configmap-test-34448ae0-0882-4b41-82b4-f8476047e1c9
STEP: Creating a pod to test consume configMaps
Mar 17 10:04:34.560: INFO: Waiting up to 5m0s for pod "pod-configmaps-19337c2f-2dbd-4e2c-bcef-7f3a219a5a26" in namespace "configmap-5178" to be "Succeeded or Failed"
Mar 17 10:04:34.568: INFO: Pod "pod-configmaps-19337c2f-2dbd-4e2c-bcef-7f3a219a5a26": Phase="Pending", Reason="", readiness=false. Elapsed: 7.595957ms
Mar 17 10:04:36.572: INFO: Pod "pod-configmaps-19337c2f-2dbd-4e2c-bcef-7f3a219a5a26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01158363s
STEP: Saw pod success
Mar 17 10:04:36.572: INFO: Pod "pod-configmaps-19337c2f-2dbd-4e2c-bcef-7f3a219a5a26" satisfied condition "Succeeded or Failed"
Mar 17 10:04:36.579: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-19337c2f-2dbd-4e2c-bcef-7f3a219a5a26 container env-test: <nil>
STEP: delete the pod
Mar 17 10:04:36.610: INFO: Waiting for pod pod-configmaps-19337c2f-2dbd-4e2c-bcef-7f3a219a5a26 to disappear
Mar 17 10:04:36.623: INFO: Pod pod-configmaps-19337c2f-2dbd-4e2c-bcef-7f3a219a5a26 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:04:36.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5178" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":5445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:04:36.651: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:04:36.693: INFO: Creating pod...
Mar 17 10:04:36.715: INFO: Pod Quantity: 1 Status: Pending
Mar 17 10:04:37.720: INFO: Pod Quantity: 1 Status: Pending
Mar 17 10:04:38.720: INFO: Pod Status: Running
Mar 17 10:04:38.720: INFO: Creating service...
Mar 17 10:04:38.734: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/pods/agnhost/proxy/some/path/with/DELETE
Mar 17 10:04:38.743: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 17 10:04:38.743: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/pods/agnhost/proxy/some/path/with/GET
Mar 17 10:04:38.750: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 17 10:04:38.750: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/pods/agnhost/proxy/some/path/with/HEAD
Mar 17 10:04:38.753: INFO: http.Client request:HEAD | StatusCode:200
Mar 17 10:04:38.753: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 17 10:04:38.757: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 17 10:04:38.757: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/pods/agnhost/proxy/some/path/with/PATCH
Mar 17 10:04:38.768: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 17 10:04:38.768: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/pods/agnhost/proxy/some/path/with/POST
Mar 17 10:04:38.771: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 17 10:04:38.771: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/pods/agnhost/proxy/some/path/with/PUT
Mar 17 10:04:38.781: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 17 10:04:38.781: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/services/test-service/proxy/some/path/with/DELETE
Mar 17 10:04:38.789: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 17 10:04:38.789: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/services/test-service/proxy/some/path/with/GET
Mar 17 10:04:38.795: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 17 10:04:38.795: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/services/test-service/proxy/some/path/with/HEAD
Mar 17 10:04:38.806: INFO: http.Client request:HEAD | StatusCode:200
Mar 17 10:04:38.806: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/services/test-service/proxy/some/path/with/OPTIONS
Mar 17 10:04:38.810: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 17 10:04:38.810: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/services/test-service/proxy/some/path/with/PATCH
Mar 17 10:04:38.813: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 17 10:04:38.813: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/services/test-service/proxy/some/path/with/POST
Mar 17 10:04:38.826: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 17 10:04:38.826: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-744/services/test-service/proxy/some/path/with/PUT
Mar 17 10:04:38.830: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:04:38.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-744" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":274,"skipped":5481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:04:38.838: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:04:38.905: INFO: Creating simple deployment test-new-deployment
Mar 17 10:04:38.925: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar 17 10:04:41.040: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-224  b160e443-31b6-4611-a3ec-ce0cd424c5e4 80727 3 2022-03-17 10:04:38 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-03-17 10:04:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:04:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b35ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-17 10:04:40 +0000 UTC,LastTransitionTime:2022-03-17 10:04:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-03-17 10:04:40 +0000 UTC,LastTransitionTime:2022-03-17 10:04:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 17 10:04:41.056: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-224  a219e6e4-7348-49e7-aa4d-194aaf15338e 80734 2 2022-03-17 10:04:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b160e443-31b6-4611-a3ec-ce0cd424c5e4 0xc0064103f7 0xc0064103f8}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:04:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b160e443-31b6-4611-a3ec-ce0cd424c5e4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:04:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006410488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:04:41.062: INFO: Pod "test-new-deployment-5d9fdcc779-2jj7d" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-2jj7d test-new-deployment-5d9fdcc779- deployment-224  72ad6d39-a81b-4ee2-8ca5-18a3b69c4ea2 80718 0 2022-03-17 10:04:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:935a3b77d8c5a85a23a536a84ea40413dda5236673090edbd264364d405f162d cni.projectcalico.org/podIP:10.42.1.125/32 cni.projectcalico.org/podIPs:10.42.1.125/32] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 a219e6e4-7348-49e7-aa4d-194aaf15338e 0xc006410877 0xc006410878}] []  [{kube-controller-manager Update v1 2022-03-17 10:04:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a219e6e4-7348-49e7-aa4d-194aaf15338e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 10:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 10:04:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sf7hg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sf7hg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:04:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:04:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:04:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:04:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.39.36,PodIP:10.42.1.125,StartTime:2022-03-17 10:04:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 10:04:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://8418d62de36ad166400e3a2f9405c28cdfc87f7ce5736b0007dcdad7187754a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 10:04:41.062: INFO: Pod "test-new-deployment-5d9fdcc779-8gpz9" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-8gpz9 test-new-deployment-5d9fdcc779- deployment-224  5903c726-7a6e-428c-bacd-8f1b35e081c9 80731 0 2022-03-17 10:04:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 a219e6e4-7348-49e7-aa4d-194aaf15338e 0xc006410a80 0xc006410a81}] []  [{kube-controller-manager Update v1 2022-03-17 10:04:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a219e6e4-7348-49e7-aa4d-194aaf15338e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts82p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts82p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:04:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:04:41.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-224" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":275,"skipped":5529,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:04:41.077: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 17 10:04:41.140: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 10:04:43.950: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:04:57.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8353" for this suite.

• [SLOW TEST:16.303 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":276,"skipped":5535,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:04:57.381: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 17 10:04:57.429: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 17 10:04:57.436: INFO: Waiting for terminating namespaces to be deleted...
Mar 17 10:04:57.443: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-33-68 before test
Mar 17 10:04:57.452: INFO: fleet-agent-68b989995b-gwpf4 from cattle-fleet-system started at 2022-03-17 06:28:25 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container fleet-agent ready: true, restart count 1
Mar 17 10:04:57.452: INFO: cattle-cluster-agent-5bdc96ddf-7rt46 from cattle-system started at 2022-03-17 06:28:37 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container cluster-register ready: true, restart count 0
Mar 17 10:04:57.452: INFO: cattle-cluster-agent-5bdc96ddf-dkrkg from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container cluster-register ready: true, restart count 12
Mar 17 10:04:57.452: INFO: cattle-node-agent-xb7rh from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container agent ready: true, restart count 0
Mar 17 10:04:57.452: INFO: kube-api-auth-htl6k from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container kube-api-auth ready: true, restart count 0
Mar 17 10:04:57.452: INFO: calico-kube-controllers-fc7fcb565-nwnrh from kube-system started at 2022-03-17 06:27:25 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container calico-kube-controllers ready: true, restart count 12
Mar 17 10:04:57.452: INFO: canal-whdbp from kube-system started at 2022-03-17 06:27:25 +0000 UTC (2 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 10:04:57.452: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 10:04:57.452: INFO: coredns-5cb46d7c6-wk9ms from kube-system started at 2022-03-17 06:27:30 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container coredns ready: true, restart count 0
Mar 17 10:04:57.452: INFO: metrics-server-5c4895ffbd-9bbrg from kube-system started at 2022-03-17 06:27:34 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container metrics-server ready: true, restart count 0
Mar 17 10:04:57.452: INFO: rke-coredns-addon-deploy-job-lj29w from kube-system started at 2022-03-17 08:23:13 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Mar 17 10:04:57.452: INFO: rke-metrics-addon-deploy-job-fpn9x from kube-system started at 2022-03-17 06:27:33 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Mar 17 10:04:57.452: INFO: rke-network-plugin-deploy-job-x6hnn from kube-system started at 2022-03-17 06:27:23 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Mar 17 10:04:57.452: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-ng79r from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 10:04:57.452: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 10:04:57.452: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 17 10:04:57.452: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-35-106 before test
Mar 17 10:04:57.462: INFO: cattle-node-agent-zbmd5 from cattle-system started at 2022-03-17 06:29:26 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.463: INFO: 	Container agent ready: true, restart count 0
Mar 17 10:04:57.463: INFO: canal-2xxhj from kube-system started at 2022-03-17 06:29:26 +0000 UTC (2 container statuses recorded)
Mar 17 10:04:57.463: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 10:04:57.463: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 10:04:57.463: INFO: coredns-5cb46d7c6-khnbz from kube-system started at 2022-03-17 08:53:25 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.463: INFO: 	Container coredns ready: true, restart count 0
Mar 17 10:04:57.463: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-n98xh from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 10:04:57.463: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 10:04:57.463: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 17 10:04:57.463: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-39-36 before test
Mar 17 10:04:57.470: INFO: cattle-node-agent-lnw2j from cattle-system started at 2022-03-17 06:28:37 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.470: INFO: 	Container agent ready: true, restart count 0
Mar 17 10:04:57.470: INFO: canal-t2wwt from kube-system started at 2022-03-17 06:28:37 +0000 UTC (2 container statuses recorded)
Mar 17 10:04:57.470: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 10:04:57.470: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 10:04:57.470: INFO: coredns-autoscaler-6cb44df646-flpwt from kube-system started at 2022-03-17 08:56:22 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.470: INFO: 	Container autoscaler ready: true, restart count 0
Mar 17 10:04:57.470: INFO: sonobuoy from sonobuoy started at 2022-03-17 08:58:32 +0000 UTC (1 container statuses recorded)
Mar 17 10:04:57.470: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 17 10:04:57.470: INFO: sonobuoy-e2e-job-3f99102f0d96453a from sonobuoy started at 2022-03-17 08:58:33 +0000 UTC (2 container statuses recorded)
Mar 17 10:04:57.470: INFO: 	Container e2e ready: true, restart count 0
Mar 17 10:04:57.470: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 10:04:57.470: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-st4gk from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 10:04:57.470: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 10:04:57.470: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d1c714ed-101c-4a2a-b5eb-fee2b65f9293 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d1c714ed-101c-4a2a-b5eb-fee2b65f9293 off the node ip-172-31-35-106
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d1c714ed-101c-4a2a-b5eb-fee2b65f9293
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:05:01.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5661" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":277,"skipped":5609,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:05:01.604: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 17 10:05:01.653: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 17 10:05:01.661: INFO: Waiting for terminating namespaces to be deleted...
Mar 17 10:05:01.666: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-33-68 before test
Mar 17 10:05:01.673: INFO: fleet-agent-68b989995b-gwpf4 from cattle-fleet-system started at 2022-03-17 06:28:25 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.673: INFO: 	Container fleet-agent ready: true, restart count 1
Mar 17 10:05:01.673: INFO: cattle-cluster-agent-5bdc96ddf-7rt46 from cattle-system started at 2022-03-17 06:28:37 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.673: INFO: 	Container cluster-register ready: true, restart count 0
Mar 17 10:05:01.673: INFO: cattle-cluster-agent-5bdc96ddf-dkrkg from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.673: INFO: 	Container cluster-register ready: true, restart count 12
Mar 17 10:05:01.673: INFO: cattle-node-agent-xb7rh from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.673: INFO: 	Container agent ready: true, restart count 0
Mar 17 10:05:01.673: INFO: kube-api-auth-htl6k from cattle-system started at 2022-03-17 06:27:47 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.673: INFO: 	Container kube-api-auth ready: true, restart count 0
Mar 17 10:05:01.673: INFO: calico-kube-controllers-fc7fcb565-nwnrh from kube-system started at 2022-03-17 06:27:25 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.673: INFO: 	Container calico-kube-controllers ready: true, restart count 12
Mar 17 10:05:01.673: INFO: canal-whdbp from kube-system started at 2022-03-17 06:27:25 +0000 UTC (2 container statuses recorded)
Mar 17 10:05:01.673: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 10:05:01.673: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 10:05:01.673: INFO: coredns-5cb46d7c6-wk9ms from kube-system started at 2022-03-17 06:27:30 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.673: INFO: 	Container coredns ready: true, restart count 0
Mar 17 10:05:01.674: INFO: metrics-server-5c4895ffbd-9bbrg from kube-system started at 2022-03-17 06:27:34 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.674: INFO: 	Container metrics-server ready: true, restart count 0
Mar 17 10:05:01.674: INFO: rke-coredns-addon-deploy-job-lj29w from kube-system started at 2022-03-17 08:23:13 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.674: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Mar 17 10:05:01.674: INFO: rke-metrics-addon-deploy-job-fpn9x from kube-system started at 2022-03-17 06:27:33 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.674: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Mar 17 10:05:01.674: INFO: rke-network-plugin-deploy-job-x6hnn from kube-system started at 2022-03-17 06:27:23 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.674: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Mar 17 10:05:01.674: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-ng79r from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 10:05:01.674: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 10:05:01.674: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 17 10:05:01.674: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-35-106 before test
Mar 17 10:05:01.679: INFO: cattle-node-agent-zbmd5 from cattle-system started at 2022-03-17 06:29:26 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.680: INFO: 	Container agent ready: true, restart count 0
Mar 17 10:05:01.680: INFO: canal-2xxhj from kube-system started at 2022-03-17 06:29:26 +0000 UTC (2 container statuses recorded)
Mar 17 10:05:01.680: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 10:05:01.680: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 10:05:01.680: INFO: coredns-5cb46d7c6-khnbz from kube-system started at 2022-03-17 08:53:25 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.680: INFO: 	Container coredns ready: true, restart count 0
Mar 17 10:05:01.680: INFO: with-labels from sched-pred-5661 started at 2022-03-17 10:04:59 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.680: INFO: 	Container with-labels ready: true, restart count 0
Mar 17 10:05:01.680: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-n98xh from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 10:05:01.680: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 10:05:01.680: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 17 10:05:01.680: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-39-36 before test
Mar 17 10:05:01.689: INFO: cattle-node-agent-lnw2j from cattle-system started at 2022-03-17 06:28:37 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.689: INFO: 	Container agent ready: true, restart count 0
Mar 17 10:05:01.689: INFO: canal-t2wwt from kube-system started at 2022-03-17 06:28:37 +0000 UTC (2 container statuses recorded)
Mar 17 10:05:01.689: INFO: 	Container calico-node ready: true, restart count 0
Mar 17 10:05:01.689: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 17 10:05:01.689: INFO: coredns-autoscaler-6cb44df646-flpwt from kube-system started at 2022-03-17 08:56:22 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.689: INFO: 	Container autoscaler ready: true, restart count 0
Mar 17 10:05:01.689: INFO: sonobuoy from sonobuoy started at 2022-03-17 08:58:32 +0000 UTC (1 container statuses recorded)
Mar 17 10:05:01.689: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 17 10:05:01.689: INFO: sonobuoy-e2e-job-3f99102f0d96453a from sonobuoy started at 2022-03-17 08:58:33 +0000 UTC (2 container statuses recorded)
Mar 17 10:05:01.689: INFO: 	Container e2e ready: true, restart count 0
Mar 17 10:05:01.689: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 10:05:01.689: INFO: sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-st4gk from sonobuoy started at 2022-03-17 08:58:34 +0000 UTC (2 container statuses recorded)
Mar 17 10:05:01.689: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 17 10:05:01.689: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-99d33d4f-af77-4930-9570-d98739cdb9a8 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.35.106 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-99d33d4f-af77-4930-9570-d98739cdb9a8 off the node ip-172-31-35-106
STEP: verifying the node doesn't have the label kubernetes.io/e2e-99d33d4f-af77-4930-9570-d98739cdb9a8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:05.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4660" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.223 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":278,"skipped":5618,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:05.827: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar 17 10:10:06.123: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 17 10:10:06.123: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 17 10:10:06.155: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 17 10:10:06.155: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 17 10:10:06.223: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 17 10:10:06.223: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 17 10:10:06.287: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 17 10:10:06.287: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 17 10:10:07.347: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 17 10:10:07.347: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 17 10:10:07.945: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar 17 10:10:07.955: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar 17 10:10:07.957: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0
Mar 17 10:10:07.957: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0
Mar 17 10:10:07.957: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0
Mar 17 10:10:07.957: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0
Mar 17 10:10:07.958: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0
Mar 17 10:10:07.958: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0
Mar 17 10:10:07.958: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0
Mar 17 10:10:07.958: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 0
Mar 17 10:10:07.958: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:07.958: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:07.959: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:07.959: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:07.959: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:07.959: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:07.967: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:07.968: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:07.988: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:07.988: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:08.038: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:08.038: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:08.049: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:08.049: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:10.147: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:10.147: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:10.164: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
STEP: listing Deployments
Mar 17 10:10:10.171: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar 17 10:10:10.184: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar 17 10:10:10.194: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:10.212: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:10.261: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:10.309: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:10.316: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:10.326: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:11.503: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:12.258: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:12.374: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:12.397: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 17 10:10:13.585: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar 17 10:10:13.637: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:13.637: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:13.637: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:13.637: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:13.637: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:13.638: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 1
Mar 17 10:10:13.638: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:13.638: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 3
Mar 17 10:10:13.638: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:13.638: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 2
Mar 17 10:10:13.638: INFO: observed Deployment test-deployment in namespace deployment-564 with ReadyReplicas 3
STEP: deleting the Deployment
Mar 17 10:10:13.647: INFO: observed event type MODIFIED
Mar 17 10:10:13.647: INFO: observed event type MODIFIED
Mar 17 10:10:13.647: INFO: observed event type MODIFIED
Mar 17 10:10:13.647: INFO: observed event type MODIFIED
Mar 17 10:10:13.647: INFO: observed event type MODIFIED
Mar 17 10:10:13.647: INFO: observed event type MODIFIED
Mar 17 10:10:13.647: INFO: observed event type MODIFIED
Mar 17 10:10:13.652: INFO: observed event type MODIFIED
Mar 17 10:10:13.652: INFO: observed event type MODIFIED
Mar 17 10:10:13.652: INFO: observed event type MODIFIED
Mar 17 10:10:13.652: INFO: observed event type MODIFIED
Mar 17 10:10:13.653: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar 17 10:10:13.660: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 17 10:10:13.663: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-564  749a6752-58e1-439b-b343-2142e308c0c7 81906 4 2022-03-17 10:10:07 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment b48f9aba-4ee9-4f80-86bd-f6a6b7d95a28 0xc006a11ad7 0xc006a11ad8}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b48f9aba-4ee9-4f80-86bd-f6a6b7d95a28\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:10:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a11b60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 17 10:10:13.669: INFO: pod: "test-deployment-5ddd8b47d8-ng2zt":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-ng2zt test-deployment-5ddd8b47d8- deployment-564  e33851b4-f93d-4c70-98f2-99588c1799ff 81896 0 2022-03-17 10:10:07 +0000 UTC 2022-03-17 10:10:13 +0000 UTC 0xc00543a868 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:315ff89ef8ded38e497763bf80b81a77437bf60018055ef8e8b11efa915cd7f3 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 749a6752-58e1-439b-b343-2142e308c0c7 0xc00543a897 0xc00543a898}] []  [{kube-controller-manager Update v1 2022-03-17 10:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"749a6752-58e1-439b-b343-2142e308c0c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 10:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 10:10:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ld4p6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ld4p6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:10.42.2.4,StartTime:2022-03-17 10:10:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 10:10:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:rancher/mirrored-pause:3.6,ImageID:docker-pullable://rancher/mirrored-pause@sha256:74c4244427b7312c5b901fe0f67cbc53683d06f4f24c6faee65d4182bf0fa893,ContainerID:docker://781556050eaa6a974d79e9075a40b994d882c0c68db276951cef20561e0babd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 17 10:10:13.670: INFO: pod: "test-deployment-5ddd8b47d8-pccm6":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-pccm6 test-deployment-5ddd8b47d8- deployment-564  db920857-dcb8-40ca-b76e-e1811f809836 81900 0 2022-03-17 10:10:10 +0000 UTC 2022-03-17 10:10:14 +0000 UTC 0xc00543aa90 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d3d5e713ee3cf6009622899ec4ccc98688b6ae184e4e43f9715bd98464272ef6 cni.projectcalico.org/podIP:10.42.1.127/32 cni.projectcalico.org/podIPs:10.42.1.127/32] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 749a6752-58e1-439b-b343-2142e308c0c7 0xc00543aae7 0xc00543aae8}] []  [{kube-controller-manager Update v1 2022-03-17 10:10:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"749a6752-58e1-439b-b343-2142e308c0c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 10:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 10:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgnsz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgnsz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.39.36,PodIP:10.42.1.127,StartTime:2022-03-17 10:10:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 10:10:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:rancher/mirrored-pause:3.6,ImageID:docker-pullable://rancher/mirrored-pause@sha256:74c4244427b7312c5b901fe0f67cbc53683d06f4f24c6faee65d4182bf0fa893,ContainerID:docker://54c380441dc6e3e0319865179d8d8be9ef5c517f0c8b2fb855e315e1fd1711b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 17 10:10:13.671: INFO: ReplicaSet "test-deployment-6cdc5bc678":
&ReplicaSet{ObjectMeta:{test-deployment-6cdc5bc678  deployment-564  8e7a42b3-dd07-4888-b4f0-da161d810556 81796 3 2022-03-17 10:10:06 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment b48f9aba-4ee9-4f80-86bd-f6a6b7d95a28 0xc006a11bc7 0xc006a11bc8}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:10:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b48f9aba-4ee9-4f80-86bd-f6a6b7d95a28\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:10:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6cdc5bc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a11c50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 17 10:10:13.676: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-564  86cdfddb-a24c-4b7f-85b9-708fbc0a6b22 81898 2 2022-03-17 10:10:10 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment b48f9aba-4ee9-4f80-86bd-f6a6b7d95a28 0xc006a11cb7 0xc006a11cb8}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:10:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b48f9aba-4ee9-4f80-86bd-f6a6b7d95a28\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:10:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a11d40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar 17 10:10:13.683: INFO: pod: "test-deployment-854fdc678-7rllh":
&Pod{ObjectMeta:{test-deployment-854fdc678-7rllh test-deployment-854fdc678- deployment-564  dc37ca0a-9bc3-41fe-a2bc-86d6e15f87b2 81897 0 2022-03-17 10:10:12 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:24718d419b917d9e6c992a8d6dc0372e250d1051f8038ca9c276793e880dbdd7 cni.projectcalico.org/podIP:10.42.1.128/32 cni.projectcalico.org/podIPs:10.42.1.128/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 86cdfddb-a24c-4b7f-85b9-708fbc0a6b22 0xc00543bf67 0xc00543bf68}] []  [{kube-controller-manager Update v1 2022-03-17 10:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86cdfddb-a24c-4b7f-85b9-708fbc0a6b22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 10:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 10:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5kzrb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kzrb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-39-36,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.39.36,PodIP:10.42.1.128,StartTime:2022-03-17 10:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 10:10:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://ac8343cf35f818bd3570f48f054b12c3c52557a3aa77d7ffa5549c64f88269b9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 17 10:10:13.683: INFO: pod: "test-deployment-854fdc678-qgnm9":
&Pod{ObjectMeta:{test-deployment-854fdc678-qgnm9 test-deployment-854fdc678- deployment-564  0054f6c5-3920-449b-9090-fcb764a88e25 81864 0 2022-03-17 10:10:10 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:f482199d69cd3ecc3f0be7415896fab54b87236235b05d989da2dba2dce42673 cni.projectcalico.org/podIP:10.42.2.5/32 cni.projectcalico.org/podIPs:10.42.2.5/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 86cdfddb-a24c-4b7f-85b9-708fbc0a6b22 0xc00443a817 0xc00443a818}] []  [{kube-controller-manager Update v1 2022-03-17 10:10:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86cdfddb-a24c-4b7f-85b9-708fbc0a6b22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-17 10:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-03-17 10:10:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2px6l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2px6l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:10:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:10.42.2.5,StartTime:2022-03-17 10:10:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 10:10:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://c0e9e0b0ea67d6c4f5a953b7a31e4dab7981b72cad53d65c951e4140a40f6fe6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:13.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-564" for this suite.

• [SLOW TEST:7.871 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":279,"skipped":5621,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:13.698: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ch56z in namespace proxy-1887
I0317 10:10:13.764497      22 runners.go:193] Created replication controller with name: proxy-service-ch56z, namespace: proxy-1887, replica count: 1
I0317 10:10:14.815895      22 runners.go:193] proxy-service-ch56z Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0317 10:10:15.816828      22 runners.go:193] proxy-service-ch56z Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 10:10:15.822: INFO: setup took 2.083825423s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 17 10:10:15.827: INFO: (0) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 4.57714ms)
Mar 17 10:10:15.828: INFO: (0) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 5.428496ms)
Mar 17 10:10:15.828: INFO: (0) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 5.822373ms)
Mar 17 10:10:15.828: INFO: (0) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 5.573103ms)
Mar 17 10:10:15.830: INFO: (0) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 8.11076ms)
Mar 17 10:10:15.831: INFO: (0) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 8.792535ms)
Mar 17 10:10:15.831: INFO: (0) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 8.900635ms)
Mar 17 10:10:15.832: INFO: (0) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 9.526811ms)
Mar 17 10:10:15.833: INFO: (0) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 10.6189ms)
Mar 17 10:10:15.836: INFO: (0) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 14.222221ms)
Mar 17 10:10:15.836: INFO: (0) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 14.233632ms)
Mar 17 10:10:15.836: INFO: (0) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 14.137416ms)
Mar 17 10:10:15.837: INFO: (0) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 14.329498ms)
Mar 17 10:10:15.837: INFO: (0) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 14.423068ms)
Mar 17 10:10:15.837: INFO: (0) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 15.231242ms)
Mar 17 10:10:15.838: INFO: (0) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 15.333007ms)
Mar 17 10:10:15.849: INFO: (1) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 11.456627ms)
Mar 17 10:10:15.850: INFO: (1) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 11.904845ms)
Mar 17 10:10:15.858: INFO: (1) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 20.615622ms)
Mar 17 10:10:15.858: INFO: (1) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 20.732175ms)
Mar 17 10:10:15.858: INFO: (1) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 20.690688ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 20.720551ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 21.058366ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 21.085242ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 21.213825ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 21.301018ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 21.508673ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 21.373241ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 21.50567ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 21.382593ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 21.435438ms)
Mar 17 10:10:15.859: INFO: (1) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 21.43349ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 15.506746ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 15.49508ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 15.530655ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 15.728203ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 15.605752ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 15.641222ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 15.679572ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 15.730597ms)
Mar 17 10:10:15.875: INFO: (2) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 15.789285ms)
Mar 17 10:10:15.876: INFO: (2) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 16.460081ms)
Mar 17 10:10:15.883: INFO: (2) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 22.866714ms)
Mar 17 10:10:15.883: INFO: (2) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 23.657243ms)
Mar 17 10:10:15.883: INFO: (2) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 23.451346ms)
Mar 17 10:10:15.883: INFO: (2) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 23.84148ms)
Mar 17 10:10:15.894: INFO: (2) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 34.153564ms)
Mar 17 10:10:15.894: INFO: (2) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 34.283107ms)
Mar 17 10:10:15.902: INFO: (3) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 8.382251ms)
Mar 17 10:10:15.905: INFO: (3) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 10.685359ms)
Mar 17 10:10:15.906: INFO: (3) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 9.026521ms)
Mar 17 10:10:15.907: INFO: (3) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 9.423331ms)
Mar 17 10:10:15.907: INFO: (3) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 9.681699ms)
Mar 17 10:10:15.907: INFO: (3) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 9.933599ms)
Mar 17 10:10:15.908: INFO: (3) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 11.02345ms)
Mar 17 10:10:15.910: INFO: (3) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 12.988036ms)
Mar 17 10:10:15.910: INFO: (3) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 12.818897ms)
Mar 17 10:10:15.912: INFO: (3) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 15.736827ms)
Mar 17 10:10:15.913: INFO: (3) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 14.68736ms)
Mar 17 10:10:15.914: INFO: (3) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 17.273168ms)
Mar 17 10:10:15.915: INFO: (3) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 17.160102ms)
Mar 17 10:10:15.915: INFO: (3) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 18.250256ms)
Mar 17 10:10:15.916: INFO: (3) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 18.818204ms)
Mar 17 10:10:15.918: INFO: (3) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 20.620095ms)
Mar 17 10:10:15.940: INFO: (4) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 21.188695ms)
Mar 17 10:10:15.941: INFO: (4) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 22.300684ms)
Mar 17 10:10:15.941: INFO: (4) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 22.493064ms)
Mar 17 10:10:15.941: INFO: (4) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 22.339734ms)
Mar 17 10:10:15.941: INFO: (4) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 23.144666ms)
Mar 17 10:10:15.941: INFO: (4) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 23.045885ms)
Mar 17 10:10:15.942: INFO: (4) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 23.698081ms)
Mar 17 10:10:15.942: INFO: (4) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 23.541077ms)
Mar 17 10:10:15.942: INFO: (4) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 23.396254ms)
Mar 17 10:10:15.946: INFO: (4) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 27.836898ms)
Mar 17 10:10:15.946: INFO: (4) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 27.450489ms)
Mar 17 10:10:15.946: INFO: (4) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 27.35742ms)
Mar 17 10:10:15.946: INFO: (4) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 27.549081ms)
Mar 17 10:10:15.946: INFO: (4) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 27.765705ms)
Mar 17 10:10:15.946: INFO: (4) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 27.893706ms)
Mar 17 10:10:15.946: INFO: (4) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 27.76438ms)
Mar 17 10:10:15.953: INFO: (5) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 6.474089ms)
Mar 17 10:10:15.967: INFO: (5) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 20.186581ms)
Mar 17 10:10:15.968: INFO: (5) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 21.149183ms)
Mar 17 10:10:15.968: INFO: (5) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 20.808979ms)
Mar 17 10:10:15.968: INFO: (5) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 20.914914ms)
Mar 17 10:10:15.969: INFO: (5) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 21.646088ms)
Mar 17 10:10:15.969: INFO: (5) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 21.679913ms)
Mar 17 10:10:15.969: INFO: (5) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 22.045412ms)
Mar 17 10:10:15.969: INFO: (5) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 22.547109ms)
Mar 17 10:10:15.969: INFO: (5) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 22.538976ms)
Mar 17 10:10:15.969: INFO: (5) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 23.011195ms)
Mar 17 10:10:15.974: INFO: (5) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 27.082158ms)
Mar 17 10:10:15.974: INFO: (5) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 27.338236ms)
Mar 17 10:10:15.983: INFO: (5) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 36.821517ms)
Mar 17 10:10:15.983: INFO: (5) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 37.092857ms)
Mar 17 10:10:15.984: INFO: (5) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 37.582923ms)
Mar 17 10:10:16.011: INFO: (6) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 26.700222ms)
Mar 17 10:10:16.015: INFO: (6) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 30.664685ms)
Mar 17 10:10:16.022: INFO: (6) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 37.7103ms)
Mar 17 10:10:16.024: INFO: (6) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 38.900486ms)
Mar 17 10:10:16.029: INFO: (6) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 44.307365ms)
Mar 17 10:10:16.033: INFO: (6) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 46.868209ms)
Mar 17 10:10:16.033: INFO: (6) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 47.392472ms)
Mar 17 10:10:16.035: INFO: (6) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 48.882648ms)
Mar 17 10:10:16.038: INFO: (6) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 53.007245ms)
Mar 17 10:10:16.038: INFO: (6) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 52.504299ms)
Mar 17 10:10:16.038: INFO: (6) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 52.605919ms)
Mar 17 10:10:16.043: INFO: (6) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 58.210019ms)
Mar 17 10:10:16.043: INFO: (6) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 57.533681ms)
Mar 17 10:10:16.043: INFO: (6) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 57.790373ms)
Mar 17 10:10:16.043: INFO: (6) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 57.845038ms)
Mar 17 10:10:16.043: INFO: (6) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 57.744444ms)
Mar 17 10:10:16.067: INFO: (7) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 23.042155ms)
Mar 17 10:10:16.067: INFO: (7) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 23.352184ms)
Mar 17 10:10:16.069: INFO: (7) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 25.477549ms)
Mar 17 10:10:16.069: INFO: (7) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 25.673778ms)
Mar 17 10:10:16.071: INFO: (7) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 26.831115ms)
Mar 17 10:10:16.071: INFO: (7) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 27.120768ms)
Mar 17 10:10:16.071: INFO: (7) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 27.032686ms)
Mar 17 10:10:16.072: INFO: (7) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 28.549708ms)
Mar 17 10:10:16.073: INFO: (7) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 28.713357ms)
Mar 17 10:10:16.073: INFO: (7) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 28.801685ms)
Mar 17 10:10:16.074: INFO: (7) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 30.267252ms)
Mar 17 10:10:16.074: INFO: (7) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 30.679603ms)
Mar 17 10:10:16.075: INFO: (7) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 31.353558ms)
Mar 17 10:10:16.076: INFO: (7) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 32.341178ms)
Mar 17 10:10:16.078: INFO: (7) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 34.195827ms)
Mar 17 10:10:16.078: INFO: (7) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 34.773207ms)
Mar 17 10:10:16.117: INFO: (8) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 38.752185ms)
Mar 17 10:10:16.117: INFO: (8) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 39.036105ms)
Mar 17 10:10:16.120: INFO: (8) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 41.16435ms)
Mar 17 10:10:16.121: INFO: (8) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 42.75461ms)
Mar 17 10:10:16.122: INFO: (8) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 43.101427ms)
Mar 17 10:10:16.124: INFO: (8) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 45.060052ms)
Mar 17 10:10:16.124: INFO: (8) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 45.167173ms)
Mar 17 10:10:16.125: INFO: (8) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 46.663746ms)
Mar 17 10:10:16.129: INFO: (8) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 49.505363ms)
Mar 17 10:10:16.129: INFO: (8) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 49.792721ms)
Mar 17 10:10:16.132: INFO: (8) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 53.480557ms)
Mar 17 10:10:16.135: INFO: (8) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 56.019894ms)
Mar 17 10:10:16.135: INFO: (8) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 56.436624ms)
Mar 17 10:10:16.136: INFO: (8) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 57.197052ms)
Mar 17 10:10:16.136: INFO: (8) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 57.628629ms)
Mar 17 10:10:16.139: INFO: (8) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 59.833181ms)
Mar 17 10:10:16.173: INFO: (9) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 33.527566ms)
Mar 17 10:10:16.174: INFO: (9) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 34.565127ms)
Mar 17 10:10:16.182: INFO: (9) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 42.689804ms)
Mar 17 10:10:16.183: INFO: (9) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 43.302023ms)
Mar 17 10:10:16.183: INFO: (9) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 43.569311ms)
Mar 17 10:10:16.189: INFO: (9) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 49.069141ms)
Mar 17 10:10:16.192: INFO: (9) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 52.052787ms)
Mar 17 10:10:16.203: INFO: (9) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 63.688952ms)
Mar 17 10:10:16.205: INFO: (9) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 64.763414ms)
Mar 17 10:10:16.205: INFO: (9) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 65.239262ms)
Mar 17 10:10:16.205: INFO: (9) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 64.677063ms)
Mar 17 10:10:16.205: INFO: (9) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 65.3314ms)
Mar 17 10:10:16.206: INFO: (9) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 66.213782ms)
Mar 17 10:10:16.206: INFO: (9) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 66.577706ms)
Mar 17 10:10:16.206: INFO: (9) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 66.407896ms)
Mar 17 10:10:16.206: INFO: (9) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 66.768062ms)
Mar 17 10:10:16.221: INFO: (10) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 14.309432ms)
Mar 17 10:10:16.221: INFO: (10) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 14.37594ms)
Mar 17 10:10:16.222: INFO: (10) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 15.056847ms)
Mar 17 10:10:16.222: INFO: (10) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 14.932189ms)
Mar 17 10:10:16.226: INFO: (10) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 19.151152ms)
Mar 17 10:10:16.226: INFO: (10) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 19.314922ms)
Mar 17 10:10:16.226: INFO: (10) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 19.111856ms)
Mar 17 10:10:16.226: INFO: (10) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 18.983987ms)
Mar 17 10:10:16.227: INFO: (10) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 19.757392ms)
Mar 17 10:10:16.227: INFO: (10) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 20.307978ms)
Mar 17 10:10:16.227: INFO: (10) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 20.022052ms)
Mar 17 10:10:16.227: INFO: (10) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 20.138973ms)
Mar 17 10:10:16.227: INFO: (10) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 20.425476ms)
Mar 17 10:10:16.228: INFO: (10) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 20.88011ms)
Mar 17 10:10:16.230: INFO: (10) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 22.508342ms)
Mar 17 10:10:16.233: INFO: (10) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 25.878ms)
Mar 17 10:10:16.240: INFO: (11) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 6.395824ms)
Mar 17 10:10:16.240: INFO: (11) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 6.875225ms)
Mar 17 10:10:16.242: INFO: (11) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 9.510477ms)
Mar 17 10:10:16.243: INFO: (11) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 10.32331ms)
Mar 17 10:10:16.243: INFO: (11) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 10.638299ms)
Mar 17 10:10:16.244: INFO: (11) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 10.917891ms)
Mar 17 10:10:16.245: INFO: (11) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 11.515617ms)
Mar 17 10:10:16.245: INFO: (11) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 11.527616ms)
Mar 17 10:10:16.245: INFO: (11) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 11.675203ms)
Mar 17 10:10:16.245: INFO: (11) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 11.486284ms)
Mar 17 10:10:16.245: INFO: (11) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 12.060316ms)
Mar 17 10:10:16.245: INFO: (11) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 12.403358ms)
Mar 17 10:10:16.245: INFO: (11) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 12.640103ms)
Mar 17 10:10:16.246: INFO: (11) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 12.481603ms)
Mar 17 10:10:16.246: INFO: (11) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 12.673531ms)
Mar 17 10:10:16.246: INFO: (11) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 12.870363ms)
Mar 17 10:10:16.254: INFO: (12) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 7.19609ms)
Mar 17 10:10:16.254: INFO: (12) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 7.21637ms)
Mar 17 10:10:16.254: INFO: (12) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 7.469983ms)
Mar 17 10:10:16.254: INFO: (12) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 7.664207ms)
Mar 17 10:10:16.255: INFO: (12) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 8.339176ms)
Mar 17 10:10:16.255: INFO: (12) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 8.84253ms)
Mar 17 10:10:16.255: INFO: (12) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 9.095121ms)
Mar 17 10:10:16.255: INFO: (12) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 9.084136ms)
Mar 17 10:10:16.256: INFO: (12) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 9.309771ms)
Mar 17 10:10:16.256: INFO: (12) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 9.563765ms)
Mar 17 10:10:16.258: INFO: (12) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 11.360412ms)
Mar 17 10:10:16.258: INFO: (12) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 11.487394ms)
Mar 17 10:10:16.258: INFO: (12) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 11.843785ms)
Mar 17 10:10:16.258: INFO: (12) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 12.102084ms)
Mar 17 10:10:16.259: INFO: (12) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 12.427865ms)
Mar 17 10:10:16.260: INFO: (12) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 13.961245ms)
Mar 17 10:10:16.268: INFO: (13) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 7.318348ms)
Mar 17 10:10:16.269: INFO: (13) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 7.845327ms)
Mar 17 10:10:16.269: INFO: (13) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 7.915325ms)
Mar 17 10:10:16.269: INFO: (13) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 8.528286ms)
Mar 17 10:10:16.273: INFO: (13) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 12.605219ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 12.754788ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 12.737595ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 12.648624ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 12.959347ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 13.047566ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 12.729293ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 12.784476ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 12.932176ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 13.018374ms)
Mar 17 10:10:16.274: INFO: (13) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 13.188104ms)
Mar 17 10:10:16.275: INFO: (13) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 14.205814ms)
Mar 17 10:10:16.281: INFO: (14) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 5.883038ms)
Mar 17 10:10:16.281: INFO: (14) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 5.936025ms)
Mar 17 10:10:16.281: INFO: (14) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 6.251171ms)
Mar 17 10:10:16.284: INFO: (14) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 8.973925ms)
Mar 17 10:10:16.284: INFO: (14) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 9.255929ms)
Mar 17 10:10:16.284: INFO: (14) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 9.183601ms)
Mar 17 10:10:16.286: INFO: (14) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 10.530374ms)
Mar 17 10:10:16.286: INFO: (14) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 10.703788ms)
Mar 17 10:10:16.286: INFO: (14) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 10.777651ms)
Mar 17 10:10:16.286: INFO: (14) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 10.912247ms)
Mar 17 10:10:16.286: INFO: (14) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 11.281123ms)
Mar 17 10:10:16.287: INFO: (14) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 11.557965ms)
Mar 17 10:10:16.287: INFO: (14) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 12.151445ms)
Mar 17 10:10:16.287: INFO: (14) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 12.101297ms)
Mar 17 10:10:16.287: INFO: (14) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 12.205435ms)
Mar 17 10:10:16.288: INFO: (14) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 12.616285ms)
Mar 17 10:10:16.290: INFO: (15) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 2.74636ms)
Mar 17 10:10:16.294: INFO: (15) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 6.024536ms)
Mar 17 10:10:16.297: INFO: (15) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 8.809142ms)
Mar 17 10:10:16.297: INFO: (15) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 8.815706ms)
Mar 17 10:10:16.297: INFO: (15) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 9.061943ms)
Mar 17 10:10:16.297: INFO: (15) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 8.948483ms)
Mar 17 10:10:16.297: INFO: (15) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 9.195219ms)
Mar 17 10:10:16.297: INFO: (15) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 9.671533ms)
Mar 17 10:10:16.298: INFO: (15) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 9.830774ms)
Mar 17 10:10:16.298: INFO: (15) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 9.983762ms)
Mar 17 10:10:16.298: INFO: (15) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 10.194204ms)
Mar 17 10:10:16.298: INFO: (15) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 10.308374ms)
Mar 17 10:10:16.301: INFO: (15) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 12.738253ms)
Mar 17 10:10:16.301: INFO: (15) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 12.891826ms)
Mar 17 10:10:16.301: INFO: (15) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 13.607671ms)
Mar 17 10:10:16.301: INFO: (15) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 13.194462ms)
Mar 17 10:10:16.309: INFO: (16) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 6.948501ms)
Mar 17 10:10:16.309: INFO: (16) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 7.128183ms)
Mar 17 10:10:16.309: INFO: (16) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 7.458058ms)
Mar 17 10:10:16.309: INFO: (16) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 7.478267ms)
Mar 17 10:10:16.310: INFO: (16) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 8.609919ms)
Mar 17 10:10:16.311: INFO: (16) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 9.243021ms)
Mar 17 10:10:16.311: INFO: (16) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 9.527138ms)
Mar 17 10:10:16.311: INFO: (16) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 9.597798ms)
Mar 17 10:10:16.311: INFO: (16) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 9.615027ms)
Mar 17 10:10:16.311: INFO: (16) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 9.719832ms)
Mar 17 10:10:16.311: INFO: (16) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 9.67912ms)
Mar 17 10:10:16.312: INFO: (16) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 9.981032ms)
Mar 17 10:10:16.312: INFO: (16) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 10.117939ms)
Mar 17 10:10:16.312: INFO: (16) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 10.727657ms)
Mar 17 10:10:16.313: INFO: (16) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 11.210981ms)
Mar 17 10:10:16.313: INFO: (16) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 11.493792ms)
Mar 17 10:10:16.320: INFO: (17) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 7.167415ms)
Mar 17 10:10:16.321: INFO: (17) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 8.105116ms)
Mar 17 10:10:16.322: INFO: (17) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 9.01116ms)
Mar 17 10:10:16.323: INFO: (17) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 9.762894ms)
Mar 17 10:10:16.323: INFO: (17) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 10.022782ms)
Mar 17 10:10:16.324: INFO: (17) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 11.01187ms)
Mar 17 10:10:16.324: INFO: (17) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 10.744298ms)
Mar 17 10:10:16.324: INFO: (17) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 11.04784ms)
Mar 17 10:10:16.325: INFO: (17) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 11.304014ms)
Mar 17 10:10:16.325: INFO: (17) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 11.575337ms)
Mar 17 10:10:16.325: INFO: (17) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 11.451617ms)
Mar 17 10:10:16.325: INFO: (17) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 11.561848ms)
Mar 17 10:10:16.325: INFO: (17) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 11.849043ms)
Mar 17 10:10:16.325: INFO: (17) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 11.645543ms)
Mar 17 10:10:16.325: INFO: (17) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 11.807244ms)
Mar 17 10:10:16.326: INFO: (17) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 12.750479ms)
Mar 17 10:10:16.331: INFO: (18) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 4.877778ms)
Mar 17 10:10:16.338: INFO: (18) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 11.679231ms)
Mar 17 10:10:16.338: INFO: (18) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 11.724526ms)
Mar 17 10:10:16.338: INFO: (18) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 11.537969ms)
Mar 17 10:10:16.338: INFO: (18) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 11.739176ms)
Mar 17 10:10:16.338: INFO: (18) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 11.66655ms)
Mar 17 10:10:16.339: INFO: (18) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 12.597984ms)
Mar 17 10:10:16.339: INFO: (18) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 12.549775ms)
Mar 17 10:10:16.339: INFO: (18) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 12.645708ms)
Mar 17 10:10:16.343: INFO: (18) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 16.328988ms)
Mar 17 10:10:16.343: INFO: (18) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 16.675303ms)
Mar 17 10:10:16.343: INFO: (18) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 17.034646ms)
Mar 17 10:10:16.343: INFO: (18) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 16.885172ms)
Mar 17 10:10:16.344: INFO: (18) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 16.843111ms)
Mar 17 10:10:16.344: INFO: (18) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 17.267411ms)
Mar 17 10:10:16.344: INFO: (18) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 17.473207ms)
Mar 17 10:10:16.355: INFO: (19) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg/proxy/rewriteme">test</a> (200; 10.830436ms)
Mar 17 10:10:16.358: INFO: (19) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">... (200; 13.075674ms)
Mar 17 10:10:16.358: INFO: (19) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 13.566874ms)
Mar 17 10:10:16.360: INFO: (19) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:443/proxy/tlsrewritem... (200; 15.365468ms)
Mar 17 10:10:16.360: INFO: (19) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname2/proxy/: tls qux (200; 15.679033ms)
Mar 17 10:10:16.360: INFO: (19) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 15.7465ms)
Mar 17 10:10:16.360: INFO: (19) /api/v1/namespaces/proxy-1887/services/https:proxy-service-ch56z:tlsportname1/proxy/: tls baz (200; 16.113103ms)
Mar 17 10:10:16.361: INFO: (19) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:460/proxy/: tls baz (200; 15.797418ms)
Mar 17 10:10:16.361: INFO: (19) /api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/: <a href="/api/v1/namespaces/proxy-1887/pods/proxy-service-ch56z-b7xzg:1080/proxy/rewriteme">test<... (200; 16.057874ms)
Mar 17 10:10:16.361: INFO: (19) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:160/proxy/: foo (200; 16.206506ms)
Mar 17 10:10:16.361: INFO: (19) /api/v1/namespaces/proxy-1887/pods/http:proxy-service-ch56z-b7xzg:162/proxy/: bar (200; 15.964118ms)
Mar 17 10:10:16.361: INFO: (19) /api/v1/namespaces/proxy-1887/pods/https:proxy-service-ch56z-b7xzg:462/proxy/: tls qux (200; 15.795947ms)
Mar 17 10:10:16.361: INFO: (19) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname2/proxy/: bar (200; 16.424342ms)
Mar 17 10:10:16.361: INFO: (19) /api/v1/namespaces/proxy-1887/services/http:proxy-service-ch56z:portname1/proxy/: foo (200; 16.366998ms)
Mar 17 10:10:16.362: INFO: (19) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname1/proxy/: foo (200; 17.474757ms)
Mar 17 10:10:16.362: INFO: (19) /api/v1/namespaces/proxy-1887/services/proxy-service-ch56z:portname2/proxy/: bar (200; 16.690699ms)
STEP: deleting ReplicationController proxy-service-ch56z in namespace proxy-1887, will wait for the garbage collector to delete the pods
Mar 17 10:10:16.422: INFO: Deleting ReplicationController proxy-service-ch56z took: 6.637494ms
Mar 17 10:10:16.523: INFO: Terminating ReplicationController proxy-service-ch56z pods took: 100.805712ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:18.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1887" for this suite.

• [SLOW TEST:5.053 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":280,"skipped":5623,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:18.752: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Mar 17 10:10:18.814: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-4032 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:18.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4032" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":281,"skipped":5623,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:18.877: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 17 10:10:19.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:10:19.052: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:10:20.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:10:20.061: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:10:21.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 10:10:21.071: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:10:22.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 10:10:22.064: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 17 10:10:22.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 10:10:22.094: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:10:23.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 10:10:23.106: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:10:24.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 10:10:24.102: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:10:25.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 17 10:10:25.103: INFO: Node ip-172-31-33-68 is running 0 daemon pod, expected 1
Mar 17 10:10:26.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 17 10:10:26.105: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9059, will wait for the garbage collector to delete the pods
Mar 17 10:10:26.173: INFO: Deleting DaemonSet.extensions daemon-set took: 8.579772ms
Mar 17 10:10:26.274: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.898783ms
Mar 17 10:10:28.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 17 10:10:28.379: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 17 10:10:28.382: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"82187"},"items":null}

Mar 17 10:10:28.386: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"82187"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:28.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9059" for this suite.

• [SLOW TEST:9.533 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":282,"skipped":5640,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:28.411: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar 17 10:10:28.458: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:32.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9035" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":283,"skipped":5654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:32.343: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 17 10:10:33.075: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 10:10:36.099: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:10:36.105: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7082" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.284 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":284,"skipped":5678,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:39.627: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:10:39.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-616c5222-6022-47c2-a5e2-15cfb71e12b7" in namespace "projected-8695" to be "Succeeded or Failed"
Mar 17 10:10:39.727: INFO: Pod "downwardapi-volume-616c5222-6022-47c2-a5e2-15cfb71e12b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.784731ms
Mar 17 10:10:41.734: INFO: Pod "downwardapi-volume-616c5222-6022-47c2-a5e2-15cfb71e12b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012322111s
Mar 17 10:10:43.737: INFO: Pod "downwardapi-volume-616c5222-6022-47c2-a5e2-15cfb71e12b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015705287s
STEP: Saw pod success
Mar 17 10:10:43.737: INFO: Pod "downwardapi-volume-616c5222-6022-47c2-a5e2-15cfb71e12b7" satisfied condition "Succeeded or Failed"
Mar 17 10:10:43.740: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-616c5222-6022-47c2-a5e2-15cfb71e12b7 container client-container: <nil>
STEP: delete the pod
Mar 17 10:10:43.770: INFO: Waiting for pod downwardapi-volume-616c5222-6022-47c2-a5e2-15cfb71e12b7 to disappear
Mar 17 10:10:43.772: INFO: Pod downwardapi-volume-616c5222-6022-47c2-a5e2-15cfb71e12b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:43.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8695" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:43.786: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Mar 17 10:10:43.862: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:10:45.866: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar 17 10:10:45.891: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:10:47.895: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar 17 10:10:47.903: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 17 10:10:47.909: INFO: Pod pod-with-prestop-http-hook still exists
Mar 17 10:10:49.910: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 17 10:10:49.915: INFO: Pod pod-with-prestop-http-hook still exists
Mar 17 10:10:51.910: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 17 10:10:51.914: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:10:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1392" for this suite.

• [SLOW TEST:8.158 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":286,"skipped":5717,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:10:51.944: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:10:52.013: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 17 10:10:55.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-2723 --namespace=crd-publish-openapi-2723 create -f -'
Mar 17 10:10:56.067: INFO: stderr: ""
Mar 17 10:10:56.067: INFO: stdout: "e2e-test-crd-publish-openapi-5011-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 17 10:10:56.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-2723 --namespace=crd-publish-openapi-2723 delete e2e-test-crd-publish-openapi-5011-crds test-cr'
Mar 17 10:10:56.133: INFO: stderr: ""
Mar 17 10:10:56.133: INFO: stdout: "e2e-test-crd-publish-openapi-5011-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 17 10:10:56.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-2723 --namespace=crd-publish-openapi-2723 apply -f -'
Mar 17 10:10:56.332: INFO: stderr: ""
Mar 17 10:10:56.332: INFO: stdout: "e2e-test-crd-publish-openapi-5011-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 17 10:10:56.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-2723 --namespace=crd-publish-openapi-2723 delete e2e-test-crd-publish-openapi-5011-crds test-cr'
Mar 17 10:10:56.397: INFO: stderr: ""
Mar 17 10:10:56.397: INFO: stdout: "e2e-test-crd-publish-openapi-5011-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 17 10:10:56.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=crd-publish-openapi-2723 explain e2e-test-crd-publish-openapi-5011-crds'
Mar 17 10:10:56.575: INFO: stderr: ""
Mar 17 10:10:56.575: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5011-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:11:00.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2723" for this suite.

• [SLOW TEST:8.347 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":287,"skipped":5738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:11:00.293: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Mar 17 10:11:00.406: INFO: Waiting up to 5m0s for pod "var-expansion-3598d042-7cd7-4dfa-b9fc-a0c990c0716e" in namespace "var-expansion-934" to be "Succeeded or Failed"
Mar 17 10:11:00.414: INFO: Pod "var-expansion-3598d042-7cd7-4dfa-b9fc-a0c990c0716e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.70914ms
Mar 17 10:11:02.419: INFO: Pod "var-expansion-3598d042-7cd7-4dfa-b9fc-a0c990c0716e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012959781s
Mar 17 10:11:04.427: INFO: Pod "var-expansion-3598d042-7cd7-4dfa-b9fc-a0c990c0716e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021022057s
STEP: Saw pod success
Mar 17 10:11:04.428: INFO: Pod "var-expansion-3598d042-7cd7-4dfa-b9fc-a0c990c0716e" satisfied condition "Succeeded or Failed"
Mar 17 10:11:04.438: INFO: Trying to get logs from node ip-172-31-35-106 pod var-expansion-3598d042-7cd7-4dfa-b9fc-a0c990c0716e container dapi-container: <nil>
STEP: delete the pod
Mar 17 10:11:04.472: INFO: Waiting for pod var-expansion-3598d042-7cd7-4dfa-b9fc-a0c990c0716e to disappear
Mar 17 10:11:04.475: INFO: Pod var-expansion-3598d042-7cd7-4dfa-b9fc-a0c990c0716e no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:11:04.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-934" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":288,"skipped":5768,"failed":0}
SSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:11:04.487: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Mar 17 10:11:06.643: INFO: running pods: 0 < 3
Mar 17 10:11:08.649: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:11:10.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2506" for this suite.

• [SLOW TEST:6.171 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":289,"skipped":5771,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:11:10.659: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8155
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-8155
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8155
Mar 17 10:11:10.716: INFO: Found 0 stateful pods, waiting for 1
Mar 17 10:11:20.725: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 17 10:11:20.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-8155 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 10:11:20.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 10:11:20.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 10:11:20.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 10:11:20.923: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 17 10:11:30.927: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 17 10:11:30.927: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 10:11:30.939: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Mar 17 10:11:30.939: INFO: ss-0  ip-172-31-35-106  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:10 +0000 UTC  }]
Mar 17 10:11:30.939: INFO: 
Mar 17 10:11:30.939: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 17 10:11:31.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996900188s
Mar 17 10:11:32.958: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983498702s
Mar 17 10:11:33.965: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978090056s
Mar 17 10:11:34.969: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970729366s
Mar 17 10:11:35.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966345135s
Mar 17 10:11:36.985: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954266071s
Mar 17 10:11:37.991: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950593107s
Mar 17 10:11:38.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944774786s
Mar 17 10:11:40.005: INFO: Verifying statefulset ss doesn't scale past 3 for another 938.851159ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8155
Mar 17 10:11:41.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-8155 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 10:11:41.205: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 17 10:11:41.205: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 10:11:41.205: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 10:11:41.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-8155 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 10:11:41.374: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 17 10:11:41.374: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 10:11:41.374: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 10:11:41.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-8155 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 10:11:41.617: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 17 10:11:41.617: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 10:11:41.617: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 10:11:41.622: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 17 10:11:51.628: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 10:11:51.628: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 10:11:51.628: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 17 10:11:51.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-8155 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 10:11:51.818: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 10:11:51.818: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 10:11:51.818: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 10:11:51.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-8155 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 10:11:51.975: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 10:11:51.975: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 10:11:51.975: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 10:11:51.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-8155 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 10:11:52.156: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 10:11:52.156: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 10:11:52.156: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 10:11:52.156: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 10:11:52.159: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 17 10:12:02.166: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 17 10:12:02.166: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 17 10:12:02.166: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 17 10:12:02.177: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Mar 17 10:12:02.177: INFO: ss-0  ip-172-31-35-106  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:10 +0000 UTC  }]
Mar 17 10:12:02.177: INFO: ss-1  ip-172-31-39-36   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:30 +0000 UTC  }]
Mar 17 10:12:02.177: INFO: ss-2  ip-172-31-33-68   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:30 +0000 UTC  }]
Mar 17 10:12:02.177: INFO: 
Mar 17 10:12:02.177: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 17 10:12:03.182: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 17 10:12:03.182: INFO: ss-2  ip-172-31-33-68  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-17 10:11:30 +0000 UTC  }]
Mar 17 10:12:03.182: INFO: 
Mar 17 10:12:03.182: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 17 10:12:04.185: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.991953537s
Mar 17 10:12:05.191: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98820383s
Mar 17 10:12:06.197: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981731979s
Mar 17 10:12:07.202: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97727416s
Mar 17 10:12:08.206: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972016594s
Mar 17 10:12:09.210: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968145962s
Mar 17 10:12:10.215: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.963579953s
Mar 17 10:12:11.221: INFO: Verifying statefulset ss doesn't scale past 0 for another 958.743465ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8155
Mar 17 10:12:12.225: INFO: Scaling statefulset ss to 0
Mar 17 10:12:12.238: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Mar 17 10:12:12.241: INFO: Deleting all statefulset in ns statefulset-8155
Mar 17 10:12:12.243: INFO: Scaling statefulset ss to 0
Mar 17 10:12:12.251: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 10:12:12.253: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:12.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8155" for this suite.

• [SLOW TEST:61.625 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":290,"skipped":5779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:12.284: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5564
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5564
STEP: creating replication controller externalsvc in namespace services-5564
I0317 10:12:12.450308      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5564, replica count: 2
I0317 10:12:15.501526      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 17 10:12:15.519: INFO: Creating new exec pod
Mar 17 10:12:19.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=services-5564 exec execpodp9s7h -- /bin/sh -x -c nslookup nodeport-service.services-5564.svc.cluster.local'
Mar 17 10:12:19.766: INFO: stderr: "+ nslookup nodeport-service.services-5564.svc.cluster.local\n"
Mar 17 10:12:19.766: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nnodeport-service.services-5564.svc.cluster.local\tcanonical name = externalsvc.services-5564.svc.cluster.local.\nName:\texternalsvc.services-5564.svc.cluster.local\nAddress: 10.43.126.21\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5564, will wait for the garbage collector to delete the pods
Mar 17 10:12:19.826: INFO: Deleting ReplicationController externalsvc took: 5.503644ms
Mar 17 10:12:19.926: INFO: Terminating ReplicationController externalsvc pods took: 100.910142ms
Mar 17 10:12:22.048: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:22.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5564" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.798 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":291,"skipped":5812,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:22.082: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar 17 10:12:23.196: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0317 10:12:23.196841      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:23.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2063" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":292,"skipped":5832,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:23.203: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:12:23.272: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"70301579-3487-4cc9-8783-28399a070169", Controller:(*bool)(0xc009743a66), BlockOwnerDeletion:(*bool)(0xc009743a67)}}
Mar 17 10:12:23.286: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2a0ee1f0-d912-4feb-8c7a-36efba1838bd", Controller:(*bool)(0xc009743cde), BlockOwnerDeletion:(*bool)(0xc009743cdf)}}
Mar 17 10:12:23.300: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d1d51840-69de-41e3-921a-570e89bdd416", Controller:(*bool)(0xc009743f56), BlockOwnerDeletion:(*bool)(0xc009743f57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:28.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2284" for this suite.

• [SLOW TEST:5.125 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":293,"skipped":5838,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:28.328: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:12:28.412: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a85fc5a-be53-4bdd-a56b-18f3771a32b2" in namespace "projected-5649" to be "Succeeded or Failed"
Mar 17 10:12:28.430: INFO: Pod "downwardapi-volume-9a85fc5a-be53-4bdd-a56b-18f3771a32b2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.54061ms
Mar 17 10:12:30.438: INFO: Pod "downwardapi-volume-9a85fc5a-be53-4bdd-a56b-18f3771a32b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026119369s
STEP: Saw pod success
Mar 17 10:12:30.438: INFO: Pod "downwardapi-volume-9a85fc5a-be53-4bdd-a56b-18f3771a32b2" satisfied condition "Succeeded or Failed"
Mar 17 10:12:30.443: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-9a85fc5a-be53-4bdd-a56b-18f3771a32b2 container client-container: <nil>
STEP: delete the pod
Mar 17 10:12:30.471: INFO: Waiting for pod downwardapi-volume-9a85fc5a-be53-4bdd-a56b-18f3771a32b2 to disappear
Mar 17 10:12:30.479: INFO: Pod downwardapi-volume-9a85fc5a-be53-4bdd-a56b-18f3771a32b2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:30.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5649" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5844,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:30.511: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:30.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5347" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":295,"skipped":5854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:30.618: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:30.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8095" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":296,"skipped":5912,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:30.745: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Mar 17 10:12:30.837: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Mar 17 10:12:32.859: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Mar 17 10:12:34.875: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:36.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-7150" for this suite.

• [SLOW TEST:6.144 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":297,"skipped":5924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:36.889: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2502
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2502
Mar 17 10:12:36.945: INFO: Found 0 stateful pods, waiting for 1
Mar 17 10:12:46.949: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Mar 17 10:12:46.989: INFO: Deleting all statefulset in ns statefulset-2502
Mar 17 10:12:46.997: INFO: Scaling statefulset ss to 0
Mar 17 10:12:57.070: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 10:12:57.073: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:57.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2502" for this suite.

• [SLOW TEST:20.214 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":298,"skipped":5958,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:57.104: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Mar 17 10:12:57.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-1065 create -f -'
Mar 17 10:12:58.221: INFO: stderr: ""
Mar 17 10:12:58.221: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar 17 10:12:58.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-1065 diff -f -'
Mar 17 10:12:58.401: INFO: rc: 1
Mar 17 10:12:58.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-1065 delete -f -'
Mar 17 10:12:58.460: INFO: stderr: ""
Mar 17 10:12:58.460: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:12:58.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1065" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":299,"skipped":6006,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:12:58.471: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:12:58.531: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4732a464-6249-4a71-801c-8c228bf0f8a7" in namespace "security-context-test-6850" to be "Succeeded or Failed"
Mar 17 10:12:58.537: INFO: Pod "busybox-readonly-false-4732a464-6249-4a71-801c-8c228bf0f8a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.603645ms
Mar 17 10:13:00.545: INFO: Pod "busybox-readonly-false-4732a464-6249-4a71-801c-8c228bf0f8a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014030075s
Mar 17 10:13:00.545: INFO: Pod "busybox-readonly-false-4732a464-6249-4a71-801c-8c228bf0f8a7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:13:00.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6850" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":6010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:13:00.558: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:00.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9047" for this suite.

• [SLOW TEST:120.083 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":301,"skipped":6046,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:00.640: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-0cd74941-4431-4ccd-a0d5-d24fae01dca9
STEP: Creating a pod to test consume secrets
Mar 17 10:15:00.722: INFO: Waiting up to 5m0s for pod "pod-secrets-90db64f3-b1da-4b1d-b0b3-d2634876ed90" in namespace "secrets-7236" to be "Succeeded or Failed"
Mar 17 10:15:00.752: INFO: Pod "pod-secrets-90db64f3-b1da-4b1d-b0b3-d2634876ed90": Phase="Pending", Reason="", readiness=false. Elapsed: 30.109886ms
Mar 17 10:15:02.759: INFO: Pod "pod-secrets-90db64f3-b1da-4b1d-b0b3-d2634876ed90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036985212s
STEP: Saw pod success
Mar 17 10:15:02.760: INFO: Pod "pod-secrets-90db64f3-b1da-4b1d-b0b3-d2634876ed90" satisfied condition "Succeeded or Failed"
Mar 17 10:15:02.763: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-secrets-90db64f3-b1da-4b1d-b0b3-d2634876ed90 container secret-volume-test: <nil>
STEP: delete the pod
Mar 17 10:15:02.813: INFO: Waiting for pod pod-secrets-90db64f3-b1da-4b1d-b0b3-d2634876ed90 to disappear
Mar 17 10:15:02.825: INFO: Pod pod-secrets-90db64f3-b1da-4b1d-b0b3-d2634876ed90 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:02.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7236" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":6050,"failed":0}
SSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:02.848: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:15:02.907: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3322
I0317 10:15:02.920390      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3322, replica count: 1
I0317 10:15:03.972202      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0317 10:15:04.973192      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 17 10:15:05.083: INFO: Created: latency-svc-6b9d2
Mar 17 10:15:05.097: INFO: Got endpoints: latency-svc-6b9d2 [23.657265ms]
Mar 17 10:15:05.118: INFO: Created: latency-svc-x54vs
Mar 17 10:15:05.128: INFO: Got endpoints: latency-svc-x54vs [30.757424ms]
Mar 17 10:15:05.132: INFO: Created: latency-svc-zjv8f
Mar 17 10:15:05.135: INFO: Created: latency-svc-275kp
Mar 17 10:15:05.141: INFO: Created: latency-svc-pn79m
Mar 17 10:15:05.153: INFO: Got endpoints: latency-svc-zjv8f [55.204369ms]
Mar 17 10:15:05.153: INFO: Got endpoints: latency-svc-275kp [55.103071ms]
Mar 17 10:15:05.158: INFO: Created: latency-svc-dxmph
Mar 17 10:15:05.166: INFO: Got endpoints: latency-svc-pn79m [68.713301ms]
Mar 17 10:15:05.167: INFO: Got endpoints: latency-svc-dxmph [68.983041ms]
Mar 17 10:15:05.171: INFO: Created: latency-svc-lrb77
Mar 17 10:15:05.181: INFO: Got endpoints: latency-svc-lrb77 [83.687274ms]
Mar 17 10:15:05.196: INFO: Created: latency-svc-6lpdr
Mar 17 10:15:05.202: INFO: Got endpoints: latency-svc-6lpdr [104.086087ms]
Mar 17 10:15:05.206: INFO: Created: latency-svc-4xt8m
Mar 17 10:15:05.215: INFO: Got endpoints: latency-svc-4xt8m [117.307119ms]
Mar 17 10:15:05.299: INFO: Created: latency-svc-rx5rq
Mar 17 10:15:05.307: INFO: Created: latency-svc-f769s
Mar 17 10:15:05.307: INFO: Created: latency-svc-4llpn
Mar 17 10:15:05.312: INFO: Created: latency-svc-tgskk
Mar 17 10:15:05.312: INFO: Created: latency-svc-7c9b2
Mar 17 10:15:05.316: INFO: Created: latency-svc-g4lxt
Mar 17 10:15:05.322: INFO: Created: latency-svc-wbjhz
Mar 17 10:15:05.322: INFO: Created: latency-svc-6d9z6
Mar 17 10:15:05.322: INFO: Created: latency-svc-vkr5x
Mar 17 10:15:05.322: INFO: Created: latency-svc-5r7lq
Mar 17 10:15:05.322: INFO: Created: latency-svc-8qpnn
Mar 17 10:15:05.326: INFO: Created: latency-svc-ql6cs
Mar 17 10:15:05.326: INFO: Created: latency-svc-jn9m8
Mar 17 10:15:05.328: INFO: Created: latency-svc-h2xnx
Mar 17 10:15:05.328: INFO: Created: latency-svc-w7xhz
Mar 17 10:15:05.330: INFO: Got endpoints: latency-svc-jn9m8 [148.542569ms]
Mar 17 10:15:05.345: INFO: Got endpoints: latency-svc-6d9z6 [246.704988ms]
Mar 17 10:15:05.345: INFO: Got endpoints: latency-svc-rx5rq [216.241906ms]
Mar 17 10:15:05.347: INFO: Got endpoints: latency-svc-w7xhz [249.164705ms]
Mar 17 10:15:05.352: INFO: Got endpoints: latency-svc-8qpnn [136.470778ms]
Mar 17 10:15:05.364: INFO: Created: latency-svc-8g422
Mar 17 10:15:05.367: INFO: Got endpoints: latency-svc-ql6cs [268.721424ms]
Mar 17 10:15:05.367: INFO: Got endpoints: latency-svc-f769s [164.84712ms]
Mar 17 10:15:05.367: INFO: Got endpoints: latency-svc-h2xnx [269.240568ms]
Mar 17 10:15:05.379: INFO: Got endpoints: latency-svc-5r7lq [281.399375ms]
Mar 17 10:15:05.409: INFO: Got endpoints: latency-svc-8g422 [78.546956ms]
Mar 17 10:15:05.409: INFO: Got endpoints: latency-svc-g4lxt [255.693611ms]
Mar 17 10:15:05.409: INFO: Got endpoints: latency-svc-wbjhz [255.955765ms]
Mar 17 10:15:05.409: INFO: Got endpoints: latency-svc-vkr5x [311.041825ms]
Mar 17 10:15:05.409: INFO: Created: latency-svc-pbtj2
Mar 17 10:15:05.415: INFO: Got endpoints: latency-svc-7c9b2 [248.476562ms]
Mar 17 10:15:05.415: INFO: Got endpoints: latency-svc-tgskk [248.241989ms]
Mar 17 10:15:05.418: INFO: Got endpoints: latency-svc-pbtj2 [70.409822ms]
Mar 17 10:15:05.418: INFO: Got endpoints: latency-svc-4llpn [319.914707ms]
Mar 17 10:15:05.465: INFO: Created: latency-svc-7cdvt
Mar 17 10:15:05.465: INFO: Created: latency-svc-w5hg8
Mar 17 10:15:05.465: INFO: Created: latency-svc-w8nps
Mar 17 10:15:05.465: INFO: Created: latency-svc-vv2c9
Mar 17 10:15:05.465: INFO: Created: latency-svc-tsmhl
Mar 17 10:15:05.469: INFO: Created: latency-svc-9x6gc
Mar 17 10:15:05.483: INFO: Created: latency-svc-bg6p2
Mar 17 10:15:05.483: INFO: Created: latency-svc-6wgwp
Mar 17 10:15:05.483: INFO: Created: latency-svc-zvl8j
Mar 17 10:15:05.483: INFO: Created: latency-svc-tm6f8
Mar 17 10:15:05.483: INFO: Created: latency-svc-95krt
Mar 17 10:15:05.483: INFO: Created: latency-svc-z8bn7
Mar 17 10:15:05.483: INFO: Created: latency-svc-4rspn
Mar 17 10:15:05.483: INFO: Created: latency-svc-h5wzg
Mar 17 10:15:05.483: INFO: Created: latency-svc-sdnvs
Mar 17 10:15:05.495: INFO: Got endpoints: latency-svc-tsmhl [150.059388ms]
Mar 17 10:15:05.505: INFO: Got endpoints: latency-svc-vv2c9 [138.680122ms]
Mar 17 10:15:05.544: INFO: Created: latency-svc-nqpjf
Mar 17 10:15:05.544: INFO: Got endpoints: latency-svc-w5hg8 [135.477259ms]
Mar 17 10:15:05.545: INFO: Got endpoints: latency-svc-9x6gc [135.818729ms]
Mar 17 10:15:05.545: INFO: Got endpoints: latency-svc-sdnvs [200.080274ms]
Mar 17 10:15:05.544: INFO: Got endpoints: latency-svc-bg6p2 [177.553053ms]
Mar 17 10:15:05.545: INFO: Got endpoints: latency-svc-6wgwp [193.313494ms]
Mar 17 10:15:05.545: INFO: Got endpoints: latency-svc-7cdvt [165.715998ms]
Mar 17 10:15:05.545: INFO: Got endpoints: latency-svc-w8nps [136.155411ms]
Mar 17 10:15:05.545: INFO: Got endpoints: latency-svc-95krt [130.099236ms]
Mar 17 10:15:05.545: INFO: Got endpoints: latency-svc-z8bn7 [127.637103ms]
Mar 17 10:15:05.545: INFO: Got endpoints: latency-svc-4rspn [178.672636ms]
Mar 17 10:15:05.550: INFO: Got endpoints: latency-svc-h5wzg [132.675174ms]
Mar 17 10:15:05.566: INFO: Created: latency-svc-dwf6d
Mar 17 10:15:05.572: INFO: Created: latency-svc-jtkgz
Mar 17 10:15:05.581: INFO: Created: latency-svc-6ppbr
Mar 17 10:15:05.588: INFO: Created: latency-svc-qpwhv
Mar 17 10:15:05.593: INFO: Got endpoints: latency-svc-zvl8j [177.551397ms]
Mar 17 10:15:05.602: INFO: Created: latency-svc-wssd4
Mar 17 10:15:05.607: INFO: Created: latency-svc-rsqgv
Mar 17 10:15:05.613: INFO: Created: latency-svc-sx5wj
Mar 17 10:15:05.614: INFO: Created: latency-svc-dq4qq
Mar 17 10:15:05.620: INFO: Created: latency-svc-nrzs6
Mar 17 10:15:05.625: INFO: Created: latency-svc-qf95p
Mar 17 10:15:05.631: INFO: Created: latency-svc-kjcgg
Mar 17 10:15:05.644: INFO: Created: latency-svc-7xdjx
Mar 17 10:15:05.644: INFO: Got endpoints: latency-svc-tm6f8 [235.712392ms]
Mar 17 10:15:05.646: INFO: Created: latency-svc-dfgpx
Mar 17 10:15:05.658: INFO: Created: latency-svc-bc69h
Mar 17 10:15:05.691: INFO: Got endpoints: latency-svc-nqpjf [196.508673ms]
Mar 17 10:15:05.703: INFO: Created: latency-svc-k885l
Mar 17 10:15:05.744: INFO: Got endpoints: latency-svc-dwf6d [238.680052ms]
Mar 17 10:15:05.753: INFO: Created: latency-svc-9zg48
Mar 17 10:15:05.796: INFO: Got endpoints: latency-svc-jtkgz [251.988415ms]
Mar 17 10:15:05.811: INFO: Created: latency-svc-9k4sr
Mar 17 10:15:05.844: INFO: Got endpoints: latency-svc-6ppbr [299.519769ms]
Mar 17 10:15:05.875: INFO: Created: latency-svc-s6xcp
Mar 17 10:15:05.899: INFO: Got endpoints: latency-svc-qpwhv [354.338756ms]
Mar 17 10:15:05.924: INFO: Created: latency-svc-hntnr
Mar 17 10:15:05.950: INFO: Got endpoints: latency-svc-wssd4 [404.820552ms]
Mar 17 10:15:05.974: INFO: Created: latency-svc-7b4lc
Mar 17 10:15:05.991: INFO: Got endpoints: latency-svc-rsqgv [445.621213ms]
Mar 17 10:15:06.006: INFO: Created: latency-svc-fbjmd
Mar 17 10:15:06.065: INFO: Got endpoints: latency-svc-sx5wj [520.109151ms]
Mar 17 10:15:06.083: INFO: Created: latency-svc-659xw
Mar 17 10:15:06.108: INFO: Got endpoints: latency-svc-dq4qq [563.463959ms]
Mar 17 10:15:06.126: INFO: Created: latency-svc-c9x8g
Mar 17 10:15:06.138: INFO: Got endpoints: latency-svc-nrzs6 [592.753641ms]
Mar 17 10:15:06.155: INFO: Created: latency-svc-9b6ln
Mar 17 10:15:06.191: INFO: Got endpoints: latency-svc-qf95p [645.225212ms]
Mar 17 10:15:06.197: INFO: Created: latency-svc-km8sl
Mar 17 10:15:06.241: INFO: Got endpoints: latency-svc-kjcgg [695.54667ms]
Mar 17 10:15:06.249: INFO: Created: latency-svc-r87kt
Mar 17 10:15:06.288: INFO: Got endpoints: latency-svc-dfgpx [737.454988ms]
Mar 17 10:15:06.318: INFO: Created: latency-svc-6h85r
Mar 17 10:15:06.340: INFO: Got endpoints: latency-svc-7xdjx [747.249804ms]
Mar 17 10:15:06.350: INFO: Created: latency-svc-vpfb5
Mar 17 10:15:06.391: INFO: Got endpoints: latency-svc-bc69h [746.857289ms]
Mar 17 10:15:06.398: INFO: Created: latency-svc-qpqf4
Mar 17 10:15:06.438: INFO: Got endpoints: latency-svc-k885l [746.877152ms]
Mar 17 10:15:06.446: INFO: Created: latency-svc-8wltw
Mar 17 10:15:06.488: INFO: Got endpoints: latency-svc-9zg48 [744.123322ms]
Mar 17 10:15:06.497: INFO: Created: latency-svc-996dv
Mar 17 10:15:06.540: INFO: Got endpoints: latency-svc-9k4sr [743.709182ms]
Mar 17 10:15:06.548: INFO: Created: latency-svc-n9q9j
Mar 17 10:15:06.591: INFO: Got endpoints: latency-svc-s6xcp [747.076272ms]
Mar 17 10:15:06.599: INFO: Created: latency-svc-h8gg4
Mar 17 10:15:06.640: INFO: Got endpoints: latency-svc-hntnr [740.505932ms]
Mar 17 10:15:06.648: INFO: Created: latency-svc-6fk9t
Mar 17 10:15:06.687: INFO: Got endpoints: latency-svc-7b4lc [736.86526ms]
Mar 17 10:15:06.695: INFO: Created: latency-svc-mdkmm
Mar 17 10:15:06.738: INFO: Got endpoints: latency-svc-fbjmd [746.789258ms]
Mar 17 10:15:06.745: INFO: Created: latency-svc-r4pg4
Mar 17 10:15:06.788: INFO: Got endpoints: latency-svc-659xw [722.519349ms]
Mar 17 10:15:06.795: INFO: Created: latency-svc-bgnxt
Mar 17 10:15:06.838: INFO: Got endpoints: latency-svc-c9x8g [729.044967ms]
Mar 17 10:15:06.846: INFO: Created: latency-svc-qv8jn
Mar 17 10:15:06.887: INFO: Got endpoints: latency-svc-9b6ln [749.382127ms]
Mar 17 10:15:06.895: INFO: Created: latency-svc-jslsr
Mar 17 10:15:06.938: INFO: Got endpoints: latency-svc-km8sl [747.882004ms]
Mar 17 10:15:06.946: INFO: Created: latency-svc-kd5p5
Mar 17 10:15:06.988: INFO: Got endpoints: latency-svc-r87kt [746.51272ms]
Mar 17 10:15:06.998: INFO: Created: latency-svc-d2c2d
Mar 17 10:15:07.041: INFO: Got endpoints: latency-svc-6h85r [753.373436ms]
Mar 17 10:15:07.048: INFO: Created: latency-svc-h2czk
Mar 17 10:15:07.088: INFO: Got endpoints: latency-svc-vpfb5 [747.584924ms]
Mar 17 10:15:07.095: INFO: Created: latency-svc-k5vsh
Mar 17 10:15:07.157: INFO: Got endpoints: latency-svc-qpqf4 [765.964677ms]
Mar 17 10:15:07.181: INFO: Created: latency-svc-p5tjn
Mar 17 10:15:07.201: INFO: Got endpoints: latency-svc-8wltw [762.355335ms]
Mar 17 10:15:07.227: INFO: Created: latency-svc-ntrhn
Mar 17 10:15:07.239: INFO: Got endpoints: latency-svc-996dv [750.511409ms]
Mar 17 10:15:07.256: INFO: Created: latency-svc-w9nxg
Mar 17 10:15:07.291: INFO: Got endpoints: latency-svc-n9q9j [750.469353ms]
Mar 17 10:15:07.300: INFO: Created: latency-svc-cc8wp
Mar 17 10:15:07.340: INFO: Got endpoints: latency-svc-h8gg4 [748.714024ms]
Mar 17 10:15:07.359: INFO: Created: latency-svc-9gdfw
Mar 17 10:15:07.391: INFO: Got endpoints: latency-svc-6fk9t [751.715299ms]
Mar 17 10:15:07.402: INFO: Created: latency-svc-67pc5
Mar 17 10:15:07.442: INFO: Got endpoints: latency-svc-mdkmm [754.135459ms]
Mar 17 10:15:07.453: INFO: Created: latency-svc-wwxs6
Mar 17 10:15:07.487: INFO: Got endpoints: latency-svc-r4pg4 [749.460525ms]
Mar 17 10:15:07.501: INFO: Created: latency-svc-4mf7q
Mar 17 10:15:07.540: INFO: Got endpoints: latency-svc-bgnxt [752.689077ms]
Mar 17 10:15:07.553: INFO: Created: latency-svc-n28r8
Mar 17 10:15:07.589: INFO: Got endpoints: latency-svc-qv8jn [751.22394ms]
Mar 17 10:15:07.610: INFO: Created: latency-svc-s296s
Mar 17 10:15:07.647: INFO: Got endpoints: latency-svc-jslsr [760.033876ms]
Mar 17 10:15:07.667: INFO: Created: latency-svc-9tmc4
Mar 17 10:15:07.696: INFO: Got endpoints: latency-svc-kd5p5 [757.578647ms]
Mar 17 10:15:07.712: INFO: Created: latency-svc-t7bxr
Mar 17 10:15:07.738: INFO: Got endpoints: latency-svc-d2c2d [750.854463ms]
Mar 17 10:15:07.762: INFO: Created: latency-svc-8tznk
Mar 17 10:15:07.791: INFO: Got endpoints: latency-svc-h2czk [749.524046ms]
Mar 17 10:15:07.811: INFO: Created: latency-svc-rdcsn
Mar 17 10:15:07.845: INFO: Got endpoints: latency-svc-k5vsh [756.511239ms]
Mar 17 10:15:07.857: INFO: Created: latency-svc-flzlz
Mar 17 10:15:07.893: INFO: Got endpoints: latency-svc-p5tjn [735.90546ms]
Mar 17 10:15:07.906: INFO: Created: latency-svc-cf7lp
Mar 17 10:15:07.949: INFO: Got endpoints: latency-svc-ntrhn [748.094654ms]
Mar 17 10:15:07.982: INFO: Created: latency-svc-n5xvg
Mar 17 10:15:08.002: INFO: Got endpoints: latency-svc-w9nxg [763.356475ms]
Mar 17 10:15:08.021: INFO: Created: latency-svc-7zppq
Mar 17 10:15:08.042: INFO: Got endpoints: latency-svc-cc8wp [751.669566ms]
Mar 17 10:15:08.058: INFO: Created: latency-svc-jg5f4
Mar 17 10:15:08.089: INFO: Got endpoints: latency-svc-9gdfw [749.036817ms]
Mar 17 10:15:08.107: INFO: Created: latency-svc-wrfzp
Mar 17 10:15:08.144: INFO: Got endpoints: latency-svc-67pc5 [752.93331ms]
Mar 17 10:15:08.154: INFO: Created: latency-svc-qv6kg
Mar 17 10:15:08.191: INFO: Got endpoints: latency-svc-wwxs6 [749.796452ms]
Mar 17 10:15:08.206: INFO: Created: latency-svc-n55t4
Mar 17 10:15:08.239: INFO: Got endpoints: latency-svc-4mf7q [751.989956ms]
Mar 17 10:15:08.277: INFO: Created: latency-svc-sztdw
Mar 17 10:15:08.295: INFO: Got endpoints: latency-svc-n28r8 [755.016269ms]
Mar 17 10:15:08.303: INFO: Created: latency-svc-vn5ld
Mar 17 10:15:08.337: INFO: Got endpoints: latency-svc-s296s [747.67091ms]
Mar 17 10:15:08.355: INFO: Created: latency-svc-txxm5
Mar 17 10:15:08.392: INFO: Got endpoints: latency-svc-9tmc4 [743.910971ms]
Mar 17 10:15:08.412: INFO: Created: latency-svc-cszdq
Mar 17 10:15:08.443: INFO: Got endpoints: latency-svc-t7bxr [746.257012ms]
Mar 17 10:15:08.453: INFO: Created: latency-svc-fppd4
Mar 17 10:15:08.491: INFO: Got endpoints: latency-svc-8tznk [752.056076ms]
Mar 17 10:15:08.497: INFO: Created: latency-svc-67wbm
Mar 17 10:15:08.539: INFO: Got endpoints: latency-svc-rdcsn [747.600066ms]
Mar 17 10:15:08.568: INFO: Created: latency-svc-v6nhh
Mar 17 10:15:08.594: INFO: Got endpoints: latency-svc-flzlz [749.731673ms]
Mar 17 10:15:08.601: INFO: Created: latency-svc-phwdn
Mar 17 10:15:08.640: INFO: Got endpoints: latency-svc-cf7lp [746.756838ms]
Mar 17 10:15:08.652: INFO: Created: latency-svc-cn6q7
Mar 17 10:15:08.690: INFO: Got endpoints: latency-svc-n5xvg [740.688658ms]
Mar 17 10:15:08.703: INFO: Created: latency-svc-6dzm6
Mar 17 10:15:08.739: INFO: Got endpoints: latency-svc-7zppq [735.928676ms]
Mar 17 10:15:08.754: INFO: Created: latency-svc-j4jmz
Mar 17 10:15:08.789: INFO: Got endpoints: latency-svc-jg5f4 [746.493119ms]
Mar 17 10:15:08.796: INFO: Created: latency-svc-92zhl
Mar 17 10:15:08.839: INFO: Got endpoints: latency-svc-wrfzp [749.804029ms]
Mar 17 10:15:08.848: INFO: Created: latency-svc-p6qdp
Mar 17 10:15:08.892: INFO: Got endpoints: latency-svc-qv6kg [747.154259ms]
Mar 17 10:15:08.908: INFO: Created: latency-svc-brl99
Mar 17 10:15:08.942: INFO: Got endpoints: latency-svc-n55t4 [750.416479ms]
Mar 17 10:15:08.952: INFO: Created: latency-svc-vrhl4
Mar 17 10:15:08.987: INFO: Got endpoints: latency-svc-sztdw [747.815041ms]
Mar 17 10:15:08.997: INFO: Created: latency-svc-fx2lw
Mar 17 10:15:09.039: INFO: Got endpoints: latency-svc-vn5ld [743.467351ms]
Mar 17 10:15:09.049: INFO: Created: latency-svc-6g9t6
Mar 17 10:15:09.089: INFO: Got endpoints: latency-svc-txxm5 [752.453773ms]
Mar 17 10:15:09.100: INFO: Created: latency-svc-7xh8x
Mar 17 10:15:09.142: INFO: Got endpoints: latency-svc-cszdq [749.963524ms]
Mar 17 10:15:09.152: INFO: Created: latency-svc-brx5p
Mar 17 10:15:09.188: INFO: Got endpoints: latency-svc-fppd4 [745.517858ms]
Mar 17 10:15:09.198: INFO: Created: latency-svc-2pj22
Mar 17 10:15:09.239: INFO: Got endpoints: latency-svc-67wbm [748.189522ms]
Mar 17 10:15:09.245: INFO: Created: latency-svc-wbmjr
Mar 17 10:15:09.289: INFO: Got endpoints: latency-svc-v6nhh [750.150943ms]
Mar 17 10:15:09.298: INFO: Created: latency-svc-v6ffb
Mar 17 10:15:09.339: INFO: Got endpoints: latency-svc-phwdn [744.488758ms]
Mar 17 10:15:09.346: INFO: Created: latency-svc-gmv25
Mar 17 10:15:09.389: INFO: Got endpoints: latency-svc-cn6q7 [748.832927ms]
Mar 17 10:15:09.396: INFO: Created: latency-svc-d5plk
Mar 17 10:15:09.438: INFO: Got endpoints: latency-svc-6dzm6 [748.488988ms]
Mar 17 10:15:09.447: INFO: Created: latency-svc-jg6sm
Mar 17 10:15:09.491: INFO: Got endpoints: latency-svc-j4jmz [751.938447ms]
Mar 17 10:15:09.501: INFO: Created: latency-svc-mxhts
Mar 17 10:15:09.537: INFO: Got endpoints: latency-svc-92zhl [748.257149ms]
Mar 17 10:15:09.546: INFO: Created: latency-svc-jzvtx
Mar 17 10:15:09.588: INFO: Got endpoints: latency-svc-p6qdp [748.405474ms]
Mar 17 10:15:09.596: INFO: Created: latency-svc-6q7z9
Mar 17 10:15:09.639: INFO: Got endpoints: latency-svc-brl99 [747.664029ms]
Mar 17 10:15:09.656: INFO: Created: latency-svc-l4zj8
Mar 17 10:15:09.690: INFO: Got endpoints: latency-svc-vrhl4 [748.061155ms]
Mar 17 10:15:09.696: INFO: Created: latency-svc-7mxm5
Mar 17 10:15:09.740: INFO: Got endpoints: latency-svc-fx2lw [752.361604ms]
Mar 17 10:15:09.750: INFO: Created: latency-svc-wtfnn
Mar 17 10:15:09.789: INFO: Got endpoints: latency-svc-6g9t6 [750.232967ms]
Mar 17 10:15:09.796: INFO: Created: latency-svc-pf4zd
Mar 17 10:15:09.839: INFO: Got endpoints: latency-svc-7xh8x [749.349666ms]
Mar 17 10:15:09.850: INFO: Created: latency-svc-w9bjm
Mar 17 10:15:09.889: INFO: Got endpoints: latency-svc-brx5p [747.29887ms]
Mar 17 10:15:09.898: INFO: Created: latency-svc-vhgvk
Mar 17 10:15:09.940: INFO: Got endpoints: latency-svc-2pj22 [752.023755ms]
Mar 17 10:15:09.949: INFO: Created: latency-svc-btgrk
Mar 17 10:15:09.988: INFO: Got endpoints: latency-svc-wbmjr [748.711705ms]
Mar 17 10:15:09.999: INFO: Created: latency-svc-xvzxs
Mar 17 10:15:10.039: INFO: Got endpoints: latency-svc-v6ffb [749.674915ms]
Mar 17 10:15:10.047: INFO: Created: latency-svc-hsfj6
Mar 17 10:15:10.089: INFO: Got endpoints: latency-svc-gmv25 [749.556963ms]
Mar 17 10:15:10.111: INFO: Created: latency-svc-s9kww
Mar 17 10:15:10.143: INFO: Got endpoints: latency-svc-d5plk [754.385335ms]
Mar 17 10:15:10.153: INFO: Created: latency-svc-s79v7
Mar 17 10:15:10.191: INFO: Got endpoints: latency-svc-jg6sm [752.677336ms]
Mar 17 10:15:10.204: INFO: Created: latency-svc-qr42c
Mar 17 10:15:10.239: INFO: Got endpoints: latency-svc-mxhts [748.462594ms]
Mar 17 10:15:10.247: INFO: Created: latency-svc-4pbtx
Mar 17 10:15:10.289: INFO: Got endpoints: latency-svc-jzvtx [751.401177ms]
Mar 17 10:15:10.301: INFO: Created: latency-svc-6d2cq
Mar 17 10:15:10.341: INFO: Got endpoints: latency-svc-6q7z9 [752.879051ms]
Mar 17 10:15:10.352: INFO: Created: latency-svc-wb5bs
Mar 17 10:15:10.388: INFO: Got endpoints: latency-svc-l4zj8 [748.707784ms]
Mar 17 10:15:10.401: INFO: Created: latency-svc-bdm84
Mar 17 10:15:10.451: INFO: Got endpoints: latency-svc-7mxm5 [761.020306ms]
Mar 17 10:15:10.468: INFO: Created: latency-svc-cp6p2
Mar 17 10:15:10.492: INFO: Got endpoints: latency-svc-wtfnn [752.487805ms]
Mar 17 10:15:10.502: INFO: Created: latency-svc-5tkbb
Mar 17 10:15:10.540: INFO: Got endpoints: latency-svc-pf4zd [750.654106ms]
Mar 17 10:15:10.549: INFO: Created: latency-svc-9b2pd
Mar 17 10:15:10.589: INFO: Got endpoints: latency-svc-w9bjm [750.373735ms]
Mar 17 10:15:10.601: INFO: Created: latency-svc-cqncj
Mar 17 10:15:10.642: INFO: Got endpoints: latency-svc-vhgvk [753.197493ms]
Mar 17 10:15:10.651: INFO: Created: latency-svc-b7tkb
Mar 17 10:15:10.692: INFO: Got endpoints: latency-svc-btgrk [751.99092ms]
Mar 17 10:15:10.699: INFO: Created: latency-svc-642rp
Mar 17 10:15:10.739: INFO: Got endpoints: latency-svc-xvzxs [751.052869ms]
Mar 17 10:15:10.768: INFO: Created: latency-svc-8cb96
Mar 17 10:15:10.792: INFO: Got endpoints: latency-svc-hsfj6 [752.745364ms]
Mar 17 10:15:10.805: INFO: Created: latency-svc-76cxf
Mar 17 10:15:10.841: INFO: Got endpoints: latency-svc-s9kww [752.334184ms]
Mar 17 10:15:10.854: INFO: Created: latency-svc-q47z7
Mar 17 10:15:10.889: INFO: Got endpoints: latency-svc-s79v7 [745.077162ms]
Mar 17 10:15:10.901: INFO: Created: latency-svc-g9thk
Mar 17 10:15:10.938: INFO: Got endpoints: latency-svc-qr42c [746.785921ms]
Mar 17 10:15:10.949: INFO: Created: latency-svc-zh94q
Mar 17 10:15:10.989: INFO: Got endpoints: latency-svc-4pbtx [749.038035ms]
Mar 17 10:15:10.999: INFO: Created: latency-svc-6s7n7
Mar 17 10:15:11.041: INFO: Got endpoints: latency-svc-6d2cq [751.521771ms]
Mar 17 10:15:11.051: INFO: Created: latency-svc-mxnbd
Mar 17 10:15:11.090: INFO: Got endpoints: latency-svc-wb5bs [749.156542ms]
Mar 17 10:15:11.108: INFO: Created: latency-svc-ppt52
Mar 17 10:15:11.140: INFO: Got endpoints: latency-svc-bdm84 [751.72873ms]
Mar 17 10:15:11.149: INFO: Created: latency-svc-gwjlt
Mar 17 10:15:11.189: INFO: Got endpoints: latency-svc-cp6p2 [737.84054ms]
Mar 17 10:15:11.238: INFO: Created: latency-svc-ztvkk
Mar 17 10:15:11.242: INFO: Got endpoints: latency-svc-5tkbb [749.488246ms]
Mar 17 10:15:11.251: INFO: Created: latency-svc-bwn2r
Mar 17 10:15:11.291: INFO: Got endpoints: latency-svc-9b2pd [751.24191ms]
Mar 17 10:15:11.302: INFO: Created: latency-svc-vqlzh
Mar 17 10:15:11.338: INFO: Got endpoints: latency-svc-cqncj [748.874033ms]
Mar 17 10:15:11.349: INFO: Created: latency-svc-twvvs
Mar 17 10:15:11.389: INFO: Got endpoints: latency-svc-b7tkb [746.45648ms]
Mar 17 10:15:11.398: INFO: Created: latency-svc-qzm5g
Mar 17 10:15:11.438: INFO: Got endpoints: latency-svc-642rp [745.961707ms]
Mar 17 10:15:11.454: INFO: Created: latency-svc-7ddth
Mar 17 10:15:11.492: INFO: Got endpoints: latency-svc-8cb96 [752.781492ms]
Mar 17 10:15:11.503: INFO: Created: latency-svc-vnr5l
Mar 17 10:15:11.541: INFO: Got endpoints: latency-svc-76cxf [749.116233ms]
Mar 17 10:15:11.549: INFO: Created: latency-svc-4tzbn
Mar 17 10:15:11.589: INFO: Got endpoints: latency-svc-q47z7 [747.676886ms]
Mar 17 10:15:11.597: INFO: Created: latency-svc-sg2tq
Mar 17 10:15:11.643: INFO: Got endpoints: latency-svc-g9thk [754.356534ms]
Mar 17 10:15:11.650: INFO: Created: latency-svc-qml5j
Mar 17 10:15:11.689: INFO: Got endpoints: latency-svc-zh94q [751.082468ms]
Mar 17 10:15:11.697: INFO: Created: latency-svc-zdplq
Mar 17 10:15:11.737: INFO: Got endpoints: latency-svc-6s7n7 [748.405402ms]
Mar 17 10:15:11.747: INFO: Created: latency-svc-752wl
Mar 17 10:15:11.787: INFO: Got endpoints: latency-svc-mxnbd [746.380386ms]
Mar 17 10:15:11.798: INFO: Created: latency-svc-wwjbx
Mar 17 10:15:11.841: INFO: Got endpoints: latency-svc-ppt52 [750.854709ms]
Mar 17 10:15:11.853: INFO: Created: latency-svc-xtp67
Mar 17 10:15:11.890: INFO: Got endpoints: latency-svc-gwjlt [749.658174ms]
Mar 17 10:15:11.899: INFO: Created: latency-svc-5qnd2
Mar 17 10:15:11.943: INFO: Got endpoints: latency-svc-ztvkk [753.447248ms]
Mar 17 10:15:11.952: INFO: Created: latency-svc-5pp68
Mar 17 10:15:11.999: INFO: Got endpoints: latency-svc-bwn2r [757.225238ms]
Mar 17 10:15:12.013: INFO: Created: latency-svc-bwfrl
Mar 17 10:15:12.043: INFO: Got endpoints: latency-svc-vqlzh [751.523471ms]
Mar 17 10:15:12.061: INFO: Created: latency-svc-g9lpz
Mar 17 10:15:12.088: INFO: Got endpoints: latency-svc-twvvs [749.464419ms]
Mar 17 10:15:12.108: INFO: Created: latency-svc-t8hzf
Mar 17 10:15:12.140: INFO: Got endpoints: latency-svc-qzm5g [750.701966ms]
Mar 17 10:15:12.151: INFO: Created: latency-svc-pllcf
Mar 17 10:15:12.192: INFO: Got endpoints: latency-svc-7ddth [752.916691ms]
Mar 17 10:15:12.217: INFO: Created: latency-svc-k4bkg
Mar 17 10:15:12.240: INFO: Got endpoints: latency-svc-vnr5l [747.807493ms]
Mar 17 10:15:12.246: INFO: Created: latency-svc-s5648
Mar 17 10:15:12.291: INFO: Got endpoints: latency-svc-4tzbn [749.627238ms]
Mar 17 10:15:12.301: INFO: Created: latency-svc-xj7hs
Mar 17 10:15:12.338: INFO: Got endpoints: latency-svc-sg2tq [749.155481ms]
Mar 17 10:15:12.344: INFO: Created: latency-svc-7jcv8
Mar 17 10:15:12.390: INFO: Got endpoints: latency-svc-qml5j [746.927527ms]
Mar 17 10:15:12.399: INFO: Created: latency-svc-hvzxr
Mar 17 10:15:12.438: INFO: Got endpoints: latency-svc-zdplq [748.785572ms]
Mar 17 10:15:12.447: INFO: Created: latency-svc-ds8hc
Mar 17 10:15:12.489: INFO: Got endpoints: latency-svc-752wl [751.441003ms]
Mar 17 10:15:12.501: INFO: Created: latency-svc-n8gj4
Mar 17 10:15:12.541: INFO: Got endpoints: latency-svc-wwjbx [753.25814ms]
Mar 17 10:15:12.549: INFO: Created: latency-svc-bmkmj
Mar 17 10:15:12.588: INFO: Got endpoints: latency-svc-xtp67 [747.042982ms]
Mar 17 10:15:12.597: INFO: Created: latency-svc-jphkm
Mar 17 10:15:12.641: INFO: Got endpoints: latency-svc-5qnd2 [751.607589ms]
Mar 17 10:15:12.649: INFO: Created: latency-svc-7gpnt
Mar 17 10:15:12.693: INFO: Got endpoints: latency-svc-5pp68 [750.27837ms]
Mar 17 10:15:12.699: INFO: Created: latency-svc-pnxxc
Mar 17 10:15:12.739: INFO: Got endpoints: latency-svc-bwfrl [739.840836ms]
Mar 17 10:15:12.745: INFO: Created: latency-svc-s2pnl
Mar 17 10:15:12.790: INFO: Got endpoints: latency-svc-g9lpz [746.986039ms]
Mar 17 10:15:12.797: INFO: Created: latency-svc-lqmh7
Mar 17 10:15:12.840: INFO: Got endpoints: latency-svc-t8hzf [752.584114ms]
Mar 17 10:15:12.848: INFO: Created: latency-svc-l84rp
Mar 17 10:15:12.891: INFO: Got endpoints: latency-svc-pllcf [751.004108ms]
Mar 17 10:15:12.898: INFO: Created: latency-svc-nnqsc
Mar 17 10:15:12.939: INFO: Got endpoints: latency-svc-k4bkg [747.039093ms]
Mar 17 10:15:12.989: INFO: Got endpoints: latency-svc-s5648 [748.999693ms]
Mar 17 10:15:13.040: INFO: Got endpoints: latency-svc-xj7hs [749.420918ms]
Mar 17 10:15:13.092: INFO: Got endpoints: latency-svc-7jcv8 [753.66361ms]
Mar 17 10:15:13.140: INFO: Got endpoints: latency-svc-hvzxr [749.505696ms]
Mar 17 10:15:13.189: INFO: Got endpoints: latency-svc-ds8hc [750.378647ms]
Mar 17 10:15:13.239: INFO: Got endpoints: latency-svc-n8gj4 [749.856607ms]
Mar 17 10:15:13.288: INFO: Got endpoints: latency-svc-bmkmj [747.317644ms]
Mar 17 10:15:13.340: INFO: Got endpoints: latency-svc-jphkm [751.662887ms]
Mar 17 10:15:13.390: INFO: Got endpoints: latency-svc-7gpnt [748.670176ms]
Mar 17 10:15:13.439: INFO: Got endpoints: latency-svc-pnxxc [746.417788ms]
Mar 17 10:15:13.488: INFO: Got endpoints: latency-svc-s2pnl [748.397047ms]
Mar 17 10:15:13.542: INFO: Got endpoints: latency-svc-lqmh7 [751.854376ms]
Mar 17 10:15:13.587: INFO: Got endpoints: latency-svc-l84rp [746.694104ms]
Mar 17 10:15:13.640: INFO: Got endpoints: latency-svc-nnqsc [749.429509ms]
Mar 17 10:15:13.640: INFO: Latencies: [30.757424ms 55.103071ms 55.204369ms 68.713301ms 68.983041ms 70.409822ms 78.546956ms 83.687274ms 104.086087ms 117.307119ms 127.637103ms 130.099236ms 132.675174ms 135.477259ms 135.818729ms 136.155411ms 136.470778ms 138.680122ms 148.542569ms 150.059388ms 164.84712ms 165.715998ms 177.551397ms 177.553053ms 178.672636ms 193.313494ms 196.508673ms 200.080274ms 216.241906ms 235.712392ms 238.680052ms 246.704988ms 248.241989ms 248.476562ms 249.164705ms 251.988415ms 255.693611ms 255.955765ms 268.721424ms 269.240568ms 281.399375ms 299.519769ms 311.041825ms 319.914707ms 354.338756ms 404.820552ms 445.621213ms 520.109151ms 563.463959ms 592.753641ms 645.225212ms 695.54667ms 722.519349ms 729.044967ms 735.90546ms 735.928676ms 736.86526ms 737.454988ms 737.84054ms 739.840836ms 740.505932ms 740.688658ms 743.467351ms 743.709182ms 743.910971ms 744.123322ms 744.488758ms 745.077162ms 745.517858ms 745.961707ms 746.257012ms 746.380386ms 746.417788ms 746.45648ms 746.493119ms 746.51272ms 746.694104ms 746.756838ms 746.785921ms 746.789258ms 746.857289ms 746.877152ms 746.927527ms 746.986039ms 747.039093ms 747.042982ms 747.076272ms 747.154259ms 747.249804ms 747.29887ms 747.317644ms 747.584924ms 747.600066ms 747.664029ms 747.67091ms 747.676886ms 747.807493ms 747.815041ms 747.882004ms 748.061155ms 748.094654ms 748.189522ms 748.257149ms 748.397047ms 748.405402ms 748.405474ms 748.462594ms 748.488988ms 748.670176ms 748.707784ms 748.711705ms 748.714024ms 748.785572ms 748.832927ms 748.874033ms 748.999693ms 749.036817ms 749.038035ms 749.116233ms 749.155481ms 749.156542ms 749.349666ms 749.382127ms 749.420918ms 749.429509ms 749.460525ms 749.464419ms 749.488246ms 749.505696ms 749.524046ms 749.556963ms 749.627238ms 749.658174ms 749.674915ms 749.731673ms 749.796452ms 749.804029ms 749.856607ms 749.963524ms 750.150943ms 750.232967ms 750.27837ms 750.373735ms 750.378647ms 750.416479ms 750.469353ms 750.511409ms 750.654106ms 750.701966ms 750.854463ms 750.854709ms 751.004108ms 751.052869ms 751.082468ms 751.22394ms 751.24191ms 751.401177ms 751.441003ms 751.521771ms 751.523471ms 751.607589ms 751.662887ms 751.669566ms 751.715299ms 751.72873ms 751.854376ms 751.938447ms 751.989956ms 751.99092ms 752.023755ms 752.056076ms 752.334184ms 752.361604ms 752.453773ms 752.487805ms 752.584114ms 752.677336ms 752.689077ms 752.745364ms 752.781492ms 752.879051ms 752.916691ms 752.93331ms 753.197493ms 753.25814ms 753.373436ms 753.447248ms 753.66361ms 754.135459ms 754.356534ms 754.385335ms 755.016269ms 756.511239ms 757.225238ms 757.578647ms 760.033876ms 761.020306ms 762.355335ms 763.356475ms 765.964677ms]
Mar 17 10:15:13.640: INFO: 50 %ile: 748.094654ms
Mar 17 10:15:13.640: INFO: 90 %ile: 752.879051ms
Mar 17 10:15:13.640: INFO: 99 %ile: 763.356475ms
Mar 17 10:15:13.640: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:13.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3322" for this suite.

• [SLOW TEST:10.802 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":303,"skipped":6057,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:13.651: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Mar 17 10:15:13.741: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:15:15.746: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:16.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9234" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":304,"skipped":6093,"failed":0}
SSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:16.770: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:16.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8859" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":305,"skipped":6100,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:16.845: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 10:15:17.483: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 10:15:20.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Mar 17 10:15:21.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Mar 17 10:15:22.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Mar 17 10:15:23.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Mar 17 10:15:24.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Mar 17 10:15:25.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Mar 17 10:15:26.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:26.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7002" for this suite.
STEP: Destroying namespace "webhook-7002-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.835 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":306,"skipped":6105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:26.684: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:15:26.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1852926-9ad9-40a7-a761-6b2e4937484b" in namespace "projected-6458" to be "Succeeded or Failed"
Mar 17 10:15:26.757: INFO: Pod "downwardapi-volume-c1852926-9ad9-40a7-a761-6b2e4937484b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.228281ms
Mar 17 10:15:28.762: INFO: Pod "downwardapi-volume-c1852926-9ad9-40a7-a761-6b2e4937484b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013654432s
STEP: Saw pod success
Mar 17 10:15:28.762: INFO: Pod "downwardapi-volume-c1852926-9ad9-40a7-a761-6b2e4937484b" satisfied condition "Succeeded or Failed"
Mar 17 10:15:28.765: INFO: Trying to get logs from node ip-172-31-39-36 pod downwardapi-volume-c1852926-9ad9-40a7-a761-6b2e4937484b container client-container: <nil>
STEP: delete the pod
Mar 17 10:15:28.790: INFO: Waiting for pod downwardapi-volume-c1852926-9ad9-40a7-a761-6b2e4937484b to disappear
Mar 17 10:15:28.798: INFO: Pod downwardapi-volume-c1852926-9ad9-40a7-a761-6b2e4937484b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:28.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6458" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":307,"skipped":6161,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:28.808: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 17 10:15:29.538: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0317 10:15:29.538717      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:29.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4757" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":308,"skipped":6163,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:29.549: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 17 10:15:29.649: INFO: Waiting up to 5m0s for pod "downward-api-e3e3f3b3-d698-4b47-a1cc-8cd4c581dd19" in namespace "downward-api-3312" to be "Succeeded or Failed"
Mar 17 10:15:29.653: INFO: Pod "downward-api-e3e3f3b3-d698-4b47-a1cc-8cd4c581dd19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218892ms
Mar 17 10:15:31.666: INFO: Pod "downward-api-e3e3f3b3-d698-4b47-a1cc-8cd4c581dd19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017470425s
Mar 17 10:15:33.671: INFO: Pod "downward-api-e3e3f3b3-d698-4b47-a1cc-8cd4c581dd19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022274748s
STEP: Saw pod success
Mar 17 10:15:33.671: INFO: Pod "downward-api-e3e3f3b3-d698-4b47-a1cc-8cd4c581dd19" satisfied condition "Succeeded or Failed"
Mar 17 10:15:33.673: INFO: Trying to get logs from node ip-172-31-39-36 pod downward-api-e3e3f3b3-d698-4b47-a1cc-8cd4c581dd19 container dapi-container: <nil>
STEP: delete the pod
Mar 17 10:15:33.707: INFO: Waiting for pod downward-api-e3e3f3b3-d698-4b47-a1cc-8cd4c581dd19 to disappear
Mar 17 10:15:33.715: INFO: Pod downward-api-e3e3f3b3-d698-4b47-a1cc-8cd4c581dd19 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:33.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3312" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":309,"skipped":6170,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:33.738: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Mar 17 10:15:33.781: INFO: namespace kubectl-3978
Mar 17 10:15:33.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-3978 create -f -'
Mar 17 10:15:33.988: INFO: stderr: ""
Mar 17 10:15:33.988: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 17 10:15:34.995: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:15:34.995: INFO: Found 0 / 1
Mar 17 10:15:35.998: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:15:35.998: INFO: Found 1 / 1
Mar 17 10:15:35.998: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 17 10:15:36.001: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:15:36.001: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 17 10:15:36.001: INFO: wait on agnhost-primary startup in kubectl-3978 
Mar 17 10:15:36.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-3978 logs agnhost-primary-cfm44 agnhost-primary'
Mar 17 10:15:36.077: INFO: stderr: ""
Mar 17 10:15:36.077: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 17 10:15:36.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-3978 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 17 10:15:36.144: INFO: stderr: ""
Mar 17 10:15:36.144: INFO: stdout: "service/rm2 exposed\n"
Mar 17 10:15:36.151: INFO: Service rm2 in namespace kubectl-3978 found.
STEP: exposing service
Mar 17 10:15:38.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-3978 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 17 10:15:38.249: INFO: stderr: ""
Mar 17 10:15:38.249: INFO: stdout: "service/rm3 exposed\n"
Mar 17 10:15:38.265: INFO: Service rm3 in namespace kubectl-3978 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:40.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3978" for this suite.

• [SLOW TEST:6.541 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":310,"skipped":6172,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:40.279: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-591217b8-58a7-4f1e-a359-27f2c54b2f50
STEP: Creating a pod to test consume configMaps
Mar 17 10:15:40.344: INFO: Waiting up to 5m0s for pod "pod-configmaps-117cdefe-e6d9-4429-8393-f4c67ec0e9f0" in namespace "configmap-5778" to be "Succeeded or Failed"
Mar 17 10:15:40.360: INFO: Pod "pod-configmaps-117cdefe-e6d9-4429-8393-f4c67ec0e9f0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.843519ms
Mar 17 10:15:42.366: INFO: Pod "pod-configmaps-117cdefe-e6d9-4429-8393-f4c67ec0e9f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02237349s
STEP: Saw pod success
Mar 17 10:15:42.366: INFO: Pod "pod-configmaps-117cdefe-e6d9-4429-8393-f4c67ec0e9f0" satisfied condition "Succeeded or Failed"
Mar 17 10:15:42.372: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-117cdefe-e6d9-4429-8393-f4c67ec0e9f0 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 17 10:15:42.388: INFO: Waiting for pod pod-configmaps-117cdefe-e6d9-4429-8393-f4c67ec0e9f0 to disappear
Mar 17 10:15:42.395: INFO: Pod pod-configmaps-117cdefe-e6d9-4429-8393-f4c67ec0e9f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:42.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5778" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":6177,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:42.405: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:15:42.463: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb3f72e9-88a6-4ecc-9ce5-c879aa88acd6" in namespace "downward-api-7013" to be "Succeeded or Failed"
Mar 17 10:15:42.466: INFO: Pod "downwardapi-volume-cb3f72e9-88a6-4ecc-9ce5-c879aa88acd6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.221306ms
Mar 17 10:15:44.470: INFO: Pod "downwardapi-volume-cb3f72e9-88a6-4ecc-9ce5-c879aa88acd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007140419s
STEP: Saw pod success
Mar 17 10:15:44.470: INFO: Pod "downwardapi-volume-cb3f72e9-88a6-4ecc-9ce5-c879aa88acd6" satisfied condition "Succeeded or Failed"
Mar 17 10:15:44.472: INFO: Trying to get logs from node ip-172-31-39-36 pod downwardapi-volume-cb3f72e9-88a6-4ecc-9ce5-c879aa88acd6 container client-container: <nil>
STEP: delete the pod
Mar 17 10:15:44.489: INFO: Waiting for pod downwardapi-volume-cb3f72e9-88a6-4ecc-9ce5-c879aa88acd6 to disappear
Mar 17 10:15:44.491: INFO: Pod downwardapi-volume-cb3f72e9-88a6-4ecc-9ce5-c879aa88acd6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:44.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7013" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":6192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:44.498: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:15:44.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67bc5ccb-f70b-4fe1-9690-2ecf2478ca7b" in namespace "projected-3779" to be "Succeeded or Failed"
Mar 17 10:15:44.568: INFO: Pod "downwardapi-volume-67bc5ccb-f70b-4fe1-9690-2ecf2478ca7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.579605ms
Mar 17 10:15:46.574: INFO: Pod "downwardapi-volume-67bc5ccb-f70b-4fe1-9690-2ecf2478ca7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011371766s
STEP: Saw pod success
Mar 17 10:15:46.575: INFO: Pod "downwardapi-volume-67bc5ccb-f70b-4fe1-9690-2ecf2478ca7b" satisfied condition "Succeeded or Failed"
Mar 17 10:15:46.578: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-67bc5ccb-f70b-4fe1-9690-2ecf2478ca7b container client-container: <nil>
STEP: delete the pod
Mar 17 10:15:46.595: INFO: Waiting for pod downwardapi-volume-67bc5ccb-f70b-4fe1-9690-2ecf2478ca7b to disappear
Mar 17 10:15:46.597: INFO: Pod downwardapi-volume-67bc5ccb-f70b-4fe1-9690-2ecf2478ca7b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:46.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3779" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":6265,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:46.605: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 17 10:15:46.643: INFO: Waiting up to 5m0s for pod "pod-80ddbb32-1d6a-4fa2-91e8-d5776535b555" in namespace "emptydir-9225" to be "Succeeded or Failed"
Mar 17 10:15:46.648: INFO: Pod "pod-80ddbb32-1d6a-4fa2-91e8-d5776535b555": Phase="Pending", Reason="", readiness=false. Elapsed: 4.813012ms
Mar 17 10:15:48.652: INFO: Pod "pod-80ddbb32-1d6a-4fa2-91e8-d5776535b555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008866147s
STEP: Saw pod success
Mar 17 10:15:48.652: INFO: Pod "pod-80ddbb32-1d6a-4fa2-91e8-d5776535b555" satisfied condition "Succeeded or Failed"
Mar 17 10:15:48.655: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-80ddbb32-1d6a-4fa2-91e8-d5776535b555 container test-container: <nil>
STEP: delete the pod
Mar 17 10:15:48.672: INFO: Waiting for pod pod-80ddbb32-1d6a-4fa2-91e8-d5776535b555 to disappear
Mar 17 10:15:48.675: INFO: Pod pod-80ddbb32-1d6a-4fa2-91e8-d5776535b555 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:48.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9225" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":6282,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:48.681: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-c092ab99-bc0c-43b9-a846-c6fe6d38841e
STEP: Creating a pod to test consume configMaps
Mar 17 10:15:48.740: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5586583b-e407-4200-8014-6bab89e51c92" in namespace "projected-9136" to be "Succeeded or Failed"
Mar 17 10:15:48.744: INFO: Pod "pod-projected-configmaps-5586583b-e407-4200-8014-6bab89e51c92": Phase="Pending", Reason="", readiness=false. Elapsed: 3.030397ms
Mar 17 10:15:50.746: INFO: Pod "pod-projected-configmaps-5586583b-e407-4200-8014-6bab89e51c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005660695s
STEP: Saw pod success
Mar 17 10:15:50.746: INFO: Pod "pod-projected-configmaps-5586583b-e407-4200-8014-6bab89e51c92" satisfied condition "Succeeded or Failed"
Mar 17 10:15:50.748: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-configmaps-5586583b-e407-4200-8014-6bab89e51c92 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 10:15:50.776: INFO: Waiting for pod pod-projected-configmaps-5586583b-e407-4200-8014-6bab89e51c92 to disappear
Mar 17 10:15:50.778: INFO: Pod pod-projected-configmaps-5586583b-e407-4200-8014-6bab89e51c92 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:15:50.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9136" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":6284,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:15:50.787: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar 17 10:16:01.008: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0317 10:16:01.008923      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:16:01.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6559" for this suite.

• [SLOW TEST:10.237 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":316,"skipped":6289,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:16:01.027: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Mar 17 10:16:01.102: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:16:03.108: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar 17 10:16:03.126: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:16:05.130: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 17 10:16:05.152: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 17 10:16:05.159: INFO: Pod pod-with-poststart-http-hook still exists
Mar 17 10:16:07.160: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 17 10:16:07.165: INFO: Pod pod-with-poststart-http-hook still exists
Mar 17 10:16:09.160: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 17 10:16:09.173: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:16:09.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8731" for this suite.

• [SLOW TEST:8.161 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":317,"skipped":6302,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:16:09.189: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:18:01.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8316" for this suite.

• [SLOW TEST:112.089 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":318,"skipped":6320,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:18:01.278: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:18:29.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4077" for this suite.

• [SLOW TEST:28.152 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":319,"skipped":6321,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:18:29.430: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-a9644733-10fd-4b3d-83f0-6fcb2d4e031e
STEP: Creating a pod to test consume configMaps
Mar 17 10:18:29.512: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc5d9cb4-faa6-40b8-b935-100968c0836e" in namespace "projected-4956" to be "Succeeded or Failed"
Mar 17 10:18:29.518: INFO: Pod "pod-projected-configmaps-fc5d9cb4-faa6-40b8-b935-100968c0836e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.360295ms
Mar 17 10:18:31.527: INFO: Pod "pod-projected-configmaps-fc5d9cb4-faa6-40b8-b935-100968c0836e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01546387s
STEP: Saw pod success
Mar 17 10:18:31.527: INFO: Pod "pod-projected-configmaps-fc5d9cb4-faa6-40b8-b935-100968c0836e" satisfied condition "Succeeded or Failed"
Mar 17 10:18:31.532: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-projected-configmaps-fc5d9cb4-faa6-40b8-b935-100968c0836e container agnhost-container: <nil>
STEP: delete the pod
Mar 17 10:18:31.557: INFO: Waiting for pod pod-projected-configmaps-fc5d9cb4-faa6-40b8-b935-100968c0836e to disappear
Mar 17 10:18:31.562: INFO: Pod pod-projected-configmaps-fc5d9cb4-faa6-40b8-b935-100968c0836e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:18:31.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4956" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":320,"skipped":6330,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:18:31.584: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7474
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7474
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7474
Mar 17 10:18:31.678: INFO: Found 0 stateful pods, waiting for 1
Mar 17 10:18:41.684: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 17 10:18:41.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-7474 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 10:18:41.895: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 10:18:41.895: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 10:18:41.895: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 10:18:41.898: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 17 10:18:51.903: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 17 10:18:51.903: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 10:18:51.917: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999801s
Mar 17 10:18:52.922: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997383995s
Mar 17 10:18:53.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992077401s
Mar 17 10:18:54.937: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982628498s
Mar 17 10:18:55.947: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976248489s
Mar 17 10:18:56.951: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.967391772s
Mar 17 10:18:57.956: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.963125273s
Mar 17 10:18:58.962: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.957598418s
Mar 17 10:18:59.969: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.95193795s
Mar 17 10:19:00.976: INFO: Verifying statefulset ss doesn't scale past 1 for another 944.467239ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7474
Mar 17 10:19:01.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-7474 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 10:19:02.144: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 17 10:19:02.144: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 10:19:02.144: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 10:19:02.148: INFO: Found 1 stateful pods, waiting for 3
Mar 17 10:19:12.155: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 10:19:12.155: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 17 10:19:12.155: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 17 10:19:12.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-7474 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 10:19:12.332: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 10:19:12.332: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 10:19:12.332: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 10:19:12.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-7474 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 10:19:12.522: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 10:19:12.522: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 10:19:12.522: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 10:19:12.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-7474 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 17 10:19:12.721: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 17 10:19:12.721: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 17 10:19:12.721: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 17 10:19:12.721: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 10:19:12.730: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 17 10:19:22.740: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 17 10:19:22.740: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 17 10:19:22.740: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 17 10:19:22.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999813s
Mar 17 10:19:23.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990605543s
Mar 17 10:19:24.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985470987s
Mar 17 10:19:25.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978847296s
Mar 17 10:19:26.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.973810988s
Mar 17 10:19:27.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968985122s
Mar 17 10:19:28.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962757472s
Mar 17 10:19:29.791: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958753272s
Mar 17 10:19:30.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954777203s
Mar 17 10:19:31.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.48749ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7474
Mar 17 10:19:32.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-7474 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 10:19:32.961: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 17 10:19:32.961: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 10:19:32.961: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 10:19:32.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-7474 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 10:19:33.125: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 17 10:19:33.125: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 10:19:33.125: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 10:19:33.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=statefulset-7474 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 17 10:19:33.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 17 10:19:33.376: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 17 10:19:33.376: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 17 10:19:33.376: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Mar 17 10:19:43.391: INFO: Deleting all statefulset in ns statefulset-7474
Mar 17 10:19:43.393: INFO: Scaling statefulset ss to 0
Mar 17 10:19:43.405: INFO: Waiting for statefulset status.replicas updated to 0
Mar 17 10:19:43.409: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:19:43.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7474" for this suite.

• [SLOW TEST:71.859 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":321,"skipped":6334,"failed":0}
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:19:43.444: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:19:43.538: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Mar 17 10:19:45.583: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Mar 17 10:19:45.617: INFO: observed ReplicaSet test-rs in namespace replicaset-1122 with ReadyReplicas 1, AvailableReplicas 1
Mar 17 10:19:45.626: INFO: observed ReplicaSet test-rs in namespace replicaset-1122 with ReadyReplicas 1, AvailableReplicas 1
Mar 17 10:19:45.694: INFO: observed ReplicaSet test-rs in namespace replicaset-1122 with ReadyReplicas 1, AvailableReplicas 1
Mar 17 10:19:45.704: INFO: observed ReplicaSet test-rs in namespace replicaset-1122 with ReadyReplicas 1, AvailableReplicas 1
Mar 17 10:19:47.480: INFO: observed ReplicaSet test-rs in namespace replicaset-1122 with ReadyReplicas 2, AvailableReplicas 2
Mar 17 10:19:47.983: INFO: observed Replicaset test-rs in namespace replicaset-1122 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:19:47.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1122" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":322,"skipped":6334,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:19:47.994: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 10:19:48.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 10:19:51.643: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:01.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3434" for this suite.
STEP: Destroying namespace "webhook-3434-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.024 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":323,"skipped":6350,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:02.019: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 17 10:20:02.260: INFO: Waiting up to 5m0s for pod "pod-885d76a4-4571-4d82-8607-0850ff8c6234" in namespace "emptydir-6934" to be "Succeeded or Failed"
Mar 17 10:20:02.263: INFO: Pod "pod-885d76a4-4571-4d82-8607-0850ff8c6234": Phase="Pending", Reason="", readiness=false. Elapsed: 2.775511ms
Mar 17 10:20:04.267: INFO: Pod "pod-885d76a4-4571-4d82-8607-0850ff8c6234": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007315344s
STEP: Saw pod success
Mar 17 10:20:04.268: INFO: Pod "pod-885d76a4-4571-4d82-8607-0850ff8c6234" satisfied condition "Succeeded or Failed"
Mar 17 10:20:04.270: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-885d76a4-4571-4d82-8607-0850ff8c6234 container test-container: <nil>
STEP: delete the pod
Mar 17 10:20:04.299: INFO: Waiting for pod pod-885d76a4-4571-4d82-8607-0850ff8c6234 to disappear
Mar 17 10:20:04.309: INFO: Pod pod-885d76a4-4571-4d82-8607-0850ff8c6234 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:04.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6934" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":324,"skipped":6368,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:04.318: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:04.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6075" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":325,"skipped":6379,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:04.410: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:20:04.509: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ea09f75-1296-4b92-87e8-e8a75efe313b" in namespace "downward-api-4158" to be "Succeeded or Failed"
Mar 17 10:20:04.535: INFO: Pod "downwardapi-volume-0ea09f75-1296-4b92-87e8-e8a75efe313b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.779551ms
Mar 17 10:20:06.541: INFO: Pod "downwardapi-volume-0ea09f75-1296-4b92-87e8-e8a75efe313b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031886243s
STEP: Saw pod success
Mar 17 10:20:06.541: INFO: Pod "downwardapi-volume-0ea09f75-1296-4b92-87e8-e8a75efe313b" satisfied condition "Succeeded or Failed"
Mar 17 10:20:06.543: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-0ea09f75-1296-4b92-87e8-e8a75efe313b container client-container: <nil>
STEP: delete the pod
Mar 17 10:20:06.563: INFO: Waiting for pod downwardapi-volume-0ea09f75-1296-4b92-87e8-e8a75efe313b to disappear
Mar 17 10:20:06.568: INFO: Pod downwardapi-volume-0ea09f75-1296-4b92-87e8-e8a75efe313b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:06.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4158" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6379,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:06.576: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 17 10:20:06.651: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1773  aed255f6-2049-43cd-8fa4-d7d5a9234167 87839 0 2022-03-17 10:20:06 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-03-17 10:20:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2x9w5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2x9w5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 17 10:20:06.658: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:20:08.663: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 17 10:20:08.663: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1773 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 10:20:08.663: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 10:20:08.664: INFO: ExecWithOptions: Clientset creation
Mar 17 10:20:08.664: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-1773/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Mar 17 10:20:08.767: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1773 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 10:20:08.767: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 10:20:08.768: INFO: ExecWithOptions: Clientset creation
Mar 17 10:20:08.768: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-1773/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Mar 17 10:20:08.856: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:08.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1773" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":327,"skipped":6379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Mar 17 10:20:08.940: INFO: created test-pod-1
Mar 17 10:20:10.958: INFO: running and ready test-pod-1
Mar 17 10:20:10.964: INFO: created test-pod-2
Mar 17 10:20:12.974: INFO: running and ready test-pod-2
Mar 17 10:20:12.980: INFO: created test-pod-3
Mar 17 10:20:14.989: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Mar 17 10:20:15.025: INFO: Pod quantity 3 is different from expected quantity 0
Mar 17 10:20:16.029: INFO: Pod quantity 1 is different from expected quantity 0
Mar 17 10:20:17.030: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:18.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1270" for this suite.

• [SLOW TEST:9.164 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":328,"skipped":6416,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:18.050: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1717
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 17 10:20:18.106: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 17 10:20:18.160: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:20:20.164: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:22.165: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:24.165: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:26.165: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:28.165: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:30.164: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:32.165: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:34.170: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:36.170: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 17 10:20:38.176: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 17 10:20:38.187: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 17 10:20:38.202: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 17 10:20:40.222: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 17 10:20:40.222: INFO: Breadth first check of 10.42.0.45 on host 172.31.33.68...
Mar 17 10:20:40.225: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.44:9080/dial?request=hostname&protocol=udp&host=10.42.0.45&port=8081&tries=1'] Namespace:pod-network-test-1717 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 10:20:40.225: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 10:20:40.226: INFO: ExecWithOptions: Clientset creation
Mar 17 10:20:40.226: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-1717/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.44%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.0.45%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Mar 17 10:20:40.307: INFO: Waiting for responses: map[]
Mar 17 10:20:40.307: INFO: reached 10.42.0.45 after 0/1 tries
Mar 17 10:20:40.307: INFO: Breadth first check of 10.42.2.43 on host 172.31.35.106...
Mar 17 10:20:40.311: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.44:9080/dial?request=hostname&protocol=udp&host=10.42.2.43&port=8081&tries=1'] Namespace:pod-network-test-1717 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 10:20:40.311: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 10:20:40.311: INFO: ExecWithOptions: Clientset creation
Mar 17 10:20:40.311: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-1717/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.44%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.2.43%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Mar 17 10:20:40.414: INFO: Waiting for responses: map[]
Mar 17 10:20:40.414: INFO: reached 10.42.2.43 after 0/1 tries
Mar 17 10:20:40.414: INFO: Breadth first check of 10.42.1.144 on host 172.31.39.36...
Mar 17 10:20:40.417: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.44:9080/dial?request=hostname&protocol=udp&host=10.42.1.144&port=8081&tries=1'] Namespace:pod-network-test-1717 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 17 10:20:40.417: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
Mar 17 10:20:40.418: INFO: ExecWithOptions: Clientset creation
Mar 17 10:20:40.418: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-1717/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.44%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.1.144%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Mar 17 10:20:40.530: INFO: Waiting for responses: map[]
Mar 17 10:20:40.530: INFO: reached 10.42.1.144 after 0/1 tries
Mar 17 10:20:40.530: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:40.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1717" for this suite.

• [SLOW TEST:22.497 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":6417,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:40.547: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:20:40.622: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 17 10:20:40.660: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 17 10:20:45.667: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 17 10:20:45.667: INFO: Creating deployment "test-rolling-update-deployment"
Mar 17 10:20:45.673: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 17 10:20:45.695: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 17 10:20:47.701: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 17 10:20:47.703: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar 17 10:20:47.714: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2143  3c40c0cb-e8b7-4ed8-b0f6-c5e54ab56a8c 88236 1 2022-03-17 10:20:45 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-03-17 10:20:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:20:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006999f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-17 10:20:45 +0000 UTC,LastTransitionTime:2022-03-17 10:20:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-03-17 10:20:47 +0000 UTC,LastTransitionTime:2022-03-17 10:20:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 17 10:20:47.719: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-2143  3ee0eefc-1c2f-4942-beb9-5b65a5eb3311 88224 1 2022-03-17 10:20:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3c40c0cb-e8b7-4ed8-b0f6-c5e54ab56a8c 0xc005896427 0xc005896428}] []  [{kube-controller-manager Update apps/v1 2022-03-17 10:20:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c40c0cb-e8b7-4ed8-b0f6-c5e54ab56a8c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:20:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058964e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:20:47.719: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 17 10:20:47.719: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2143  f3fbdae3-6785-4efb-8879-973036c3d2ac 88235 2 2022-03-17 10:20:40 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3c40c0cb-e8b7-4ed8-b0f6-c5e54ab56a8c 0xc0058962f7 0xc0058962f8}] []  [{e2e.test Update apps/v1 2022-03-17 10:20:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c40c0cb-e8b7-4ed8-b0f6-c5e54ab56a8c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-03-17 10:20:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0058963b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 17 10:20:47.722: INFO: Pod "test-rolling-update-deployment-796dbc4547-n4h8s" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-n4h8s test-rolling-update-deployment-796dbc4547- deployment-2143  7a26033b-f70c-45ca-9ae2-0ab2a873f113 88223 0 2022-03-17 10:20:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[cni.projectcalico.org/containerID:6a5ae9ec6e577fadbb39ff9bb822369eb2899040cff8f94ffb8bb3e7a5fe36ca cni.projectcalico.org/podIP:10.42.2.46/32 cni.projectcalico.org/podIPs:10.42.2.46/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 3ee0eefc-1c2f-4942-beb9-5b65a5eb3311 0xc005896957 0xc005896958}] []  [{kube-controller-manager Update v1 2022-03-17 10:20:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee0eefc-1c2f-4942-beb9-5b65a5eb3311\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-03-17 10:20:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-03-17 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lj4sf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lj4sf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:20:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:20:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:20:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-17 10:20:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.106,PodIP:10.42.2.46,StartTime:2022-03-17 10:20:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-17 10:20:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:docker://b7c129185d50031a3f86a241dfb948d00c33b9cf26ea0369573e0036acc20681,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:47.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2143" for this suite.

• [SLOW TEST:7.183 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":330,"skipped":6420,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:47.730: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-5cab6a32-39aa-4b4d-8bb4-1d65665555d4
STEP: Creating a pod to test consume secrets
Mar 17 10:20:47.831: INFO: Waiting up to 5m0s for pod "pod-secrets-ac0bf3e6-2a45-4d52-a021-435503cec63c" in namespace "secrets-4760" to be "Succeeded or Failed"
Mar 17 10:20:47.837: INFO: Pod "pod-secrets-ac0bf3e6-2a45-4d52-a021-435503cec63c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.774123ms
Mar 17 10:20:49.841: INFO: Pod "pod-secrets-ac0bf3e6-2a45-4d52-a021-435503cec63c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009552246s
STEP: Saw pod success
Mar 17 10:20:49.841: INFO: Pod "pod-secrets-ac0bf3e6-2a45-4d52-a021-435503cec63c" satisfied condition "Succeeded or Failed"
Mar 17 10:20:49.844: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-secrets-ac0bf3e6-2a45-4d52-a021-435503cec63c container secret-volume-test: <nil>
STEP: delete the pod
Mar 17 10:20:49.864: INFO: Waiting for pod pod-secrets-ac0bf3e6-2a45-4d52-a021-435503cec63c to disappear
Mar 17 10:20:49.871: INFO: Pod pod-secrets-ac0bf3e6-2a45-4d52-a021-435503cec63c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:49.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4760" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":6433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:49.878: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:20:50.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a7db227-00dd-4798-89e2-71b737f2e9b0" in namespace "downward-api-427" to be "Succeeded or Failed"
Mar 17 10:20:50.028: INFO: Pod "downwardapi-volume-3a7db227-00dd-4798-89e2-71b737f2e9b0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008102ms
Mar 17 10:20:52.033: INFO: Pod "downwardapi-volume-3a7db227-00dd-4798-89e2-71b737f2e9b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025918886s
STEP: Saw pod success
Mar 17 10:20:52.033: INFO: Pod "downwardapi-volume-3a7db227-00dd-4798-89e2-71b737f2e9b0" satisfied condition "Succeeded or Failed"
Mar 17 10:20:52.036: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-3a7db227-00dd-4798-89e2-71b737f2e9b0 container client-container: <nil>
STEP: delete the pod
Mar 17 10:20:52.061: INFO: Waiting for pod downwardapi-volume-3a7db227-00dd-4798-89e2-71b737f2e9b0 to disappear
Mar 17 10:20:52.066: INFO: Pod downwardapi-volume-3a7db227-00dd-4798-89e2-71b737f2e9b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:52.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-427" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":6483,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:52.077: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-995cfba8-de5f-4648-91a9-362bdae836d3
STEP: Creating a pod to test consume configMaps
Mar 17 10:20:52.166: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c54bdc6-5c90-4e25-949b-15d10d8a2bed" in namespace "configmap-5271" to be "Succeeded or Failed"
Mar 17 10:20:52.171: INFO: Pod "pod-configmaps-2c54bdc6-5c90-4e25-949b-15d10d8a2bed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.002963ms
Mar 17 10:20:54.176: INFO: Pod "pod-configmaps-2c54bdc6-5c90-4e25-949b-15d10d8a2bed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00982233s
STEP: Saw pod success
Mar 17 10:20:54.176: INFO: Pod "pod-configmaps-2c54bdc6-5c90-4e25-949b-15d10d8a2bed" satisfied condition "Succeeded or Failed"
Mar 17 10:20:54.180: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-configmaps-2c54bdc6-5c90-4e25-949b-15d10d8a2bed container agnhost-container: <nil>
STEP: delete the pod
Mar 17 10:20:54.199: INFO: Waiting for pod pod-configmaps-2c54bdc6-5c90-4e25-949b-15d10d8a2bed to disappear
Mar 17 10:20:54.204: INFO: Pod pod-configmaps-2c54bdc6-5c90-4e25-949b-15d10d8a2bed no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:20:54.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5271" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":6487,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:20:54.219: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 10:20:54.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 10:20:57.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:20:57.862: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:21:01.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5401" for this suite.
STEP: Destroying namespace "webhook-5401-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.039 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":334,"skipped":6487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:21:01.259: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:21:01.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-5077 create -f -'
Mar 17 10:21:02.817: INFO: stderr: ""
Mar 17 10:21:02.817: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 17 10:21:02.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-5077 create -f -'
Mar 17 10:21:03.799: INFO: stderr: ""
Mar 17 10:21:03.799: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 17 10:21:04.811: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:21:04.811: INFO: Found 1 / 1
Mar 17 10:21:04.811: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 17 10:21:04.814: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:21:04.815: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 17 10:21:04.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-5077 describe pod agnhost-primary-zhbzx'
Mar 17 10:21:04.897: INFO: stderr: ""
Mar 17 10:21:04.897: INFO: stdout: "Name:         agnhost-primary-zhbzx\nNamespace:    kubectl-5077\nPriority:     0\nNode:         ip-172-31-35-106/172.31.35.106\nStart Time:   Thu, 17 Mar 2022 10:21:02 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 8d28f4bd57fde1647e263eeadd5633b0f17a4b650682fa70f49dcd98985d8bd6\n              cni.projectcalico.org/podIP: 10.42.2.51/32\n              cni.projectcalico.org/podIPs: 10.42.2.51/32\nStatus:       Running\nIP:           10.42.2.51\nIPs:\n  IP:           10.42.2.51\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://232d9f2a313da77963bc129ad29e753f19b58a14a5cb14ac9e16ec57b0ec1f8f\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 17 Mar 2022 10:21:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xjffg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xjffg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5077/agnhost-primary-zhbzx to ip-172-31-35-106\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar 17 10:21:04.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-5077 describe rc agnhost-primary'
Mar 17 10:21:04.981: INFO: stderr: ""
Mar 17 10:21:04.981: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5077\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-zhbzx\n"
Mar 17 10:21:04.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-5077 describe service agnhost-primary'
Mar 17 10:21:05.055: INFO: stderr: ""
Mar 17 10:21:05.055: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5077\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.43.96.98\nIPs:               10.43.96.98\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.2.51:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 17 10:21:05.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-5077 describe node ip-172-31-33-68'
Mar 17 10:21:05.165: INFO: stderr: ""
Mar 17 10:21:05.165: INFO: stdout: "Name:               ip-172-31-33-68\nRoles:              controlplane,etcd,worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-33-68\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/controlplane=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/worker=true\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"46:ed:7e:45:4d:41\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.33.68\n                    management.cattle.io/pod-limits: {\"memory\":\"170Mi\"}\n                    management.cattle.io/pod-requests: {\"cpu\":\"450m\",\"memory\":\"270Mi\",\"pods\":\"10\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.33.68/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.42.0.1\n                    rke.cattle.io/external-ip: 3.21.53.65\n                    rke.cattle.io/internal-ip: 172.31.33.68\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 17 Mar 2022 06:27:11 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-33-68\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 17 Mar 2022 10:21:03 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 17 Mar 2022 06:27:47 +0000   Thu, 17 Mar 2022 06:27:47 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Thu, 17 Mar 2022 10:20:29 +0000   Thu, 17 Mar 2022 06:27:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 17 Mar 2022 10:20:29 +0000   Thu, 17 Mar 2022 06:27:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 17 Mar 2022 10:20:29 +0000   Thu, 17 Mar 2022 06:27:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 17 Mar 2022 10:20:29 +0000   Thu, 17 Mar 2022 06:27:41 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.33.68\n  Hostname:    ip-172-31-33-68\nCapacity:\n  cpu:                4\n  ephemeral-storage:  152409020Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16198084Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  140460152600\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16095684Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 \n  System UUID:                ec2874b4-b6fb-66e2-deca-faa6d575d07c\n  Boot ID:                    21bb2941-dc76-4db5-acfb-f890a7ff86a7\n  Kernel Version:             5.11.0-1022-aws\n  OS Image:                   Ubuntu 20.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.7\n  Kubelet Version:            v1.23.4\n  Kube-Proxy Version:         v1.23.4\nPodCIDR:                      10.42.0.0/24\nPodCIDRs:                     10.42.0.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  cattle-fleet-system         fleet-agent-68b989995b-gwpf4                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h52m\n  cattle-system               cattle-cluster-agent-5bdc96ddf-7rt46                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h52m\n  cattle-system               cattle-cluster-agent-5bdc96ddf-dkrkg                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h53m\n  cattle-system               cattle-node-agent-xb7rh                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h53m\n  cattle-system               kube-api-auth-htl6k                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h53m\n  kube-system                 calico-kube-controllers-fc7fcb565-nwnrh                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h53m\n  kube-system                 canal-whdbp                                                250m (6%)     0 (0%)      0 (0%)           0 (0%)         3h53m\n  kube-system                 coredns-5cb46d7c6-wk9ms                                    100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     3h53m\n  kube-system                 metrics-server-5c4895ffbd-9bbrg                            100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         3h53m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-35d9a9c5010b483c-ng79r    0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                450m (11%)  0 (0%)\n  memory             270Mi (1%)  170Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                                            Age    From        Message\n  ----     ------                                            ----   ----        -------\n  Warning  listen tcp4 :31192: bind: address already in use  57m    kube-proxy  can't open port \"nodePort for services-6588/affinity-nodeport-timeout\" (:31192/tcp4), skipping it\n  Warning  listen tcp4 :32567: bind: address already in use  34m    kube-proxy  can't open port \"nodePort for services-147/affinity-nodeport-transition\" (:32567/tcp4), skipping it\n  Warning  listen tcp4 :32075: bind: address already in use  30m    kube-proxy  can't open port \"nodePort for resourcequota-2422/test-service-np\" (:32075/tcp4), skipping it\n  Warning  listen tcp4 :30991: bind: address already in use  24m    kube-proxy  can't open port \"nodePort for services-6041/affinity-nodeport\" (:30991/tcp4), skipping it\n  Warning  listen tcp4 :31506: bind: address already in use  8m52s  kube-proxy  can't open port \"nodePort for services-5564/nodeport-service\" (:31506/tcp4), skipping it\n"
Mar 17 10:21:05.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-5077 describe namespace kubectl-5077'
Mar 17 10:21:05.243: INFO: stderr: ""
Mar 17 10:21:05.243: INFO: stdout: "Name:         kubectl-5077\nLabels:       e2e-framework=kubectl\n              e2e-run=5d7c7e55-32d7-4c12-801b-31dacfb4b590\n              kubernetes.io/metadata.name=kubectl-5077\nAnnotations:  cattle.io/status:\n                {\"Conditions\":[{\"Type\":\"ResourceQuotaInit\",\"Status\":\"True\",\"Message\":\"\",\"LastUpdateTime\":\"2022-03-17T10:21:02Z\"},{\"Type\":\"InitialRolesPopu...\n              lifecycle.cattle.io/create.namespace-auth: true\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:21:05.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5077" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":335,"skipped":6518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:21:05.254: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-49874e1b-c658-4283-bcdd-ca8b7cba84d1 in namespace container-probe-9234
Mar 17 10:21:07.316: INFO: Started pod liveness-49874e1b-c658-4283-bcdd-ca8b7cba84d1 in namespace container-probe-9234
STEP: checking the pod's current state and verifying that restartCount is present
Mar 17 10:21:07.318: INFO: Initial restart count of pod liveness-49874e1b-c658-4283-bcdd-ca8b7cba84d1 is 0
Mar 17 10:21:27.383: INFO: Restart count of pod container-probe-9234/liveness-49874e1b-c658-4283-bcdd-ca8b7cba84d1 is now 1 (20.064737878s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:21:27.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9234" for this suite.

• [SLOW TEST:22.171 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":6575,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:21:27.426: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Mar 17 10:21:27.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7970 create -f -'
Mar 17 10:21:27.669: INFO: stderr: ""
Mar 17 10:21:27.669: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 17 10:21:28.675: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:21:28.675: INFO: Found 0 / 1
Mar 17 10:21:29.674: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:21:29.674: INFO: Found 1 / 1
Mar 17 10:21:29.674: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 17 10:21:29.677: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:21:29.677: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 17 10:21:29.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-472717657 --namespace=kubectl-7970 patch pod agnhost-primary-w77fk -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 17 10:21:29.751: INFO: stderr: ""
Mar 17 10:21:29.751: INFO: stdout: "pod/agnhost-primary-w77fk patched\n"
STEP: checking annotations
Mar 17 10:21:29.756: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 17 10:21:29.756: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:21:29.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7970" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":337,"skipped":6591,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:21:29.764: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 17 10:21:30.482: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 17 10:21:33.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 17 10:21:33.557: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:21:33.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4118" for this suite.
STEP: Destroying namespace "webhook-4118-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":338,"skipped":6600,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:21:33.659: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar 17 10:21:33.712: INFO: PodSpec: initContainers in spec.initContainers
Mar 17 10:22:16.275: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b48fc345-7226-4b75-a6a0-6e267defa68d", GenerateName:"", Namespace:"init-container-8623", SelfLink:"", UID:"1ce45ebc-9996-4212-997d-54100968d1c2", ResourceVersion:"88953", Generation:0, CreationTimestamp:time.Date(2022, time.March, 17, 10, 21, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"712020495"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"d89dc51423bdbe65129bb714aa4d2818b84f05938e3bd8f207b61b3bac79b154", "cni.projectcalico.org/podIP":"10.42.2.55/32", "cni.projectcalico.org/podIPs":"10.42.2.55/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.March, 17, 10, 21, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003302648), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.March, 17, 10, 21, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003302678), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.March, 17, 10, 21, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0033026a8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-fr7rr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc009cb1e80), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fr7rr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fr7rr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fr7rr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0057057d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-35-106", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0035adce0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005705860)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005705880)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005705888), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00570588c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005a90bc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.March, 17, 10, 21, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.March, 17, 10, 21, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.March, 17, 10, 21, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.March, 17, 10, 21, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.35.106", PodIP:"10.42.2.55", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.42.2.55"}}, StartTime:time.Date(2022, time.March, 17, 10, 21, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035ade30)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035adea0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"docker://fc91070eefb4d7cb2a36e7d659becb68ba32304f5f6aad3c6570d1188be2a6d0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009cb1f00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009cb1ee0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc00570591f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:22:16.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8623" for this suite.

• [SLOW TEST:42.631 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":339,"skipped":6611,"failed":0}
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:22:16.290: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-2f7e9681-6994-46fe-818b-90da10615cbf in namespace container-probe-84
Mar 17 10:22:18.381: INFO: Started pod liveness-2f7e9681-6994-46fe-818b-90da10615cbf in namespace container-probe-84
STEP: checking the pod's current state and verifying that restartCount is present
Mar 17 10:22:18.386: INFO: Initial restart count of pod liveness-2f7e9681-6994-46fe-818b-90da10615cbf is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:26:19.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-84" for this suite.

• [SLOW TEST:242.822 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":6611,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:26:19.113: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 17 10:26:19.320: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bca28a3-660b-4b05-b733-0631c2dfeb21" in namespace "downward-api-2533" to be "Succeeded or Failed"
Mar 17 10:26:19.328: INFO: Pod "downwardapi-volume-1bca28a3-660b-4b05-b733-0631c2dfeb21": Phase="Pending", Reason="", readiness=false. Elapsed: 8.383519ms
Mar 17 10:26:21.333: INFO: Pod "downwardapi-volume-1bca28a3-660b-4b05-b733-0631c2dfeb21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012926693s
STEP: Saw pod success
Mar 17 10:26:21.333: INFO: Pod "downwardapi-volume-1bca28a3-660b-4b05-b733-0631c2dfeb21" satisfied condition "Succeeded or Failed"
Mar 17 10:26:21.336: INFO: Trying to get logs from node ip-172-31-35-106 pod downwardapi-volume-1bca28a3-660b-4b05-b733-0631c2dfeb21 container client-container: <nil>
STEP: delete the pod
Mar 17 10:26:21.371: INFO: Waiting for pod downwardapi-volume-1bca28a3-660b-4b05-b733-0631c2dfeb21 to disappear
Mar 17 10:26:21.383: INFO: Pod downwardapi-volume-1bca28a3-660b-4b05-b733-0631c2dfeb21 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:26:21.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2533" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":341,"skipped":6613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:26:21.395: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Mar 17 10:26:21.476: INFO: created test-event-1
Mar 17 10:26:21.502: INFO: created test-event-2
Mar 17 10:26:21.517: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar 17 10:26:21.522: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar 17 10:26:21.548: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:26:21.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4493" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":342,"skipped":6653,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:26:21.565: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Mar 17 10:26:21.643: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:26:23.649: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar 17 10:26:23.662: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 17 10:26:25.666: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar 17 10:26:25.672: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 17 10:26:25.677: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 17 10:26:27.678: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 17 10:26:27.685: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 17 10:26:29.678: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 17 10:26:29.681: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:26:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6657" for this suite.

• [SLOW TEST:8.149 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6655,"failed":0}
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:26:29.715: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar 17 10:26:29.764: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 17 10:27:29.804: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 17 10:27:29.808: INFO: Starting informer...
STEP: Starting pod...
Mar 17 10:27:30.029: INFO: Pod is running on ip-172-31-35-106. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 17 10:27:30.051: INFO: Pod wasn't evicted. Proceeding
Mar 17 10:27:30.051: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 17 10:28:45.089: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:28:45.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2449" for this suite.

• [SLOW TEST:135.390 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":344,"skipped":6660,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:28:45.106: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-f1c2af03-62be-4713-9f9b-a34124c2cfc7
STEP: Creating a pod to test consume secrets
Mar 17 10:28:45.177: INFO: Waiting up to 5m0s for pod "pod-secrets-30370c73-f47f-407d-8705-71860ceb4cdc" in namespace "secrets-1525" to be "Succeeded or Failed"
Mar 17 10:28:45.187: INFO: Pod "pod-secrets-30370c73-f47f-407d-8705-71860ceb4cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.737264ms
Mar 17 10:28:47.194: INFO: Pod "pod-secrets-30370c73-f47f-407d-8705-71860ceb4cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016463364s
STEP: Saw pod success
Mar 17 10:28:47.194: INFO: Pod "pod-secrets-30370c73-f47f-407d-8705-71860ceb4cdc" satisfied condition "Succeeded or Failed"
Mar 17 10:28:47.198: INFO: Trying to get logs from node ip-172-31-35-106 pod pod-secrets-30370c73-f47f-407d-8705-71860ceb4cdc container secret-volume-test: <nil>
STEP: delete the pod
Mar 17 10:28:47.231: INFO: Waiting for pod pod-secrets-30370c73-f47f-407d-8705-71860ceb4cdc to disappear
Mar 17 10:28:47.236: INFO: Pod pod-secrets-30370c73-f47f-407d-8705-71860ceb4cdc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:28:47.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1525" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 17 10:28:47.249: INFO: >>> kubeConfig: /tmp/kubeconfig-472717657
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Mar 17 10:28:47.324: INFO: Waiting up to 5m0s for pod "test-pod-ba87115c-5a22-405f-ba93-850f18940dc9" in namespace "svcaccounts-7966" to be "Succeeded or Failed"
Mar 17 10:28:47.336: INFO: Pod "test-pod-ba87115c-5a22-405f-ba93-850f18940dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.995388ms
Mar 17 10:28:49.341: INFO: Pod "test-pod-ba87115c-5a22-405f-ba93-850f18940dc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017191835s
STEP: Saw pod success
Mar 17 10:28:49.341: INFO: Pod "test-pod-ba87115c-5a22-405f-ba93-850f18940dc9" satisfied condition "Succeeded or Failed"
Mar 17 10:28:49.345: INFO: Trying to get logs from node ip-172-31-35-106 pod test-pod-ba87115c-5a22-405f-ba93-850f18940dc9 container agnhost-container: <nil>
STEP: delete the pod
Mar 17 10:28:49.361: INFO: Waiting for pod test-pod-ba87115c-5a22-405f-ba93-850f18940dc9 to disappear
Mar 17 10:28:49.374: INFO: Pod test-pod-ba87115c-5a22-405f-ba93-850f18940dc9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 17 10:28:49.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7966" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":346,"skipped":6696,"failed":0}
Mar 17 10:28:49.388: INFO: Running AfterSuite actions on all nodes
Mar 17 10:28:49.388: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Mar 17 10:28:49.388: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Mar 17 10:28:49.388: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Mar 17 10:28:49.388: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar 17 10:28:49.388: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar 17 10:28:49.388: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar 17 10:28:49.388: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Mar 17 10:28:49.388: INFO: Running AfterSuite actions on node 1
Mar 17 10:28:49.388: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6696,"failed":0}

Ran 346 of 7042 Specs in 5412.439 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6696 Skipped
PASS

Ginkgo ran 1 suite in 1h30m14.5888112s
Test Suite Passed
