I0302 21:45:56.729704      25 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-927863346
I0302 21:45:56.729742      25 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0302 21:45:56.729929      25 e2e.go:129] Starting e2e run "3cbcf0b8-bba3-47d5-ace4-ef87f2414ab2" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1646257555 - Will randomize all specs
Will run 311 of 5668 specs

Mar  2 21:45:56.761: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
E0302 21:45:56.763242      25 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar  2 21:45:56.764: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  2 21:45:56.885: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 21:45:56.993: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 21:45:56.993: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Mar  2 21:45:56.993: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 21:45:57.022: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Mar  2 21:45:57.023: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Mar  2 21:45:57.023: INFO: e2e test version: v1.20.11
Mar  2 21:45:57.030: INFO: kube-apiserver version: v1.20.11+e880017
Mar  2 21:45:57.031: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 21:45:57.053: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:45:57.053: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
Mar  2 21:45:57.330: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 21:45:57.347: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  2 21:46:06.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 --namespace=crd-publish-openapi-6385 create -f -'
Mar  2 21:46:07.722: INFO: stderr: ""
Mar  2 21:46:07.722: INFO: stdout: "e2e-test-crd-publish-openapi-2884-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 21:46:07.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 --namespace=crd-publish-openapi-6385 delete e2e-test-crd-publish-openapi-2884-crds test-foo'
Mar  2 21:46:07.926: INFO: stderr: ""
Mar  2 21:46:07.926: INFO: stdout: "e2e-test-crd-publish-openapi-2884-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  2 21:46:07.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 --namespace=crd-publish-openapi-6385 apply -f -'
Mar  2 21:46:08.692: INFO: stderr: ""
Mar  2 21:46:08.692: INFO: stdout: "e2e-test-crd-publish-openapi-2884-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 21:46:08.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 --namespace=crd-publish-openapi-6385 delete e2e-test-crd-publish-openapi-2884-crds test-foo'
Mar  2 21:46:08.867: INFO: stderr: ""
Mar  2 21:46:08.867: INFO: stdout: "e2e-test-crd-publish-openapi-2884-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  2 21:46:08.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 --namespace=crd-publish-openapi-6385 create -f -'
Mar  2 21:46:09.543: INFO: rc: 1
Mar  2 21:46:09.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 --namespace=crd-publish-openapi-6385 apply -f -'
Mar  2 21:46:10.179: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar  2 21:46:10.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 --namespace=crd-publish-openapi-6385 create -f -'
Mar  2 21:46:10.803: INFO: rc: 1
Mar  2 21:46:10.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 --namespace=crd-publish-openapi-6385 apply -f -'
Mar  2 21:46:11.452: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar  2 21:46:11.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 explain e2e-test-crd-publish-openapi-2884-crds'
Mar  2 21:46:12.137: INFO: stderr: ""
Mar  2 21:46:12.137: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2884-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar  2 21:46:12.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 explain e2e-test-crd-publish-openapi-2884-crds.metadata'
Mar  2 21:46:12.558: INFO: stderr: ""
Mar  2 21:46:12.558: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2884-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  2 21:46:12.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 explain e2e-test-crd-publish-openapi-2884-crds.spec'
Mar  2 21:46:13.148: INFO: stderr: ""
Mar  2 21:46:13.148: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2884-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  2 21:46:13.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 explain e2e-test-crd-publish-openapi-2884-crds.spec.bars'
Mar  2 21:46:13.519: INFO: stderr: ""
Mar  2 21:46:13.519: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2884-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  2 21:46:13.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-6385 explain e2e-test-crd-publish-openapi-2884-crds.spec.bars2'
Mar  2 21:46:14.169: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:46:23.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6385" for this suite.

• [SLOW TEST:26.234 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:46:23.288: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Mar  2 21:48:24.251: INFO: Successfully updated pod "var-expansion-b9616c7c-0525-4353-8f80-c2486b0eb7e1"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar  2 21:48:26.279: INFO: Deleting pod "var-expansion-b9616c7c-0525-4353-8f80-c2486b0eb7e1" in namespace "var-expansion-6470"
Mar  2 21:48:26.333: INFO: Wait up to 5m0s for pod "var-expansion-b9616c7c-0525-4353-8f80-c2486b0eb7e1" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:49:08.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6470" for this suite.

• [SLOW TEST:165.123 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":2,"skipped":17,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:49:08.413: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Mar  2 21:49:08.729: INFO: Waiting up to 5m0s for pod "var-expansion-c064d1af-4116-4e51-8960-4832fdec950d" in namespace "var-expansion-1806" to be "Succeeded or Failed"
Mar  2 21:49:08.742: INFO: Pod "var-expansion-c064d1af-4116-4e51-8960-4832fdec950d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.320823ms
Mar  2 21:49:10.753: INFO: Pod "var-expansion-c064d1af-4116-4e51-8960-4832fdec950d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024213787s
Mar  2 21:49:12.770: INFO: Pod "var-expansion-c064d1af-4116-4e51-8960-4832fdec950d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041317273s
STEP: Saw pod success
Mar  2 21:49:12.770: INFO: Pod "var-expansion-c064d1af-4116-4e51-8960-4832fdec950d" satisfied condition "Succeeded or Failed"
Mar  2 21:49:12.782: INFO: Trying to get logs from node 10.138.244.159 pod var-expansion-c064d1af-4116-4e51-8960-4832fdec950d container dapi-container: <nil>
STEP: delete the pod
Mar  2 21:49:12.879: INFO: Waiting for pod var-expansion-c064d1af-4116-4e51-8960-4832fdec950d to disappear
Mar  2 21:49:12.892: INFO: Pod var-expansion-c064d1af-4116-4e51-8960-4832fdec950d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:49:12.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1806" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":26,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:49:12.931: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 21:49:13.141: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 21:49:13.212: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 21:49:13.268: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.150 before test
Mar  2 21:49:13.381: INFO: calico-node-sfwd8 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.381: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 21:49:13.381: INFO: calico-typha-747778ff7-7mfzl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.381: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 21:49:13.381: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-03-02 20:09:15 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.381: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  2 21:49:13.381: INFO: managed-storage-validation-webhooks-7d645c9954-9tz2t from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.381: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 21:49:13.381: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-xx9p2 from ibm-system started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.381: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 21:49:13.381: INFO: ibm-file-plugin-58c5ccc6d4-mhk8g from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.381: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 21:49:13.381: INFO: ibm-keepalived-watcher-lzm26 from kube-system started at 2022-03-02 20:04:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.381: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 21:49:13.382: INFO: ibm-master-proxy-static-10.138.244.150 from kube-system started at 2022-03-02 20:04:27 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container pause ready: true, restart count 0
Mar  2 21:49:13.382: INFO: ibmcloud-block-storage-driver-n9lmx from kube-system started at 2022-03-02 20:04:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 21:49:13.382: INFO: ibmcloud-block-storage-plugin-67c5f49db6-wlj2j from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 21:49:13.382: INFO: tuned-fqxdn from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:49:13.382: INFO: cluster-samples-operator-69fbcc775-4zjvg from openshift-cluster-samples-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 21:49:13.382: INFO: cluster-storage-operator-56787c5bcb-9f5hs from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 21:49:13.382: INFO: csi-snapshot-controller-operator-6977df7ddd-dkdjh from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 21:49:13.382: INFO: console-78b4c56c55-ndmlz from openshift-console started at 2022-03-02 20:13:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container console ready: true, restart count 0
Mar  2 21:49:13.382: INFO: downloads-79b8c4c9f8-hqr26 from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:49:13.382: INFO: dns-default-jf6bp from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.382: INFO: cluster-image-registry-operator-58888444b7-jsm7m from openshift-image-registry started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 21:49:13.382: INFO: image-registry-78c5f597d5-rfdtq from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container registry ready: true, restart count 0
Mar  2 21:49:13.382: INFO: node-ca-2d6rp from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:49:13.382: INFO: ingress-canary-wptt6 from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 21:49:13.382: INFO: openshift-kube-proxy-9mnl6 from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.382: INFO: kube-storage-version-migrator-operator-68756f7898-kvfhq from openshift-kube-storage-version-migrator-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 21:49:13.382: INFO: certified-operators-4lfsq from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:49:13.382: INFO: community-operators-fj9gp from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:49:13.382: INFO: marketplace-operator-8689886944-xlcrw from openshift-marketplace started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 21:49:13.382: INFO: redhat-marketplace-tvwc6 from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:49:13.382: INFO: redhat-operators-87vss from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:49:13.382: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:49:13.382: INFO: cluster-monitoring-operator-668f4b779-pt5v7 from openshift-monitoring started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container cluster-monitoring-operator ready: true, restart count 1
Mar  2 21:49:13.382: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Mar  2 21:49:13.382: INFO: grafana-77dc549b6f-nn2b2 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container grafana ready: true, restart count 0
Mar  2 21:49:13.382: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 21:49:13.382: INFO: node-exporter-dff24 from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.382: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:49:13.383: INFO: thanos-querier-544bf8c477-jdvqv from openshift-monitoring started at 2022-03-02 20:07:50 +0000 UTC (5 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 21:49:13.383: INFO: multus-admission-controller-2wfsf from openshift-multus started at 2022-03-02 20:06:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 21:49:13.383: INFO: multus-zgnwz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:49:13.383: INFO: network-metrics-daemon-qmb7k from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 21:49:13.383: INFO: network-check-target-rmfwb from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 21:49:13.383: INFO: catalog-operator-777865977d-2fqfw from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 21:49:13.383: INFO: olm-operator-5cb6cff486-wld6q from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 21:49:13.383: INFO: packageserver-67967c9875-524hv from openshift-operator-lifecycle-manager started at 2022-03-02 20:07:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 21:49:13.383: INFO: service-ca-operator-5b668f5895-69p65 from openshift-service-ca-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 21:49:13.383: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-pl9fl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 21:49:13.383: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.159 before test
Mar  2 21:49:13.487: INFO: calico-kube-controllers-64bc47d78c-n2mcv from calico-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 21:49:13.487: INFO: calico-node-fhc29 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 21:49:13.487: INFO: calico-typha-747778ff7-sgxvl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 21:49:13.487: INFO: managed-storage-validation-webhooks-7d645c9954-2dqd8 from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 21:49:13.487: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-lt8lm from ibm-system started at 2022-03-02 20:07:28 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 21:49:13.487: INFO: ibm-keepalived-watcher-drf4g from kube-system started at 2022-03-02 20:04:20 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 21:49:13.487: INFO: ibm-master-proxy-static-10.138.244.159 from kube-system started at 2022-03-02 20:04:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 21:49:13.487: INFO: 	Container pause ready: true, restart count 0
Mar  2 21:49:13.487: INFO: ibmcloud-block-storage-driver-85npp from kube-system started at 2022-03-02 20:04:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 21:49:13.487: INFO: vpn-7bf7499b46-c92qv from kube-system started at 2022-03-02 20:06:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container vpn ready: true, restart count 0
Mar  2 21:49:13.487: INFO: tuned-xv4r5 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:49:13.487: INFO: csi-snapshot-controller-69454d4b56-d855w from openshift-cluster-storage-operator started at 2022-03-02 20:06:49 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 21:49:13.487: INFO: csi-snapshot-webhook-788b7d55dd-bqvnb from openshift-cluster-storage-operator started at 2022-03-02 20:06:48 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container webhook ready: true, restart count 0
Mar  2 21:49:13.487: INFO: dns-default-224cd from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:49:13.487: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:49:13.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.487: INFO: node-ca-bc4lw from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:49:13.487: INFO: registry-pvc-permissions-kstcw from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 21:49:13.487: INFO: ingress-canary-2f8b6 from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 21:49:13.487: INFO: router-default-678545d6db-hl5tg from openshift-ingress started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container router ready: true, restart count 0
Mar  2 21:49:13.487: INFO: openshift-kube-proxy-lxhnl from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 21:49:13.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.487: INFO: migrator-76bc956454-h9h2j from openshift-kube-storage-version-migrator started at 2022-03-02 20:06:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container migrator ready: true, restart count 0
Mar  2 21:49:13.487: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 21:49:13.487: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:49:13.487: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:49:13.487: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:49:13.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.487: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:49:13.487: INFO: kube-state-metrics-7b6c6d96b-tx74c from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (3 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 21:49:13.488: INFO: node-exporter-t8msj from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:49:13.488: INFO: openshift-state-metrics-84c4bdd485-9xfkj from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (3 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 21:49:13.488: INFO: prometheus-adapter-68858877cc-2w7tw from openshift-monitoring started at 2022-03-02 20:09:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:49:13.488: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-03-02 20:14:02 +0000 UTC (7 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 21:49:13.488: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:49:13.488: INFO: prometheus-operator-7797d58ccd-xbbnz from openshift-monitoring started at 2022-03-02 20:09:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 21:49:13.488: INFO: thanos-querier-544bf8c477-rvftb from openshift-monitoring started at 2022-03-02 20:07:50 +0000 UTC (5 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 21:49:13.488: INFO: multus-58kxw from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:49:13.488: INFO: multus-admission-controller-955dw from openshift-multus started at 2022-03-02 20:06:00 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 21:49:13.488: INFO: network-metrics-daemon-4hdhq from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 21:49:13.488: INFO: network-check-source-758fcf9d76-9fjvn from openshift-network-diagnostics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 21:49:13.488: INFO: network-check-target-c4pgw from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 21:49:13.488: INFO: network-operator-57496bd6cc-4pzht from openshift-network-operator started at 2022-03-02 20:04:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container network-operator ready: true, restart count 0
Mar  2 21:49:13.488: INFO: packageserver-67967c9875-srph5 from openshift-operator-lifecycle-manager started at 2022-03-02 20:07:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 21:49:13.488: INFO: service-ca-b874796d6-dfnf5 from openshift-service-ca started at 2022-03-02 20:06:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 21:49:13.488: INFO: sonobuoy from sonobuoy started at 2022-03-02 21:45:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 21:49:13.488: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-xczvl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 21:49:13.488: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 21:49:13.488: INFO: tigera-operator-597f8644c9-57nwz from tigera-operator started at 2022-03-02 20:04:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.488: INFO: 	Container tigera-operator ready: true, restart count 1
Mar  2 21:49:13.488: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.162 before test
Mar  2 21:49:13.555: INFO: calico-node-8mbgb from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 21:49:13.555: INFO: calico-typha-747778ff7-cxk8p from calico-system started at 2022-03-02 20:05:22 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 21:49:13.555: INFO: managed-storage-validation-webhooks-7d645c9954-hxlwh from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 21:49:13.555: INFO: ibm-keepalived-watcher-g4r68 from kube-system started at 2022-03-02 20:04:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 21:49:13.555: INFO: ibm-master-proxy-static-10.138.244.162 from kube-system started at 2022-03-02 20:04:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container pause ready: true, restart count 0
Mar  2 21:49:13.555: INFO: ibm-storage-metrics-agent-5686f759d8-6tq4n from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 21:49:13.555: INFO: ibm-storage-watcher-6488446f7b-nmfdr from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 21:49:13.555: INFO: ibmcloud-block-storage-driver-bz98p from kube-system started at 2022-03-02 20:04:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 21:49:13.555: INFO: cluster-node-tuning-operator-5cb4c7f989-xchxd from openshift-cluster-node-tuning-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 21:49:13.555: INFO: tuned-krv52 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:49:13.555: INFO: console-operator-58ff8dbf6b-bz5q9 from openshift-console-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 21:49:13.555: INFO: console-78b4c56c55-kxx8b from openshift-console started at 2022-03-02 20:13:09 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container console ready: true, restart count 0
Mar  2 21:49:13.555: INFO: downloads-79b8c4c9f8-djqwn from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:49:13.555: INFO: dns-operator-55f6c97874-bfkv2 from openshift-dns-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: dns-default-p6v58 from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: node-ca-bgqw9 from openshift-image-registry started at 2022-03-02 20:07:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:49:13.555: INFO: ingress-canary-wsb4b from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 21:49:13.555: INFO: ingress-operator-bc8cf4dbb-pqp88 from openshift-ingress-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: router-default-678545d6db-zh9tr from openshift-ingress started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container router ready: true, restart count 0
Mar  2 21:49:13.555: INFO: openshift-kube-proxy-ks9sn from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: node-exporter-gtb4f from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.555: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:49:13.555: INFO: prometheus-adapter-68858877cc-cxjl2 from openshift-monitoring started at 2022-03-02 20:09:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.555: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:49:13.556: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-03-02 20:14:02 +0000 UTC (7 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 21:49:13.556: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:49:13.556: INFO: telemeter-client-765c47dd74-4s7d4 from openshift-monitoring started at 2022-03-02 20:06:40 +0000 UTC (3 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container reload ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 21:49:13.556: INFO: multus-admission-controller-ljkq4 from openshift-multus started at 2022-03-02 20:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 21:49:13.556: INFO: multus-bkzqz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:49:13.556: INFO: network-metrics-daemon-2bsdl from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 21:49:13.556: INFO: network-check-target-t499h from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 21:49:13.556: INFO: metrics-78d8646588-dvms5 from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container metrics ready: true, restart count 2
Mar  2 21:49:13.556: INFO: push-gateway-7b4b958bfb-dmwzc from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 21:49:13.556: INFO: sonobuoy-e2e-job-87017147060647d2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container e2e ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 21:49:13.556: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-szbl2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 21:49:13.556: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 21:49:13.556: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fb50a1c9-51d4-4818-840f-1229134e903d 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.138.244.159 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.138.244.159 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 21:49:52.109: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.244.159 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:49:52.110: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321
Mar  2 21:49:52.521: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.244.159:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:49:52.521: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321 UDP
Mar  2 21:49:53.057: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.244.159 54321] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:49:53.057: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 21:49:58.309: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.244.159 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:49:58.309: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321
Mar  2 21:49:58.639: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.244.159:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:49:58.639: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321 UDP
Mar  2 21:49:58.909: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.244.159 54321] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:49:58.909: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 21:50:04.192: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.244.159 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:04.192: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321
Mar  2 21:50:04.469: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.244.159:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:04.469: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321 UDP
Mar  2 21:50:04.766: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.244.159 54321] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:04.766: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 21:50:10.065: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.244.159 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:10.065: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321
Mar  2 21:50:10.546: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.244.159:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:10.546: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321 UDP
Mar  2 21:50:10.825: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.244.159 54321] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:10.825: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 21:50:16.051: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.244.159 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:16.051: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321
Mar  2 21:50:16.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.244.159:54321/hostname] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:16.316: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.244.159, port: 54321 UDP
Mar  2 21:50:16.551: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.244.159 54321] Namespace:sched-pred-9511 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:50:16.552: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: removing the label kubernetes.io/e2e-fb50a1c9-51d4-4818-840f-1229134e903d off the node 10.138.244.159
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fb50a1c9-51d4-4818-840f-1229134e903d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:50:21.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9511" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:69.064 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":4,"skipped":42,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:50:21.995: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:50:23.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-417" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":5,"skipped":50,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:50:23.709: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Mar  2 21:50:23.989: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-5063 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:50:24.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5063" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":6,"skipped":68,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:50:24.196: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 21:50:24.443: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 21:50:24.505: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 21:50:24.555: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.150 before test
Mar  2 21:50:24.633: INFO: calico-node-sfwd8 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 21:50:24.633: INFO: calico-typha-747778ff7-7mfzl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 21:50:24.633: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-03-02 20:09:15 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  2 21:50:24.633: INFO: managed-storage-validation-webhooks-7d645c9954-9tz2t from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 21:50:24.633: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-xx9p2 from ibm-system started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 21:50:24.633: INFO: ibm-file-plugin-58c5ccc6d4-mhk8g from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 21:50:24.633: INFO: ibm-keepalived-watcher-lzm26 from kube-system started at 2022-03-02 20:04:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 21:50:24.633: INFO: ibm-master-proxy-static-10.138.244.150 from kube-system started at 2022-03-02 20:04:27 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 21:50:24.633: INFO: 	Container pause ready: true, restart count 0
Mar  2 21:50:24.633: INFO: ibmcloud-block-storage-driver-n9lmx from kube-system started at 2022-03-02 20:04:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 21:50:24.633: INFO: ibmcloud-block-storage-plugin-67c5f49db6-wlj2j from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 21:50:24.633: INFO: tuned-fqxdn from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:50:24.633: INFO: cluster-samples-operator-69fbcc775-4zjvg from openshift-cluster-samples-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 21:50:24.633: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 21:50:24.633: INFO: cluster-storage-operator-56787c5bcb-9f5hs from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 21:50:24.633: INFO: csi-snapshot-controller-operator-6977df7ddd-dkdjh from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 21:50:24.633: INFO: console-78b4c56c55-ndmlz from openshift-console started at 2022-03-02 20:13:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container console ready: true, restart count 0
Mar  2 21:50:24.633: INFO: downloads-79b8c4c9f8-hqr26 from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:50:24.633: INFO: dns-default-jf6bp from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 21:50:24.633: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:50:24.633: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:50:24.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.633: INFO: cluster-image-registry-operator-58888444b7-jsm7m from openshift-image-registry started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 21:50:24.634: INFO: image-registry-78c5f597d5-rfdtq from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container registry ready: true, restart count 0
Mar  2 21:50:24.634: INFO: node-ca-2d6rp from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:50:24.634: INFO: ingress-canary-wptt6 from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 21:50:24.634: INFO: openshift-kube-proxy-9mnl6 from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: kube-storage-version-migrator-operator-68756f7898-kvfhq from openshift-kube-storage-version-migrator-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 21:50:24.634: INFO: certified-operators-4lfsq from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:50:24.634: INFO: community-operators-fj9gp from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:50:24.634: INFO: marketplace-operator-8689886944-xlcrw from openshift-marketplace started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 21:50:24.634: INFO: redhat-marketplace-tvwc6 from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:50:24.634: INFO: redhat-operators-87vss from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:50:24.634: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: cluster-monitoring-operator-668f4b779-pt5v7 from openshift-monitoring started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container cluster-monitoring-operator ready: true, restart count 1
Mar  2 21:50:24.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Mar  2 21:50:24.634: INFO: grafana-77dc549b6f-nn2b2 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container grafana ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: node-exporter-dff24 from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:50:24.634: INFO: thanos-querier-544bf8c477-jdvqv from openshift-monitoring started at 2022-03-02 20:07:50 +0000 UTC (5 container statuses recorded)
Mar  2 21:50:24.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:50:24.634: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:50:24.635: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 21:50:24.635: INFO: multus-admission-controller-2wfsf from openshift-multus started at 2022-03-02 20:06:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.635: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 21:50:24.635: INFO: multus-zgnwz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:50:24.635: INFO: network-metrics-daemon-qmb7k from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.635: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 21:50:24.635: INFO: network-check-target-rmfwb from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 21:50:24.635: INFO: catalog-operator-777865977d-2fqfw from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 21:50:24.635: INFO: olm-operator-5cb6cff486-wld6q from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 21:50:24.635: INFO: packageserver-67967c9875-524hv from openshift-operator-lifecycle-manager started at 2022-03-02 20:07:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 21:50:24.635: INFO: service-ca-operator-5b668f5895-69p65 from openshift-service-ca-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 21:50:24.635: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-pl9fl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.635: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 21:50:24.635: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 21:50:24.635: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.159 before test
Mar  2 21:50:24.711: INFO: calico-kube-controllers-64bc47d78c-n2mcv from calico-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 21:50:24.711: INFO: calico-node-fhc29 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 21:50:24.711: INFO: calico-typha-747778ff7-sgxvl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 21:50:24.711: INFO: managed-storage-validation-webhooks-7d645c9954-2dqd8 from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 21:50:24.711: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-lt8lm from ibm-system started at 2022-03-02 20:07:28 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 21:50:24.711: INFO: ibm-keepalived-watcher-drf4g from kube-system started at 2022-03-02 20:04:20 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 21:50:24.711: INFO: ibm-master-proxy-static-10.138.244.159 from kube-system started at 2022-03-02 20:04:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 21:50:24.711: INFO: 	Container pause ready: true, restart count 0
Mar  2 21:50:24.711: INFO: ibmcloud-block-storage-driver-85npp from kube-system started at 2022-03-02 20:04:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 21:50:24.711: INFO: vpn-7bf7499b46-c92qv from kube-system started at 2022-03-02 20:06:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container vpn ready: true, restart count 0
Mar  2 21:50:24.711: INFO: tuned-xv4r5 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:50:24.711: INFO: csi-snapshot-controller-69454d4b56-d855w from openshift-cluster-storage-operator started at 2022-03-02 20:06:49 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 21:50:24.711: INFO: csi-snapshot-webhook-788b7d55dd-bqvnb from openshift-cluster-storage-operator started at 2022-03-02 20:06:48 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container webhook ready: true, restart count 0
Mar  2 21:50:24.711: INFO: dns-default-224cd from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:50:24.711: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:50:24.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.711: INFO: node-ca-bc4lw from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:50:24.711: INFO: registry-pvc-permissions-kstcw from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 21:50:24.711: INFO: ingress-canary-2f8b6 from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 21:50:24.711: INFO: router-default-678545d6db-hl5tg from openshift-ingress started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.711: INFO: 	Container router ready: true, restart count 0
Mar  2 21:50:24.712: INFO: openshift-kube-proxy-lxhnl from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: migrator-76bc956454-h9h2j from openshift-kube-storage-version-migrator started at 2022-03-02 20:06:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container migrator ready: true, restart count 0
Mar  2 21:50:24.712: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: kube-state-metrics-7b6c6d96b-tx74c from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (3 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 21:50:24.712: INFO: node-exporter-t8msj from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:50:24.712: INFO: openshift-state-metrics-84c4bdd485-9xfkj from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (3 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 21:50:24.712: INFO: prometheus-adapter-68858877cc-2w7tw from openshift-monitoring started at 2022-03-02 20:09:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:50:24.712: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-03-02 20:14:02 +0000 UTC (7 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 21:50:24.712: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:50:24.712: INFO: prometheus-operator-7797d58ccd-xbbnz from openshift-monitoring started at 2022-03-02 20:09:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 21:50:24.712: INFO: thanos-querier-544bf8c477-rvftb from openshift-monitoring started at 2022-03-02 20:07:50 +0000 UTC (5 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 21:50:24.712: INFO: multus-58kxw from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:50:24.712: INFO: multus-admission-controller-955dw from openshift-multus started at 2022-03-02 20:06:00 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 21:50:24.712: INFO: network-metrics-daemon-4hdhq from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.712: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 21:50:24.712: INFO: network-check-source-758fcf9d76-9fjvn from openshift-network-diagnostics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 21:50:24.712: INFO: network-check-target-c4pgw from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 21:50:24.712: INFO: network-operator-57496bd6cc-4pzht from openshift-network-operator started at 2022-03-02 20:04:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container network-operator ready: true, restart count 0
Mar  2 21:50:24.712: INFO: packageserver-67967c9875-srph5 from openshift-operator-lifecycle-manager started at 2022-03-02 20:07:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 21:50:24.712: INFO: service-ca-b874796d6-dfnf5 from openshift-service-ca started at 2022-03-02 20:06:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.712: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 21:50:24.712: INFO: e2e-host-exec from sched-pred-9511 started at 2022-03-02 21:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.713: INFO: 	Container e2e-host-exec ready: true, restart count 0
Mar  2 21:50:24.713: INFO: pod1 from sched-pred-9511 started at 2022-03-02 21:49:23 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.713: INFO: 	Container agnhost ready: true, restart count 0
Mar  2 21:50:24.713: INFO: pod2 from sched-pred-9511 started at 2022-03-02 21:49:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.713: INFO: 	Container agnhost ready: true, restart count 0
Mar  2 21:50:24.713: INFO: pod3 from sched-pred-9511 started at 2022-03-02 21:49:45 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.713: INFO: 	Container agnhost ready: true, restart count 0
Mar  2 21:50:24.713: INFO: sonobuoy from sonobuoy started at 2022-03-02 21:45:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.713: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 21:50:24.713: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-xczvl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.713: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 21:50:24.713: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 21:50:24.713: INFO: tigera-operator-597f8644c9-57nwz from tigera-operator started at 2022-03-02 20:04:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.713: INFO: 	Container tigera-operator ready: true, restart count 1
Mar  2 21:50:24.713: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.162 before test
Mar  2 21:50:24.783: INFO: calico-node-8mbgb from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 21:50:24.783: INFO: calico-typha-747778ff7-cxk8p from calico-system started at 2022-03-02 20:05:22 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 21:50:24.783: INFO: managed-storage-validation-webhooks-7d645c9954-hxlwh from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 21:50:24.783: INFO: ibm-keepalived-watcher-g4r68 from kube-system started at 2022-03-02 20:04:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 21:50:24.783: INFO: ibm-master-proxy-static-10.138.244.162 from kube-system started at 2022-03-02 20:04:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 21:50:24.783: INFO: 	Container pause ready: true, restart count 0
Mar  2 21:50:24.783: INFO: ibm-storage-metrics-agent-5686f759d8-6tq4n from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 21:50:24.783: INFO: ibm-storage-watcher-6488446f7b-nmfdr from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 21:50:24.783: INFO: ibmcloud-block-storage-driver-bz98p from kube-system started at 2022-03-02 20:04:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 21:50:24.783: INFO: cluster-node-tuning-operator-5cb4c7f989-xchxd from openshift-cluster-node-tuning-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 21:50:24.783: INFO: tuned-krv52 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:50:24.783: INFO: console-operator-58ff8dbf6b-bz5q9 from openshift-console-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 21:50:24.783: INFO: console-78b4c56c55-kxx8b from openshift-console started at 2022-03-02 20:13:09 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container console ready: true, restart count 0
Mar  2 21:50:24.783: INFO: downloads-79b8c4c9f8-djqwn from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:50:24.783: INFO: dns-operator-55f6c97874-bfkv2 from openshift-dns-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 21:50:24.783: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.783: INFO: dns-default-p6v58 from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:50:24.783: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:50:24.783: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.783: INFO: node-ca-bgqw9 from openshift-image-registry started at 2022-03-02 20:07:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:50:24.783: INFO: ingress-canary-wsb4b from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 21:50:24.783: INFO: ingress-operator-bc8cf4dbb-pqp88 from openshift-ingress-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.783: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 21:50:24.783: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.783: INFO: router-default-678545d6db-zh9tr from openshift-ingress started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container router ready: true, restart count 0
Mar  2 21:50:24.784: INFO: openshift-kube-proxy-ks9sn from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: node-exporter-gtb4f from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:50:24.784: INFO: prometheus-adapter-68858877cc-cxjl2 from openshift-monitoring started at 2022-03-02 20:09:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:50:24.784: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-03-02 20:14:02 +0000 UTC (7 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 21:50:24.784: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:50:24.784: INFO: telemeter-client-765c47dd74-4s7d4 from openshift-monitoring started at 2022-03-02 20:06:40 +0000 UTC (3 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container reload ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 21:50:24.784: INFO: multus-admission-controller-ljkq4 from openshift-multus started at 2022-03-02 20:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 21:50:24.784: INFO: multus-bkzqz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:50:24.784: INFO: network-metrics-daemon-2bsdl from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 21:50:24.784: INFO: network-check-target-t499h from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 21:50:24.784: INFO: metrics-78d8646588-dvms5 from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container metrics ready: true, restart count 2
Mar  2 21:50:24.784: INFO: push-gateway-7b4b958bfb-dmwzc from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 21:50:24.784: INFO: sonobuoy-e2e-job-87017147060647d2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container e2e ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 21:50:24.784: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-szbl2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 21:50:24.784: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 21:50:24.784: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6e52d0ae-ddb4-4b2c-be9a-612013f012cf 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.138.244.150 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6e52d0ae-ddb4-4b2c-be9a-612013f012cf off the node 10.138.244.150
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6e52d0ae-ddb4-4b2c-be9a-612013f012cf
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:55:51.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6433" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:327.138 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":7,"skipped":69,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:55:51.340: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 21:55:51.618: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 21:56:51.803: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Mar  2 21:56:51.984: INFO: Created pod: pod0-sched-preemption-low-priority
Mar  2 21:56:52.132: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar  2 21:56:52.203: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:57:20.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8283" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:89.256 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":8,"skipped":75,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:57:20.600: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-a8937d6a-7c49-4022-b0a8-658c1879dd1a
STEP: Creating a pod to test consume configMaps
Mar  2 21:57:20.918: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3b9ff27-7a02-4f43-982b-95978e3a80f6" in namespace "configmap-7810" to be "Succeeded or Failed"
Mar  2 21:57:20.928: INFO: Pod "pod-configmaps-d3b9ff27-7a02-4f43-982b-95978e3a80f6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.14135ms
Mar  2 21:57:22.940: INFO: Pod "pod-configmaps-d3b9ff27-7a02-4f43-982b-95978e3a80f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020914751s
Mar  2 21:57:24.955: INFO: Pod "pod-configmaps-d3b9ff27-7a02-4f43-982b-95978e3a80f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036201792s
STEP: Saw pod success
Mar  2 21:57:24.955: INFO: Pod "pod-configmaps-d3b9ff27-7a02-4f43-982b-95978e3a80f6" satisfied condition "Succeeded or Failed"
Mar  2 21:57:24.969: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-d3b9ff27-7a02-4f43-982b-95978e3a80f6 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 21:57:25.097: INFO: Waiting for pod pod-configmaps-d3b9ff27-7a02-4f43-982b-95978e3a80f6 to disappear
Mar  2 21:57:25.107: INFO: Pod pod-configmaps-d3b9ff27-7a02-4f43-982b-95978e3a80f6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:57:25.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7810" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":84,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:57:25.154: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 21:57:25.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-665 create -f -'
Mar  2 21:57:26.486: INFO: stderr: ""
Mar  2 21:57:26.486: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  2 21:57:26.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-665 create -f -'
Mar  2 21:57:27.315: INFO: stderr: ""
Mar  2 21:57:27.315: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 21:57:28.325: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 21:57:28.325: INFO: Found 0 / 1
Mar  2 21:57:29.334: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 21:57:29.334: INFO: Found 1 / 1
Mar  2 21:57:29.334: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 21:57:29.347: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 21:57:29.347: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 21:57:29.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-665 describe pod agnhost-primary-4wns5'
Mar  2 21:57:29.631: INFO: stderr: ""
Mar  2 21:57:29.631: INFO: stdout: "Name:         agnhost-primary-4wns5\nNamespace:    kubectl-665\nPriority:     0\nNode:         10.138.244.150/10.138.244.150\nStart Time:   Wed, 02 Mar 2022 21:57:26 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 15999035464bdbddcfe1fc599cdf57931b4c140d4bc8e488e9a10cbc3bb76fc6\n              cni.projectcalico.org/podIP: 172.30.43.118/32\n              cni.projectcalico.org/podIPs: 172.30.43.118/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.30.43.118\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.30.43.118\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.30.43.118\nIPs:\n  IP:           172.30.43.118\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://4e2b62fc591fad782a61b59501b11e8c569a8eae064c2f1789a4e5e482acd90c\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 02 Mar 2022 21:57:28 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rxmms (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rxmms:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rxmms\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-665/agnhost-primary-4wns5 to 10.138.244.150\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.43.118/32]\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Mar  2 21:57:29.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-665 describe rc agnhost-primary'
Mar  2 21:57:29.854: INFO: stderr: ""
Mar  2 21:57:29.854: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-665\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-4wns5\n"
Mar  2 21:57:29.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-665 describe service agnhost-primary'
Mar  2 21:57:30.078: INFO: stderr: ""
Mar  2 21:57:30.078: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-665\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.21.100.153\nIPs:               172.21.100.153\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.43.118:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  2 21:57:30.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-665 describe node 10.138.244.150'
Mar  2 21:57:30.617: INFO: stderr: ""
Mar  2 21:57:30.617: INFO: stdout: "Name:               10.138.244.150\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=au-syd\n                    failure-domain.beta.kubernetes.io/zone=syd01\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=168.1.11.212\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.138.244.150\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=au-syd\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-c8fsiavs0ao5oeu4vc7g-kubee2epvgp-default-0000036e\n                    ibm-cloud.kubernetes.io/worker-pool-id=c8fsiavs0ao5oeu4vc7g-14e6b7a\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.7.42_1553_openshift\n                    ibm-cloud.kubernetes.io/zone=syd01\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.138.244.150\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2723066\n                    publicVLAN=2723064\n                    topology.kubernetes.io/region=au-syd\n                    topology.kubernetes.io/zone=syd01\nAnnotations:        projectcalico.org/IPv4Address: 10.138.244.150/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.43.64\nCreationTimestamp:  Wed, 02 Mar 2022 20:04:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.138.244.150\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 02 Mar 2022 21:57:24 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 02 Mar 2022 20:05:39 +0000   Wed, 02 Mar 2022 20:05:39 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 02 Mar 2022 21:56:56 +0000   Wed, 02 Mar 2022 20:04:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 02 Mar 2022 21:56:56 +0000   Wed, 02 Mar 2022 20:04:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 02 Mar 2022 21:56:56 +0000   Wed, 02 Mar 2022 20:04:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 02 Mar 2022 21:56:56 +0000   Wed, 02 Mar 2022 20:06:00 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.138.244.150\n  ExternalIP:  168.1.11.212\n  Hostname:    10.138.244.150\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      103078840Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16253384Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    3910m\n  ephemeral-storage:      94369515442\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 13477320Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                                       fcd2d48b4c80474fb58d97fdf2c67e4f\n  System UUID:                                      4250A704-F0A5-D00A-16F5-F02F68149F17\n  Boot ID:                                          c601b1c7-9e9a-46f2-b195-0ff2b20c5153\n  Kernel Version:                                   3.10.0-1160.59.1.el7.x86_64\n  OS Image:                                         Red Hat\n  Operating System:                                 linux\n  Architecture:                                     amd64\n  Container Runtime Version:                        cri-o://1.20.6-6.rhaos4.7.gitc2b5bcc.el7\n  Kubelet Version:                                  v1.20.11+e880017\n  Kube-Proxy Version:                               v1.20.11+e880017\nPodCIDR:                                            172.30.1.0/24\nPodCIDRs:                                           172.30.1.0/24\nProviderID:                                         ibm://fee034388aa6435883a1f720010ab3a2///c8fsiavs0ao5oeu4vc7g/kube-c8fsiavs0ao5oeu4vc7g-kubee2epvgp-default-0000036e\nNon-terminated Pods:                                (43 in total)\n  Namespace                                         Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                                         ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                                     calico-node-sfwd8                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         112m\n  calico-system                                     calico-typha-747778ff7-7mfzl                               250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         112m\n  default                                           test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         108m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-7d645c9954-9tz2t       10m (0%)      100m (2%)   10Mi (0%)        100Mi (0%)     116m\n  ibm-system                                        ibm-cloud-provider-ip-168-1-52-154-569658dd7b-xx9p2        5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         110m\n  kube-system                                       ibm-file-plugin-58c5ccc6d4-mhk8g                           50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     121m\n  kube-system                                       ibm-keepalived-watcher-lzm26                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         113m\n  kube-system                                       ibm-master-proxy-static-10.138.244.150                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      111m\n  kube-system                                       ibmcloud-block-storage-driver-n9lmx                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     112m\n  kube-system                                       ibmcloud-block-storage-plugin-67c5f49db6-wlj2j             50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     121m\n  kubectl-665                                       agnhost-primary-4wns5                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-node-tuning-operator            tuned-fqxdn                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         110m\n  openshift-cluster-samples-operator                cluster-samples-operator-69fbcc775-4zjvg                   20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         124m\n  openshift-cluster-storage-operator                cluster-storage-operator-56787c5bcb-9f5hs                  10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         124m\n  openshift-cluster-storage-operator                csi-snapshot-controller-operator-6977df7ddd-dkdjh          10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         124m\n  openshift-console                                 console-78b4c56c55-ndmlz                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         104m\n  openshift-console                                 downloads-79b8c4c9f8-hqr26                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         123m\n  openshift-dns                                     dns-default-jf6bp                                          65m (1%)      0 (0%)      131Mi (0%)       0 (0%)         109m\n  openshift-image-registry                          cluster-image-registry-operator-58888444b7-jsm7m           10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         124m\n  openshift-image-registry                          image-registry-78c5f597d5-rfdtq                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         109m\n  openshift-image-registry                          node-ca-2d6rp                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         110m\n  openshift-ingress-canary                          ingress-canary-wptt6                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         110m\n  openshift-kube-proxy                              openshift-kube-proxy-9mnl6                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         112m\n  openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-68756f7898-kvfhq    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         124m\n  openshift-marketplace                             certified-operators-4lfsq                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         109m\n  openshift-marketplace                             community-operators-fj9gp                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         109m\n  openshift-marketplace                             marketplace-operator-8689886944-xlcrw                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         124m\n  openshift-marketplace                             redhat-marketplace-tvwc6                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         109m\n  openshift-marketplace                             redhat-operators-87vss                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         109m\n  openshift-monitoring                              alertmanager-main-0                                        8m (0%)       0 (0%)      270Mi (2%)       0 (0%)         109m\n  openshift-monitoring                              cluster-monitoring-operator-668f4b779-pt5v7                11m (0%)      0 (0%)      70Mi (0%)        0 (0%)         124m\n  openshift-monitoring                              grafana-77dc549b6f-nn2b2                                   5m (0%)       0 (0%)      120Mi (0%)       0 (0%)         109m\n  openshift-monitoring                              node-exporter-dff24                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         110m\n  openshift-monitoring                              thanos-querier-544bf8c477-jdvqv                            9m (0%)       0 (0%)      92Mi (0%)        0 (0%)         109m\n  openshift-multus                                  multus-admission-controller-2wfsf                          20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         111m\n  openshift-multus                                  multus-zgnwz                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         112m\n  openshift-multus                                  network-metrics-daemon-qmb7k                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         112m\n  openshift-network-diagnostics                     network-check-target-rmfwb                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         112m\n  openshift-operator-lifecycle-manager              catalog-operator-777865977d-2fqfw                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         124m\n  openshift-operator-lifecycle-manager              olm-operator-5cb6cff486-wld6q                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         124m\n  openshift-operator-lifecycle-manager              packageserver-67967c9875-524hv                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         109m\n  openshift-service-ca-operator                     service-ca-operator-5b668f5895-69p65                       10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         124m\n  sonobuoy                                          sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-pl9fl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests         Limits\n  --------               --------         ------\n  cpu                    1272m (32%)      1200m (30%)\n  memory                 3409426Ki (25%)  1421600Ki (10%)\n  ephemeral-storage      0 (0%)           0 (0%)\n  hugepages-1Gi          0 (0%)           0 (0%)\n  hugepages-2Mi          0 (0%)           0 (0%)\n  scheduling.k8s.io/foo  0                0\nEvents:\n  Type    Reason                   Age                  From        Message\n  ----    ------                   ----                 ----        -------\n  Normal  Starting                 113m                 kubelet     Starting kubelet.\n  Normal  NodeAllocatableEnforced  113m                 kubelet     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientPID     113m (x7 over 113m)  kubelet     Node 10.138.244.150 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  113m (x8 over 113m)  kubelet     Node 10.138.244.150 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    113m (x8 over 113m)  kubelet     Node 10.138.244.150 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 112m                 kube-proxy  Starting kube-proxy.\n"
Mar  2 21:57:30.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-665 describe namespace kubectl-665'
Mar  2 21:57:30.866: INFO: stderr: ""
Mar  2 21:57:30.866: INFO: stdout: "Name:         kubectl-665\nLabels:       e2e-framework=kubectl\n              e2e-run=3cbcf0b8-bba3-47d5-ace4-ef87f2414ab2\nAnnotations:  openshift.io/sa.scc.mcs: s0:c31,c0\n              openshift.io/sa.scc.supplemental-groups: 1000930000/10000\n              openshift.io/sa.scc.uid-range: 1000930000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:57:30.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-665" for this suite.

• [SLOW TEST:5.750 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1090
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":10,"skipped":109,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:57:30.904: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 21:57:31.254: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 21:57:31.302: INFO: Number of nodes with available pods: 0
Mar  2 21:57:31.302: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:32.365: INFO: Number of nodes with available pods: 0
Mar  2 21:57:32.365: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:33.339: INFO: Number of nodes with available pods: 0
Mar  2 21:57:33.340: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:34.346: INFO: Number of nodes with available pods: 0
Mar  2 21:57:34.346: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:35.333: INFO: Number of nodes with available pods: 0
Mar  2 21:57:35.333: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:36.340: INFO: Number of nodes with available pods: 0
Mar  2 21:57:36.340: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:37.353: INFO: Number of nodes with available pods: 0
Mar  2 21:57:37.353: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:38.329: INFO: Number of nodes with available pods: 0
Mar  2 21:57:38.329: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:39.330: INFO: Number of nodes with available pods: 0
Mar  2 21:57:39.330: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:40.351: INFO: Number of nodes with available pods: 0
Mar  2 21:57:40.351: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:41.334: INFO: Number of nodes with available pods: 0
Mar  2 21:57:41.334: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:42.328: INFO: Number of nodes with available pods: 0
Mar  2 21:57:42.329: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:43.331: INFO: Number of nodes with available pods: 0
Mar  2 21:57:43.331: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:44.356: INFO: Number of nodes with available pods: 0
Mar  2 21:57:44.356: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:45.333: INFO: Number of nodes with available pods: 0
Mar  2 21:57:45.333: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:46.330: INFO: Number of nodes with available pods: 0
Mar  2 21:57:46.330: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:47.337: INFO: Number of nodes with available pods: 2
Mar  2 21:57:47.337: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 21:57:48.340: INFO: Number of nodes with available pods: 3
Mar  2 21:57:48.340: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  2 21:57:48.540: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:48.540: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:48.540: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:49.603: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:49.603: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:49.603: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:50.603: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:50.603: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:50.604: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:51.612: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:51.612: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:51.612: INFO: Pod daemon-set-wtcjk is not available
Mar  2 21:57:51.612: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:52.602: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:52.602: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:52.602: INFO: Pod daemon-set-wtcjk is not available
Mar  2 21:57:52.602: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:53.606: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:53.606: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:53.606: INFO: Pod daemon-set-wtcjk is not available
Mar  2 21:57:53.606: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:54.627: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:54.627: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:54.627: INFO: Pod daemon-set-wtcjk is not available
Mar  2 21:57:54.627: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:55.604: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:55.604: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:55.604: INFO: Pod daemon-set-wtcjk is not available
Mar  2 21:57:55.604: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:56.603: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:56.603: INFO: Wrong image for pod: daemon-set-wtcjk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:56.604: INFO: Pod daemon-set-wtcjk is not available
Mar  2 21:57:56.604: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:57.601: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:57.601: INFO: Pod daemon-set-bqmj8 is not available
Mar  2 21:57:57.601: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:58.603: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:58.604: INFO: Pod daemon-set-bqmj8 is not available
Mar  2 21:57:58.604: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:59.601: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:57:59.601: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:00.602: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:00.602: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:01.602: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:01.602: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:02.602: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:02.602: INFO: Pod daemon-set-44wlz is not available
Mar  2 21:58:02.602: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:03.603: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:03.603: INFO: Pod daemon-set-44wlz is not available
Mar  2 21:58:03.603: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:04.604: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:04.604: INFO: Pod daemon-set-44wlz is not available
Mar  2 21:58:04.604: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:05.610: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:05.610: INFO: Pod daemon-set-44wlz is not available
Mar  2 21:58:05.610: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:06.602: INFO: Wrong image for pod: daemon-set-44wlz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:06.602: INFO: Pod daemon-set-44wlz is not available
Mar  2 21:58:06.602: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:07.605: INFO: Pod daemon-set-kkk7p is not available
Mar  2 21:58:07.606: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:08.633: INFO: Pod daemon-set-kkk7p is not available
Mar  2 21:58:08.633: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:09.601: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:10.602: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:11.602: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:11.602: INFO: Pod daemon-set-xb2xv is not available
Mar  2 21:58:12.601: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:12.601: INFO: Pod daemon-set-xb2xv is not available
Mar  2 21:58:13.606: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:13.606: INFO: Pod daemon-set-xb2xv is not available
Mar  2 21:58:14.600: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:14.600: INFO: Pod daemon-set-xb2xv is not available
Mar  2 21:58:15.601: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:15.601: INFO: Pod daemon-set-xb2xv is not available
Mar  2 21:58:16.617: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:16.617: INFO: Pod daemon-set-xb2xv is not available
Mar  2 21:58:17.616: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:17.616: INFO: Pod daemon-set-xb2xv is not available
Mar  2 21:58:18.601: INFO: Wrong image for pod: daemon-set-xb2xv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:58:18.602: INFO: Pod daemon-set-xb2xv is not available
Mar  2 21:58:19.620: INFO: Pod daemon-set-rkvfx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  2 21:58:19.661: INFO: Number of nodes with available pods: 2
Mar  2 21:58:19.661: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:20.698: INFO: Number of nodes with available pods: 2
Mar  2 21:58:20.698: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:21.689: INFO: Number of nodes with available pods: 2
Mar  2 21:58:21.689: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:22.695: INFO: Number of nodes with available pods: 2
Mar  2 21:58:22.695: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:23.690: INFO: Number of nodes with available pods: 2
Mar  2 21:58:23.690: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:24.690: INFO: Number of nodes with available pods: 2
Mar  2 21:58:24.690: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:25.689: INFO: Number of nodes with available pods: 2
Mar  2 21:58:25.689: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:26.696: INFO: Number of nodes with available pods: 2
Mar  2 21:58:26.696: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:27.696: INFO: Number of nodes with available pods: 2
Mar  2 21:58:27.696: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:28.689: INFO: Number of nodes with available pods: 2
Mar  2 21:58:28.689: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:29.705: INFO: Number of nodes with available pods: 2
Mar  2 21:58:29.705: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:30.688: INFO: Number of nodes with available pods: 2
Mar  2 21:58:30.688: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:31.722: INFO: Number of nodes with available pods: 2
Mar  2 21:58:31.722: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:32.695: INFO: Number of nodes with available pods: 2
Mar  2 21:58:32.695: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:33.701: INFO: Number of nodes with available pods: 2
Mar  2 21:58:33.701: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 21:58:34.691: INFO: Number of nodes with available pods: 3
Mar  2 21:58:34.691: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6283, will wait for the garbage collector to delete the pods
Mar  2 21:58:34.900: INFO: Deleting DaemonSet.extensions daemon-set took: 77.220398ms
Mar  2 21:58:35.101: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.548652ms
Mar  2 21:58:49.111: INFO: Number of nodes with available pods: 0
Mar  2 21:58:49.111: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 21:58:49.123: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6283/daemonsets","resourceVersion":"58315"},"items":null}

Mar  2 21:58:49.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6283/pods","resourceVersion":"58315"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:58:49.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6283" for this suite.

• [SLOW TEST:78.359 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":11,"skipped":116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:58:49.264: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-c84b7a22-514c-4190-b917-23a1e79252b3
STEP: Creating a pod to test consume secrets
Mar  2 21:58:49.610: INFO: Waiting up to 5m0s for pod "pod-secrets-d5851137-07da-47ad-933f-8a4c69429f1f" in namespace "secrets-9090" to be "Succeeded or Failed"
Mar  2 21:58:49.626: INFO: Pod "pod-secrets-d5851137-07da-47ad-933f-8a4c69429f1f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.084333ms
Mar  2 21:58:51.645: INFO: Pod "pod-secrets-d5851137-07da-47ad-933f-8a4c69429f1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034866117s
Mar  2 21:58:53.663: INFO: Pod "pod-secrets-d5851137-07da-47ad-933f-8a4c69429f1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052807142s
STEP: Saw pod success
Mar  2 21:58:53.663: INFO: Pod "pod-secrets-d5851137-07da-47ad-933f-8a4c69429f1f" satisfied condition "Succeeded or Failed"
Mar  2 21:58:53.673: INFO: Trying to get logs from node 10.138.244.159 pod pod-secrets-d5851137-07da-47ad-933f-8a4c69429f1f container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 21:58:53.784: INFO: Waiting for pod pod-secrets-d5851137-07da-47ad-933f-8a4c69429f1f to disappear
Mar  2 21:58:53.799: INFO: Pod pod-secrets-d5851137-07da-47ad-933f-8a4c69429f1f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:58:53.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9090" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":139,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:58:53.845: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Mar  2 21:58:54.168: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:59:37.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2803" for this suite.

• [SLOW TEST:44.137 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":13,"skipped":142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:59:37.983: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  2 21:59:38.471: INFO: Waiting up to 5m0s for pod "pod-29d413d8-7848-43ab-b87b-4978b491966c" in namespace "emptydir-8121" to be "Succeeded or Failed"
Mar  2 21:59:38.532: INFO: Pod "pod-29d413d8-7848-43ab-b87b-4978b491966c": Phase="Pending", Reason="", readiness=false. Elapsed: 61.171968ms
Mar  2 21:59:40.602: INFO: Pod "pod-29d413d8-7848-43ab-b87b-4978b491966c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130873523s
Mar  2 21:59:42.612: INFO: Pod "pod-29d413d8-7848-43ab-b87b-4978b491966c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.141041387s
STEP: Saw pod success
Mar  2 21:59:42.612: INFO: Pod "pod-29d413d8-7848-43ab-b87b-4978b491966c" satisfied condition "Succeeded or Failed"
Mar  2 21:59:42.623: INFO: Trying to get logs from node 10.138.244.159 pod pod-29d413d8-7848-43ab-b87b-4978b491966c container test-container: <nil>
STEP: delete the pod
Mar  2 21:59:42.699: INFO: Waiting for pod pod-29d413d8-7848-43ab-b87b-4978b491966c to disappear
Mar  2 21:59:42.711: INFO: Pod pod-29d413d8-7848-43ab-b87b-4978b491966c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:59:42.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8121" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:59:42.754: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  2 21:59:43.039: INFO: Waiting up to 5m0s for pod "downward-api-a274a354-610a-4bdd-8399-0cb452530705" in namespace "downward-api-4136" to be "Succeeded or Failed"
Mar  2 21:59:43.077: INFO: Pod "downward-api-a274a354-610a-4bdd-8399-0cb452530705": Phase="Pending", Reason="", readiness=false. Elapsed: 37.270332ms
Mar  2 21:59:45.087: INFO: Pod "downward-api-a274a354-610a-4bdd-8399-0cb452530705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047437695s
Mar  2 21:59:47.103: INFO: Pod "downward-api-a274a354-610a-4bdd-8399-0cb452530705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063961495s
STEP: Saw pod success
Mar  2 21:59:47.103: INFO: Pod "downward-api-a274a354-610a-4bdd-8399-0cb452530705" satisfied condition "Succeeded or Failed"
Mar  2 21:59:47.113: INFO: Trying to get logs from node 10.138.244.159 pod downward-api-a274a354-610a-4bdd-8399-0cb452530705 container dapi-container: <nil>
STEP: delete the pod
Mar  2 21:59:47.181: INFO: Waiting for pod downward-api-a274a354-610a-4bdd-8399-0cb452530705 to disappear
Mar  2 21:59:47.199: INFO: Pod downward-api-a274a354-610a-4bdd-8399-0cb452530705 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 21:59:47.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4136" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":15,"skipped":206,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 21:59:47.235: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3832 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3832;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3832 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3832;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3832.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3832.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3832.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3832.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3832.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3832.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3832.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3832.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3832.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3832.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3832.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3832.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3832.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 88.116.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.116.88_udp@PTR;check="$$(dig +tcp +noall +answer +search 88.116.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.116.88_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3832 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3832;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3832 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3832;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3832.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3832.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3832.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3832.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3832.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3832.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3832.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3832.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3832.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3832.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3832.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3832.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3832.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 88.116.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.116.88_udp@PTR;check="$$(dig +tcp +noall +answer +search 88.116.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.116.88_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:00:07.742: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:07.762: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:07.783: INFO: Unable to read wheezy_udp@dns-test-service.dns-3832 from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:07.805: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3832 from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:07.841: INFO: Unable to read wheezy_udp@dns-test-service.dns-3832.svc from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:07.862: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3832.svc from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:08.069: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:08.124: INFO: Unable to read jessie_udp@dns-test-service.dns-3832 from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:08.140: INFO: Unable to read jessie_tcp@dns-test-service.dns-3832 from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:08.174: INFO: Unable to read jessie_udp@dns-test-service.dns-3832.svc from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:08.245: INFO: Unable to read jessie_tcp@dns-test-service.dns-3832.svc from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:08.409: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3832.svc from pod dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69: the server could not find the requested resource (get pods dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69)
Mar  2 22:00:08.918: INFO: Lookups using dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3832 wheezy_tcp@dns-test-service.dns-3832 wheezy_udp@dns-test-service.dns-3832.svc wheezy_tcp@dns-test-service.dns-3832.svc jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-3832 jessie_tcp@dns-test-service.dns-3832 jessie_udp@dns-test-service.dns-3832.svc jessie_tcp@dns-test-service.dns-3832.svc jessie_tcp@_http._tcp.dns-test-service.dns-3832.svc]

Mar  2 22:00:14.738: INFO: DNS probes using dns-3832/dns-test-7ee1af0d-cde2-4df5-8ab2-c79e45d59d69 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:00:14.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3832" for this suite.

• [SLOW TEST:27.734 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":16,"skipped":208,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:00:14.969: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:00:15.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c24792c-d805-45dc-b172-ec956f400fd8" in namespace "downward-api-4432" to be "Succeeded or Failed"
Mar  2 22:00:15.283: INFO: Pod "downwardapi-volume-0c24792c-d805-45dc-b172-ec956f400fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.53183ms
Mar  2 22:00:17.294: INFO: Pod "downwardapi-volume-0c24792c-d805-45dc-b172-ec956f400fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032203689s
Mar  2 22:00:19.307: INFO: Pod "downwardapi-volume-0c24792c-d805-45dc-b172-ec956f400fd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04531231s
STEP: Saw pod success
Mar  2 22:00:19.308: INFO: Pod "downwardapi-volume-0c24792c-d805-45dc-b172-ec956f400fd8" satisfied condition "Succeeded or Failed"
Mar  2 22:00:19.317: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-0c24792c-d805-45dc-b172-ec956f400fd8 container client-container: <nil>
STEP: delete the pod
Mar  2 22:00:19.388: INFO: Waiting for pod downwardapi-volume-0c24792c-d805-45dc-b172-ec956f400fd8 to disappear
Mar  2 22:00:19.397: INFO: Pod downwardapi-volume-0c24792c-d805-45dc-b172-ec956f400fd8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:00:19.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4432" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:00:19.453: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-189bd870-d5c5-4d43-a736-78ba49c818b5
STEP: Creating a pod to test consume configMaps
Mar  2 22:00:19.811: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3240dc9a-2cc3-429b-91a3-84e9f5f95ce5" in namespace "projected-4009" to be "Succeeded or Failed"
Mar  2 22:00:19.820: INFO: Pod "pod-projected-configmaps-3240dc9a-2cc3-429b-91a3-84e9f5f95ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.527308ms
Mar  2 22:00:21.833: INFO: Pod "pod-projected-configmaps-3240dc9a-2cc3-429b-91a3-84e9f5f95ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022399217s
Mar  2 22:00:23.892: INFO: Pod "pod-projected-configmaps-3240dc9a-2cc3-429b-91a3-84e9f5f95ce5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081454378s
STEP: Saw pod success
Mar  2 22:00:23.893: INFO: Pod "pod-projected-configmaps-3240dc9a-2cc3-429b-91a3-84e9f5f95ce5" satisfied condition "Succeeded or Failed"
Mar  2 22:00:23.906: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-configmaps-3240dc9a-2cc3-429b-91a3-84e9f5f95ce5 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:00:23.963: INFO: Waiting for pod pod-projected-configmaps-3240dc9a-2cc3-429b-91a3-84e9f5f95ce5 to disappear
Mar  2 22:00:23.975: INFO: Pod pod-projected-configmaps-3240dc9a-2cc3-429b-91a3-84e9f5f95ce5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:00:23.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4009" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":282,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:00:24.009: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-dfff329a-3e60-447c-b0d0-d401f4215548
STEP: Creating a pod to test consume configMaps
Mar  2 22:00:24.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-f985ea93-1d23-419a-be41-626210ed8c44" in namespace "configmap-970" to be "Succeeded or Failed"
Mar  2 22:00:24.331: INFO: Pod "pod-configmaps-f985ea93-1d23-419a-be41-626210ed8c44": Phase="Pending", Reason="", readiness=false. Elapsed: 12.687284ms
Mar  2 22:00:26.343: INFO: Pod "pod-configmaps-f985ea93-1d23-419a-be41-626210ed8c44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024197793s
Mar  2 22:00:28.358: INFO: Pod "pod-configmaps-f985ea93-1d23-419a-be41-626210ed8c44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039769077s
STEP: Saw pod success
Mar  2 22:00:28.358: INFO: Pod "pod-configmaps-f985ea93-1d23-419a-be41-626210ed8c44" satisfied condition "Succeeded or Failed"
Mar  2 22:00:28.382: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-f985ea93-1d23-419a-be41-626210ed8c44 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:00:28.437: INFO: Waiting for pod pod-configmaps-f985ea93-1d23-419a-be41-626210ed8c44 to disappear
Mar  2 22:00:28.445: INFO: Pod pod-configmaps-f985ea93-1d23-419a-be41-626210ed8c44 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:00:28.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-970" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:00:28.486: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:00:40.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-430" for this suite.

• [SLOW TEST:11.588 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":20,"skipped":319,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:00:40.075: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Mar  2 22:00:40.281: INFO: Major version: 1
STEP: Confirm minor version
Mar  2 22:00:40.281: INFO: cleanMinorVersion: 20
Mar  2 22:00:40.281: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:00:40.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2343" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":21,"skipped":333,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:00:40.363: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:00:40.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97cdd731-6792-495b-a5d7-dc35f866fa09" in namespace "projected-2948" to be "Succeeded or Failed"
Mar  2 22:00:40.684: INFO: Pod "downwardapi-volume-97cdd731-6792-495b-a5d7-dc35f866fa09": Phase="Pending", Reason="", readiness=false. Elapsed: 16.34016ms
Mar  2 22:00:42.695: INFO: Pod "downwardapi-volume-97cdd731-6792-495b-a5d7-dc35f866fa09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027179114s
Mar  2 22:00:44.709: INFO: Pod "downwardapi-volume-97cdd731-6792-495b-a5d7-dc35f866fa09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041109535s
STEP: Saw pod success
Mar  2 22:00:44.709: INFO: Pod "downwardapi-volume-97cdd731-6792-495b-a5d7-dc35f866fa09" satisfied condition "Succeeded or Failed"
Mar  2 22:00:44.719: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-97cdd731-6792-495b-a5d7-dc35f866fa09 container client-container: <nil>
STEP: delete the pod
Mar  2 22:00:44.782: INFO: Waiting for pod downwardapi-volume-97cdd731-6792-495b-a5d7-dc35f866fa09 to disappear
Mar  2 22:00:44.797: INFO: Pod downwardapi-volume-97cdd731-6792-495b-a5d7-dc35f866fa09 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:00:44.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2948" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":336,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:00:44.855: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:01:21.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2689" for this suite.
STEP: Destroying namespace "nsdeletetest-5720" for this suite.
Mar  2 22:01:21.720: INFO: Namespace nsdeletetest-5720 was already deleted
STEP: Destroying namespace "nsdeletetest-8529" for this suite.

• [SLOW TEST:36.889 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":23,"skipped":339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:01:21.752: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:01:22.764: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  2 22:01:24.798: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855282, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855282, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855282, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855282, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:01:27.841: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:01:27.860: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:01:29.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5811" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.710 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":24,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:01:29.467: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-40e2d498-5373-41ab-82ab-13f13115cb3d
STEP: Creating a pod to test consume configMaps
Mar  2 22:01:29.769: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71199ec4-4efe-460c-a3cf-b8ae03cd3bfc" in namespace "projected-2628" to be "Succeeded or Failed"
Mar  2 22:01:29.828: INFO: Pod "pod-projected-configmaps-71199ec4-4efe-460c-a3cf-b8ae03cd3bfc": Phase="Pending", Reason="", readiness=false. Elapsed: 59.023477ms
Mar  2 22:01:31.840: INFO: Pod "pod-projected-configmaps-71199ec4-4efe-460c-a3cf-b8ae03cd3bfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070989216s
Mar  2 22:01:33.869: INFO: Pod "pod-projected-configmaps-71199ec4-4efe-460c-a3cf-b8ae03cd3bfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.099917173s
STEP: Saw pod success
Mar  2 22:01:33.869: INFO: Pod "pod-projected-configmaps-71199ec4-4efe-460c-a3cf-b8ae03cd3bfc" satisfied condition "Succeeded or Failed"
Mar  2 22:01:33.883: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-configmaps-71199ec4-4efe-460c-a3cf-b8ae03cd3bfc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:01:33.981: INFO: Waiting for pod pod-projected-configmaps-71199ec4-4efe-460c-a3cf-b8ae03cd3bfc to disappear
Mar  2 22:01:33.993: INFO: Pod pod-projected-configmaps-71199ec4-4efe-460c-a3cf-b8ae03cd3bfc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:01:33.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2628" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":384,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:01:34.064: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:01:34.420: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a8d4a80-7adb-4f80-a2e9-7f43c3002755" in namespace "downward-api-2322" to be "Succeeded or Failed"
Mar  2 22:01:34.445: INFO: Pod "downwardapi-volume-6a8d4a80-7adb-4f80-a2e9-7f43c3002755": Phase="Pending", Reason="", readiness=false. Elapsed: 24.964343ms
Mar  2 22:01:36.456: INFO: Pod "downwardapi-volume-6a8d4a80-7adb-4f80-a2e9-7f43c3002755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036150496s
Mar  2 22:01:38.477: INFO: Pod "downwardapi-volume-6a8d4a80-7adb-4f80-a2e9-7f43c3002755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057408977s
STEP: Saw pod success
Mar  2 22:01:38.478: INFO: Pod "downwardapi-volume-6a8d4a80-7adb-4f80-a2e9-7f43c3002755" satisfied condition "Succeeded or Failed"
Mar  2 22:01:38.496: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-6a8d4a80-7adb-4f80-a2e9-7f43c3002755 container client-container: <nil>
STEP: delete the pod
Mar  2 22:01:38.609: INFO: Waiting for pod downwardapi-volume-6a8d4a80-7adb-4f80-a2e9-7f43c3002755 to disappear
Mar  2 22:01:38.636: INFO: Pod downwardapi-volume-6a8d4a80-7adb-4f80-a2e9-7f43c3002755 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:01:38.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2322" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":390,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:01:38.751: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:01:39.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7981" for this suite.
STEP: Destroying namespace "nspatchtest-4d210c3b-ae32-4be5-95fd-6aa5e4fad07c-5098" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":27,"skipped":391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:01:39.588: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  2 22:01:41.984: INFO: &Pod{ObjectMeta:{send-events-c4b2f3d8-8ce4-454a-979e-765dcbf8f0d9  events-3075 /api/v1/namespaces/events-3075/pods/send-events-c4b2f3d8-8ce4-454a-979e-765dcbf8f0d9 25ebac9d-97b6-4e4d-9bed-76b8511021bb 60796 0 2022-03-02 22:01:39 +0000 UTC <nil> <nil> map[name:foo time:764019165] map[cni.projectcalico.org/containerID:f1b549877c68a132617a483efb8014ec2dc19f09b05476d35a4ebf331c1cde3c cni.projectcalico.org/podIP:172.30.228.12/32 cni.projectcalico.org/podIPs:172.30.228.12/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.12"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.12"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-03-02 22:01:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:01:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-03-02 22:01:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.228.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2022-03-02 22:01:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hwjsr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hwjsr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hwjsr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:01:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:01:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:01:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:01:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:172.30.228.12,StartTime:2022-03-02 22:01:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:01:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://a11be00596707aaf807ad4407f315ed22004b74dd186e13ccc6facd6b8d3b6da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.228.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar  2 22:01:43.998: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  2 22:01:46.025: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:01:46.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3075" for this suite.

• [SLOW TEST:6.520 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":28,"skipped":462,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:01:46.108: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:01:46.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3558" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":29,"skipped":475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:01:46.623: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-61d571be-d728-47d8-a04e-86bbbba5ba2c in namespace container-probe-3792
Mar  2 22:01:51.005: INFO: Started pod liveness-61d571be-d728-47d8-a04e-86bbbba5ba2c in namespace container-probe-3792
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:01:51.015: INFO: Initial restart count of pod liveness-61d571be-d728-47d8-a04e-86bbbba5ba2c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:05:52.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3792" for this suite.

• [SLOW TEST:246.296 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:05:52.921: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:05:58.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6392" for this suite.

• [SLOW TEST:5.522 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:05:58.444: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 22:05:58.867: INFO: Waiting up to 5m0s for pod "pod-872fce6f-baab-4f00-a11c-2295a0fcab1b" in namespace "emptydir-4191" to be "Succeeded or Failed"
Mar  2 22:05:58.877: INFO: Pod "pod-872fce6f-baab-4f00-a11c-2295a0fcab1b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.971337ms
Mar  2 22:06:00.891: INFO: Pod "pod-872fce6f-baab-4f00-a11c-2295a0fcab1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023476159s
Mar  2 22:06:02.911: INFO: Pod "pod-872fce6f-baab-4f00-a11c-2295a0fcab1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043360697s
STEP: Saw pod success
Mar  2 22:06:02.911: INFO: Pod "pod-872fce6f-baab-4f00-a11c-2295a0fcab1b" satisfied condition "Succeeded or Failed"
Mar  2 22:06:02.932: INFO: Trying to get logs from node 10.138.244.159 pod pod-872fce6f-baab-4f00-a11c-2295a0fcab1b container test-container: <nil>
STEP: delete the pod
Mar  2 22:06:03.064: INFO: Waiting for pod pod-872fce6f-baab-4f00-a11c-2295a0fcab1b to disappear
Mar  2 22:06:03.095: INFO: Pod pod-872fce6f-baab-4f00-a11c-2295a0fcab1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:06:03.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4191" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:06:03.148: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar  2 22:06:03.637: INFO: observed Pod pod-test in namespace pods-4010 in phase Pending conditions []
Mar  2 22:06:03.637: INFO: observed Pod pod-test in namespace pods-4010 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC  }]
Mar  2 22:06:03.711: INFO: observed Pod pod-test in namespace pods-4010 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC  }]
Mar  2 22:06:05.005: INFO: observed Pod pod-test in namespace pods-4010 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC  }]
Mar  2 22:06:05.068: INFO: observed Pod pod-test in namespace pods-4010 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 22:06:03 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Mar  2 22:06:06.519: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar  2 22:06:06.666: INFO: observed event type ADDED
Mar  2 22:06:06.666: INFO: observed event type MODIFIED
Mar  2 22:06:06.666: INFO: observed event type MODIFIED
Mar  2 22:06:06.669: INFO: observed event type MODIFIED
Mar  2 22:06:06.669: INFO: observed event type MODIFIED
Mar  2 22:06:06.669: INFO: observed event type MODIFIED
Mar  2 22:06:06.669: INFO: observed event type MODIFIED
Mar  2 22:06:06.669: INFO: observed event type MODIFIED
Mar  2 22:06:06.669: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:06:06.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4010" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":33,"skipped":612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:06:06.754: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 22:06:07.248: INFO: Number of nodes with available pods: 0
Mar  2 22:06:07.248: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 22:06:08.282: INFO: Number of nodes with available pods: 0
Mar  2 22:06:08.282: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 22:06:09.291: INFO: Number of nodes with available pods: 0
Mar  2 22:06:09.291: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 22:06:10.275: INFO: Number of nodes with available pods: 3
Mar  2 22:06:10.275: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  2 22:06:10.361: INFO: Number of nodes with available pods: 2
Mar  2 22:06:10.361: INFO: Node 10.138.244.159 is running more than one daemon pod
Mar  2 22:06:11.399: INFO: Number of nodes with available pods: 2
Mar  2 22:06:11.399: INFO: Node 10.138.244.159 is running more than one daemon pod
Mar  2 22:06:12.402: INFO: Number of nodes with available pods: 2
Mar  2 22:06:12.402: INFO: Node 10.138.244.159 is running more than one daemon pod
Mar  2 22:06:13.409: INFO: Number of nodes with available pods: 3
Mar  2 22:06:13.409: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8293, will wait for the garbage collector to delete the pods
Mar  2 22:06:13.558: INFO: Deleting DaemonSet.extensions daemon-set took: 46.663993ms
Mar  2 22:06:13.658: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.412567ms
Mar  2 22:06:27.382: INFO: Number of nodes with available pods: 0
Mar  2 22:06:27.382: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 22:06:27.395: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8293/daemonsets","resourceVersion":"62881"},"items":null}

Mar  2 22:06:27.405: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8293/pods","resourceVersion":"62881"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:06:27.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8293" for this suite.

• [SLOW TEST:20.794 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":34,"skipped":640,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:06:27.554: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Mar  2 22:06:27.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-9965 create -f -'
Mar  2 22:06:28.603: INFO: stderr: ""
Mar  2 22:06:28.603: INFO: stdout: "pod/pause created\n"
Mar  2 22:06:28.603: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  2 22:06:28.603: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9965" to be "running and ready"
Mar  2 22:06:28.631: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 27.465848ms
Mar  2 22:06:30.656: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052403675s
Mar  2 22:06:32.671: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.067813599s
Mar  2 22:06:32.671: INFO: Pod "pause" satisfied condition "running and ready"
Mar  2 22:06:32.671: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  2 22:06:32.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-9965 label pods pause testing-label=testing-label-value'
Mar  2 22:06:32.944: INFO: stderr: ""
Mar  2 22:06:32.944: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  2 22:06:32.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-9965 get pod pause -L testing-label'
Mar  2 22:06:33.088: INFO: stderr: ""
Mar  2 22:06:33.088: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  2 22:06:33.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-9965 label pods pause testing-label-'
Mar  2 22:06:33.304: INFO: stderr: ""
Mar  2 22:06:33.304: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  2 22:06:33.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-9965 get pod pause -L testing-label'
Mar  2 22:06:33.468: INFO: stderr: ""
Mar  2 22:06:33.468: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Mar  2 22:06:33.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-9965 delete --grace-period=0 --force -f -'
Mar  2 22:06:33.693: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:06:33.693: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  2 22:06:33.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-9965 get rc,svc -l name=pause --no-headers'
Mar  2 22:06:33.866: INFO: stderr: "No resources found in kubectl-9965 namespace.\n"
Mar  2 22:06:33.866: INFO: stdout: ""
Mar  2 22:06:33.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-9965 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 22:06:34.020: INFO: stderr: ""
Mar  2 22:06:34.020: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:06:34.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9965" for this suite.

• [SLOW TEST:6.578 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":35,"skipped":643,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:06:34.132: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar  2 22:06:39.251: INFO: Successfully updated pod "labelsupdate5e926560-8581-4d80-9ef1-49b63e9a28eb"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:06:41.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6989" for this suite.

• [SLOW TEST:7.277 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":649,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:06:41.410: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:06:41.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4894" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":37,"skipped":668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:06:41.803: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:06:41.984: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:06:43.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7115" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":38,"skipped":702,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:06:43.121: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4691
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 22:06:43.377: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 22:06:43.625: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 22:06:45.636: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 22:06:47.637: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:06:49.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:06:51.641: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:06:53.641: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:06:55.637: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:06:57.673: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:06:59.639: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:07:01.639: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:07:03.636: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 22:07:05.651: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 22:07:05.672: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 22:07:05.698: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 22:07:09.876: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 22:07:09.876: INFO: Going to poll 172.30.43.127 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  2 22:07:09.886: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.43.127:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:07:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:07:10.126: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 22:07:10.126: INFO: Going to poll 172.30.228.33 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  2 22:07:10.143: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.228.33:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:07:10.143: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:07:10.387: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 22:07:10.387: INFO: Going to poll 172.30.21.162 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  2 22:07:10.398: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.21.162:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:07:10.398: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:07:10.675: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:10.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4691" for this suite.

• [SLOW TEST:27.590 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":39,"skipped":710,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:10.716: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-7a29ba70-8630-403e-bc66-7d22ed943ae4
STEP: Creating a pod to test consume secrets
Mar  2 22:07:11.025: INFO: Waiting up to 5m0s for pod "pod-secrets-f639a8da-e720-43d7-9aeb-8fe7c582b35e" in namespace "secrets-8830" to be "Succeeded or Failed"
Mar  2 22:07:11.039: INFO: Pod "pod-secrets-f639a8da-e720-43d7-9aeb-8fe7c582b35e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.876653ms
Mar  2 22:07:13.058: INFO: Pod "pod-secrets-f639a8da-e720-43d7-9aeb-8fe7c582b35e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032465999s
Mar  2 22:07:15.070: INFO: Pod "pod-secrets-f639a8da-e720-43d7-9aeb-8fe7c582b35e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045106179s
STEP: Saw pod success
Mar  2 22:07:15.070: INFO: Pod "pod-secrets-f639a8da-e720-43d7-9aeb-8fe7c582b35e" satisfied condition "Succeeded or Failed"
Mar  2 22:07:15.082: INFO: Trying to get logs from node 10.138.244.162 pod pod-secrets-f639a8da-e720-43d7-9aeb-8fe7c582b35e container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:07:15.206: INFO: Waiting for pod pod-secrets-f639a8da-e720-43d7-9aeb-8fe7c582b35e to disappear
Mar  2 22:07:15.215: INFO: Pod pod-secrets-f639a8da-e720-43d7-9aeb-8fe7c582b35e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:15.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8830" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":716,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:15.257: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:07:15.794: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  2 22:07:17.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855635, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855635, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855635, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855635, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:07:20.878: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:07:20.888: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:22.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6419" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.528 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":41,"skipped":724,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:22.784: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:07:23.697: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:07:25.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855643, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855643, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855643, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855643, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:07:28.815: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:29.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-985" for this suite.
STEP: Destroying namespace "webhook-985-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.565 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":42,"skipped":727,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:29.350: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:33.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2577" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":730,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:33.800: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:34.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9047" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":44,"skipped":737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:34.093: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:34.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7711" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":45,"skipped":766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:34.850: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9911
STEP: creating service affinity-clusterip-transition in namespace services-9911
STEP: creating replication controller affinity-clusterip-transition in namespace services-9911
I0302 22:07:35.110494      25 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-9911, replica count: 3
I0302 22:07:38.161385      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:07:41.163867      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:07:41.185: INFO: Creating new exec pod
Mar  2 22:07:46.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9911 exec execpod-affinityjdcw7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Mar  2 22:07:46.956: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  2 22:07:46.956: INFO: stdout: ""
Mar  2 22:07:46.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9911 exec execpod-affinityjdcw7 -- /bin/sh -x -c nc -zv -t -w 2 172.21.60.41 80'
Mar  2 22:07:47.418: INFO: stderr: "+ nc -zv -t -w 2 172.21.60.41 80\nConnection to 172.21.60.41 80 port [tcp/http] succeeded!\n"
Mar  2 22:07:47.418: INFO: stdout: ""
Mar  2 22:07:47.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9911 exec execpod-affinityjdcw7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.60.41:80/ ; done'
Mar  2 22:07:48.187: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n"
Mar  2 22:07:48.187: INFO: stdout: "\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vr77q\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-vr77q\naffinity-clusterip-transition-vr77q\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vr77q\naffinity-clusterip-transition-s6rlx\naffinity-clusterip-transition-s6rlx"
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-vr77q
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-vr77q
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-vr77q
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-vr77q
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.188: INFO: Received response from host: affinity-clusterip-transition-s6rlx
Mar  2 22:07:48.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9911 exec execpod-affinityjdcw7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.60.41:80/ ; done'
Mar  2 22:07:48.793: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.60.41:80/\n"
Mar  2 22:07:48.793: INFO: stdout: "\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct\naffinity-clusterip-transition-vflct"
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Received response from host: affinity-clusterip-transition-vflct
Mar  2 22:07:48.793: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9911, will wait for the garbage collector to delete the pods
Mar  2 22:07:48.937: INFO: Deleting ReplicationController affinity-clusterip-transition took: 48.821098ms
Mar  2 22:07:49.038: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.930116ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:59.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9911" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:24.442 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":46,"skipped":808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:59.293: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Mar  2 22:07:59.510: INFO: created test-pod-1
Mar  2 22:07:59.553: INFO: created test-pod-2
Mar  2 22:07:59.586: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:07:59.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4032" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":47,"skipped":831,"failed":0}
SSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:07:59.766: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:08:00.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3464" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":48,"skipped":837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:08:00.166: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:08:01.268: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:08:03.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855681, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855681, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855681, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855681, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:08:06.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:08:06.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2586" for this suite.
STEP: Destroying namespace "webhook-2586-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":49,"skipped":894,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:08:06.813: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-9227
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9227 to expose endpoints map[]
Mar  2 22:08:07.290: INFO: successfully validated that service multi-endpoint-test in namespace services-9227 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9227
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9227 to expose endpoints map[pod1:[100]]
Mar  2 22:08:10.404: INFO: successfully validated that service multi-endpoint-test in namespace services-9227 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9227
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9227 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  2 22:08:13.500: INFO: successfully validated that service multi-endpoint-test in namespace services-9227 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-9227
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9227 to expose endpoints map[pod2:[101]]
Mar  2 22:08:13.607: INFO: successfully validated that service multi-endpoint-test in namespace services-9227 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9227
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9227 to expose endpoints map[]
Mar  2 22:08:13.737: INFO: successfully validated that service multi-endpoint-test in namespace services-9227 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:08:13.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9227" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.072 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":50,"skipped":906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:08:13.890: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:08:14.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9273" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":51,"skipped":947,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:08:14.563: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar  2 22:08:54.975: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0302 22:08:54.975567      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0302 22:08:54.975624      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0302 22:08:54.975643      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 22:08:54.975: INFO: Deleting pod "simpletest.rc-7ghbz" in namespace "gc-1142"
Mar  2 22:08:55.056: INFO: Deleting pod "simpletest.rc-b4fn2" in namespace "gc-1142"
Mar  2 22:08:55.111: INFO: Deleting pod "simpletest.rc-gcddw" in namespace "gc-1142"
Mar  2 22:08:55.146: INFO: Deleting pod "simpletest.rc-gv2rl" in namespace "gc-1142"
Mar  2 22:08:55.223: INFO: Deleting pod "simpletest.rc-hq4xf" in namespace "gc-1142"
Mar  2 22:08:55.307: INFO: Deleting pod "simpletest.rc-nxc6r" in namespace "gc-1142"
Mar  2 22:08:55.391: INFO: Deleting pod "simpletest.rc-tb785" in namespace "gc-1142"
Mar  2 22:08:55.473: INFO: Deleting pod "simpletest.rc-x5mbh" in namespace "gc-1142"
Mar  2 22:08:55.516: INFO: Deleting pod "simpletest.rc-xglr7" in namespace "gc-1142"
Mar  2 22:08:55.591: INFO: Deleting pod "simpletest.rc-xqkvb" in namespace "gc-1142"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:08:55.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1142" for this suite.

• [SLOW TEST:41.327 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":52,"skipped":948,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:08:55.892: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:08:56.277: INFO: Waiting up to 5m0s for pod "busybox-user-65534-adde067b-0515-4ddf-9319-6ff2fad4a4f5" in namespace "security-context-test-2976" to be "Succeeded or Failed"
Mar  2 22:08:56.299: INFO: Pod "busybox-user-65534-adde067b-0515-4ddf-9319-6ff2fad4a4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.917423ms
Mar  2 22:08:58.309: INFO: Pod "busybox-user-65534-adde067b-0515-4ddf-9319-6ff2fad4a4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031461693s
Mar  2 22:09:00.321: INFO: Pod "busybox-user-65534-adde067b-0515-4ddf-9319-6ff2fad4a4f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043168878s
Mar  2 22:09:00.321: INFO: Pod "busybox-user-65534-adde067b-0515-4ddf-9319-6ff2fad4a4f5" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:09:00.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2976" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":965,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:09:00.367: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 22:09:00.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-389 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar  2 22:09:00.901: INFO: stderr: ""
Mar  2 22:09:00.902: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar  2 22:09:05.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-389 get pod e2e-test-httpd-pod -o json'
Mar  2 22:09:06.165: INFO: stderr: ""
Mar  2 22:09:06.165: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"5c7a54e28ad65293c81e0bcd44825a10b8b39f8062b0e86fcd8cc80f788c1f16\",\n            \"cni.projectcalico.org/podIP\": \"172.30.228.48/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.228.48/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.228.48\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.228.48\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2022-03-02T22:09:00Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2022-03-02T22:09:00Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/containerID\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2022-03-02T22:09:02Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.30.228.48\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2022-03-02T22:09:02Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2022-03-02T22:09:02Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-389\",\n        \"resourceVersion\": \"66114\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-389/pods/e2e-test-httpd-pod\",\n        \"uid\": \"c6138d52-61f9-4d93-bfe4-525c49887298\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-r9j8w\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-drpxt\"\n            }\n        ],\n        \"nodeName\": \"10.138.244.159\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c38,c7\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-r9j8w\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-r9j8w\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-02T22:09:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-02T22:09:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-02T22:09:02Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-02T22:09:00Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://12f9f1103cffe68f00aea5745492badaa06e788e532625d3f1da611df21d236a\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-03-02T22:09:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.138.244.159\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.228.48\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.228.48\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-03-02T22:09:00Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  2 22:09:06.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-389 replace -f -'
Mar  2 22:09:06.935: INFO: stderr: ""
Mar  2 22:09:06.935: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Mar  2 22:09:06.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-389 delete pods e2e-test-httpd-pod'
Mar  2 22:09:09.087: INFO: stderr: ""
Mar  2 22:09:09.087: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:09:09.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-389" for this suite.

• [SLOW TEST:8.768 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":54,"skipped":979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:09:09.137: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-09cda618-aee9-42ce-8445-108d7c7eb910
STEP: Creating a pod to test consume secrets
Mar  2 22:09:09.481: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-566fb627-446a-428f-b90e-c54151ee4929" in namespace "projected-4164" to be "Succeeded or Failed"
Mar  2 22:09:09.550: INFO: Pod "pod-projected-secrets-566fb627-446a-428f-b90e-c54151ee4929": Phase="Pending", Reason="", readiness=false. Elapsed: 68.998985ms
Mar  2 22:09:11.569: INFO: Pod "pod-projected-secrets-566fb627-446a-428f-b90e-c54151ee4929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088656901s
Mar  2 22:09:13.580: INFO: Pod "pod-projected-secrets-566fb627-446a-428f-b90e-c54151ee4929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.099317186s
STEP: Saw pod success
Mar  2 22:09:13.580: INFO: Pod "pod-projected-secrets-566fb627-446a-428f-b90e-c54151ee4929" satisfied condition "Succeeded or Failed"
Mar  2 22:09:13.589: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-secrets-566fb627-446a-428f-b90e-c54151ee4929 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:09:13.675: INFO: Waiting for pod pod-projected-secrets-566fb627-446a-428f-b90e-c54151ee4929 to disappear
Mar  2 22:09:13.686: INFO: Pod pod-projected-secrets-566fb627-446a-428f-b90e-c54151ee4929 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:09:13.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4164" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":55,"skipped":1023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:09:13.744: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  2 22:09:18.608: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5523 pod-service-account-c9abe0be-3b0f-4411-ac01-1cd3537521e8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  2 22:09:19.072: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5523 pod-service-account-c9abe0be-3b0f-4411-ac01-1cd3537521e8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  2 22:09:19.464: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5523 pod-service-account-c9abe0be-3b0f-4411-ac01-1cd3537521e8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:09:19.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5523" for this suite.

• [SLOW TEST:6.185 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":56,"skipped":1047,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:09:19.929: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  2 22:09:23.231: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:09:24.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-17" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":57,"skipped":1053,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:09:24.380: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 22:09:24.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-3754 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Mar  2 22:09:24.731: INFO: stderr: ""
Mar  2 22:09:24.731: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Mar  2 22:09:24.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-3754 delete pods e2e-test-httpd-pod'
Mar  2 22:09:29.100: INFO: stderr: ""
Mar  2 22:09:29.100: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:09:29.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3754" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":58,"skipped":1053,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:09:29.164: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:09:40.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9997" for this suite.

• [SLOW TEST:11.415 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":59,"skipped":1056,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:09:40.579: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:10:09.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5017" for this suite.

• [SLOW TEST:28.547 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":60,"skipped":1093,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:10:09.130: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:11:09.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9410" for this suite.

• [SLOW TEST:60.347 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":1101,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:11:09.478: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  2 22:11:09.755: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  2 22:11:14.780: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:11:15.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7951" for this suite.

• [SLOW TEST:6.435 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":62,"skipped":1103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:11:15.921: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 22:11:16.901: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 22:11:16.935: INFO: waiting for watch events with expected annotations
Mar  2 22:11:16.936: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:11:17.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6888" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":63,"skipped":1132,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:11:17.231: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:11:17.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c86c4e68-0b77-4a9c-b071-5d7316a307d1" in namespace "downward-api-4951" to be "Succeeded or Failed"
Mar  2 22:11:17.483: INFO: Pod "downwardapi-volume-c86c4e68-0b77-4a9c-b071-5d7316a307d1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.787001ms
Mar  2 22:11:19.495: INFO: Pod "downwardapi-volume-c86c4e68-0b77-4a9c-b071-5d7316a307d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021312005s
Mar  2 22:11:21.507: INFO: Pod "downwardapi-volume-c86c4e68-0b77-4a9c-b071-5d7316a307d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033689256s
STEP: Saw pod success
Mar  2 22:11:21.507: INFO: Pod "downwardapi-volume-c86c4e68-0b77-4a9c-b071-5d7316a307d1" satisfied condition "Succeeded or Failed"
Mar  2 22:11:21.518: INFO: Trying to get logs from node 10.138.244.162 pod downwardapi-volume-c86c4e68-0b77-4a9c-b071-5d7316a307d1 container client-container: <nil>
STEP: delete the pod
Mar  2 22:11:21.618: INFO: Waiting for pod downwardapi-volume-c86c4e68-0b77-4a9c-b071-5d7316a307d1 to disappear
Mar  2 22:11:21.627: INFO: Pod downwardapi-volume-c86c4e68-0b77-4a9c-b071-5d7316a307d1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:11:21.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4951" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":1133,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:11:21.682: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  2 22:11:28.075: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:11:28.086: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 22:11:30.087: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:11:30.099: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 22:11:32.087: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:11:32.099: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 22:11:34.087: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:11:34.101: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 22:11:36.087: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:11:36.116: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 22:11:38.087: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:11:38.098: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:11:38.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7781" for this suite.

• [SLOW TEST:16.491 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1150,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:11:38.175: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Mar  2 22:11:38.413: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8081 proxy --unix-socket=/tmp/kubectl-proxy-unix801806969/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:11:38.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8081" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":66,"skipped":1165,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:11:38.622: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:11:39.392: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:11:41.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855899, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855899, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855899, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855899, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:11:44.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:11:44.616: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:11:46.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7110" for this suite.
STEP: Destroying namespace "webhook-7110-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.894 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":67,"skipped":1176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:11:46.517: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:11:47.992: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 22:11:50.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855908, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855908, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855908, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781855907, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:11:53.131: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:11:53.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-424" for this suite.
STEP: Destroying namespace "webhook-424-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.300 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":68,"skipped":1215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:11:53.817: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar  2 22:11:54.109: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 22:12:54.305: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:12:54.330: INFO: Starting informer...
STEP: Starting pod...
Mar  2 22:12:54.586: INFO: Pod is running on 10.138.244.159. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar  2 22:12:54.636: INFO: Pod wasn't evicted. Proceeding
Mar  2 22:12:54.636: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar  2 22:14:09.748: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:09.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2314" for this suite.

• [SLOW TEST:135.968 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":69,"skipped":1237,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:09.788: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:19.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3718" for this suite.
STEP: Destroying namespace "nsdeletetest-3332" for this suite.
Mar  2 22:14:19.596: INFO: Namespace nsdeletetest-3332 was already deleted
STEP: Destroying namespace "nsdeletetest-6239" for this suite.

• [SLOW TEST:9.829 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":70,"skipped":1242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:19.618: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-5adc6fdb-83cb-48cd-8002-1cd0aa6c0638
STEP: Creating a pod to test consume configMaps
Mar  2 22:14:19.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-33bd4658-8b9c-4ad7-93ab-cb446870e980" in namespace "configmap-6304" to be "Succeeded or Failed"
Mar  2 22:14:19.981: INFO: Pod "pod-configmaps-33bd4658-8b9c-4ad7-93ab-cb446870e980": Phase="Pending", Reason="", readiness=false. Elapsed: 18.607642ms
Mar  2 22:14:21.993: INFO: Pod "pod-configmaps-33bd4658-8b9c-4ad7-93ab-cb446870e980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031185436s
Mar  2 22:14:24.015: INFO: Pod "pod-configmaps-33bd4658-8b9c-4ad7-93ab-cb446870e980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052866312s
STEP: Saw pod success
Mar  2 22:14:24.015: INFO: Pod "pod-configmaps-33bd4658-8b9c-4ad7-93ab-cb446870e980" satisfied condition "Succeeded or Failed"
Mar  2 22:14:24.027: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-33bd4658-8b9c-4ad7-93ab-cb446870e980 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:14:24.153: INFO: Waiting for pod pod-configmaps-33bd4658-8b9c-4ad7-93ab-cb446870e980 to disappear
Mar  2 22:14:24.163: INFO: Pod pod-configmaps-33bd4658-8b9c-4ad7-93ab-cb446870e980 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:24.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6304" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":71,"skipped":1288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:24.204: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:14:24.458: INFO: Creating deployment "webserver-deployment"
Mar  2 22:14:24.490: INFO: Waiting for observed generation 1
Mar  2 22:14:26.520: INFO: Waiting for all required pods to come up
Mar  2 22:14:26.544: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  2 22:14:28.581: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  2 22:14:28.615: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  2 22:14:28.683: INFO: Updating deployment webserver-deployment
Mar  2 22:14:28.683: INFO: Waiting for observed generation 2
Mar  2 22:14:30.722: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  2 22:14:30.734: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  2 22:14:30.756: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 22:14:30.829: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  2 22:14:30.829: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  2 22:14:30.839: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 22:14:30.910: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  2 22:14:30.910: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  2 22:14:30.958: INFO: Updating deployment webserver-deployment
Mar  2 22:14:30.958: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  2 22:14:30.987: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  2 22:14:33.020: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 22:14:33.052: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1547 /apis/apps/v1/namespaces/deployment-1547/deployments/webserver-deployment cb2d38a4-d7d2-4a57-9243-b4d850d7cae6 71104 3 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-02 22:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004234f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-03-02 22:14:30 +0000 UTC,LastTransitionTime:2022-03-02 22:14:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-03-02 22:14:31 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  2 22:14:33.061: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1547 /apis/apps/v1/namespaces/deployment-1547/replicasets/webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 71097 3 2022-03-02 22:14:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment cb2d38a4-d7d2-4a57-9243-b4d850d7cae6 0xc004235377 0xc004235378}] []  [{kube-controller-manager Update apps/v1 2022-03-02 22:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb2d38a4-d7d2-4a57-9243-b4d850d7cae6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042353f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:14:33.061: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  2 22:14:33.061: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1547 /apis/apps/v1/namespaces/deployment-1547/replicasets/webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 71091 3 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment cb2d38a4-d7d2-4a57-9243-b4d850d7cae6 0xc004235457 0xc004235458}] []  [{kube-controller-manager Update apps/v1 2022-03-02 22:14:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb2d38a4-d7d2-4a57-9243-b4d850d7cae6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042354c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:14:33.103: INFO: Pod "webserver-deployment-795d758f88-2722r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2722r webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-2722r 81e45ae5-26fd-4db8-b960-d990d5949264 71063 0 2022-03-02 22:14:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:a3bb52d5e2a1a47864e5079957c6046a1b936c54b60ea89612349bfd46da043d cni.projectcalico.org/podIP:172.30.228.37/32 cni.projectcalico.org/podIPs:172.30.228.37/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.37"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.37"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc003b5d9f7 0xc003b5d9f8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.105: INFO: Pod "webserver-deployment-795d758f88-5q4qd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5q4qd webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-5q4qd 905e3490-bfda-4bff-b9ef-7af6b97fdc11 71093 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc003b5dbf7 0xc003b5dbf8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.105: INFO: Pod "webserver-deployment-795d758f88-7bfqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7bfqr webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-7bfqr ae8fb4b5-05f5-468a-b811-a5714fd89ec9 71088 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc003b5ddc7 0xc003b5ddc8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.105: INFO: Pod "webserver-deployment-795d758f88-d6724" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-d6724 webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-d6724 15e61c13-703c-4048-bd97-88ee3594b6e6 70985 0 2022-03-02 22:14:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:e6a679288b48c5add2f352683b3e346ae56b1d2bbcb4c980b0063a441a3c11dd cni.projectcalico.org/podIP:172.30.43.105/32 cni.projectcalico.org/podIPs:172.30.43.105/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.43.105"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.43.105"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc003b5dfb7 0xc003b5dfb8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:,StartTime:2022-03-02 22:14:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.107: INFO: Pod "webserver-deployment-795d758f88-k2znp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k2znp webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-k2znp 03d6bfa3-78ae-43d4-af57-6089e6c2ce3c 71173 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:2a06aa14c978641b352b942089e4d161a3259d2ee911707ebcaa59709b9c030d cni.projectcalico.org/podIP:172.30.43.109/32 cni.projectcalico.org/podIPs:172.30.43.109/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.43.109"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.43.109"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc0045121d7 0xc0045121d8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.111: INFO: Pod "webserver-deployment-795d758f88-kscbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kscbp webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-kscbp 6788c6d3-5071-4e70-8452-1000e2b2d369 71011 0 2022-03-02 22:14:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:022b38c274e3d024357caa487810685ccd660438f7001c4ceec0bddfa5d12cf6 cni.projectcalico.org/podIP:172.30.228.38/32 cni.projectcalico.org/podIPs:172.30.228.38/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.38"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.38"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc0045123f7 0xc0045123f8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.111: INFO: Pod "webserver-deployment-795d758f88-kv8wl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kv8wl webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-kv8wl 51c69113-e660-4fb0-a898-c43cad50e7cf 71100 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc0045125f7 0xc0045125f8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.111: INFO: Pod "webserver-deployment-795d758f88-n59mp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n59mp webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-n59mp 3ff9a7fc-817f-48c7-9304-3815db106109 71000 0 2022-03-02 22:14:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:9b498fcd4e183b08f6292056cb012138c501af24338cd5a674ce85abb16da625 cni.projectcalico.org/podIP:172.30.43.108/32 cni.projectcalico.org/podIPs:172.30.43.108/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.43.108"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.43.108"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc0045127e7 0xc0045127e8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:,StartTime:2022-03-02 22:14:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.113: INFO: Pod "webserver-deployment-795d758f88-th7rl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-th7rl webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-th7rl 35bd13fe-8cf2-4551-9d41-4b910dd256e2 71161 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:eaea7f0ba9b94815cd1beeb0762f897af232cc6a797ef876329c833408ee17b6 cni.projectcalico.org/podIP:172.30.21.178/32 cni.projectcalico.org/podIPs:172.30.21.178/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.21.178"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.21.178"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc004512a07 0xc004512a08}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.113: INFO: Pod "webserver-deployment-795d758f88-vf658" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vf658 webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-vf658 84ae8dc2-80cd-4597-baef-33498bc735cb 71114 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc004512c07 0xc004512c08}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.114: INFO: Pod "webserver-deployment-795d758f88-vp94x" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vp94x webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-vp94x 7c0d6b17-3225-4590-8543-5a88a1c20065 71131 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc004512dd7 0xc004512dd8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.115: INFO: Pod "webserver-deployment-795d758f88-vw9mg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vw9mg webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-vw9mg af0360a1-b3db-45ad-96a0-2684d345469d 71117 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc004512fa7 0xc004512fa8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.115: INFO: Pod "webserver-deployment-795d758f88-xvp75" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xvp75 webserver-deployment-795d758f88- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-795d758f88-xvp75 4e382fbf-2f17-483c-8dfd-9cc8240374e5 70997 0 2022-03-02 22:14:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:b0b5b85d6c8abc75e04cb4593ae05fa37dd851eff9329e25e95b63a8d28b0ce2 cni.projectcalico.org/podIP:172.30.21.177/32 cni.projectcalico.org/podIPs:172.30.21.177/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.21.177"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.21.177"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c8bfce08-abc3-43ce-98ca-73f6ad9948ca 0xc004513197 0xc004513198}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bfce08-abc3-43ce-98ca-73f6ad9948ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:,StartTime:2022-03-02 22:14:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.116: INFO: Pod "webserver-deployment-dd94f59b7-2c4lp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2c4lp webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-2c4lp 7ebdc2ab-3cc4-45f2-9bdf-b6a51daef4d1 70870 0 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:766e461eb63125eddccd2adf209235d6377f4476dc62385d2b1e59c52398a0fd cni.projectcalico.org/podIP:172.30.228.39/32 cni.projectcalico.org/podIPs:172.30.228.39/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.39"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.39"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045133b7 0xc0045133b8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:14:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.228.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:172.30.228.39,StartTime:2022-03-02 22:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:14:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://5b6869a097c28ca5422debbc5f06505644a4df7fba497fc02a19ba39372b195d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.228.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.116: INFO: Pod "webserver-deployment-dd94f59b7-4h9m4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4h9m4 webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-4h9m4 45115d3b-929e-43c4-a364-f3ff8fa7f1ac 70858 0 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:772c5ed82210910b30c9af48a81c992a7cbd62024653172246a106df4effe7f2 cni.projectcalico.org/podIP:172.30.21.175/32 cni.projectcalico.org/podIPs:172.30.21.175/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.21.175"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.21.175"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045135d7 0xc0045135d8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:14:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.21.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:172.30.21.175,StartTime:2022-03-02 22:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:14:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://0d7c1ae54f3c5eeb84efc7b25b18b216a31d9425bbcd3b9a093378e75c64351e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.21.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.116: INFO: Pod "webserver-deployment-dd94f59b7-52lgf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-52lgf webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-52lgf 22e4a9b8-d703-49e5-893b-d398299a1317 71120 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045137d7 0xc0045137d8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.117: INFO: Pod "webserver-deployment-dd94f59b7-5dgzm" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5dgzm webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-5dgzm 3ac2abb9-c257-43ad-afa4-9bb1d415bdea 70888 0 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:c5cfa5f3665f47f12e70162677c0ca64b11038f9a283ade29d9024207be3185b cni.projectcalico.org/podIP:172.30.43.106/32 cni.projectcalico.org/podIPs:172.30.43.106/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.43.106"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.43.106"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045139a7 0xc0045139a8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:14:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.43.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:172.30.43.106,StartTime:2022-03-02 22:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:14:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://d6ede81ad3ea05acbbc57cba020688a04e1a8451a8b5f16956f2bc9371973309,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.43.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.118: INFO: Pod "webserver-deployment-dd94f59b7-8swcz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8swcz webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-8swcz e0424135-542c-4280-92d6-660336b19086 71123 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004513ba7 0xc004513ba8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.118: INFO: Pod "webserver-deployment-dd94f59b7-dqh8s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dqh8s webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-dqh8s f5b81f4a-5f89-44e1-bf1f-b95f8c351d7b 71128 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004513d57 0xc004513d58}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.118: INFO: Pod "webserver-deployment-dd94f59b7-fxr97" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fxr97 webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-fxr97 cbb9cf4a-b6d8-4b13-9ff6-3870c9cc4e93 71108 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004513f07 0xc004513f08}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.118: INFO: Pod "webserver-deployment-dd94f59b7-g5t7j" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-g5t7j webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-g5t7j 45188ba0-ec39-4525-8e26-3a1eb1f383f3 70866 0 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:57c11dc6035f40db92030e19b5cbdeb18ce4f3ef92d8a2781a2bf072dbefaace cni.projectcalico.org/podIP:172.30.228.40/32 cni.projectcalico.org/podIPs:172.30.228.40/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.40"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.40"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045360d7 0xc0045360d8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:14:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.228.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:172.30.228.40,StartTime:2022-03-02 22:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:14:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://89c54cfa5c234b2e2fb28199b92d3867740e21c3e5054e54faf997ec533d5e22,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.228.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.118: INFO: Pod "webserver-deployment-dd94f59b7-ggxd7" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ggxd7 webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-ggxd7 154cc117-6ab5-4604-8d02-fa64e4324153 71181 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:de679c5a741caaba9249f541a292d643a4946e1cce16eb6054f46c05edc8f81a cni.projectcalico.org/podIP:172.30.21.173/32 cni.projectcalico.org/podIPs:172.30.21.173/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045362f7 0xc0045362f8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.118: INFO: Pod "webserver-deployment-dd94f59b7-hbndq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hbndq webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-hbndq 8e68e194-8c21-49da-9f70-e85be164fe48 70881 0 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:c7227e009f3867d5987ffd63e9f2d911ff965cb5c72b7e36b7835d29838d7b35 cni.projectcalico.org/podIP:172.30.43.107/32 cni.projectcalico.org/podIPs:172.30.43.107/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.43.107"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.43.107"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045364d7 0xc0045364d8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:14:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.43.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:172.30.43.107,StartTime:2022-03-02 22:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:14:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://eed0d69d496ba635bb555079632de6b312ede46e6e09e449f8c42493cfd32f67,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.43.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.118: INFO: Pod "webserver-deployment-dd94f59b7-k9hrb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-k9hrb webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-k9hrb 30c0550a-62ef-403d-a742-2bcc5032bd3d 71101 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045366d7 0xc0045366d8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.124: INFO: Pod "webserver-deployment-dd94f59b7-kx7xw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kx7xw webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-kx7xw 967e5ad5-db89-4f3d-9973-e7373242957c 71174 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:cef115f6c7ffd33bff1f05bbc746bdda6f60cb090b32994483f8641152174051 cni.projectcalico.org/podIP:172.30.228.47/32 cni.projectcalico.org/podIPs:172.30.228.47/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045368a7 0xc0045368a8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.137: INFO: Pod "webserver-deployment-dd94f59b7-pcg7j" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pcg7j webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-pcg7j 2099ed27-fd9e-4bfa-af47-72e742f772ef 71166 0 2022-03-02 22:14:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:0bb2d66488330430521323c4e00c488c9348fe4c31ec05e3693eb00f49b605f5 cni.projectcalico.org/podIP:172.30.228.42/32 cni.projectcalico.org/podIPs:172.30.228.42/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.42"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.42"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004536a87 0xc004536a88}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.137: INFO: Pod "webserver-deployment-dd94f59b7-pxd4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pxd4v webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-pxd4v 56cd49ff-8f7a-4316-b1e5-8b90cb846cea 71094 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004536c67 0xc004536c68}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.137: INFO: Pod "webserver-deployment-dd94f59b7-sdbgn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sdbgn webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-sdbgn 2204761b-136b-41f1-b331-e661e62a0250 71110 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004536e17 0xc004536e18}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.139: INFO: Pod "webserver-deployment-dd94f59b7-shx5j" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-shx5j webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-shx5j 336c5ad2-2e1c-4f61-bfd6-2e3ceff1e8ea 71177 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:7960f385f37e6425d4b2530b0c5afc8e9e28ed0d42acbf4b48f934421e27d9e7 cni.projectcalico.org/podIP:172.30.21.179/32 cni.projectcalico.org/podIPs:172.30.21.179/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.21.179"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.21.179"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004536fe7 0xc004536fe8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.140: INFO: Pod "webserver-deployment-dd94f59b7-x6xfv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x6xfv webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-x6xfv 7123e4c9-add9-4709-beca-0435b7a69d14 70885 0 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:0eccf09f17656dca3e9ef59921b7f9f0fd9cfcdb11229b401dc17817fa3300a7 cni.projectcalico.org/podIP:172.30.43.103/32 cni.projectcalico.org/podIPs:172.30.43.103/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.43.103"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.43.103"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045371e7 0xc0045371e8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:14:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.43.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:172.30.43.103,StartTime:2022-03-02 22:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:14:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://6a175e2e315cc947bc7a00b41853c75251849c35de1c089e7fd9d76a2faa484d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.43.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.140: INFO: Pod "webserver-deployment-dd94f59b7-xdpft" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xdpft webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-xdpft bc84bccb-2ad3-466c-bf49-20161cdf406c 70854 0 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:f65bf2871c895bcb53c0b9c7995db5909a99e5f13f9cab58a73d4a7de834f5d9 cni.projectcalico.org/podIP:172.30.21.174/32 cni.projectcalico.org/podIPs:172.30.21.174/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.21.174"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.21.174"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004537407 0xc004537408}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:14:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.21.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:172.30.21.174,StartTime:2022-03-02 22:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:14:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://3d6ab869ddeff7d406deda34aae71fc1af40729e654f8bf834fb49d3e91836df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.21.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.140: INFO: Pod "webserver-deployment-dd94f59b7-xmdmg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xmdmg webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-xmdmg 8e205152-9616-4400-bb0f-855225653514 71118 0 2022-03-02 22:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc004537607 0xc004537608}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.150,PodIP:,StartTime:2022-03-02 22:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:14:33.140: INFO: Pod "webserver-deployment-dd94f59b7-zvk5k" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zvk5k webserver-deployment-dd94f59b7- deployment-1547 /api/v1/namespaces/deployment-1547/pods/webserver-deployment-dd94f59b7-zvk5k 1e75fc94-ae9d-49b0-aa4d-52b8bb54fc61 70900 0 2022-03-02 22:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/containerID:5b827abef3af485c10bfdcf7cee453f728722b51c4ba1ee7ec171e194800c383 cni.projectcalico.org/podIP:172.30.21.176/32 cni.projectcalico.org/podIPs:172.30.21.176/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.21.176"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.21.176"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2af22ae0-8f87-4392-935b-dc70470da8dd 0xc0045377d7 0xc0045377d8}] []  [{kube-controller-manager Update v1 2022-03-02 22:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2af22ae0-8f87-4392-935b-dc70470da8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:14:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.21.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kqxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kqxbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kqxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2pgvt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:172.30.21.176,StartTime:2022-03-02 22:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:14:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://3d3dad785b3f05bbba3183516291bb4a6f6c4c60264afd0646020f4d7825fa00,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.21.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:33.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1547" for this suite.

• [SLOW TEST:9.027 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":72,"skipped":1318,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:33.232: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:14:34.218: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:14:36.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856074, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856074, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856074, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856074, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:14:39.327: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:39.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8445" for this suite.
STEP: Destroying namespace "webhook-8445-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.587 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":73,"skipped":1333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:39.835: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  2 22:14:40.078: INFO: Waiting up to 5m0s for pod "pod-a20bfb38-ed6e-4f8c-9830-26cf5c64fbc6" in namespace "emptydir-7313" to be "Succeeded or Failed"
Mar  2 22:14:40.095: INFO: Pod "pod-a20bfb38-ed6e-4f8c-9830-26cf5c64fbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.905525ms
Mar  2 22:14:42.145: INFO: Pod "pod-a20bfb38-ed6e-4f8c-9830-26cf5c64fbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067173413s
Mar  2 22:14:44.168: INFO: Pod "pod-a20bfb38-ed6e-4f8c-9830-26cf5c64fbc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090379553s
STEP: Saw pod success
Mar  2 22:14:44.168: INFO: Pod "pod-a20bfb38-ed6e-4f8c-9830-26cf5c64fbc6" satisfied condition "Succeeded or Failed"
Mar  2 22:14:44.186: INFO: Trying to get logs from node 10.138.244.159 pod pod-a20bfb38-ed6e-4f8c-9830-26cf5c64fbc6 container test-container: <nil>
STEP: delete the pod
Mar  2 22:14:44.275: INFO: Waiting for pod pod-a20bfb38-ed6e-4f8c-9830-26cf5c64fbc6 to disappear
Mar  2 22:14:44.313: INFO: Pod pod-a20bfb38-ed6e-4f8c-9830-26cf5c64fbc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:44.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7313" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1395,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:44.378: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 22:14:44.646: INFO: Waiting up to 5m0s for pod "pod-4554b7e7-1406-44f2-8cf1-c21310677582" in namespace "emptydir-4022" to be "Succeeded or Failed"
Mar  2 22:14:44.688: INFO: Pod "pod-4554b7e7-1406-44f2-8cf1-c21310677582": Phase="Pending", Reason="", readiness=false. Elapsed: 41.778376ms
Mar  2 22:14:46.708: INFO: Pod "pod-4554b7e7-1406-44f2-8cf1-c21310677582": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061835486s
Mar  2 22:14:48.721: INFO: Pod "pod-4554b7e7-1406-44f2-8cf1-c21310677582": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074774856s
STEP: Saw pod success
Mar  2 22:14:48.721: INFO: Pod "pod-4554b7e7-1406-44f2-8cf1-c21310677582" satisfied condition "Succeeded or Failed"
Mar  2 22:14:48.731: INFO: Trying to get logs from node 10.138.244.159 pod pod-4554b7e7-1406-44f2-8cf1-c21310677582 container test-container: <nil>
STEP: delete the pod
Mar  2 22:14:48.818: INFO: Waiting for pod pod-4554b7e7-1406-44f2-8cf1-c21310677582 to disappear
Mar  2 22:14:48.831: INFO: Pod pod-4554b7e7-1406-44f2-8cf1-c21310677582 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:48.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4022" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":75,"skipped":1397,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:48.876: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:49.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9956" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":76,"skipped":1400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:49.463: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 22:14:54.343: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d994747e-cea5-4658-bd10-a8eff7242084"
Mar  2 22:14:54.343: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d994747e-cea5-4658-bd10-a8eff7242084" in namespace "pods-9261" to be "terminated due to deadline exceeded"
Mar  2 22:14:54.355: INFO: Pod "pod-update-activedeadlineseconds-d994747e-cea5-4658-bd10-a8eff7242084": Phase="Running", Reason="", readiness=true. Elapsed: 11.641532ms
Mar  2 22:14:56.377: INFO: Pod "pod-update-activedeadlineseconds-d994747e-cea5-4658-bd10-a8eff7242084": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.033353876s
Mar  2 22:14:56.377: INFO: Pod "pod-update-activedeadlineseconds-d994747e-cea5-4658-bd10-a8eff7242084" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:14:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9261" for this suite.

• [SLOW TEST:6.975 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1430,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:14:56.438: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-5822
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5822
STEP: Deleting pre-stop pod
Mar  2 22:15:06.050: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:15:06.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5822" for this suite.

• [SLOW TEST:9.710 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":78,"skipped":1444,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:15:06.149: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:15:07.234: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:15:09.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856107, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856107, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856107, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856107, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:15:12.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar  2 22:15:16.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=webhook-4248 attach --namespace=webhook-4248 to-be-attached-pod -i -c=container1'
Mar  2 22:15:16.810: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:15:16.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4248" for this suite.
STEP: Destroying namespace "webhook-4248-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:10.976 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":79,"skipped":1458,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:15:17.127: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4739.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4739.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:15:21.671: INFO: DNS probes using dns-4739/dns-test-79011ce9-c711-4aaf-9331-00427d2a3c22 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:15:21.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4739" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":80,"skipped":1465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:15:21.765: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar  2 22:15:22.014: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 22:15:22.014: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 22:15:22.096: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 22:15:22.096: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 22:15:22.158: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 22:15:22.158: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 22:15:22.316: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 22:15:22.316: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 22:15:24.155: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 22:15:24.155: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 22:15:24.696: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar  2 22:15:24.722: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar  2 22:15:24.729: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0
Mar  2 22:15:24.729: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0
Mar  2 22:15:24.729: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0
Mar  2 22:15:24.729: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0
Mar  2 22:15:24.729: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0
Mar  2 22:15:24.729: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0
Mar  2 22:15:24.729: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0
Mar  2 22:15:24.729: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 0
Mar  2 22:15:24.730: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:24.730: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:24.730: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2
Mar  2 22:15:24.730: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2
Mar  2 22:15:24.730: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2
Mar  2 22:15:24.730: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2
Mar  2 22:15:24.772: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2
Mar  2 22:15:24.772: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2
Mar  2 22:15:24.799: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2
Mar  2 22:15:24.799: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 2
Mar  2 22:15:24.827: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
STEP: listing Deployments
Mar  2 22:15:24.881: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar  2 22:15:24.921: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar  2 22:15:24.945: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:patched test-deployment-static:true]
Mar  2 22:15:24.945: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 22:15:24.971: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 22:15:25.042: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 22:15:25.089: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 22:15:25.118: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 22:15:25.132: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 22:15:25.157: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 22:15:25.178: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar  2 22:15:28.322: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:28.322: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:28.323: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:28.323: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:28.323: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:28.323: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:28.323: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:28.323: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
Mar  2 22:15:28.324: INFO: observed Deployment test-deployment in namespace deployment-6378 with ReadyReplicas 1
STEP: deleting the Deployment
Mar  2 22:15:28.374: INFO: observed event type MODIFIED
Mar  2 22:15:28.374: INFO: observed event type MODIFIED
Mar  2 22:15:28.374: INFO: observed event type MODIFIED
Mar  2 22:15:28.375: INFO: observed event type MODIFIED
Mar  2 22:15:28.375: INFO: observed event type MODIFIED
Mar  2 22:15:28.375: INFO: observed event type MODIFIED
Mar  2 22:15:28.375: INFO: observed event type MODIFIED
Mar  2 22:15:28.375: INFO: observed event type MODIFIED
Mar  2 22:15:28.375: INFO: observed event type MODIFIED
Mar  2 22:15:28.375: INFO: observed event type MODIFIED
Mar  2 22:15:28.376: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 22:15:28.386: INFO: Log out all the ReplicaSets if there is no deployment created
Mar  2 22:15:28.401: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-6378 /apis/apps/v1/namespaces/deployment-6378/replicasets/test-deployment-8b6954bfb d9cef36b-e05e-4e28-87e5-a0ad55a6263d 72911 2 2022-03-02 22:15:22 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 058a9537-1ba4-4586-81c5-277838f5c50f 0xc00026ee57 0xc00026ee58}] []  [{kube-controller-manager Update apps/v1 2022-03-02 22:15:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"058a9537-1ba4-4586-81c5-277838f5c50f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00026efa0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Mar  2 22:15:28.414: INFO: pod: "test-deployment-8b6954bfb-c9j7p":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-c9j7p test-deployment-8b6954bfb- deployment-6378 /api/v1/namespaces/deployment-6378/pods/test-deployment-8b6954bfb-c9j7p ff424e8f-1a03-4420-9a7c-7550c7836075 72877 0 2022-03-02 22:15:22 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[cni.projectcalico.org/containerID:7b9abd4986515f42278a7e3fb6215ac33faaaf2857558fc50299fe57f82b7e8f cni.projectcalico.org/podIP:172.30.21.184/32 cni.projectcalico.org/podIPs:172.30.21.184/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.21.184"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.21.184"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-8b6954bfb d9cef36b-e05e-4e28-87e5-a0ad55a6263d 0xc002e77637 0xc002e77638}] []  [{kube-controller-manager Update v1 2022-03-02 22:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9cef36b-e05e-4e28-87e5-a0ad55a6263d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:15:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:15:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:15:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.21.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7gvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7gvl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7gvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.162,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rcpfg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:15:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:15:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.162,PodIP:172.30.21.184,StartTime:2022-03-02 22:15:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:15:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://6dc22134c2c75b1b280ff9462c6c03eb2cc9411ae1d371e2a63e7b52f62b8c14,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.21.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:15:28.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6378" for this suite.

• [SLOW TEST:6.714 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":81,"skipped":1507,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:15:28.479: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:15:28.758: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 22:15:30.770: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 22:15:32.776: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:34.770: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:36.781: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:38.814: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:40.774: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:42.776: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:44.788: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:46.769: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:48.770: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:50.774: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = false)
Mar  2 22:15:52.772: INFO: The status of Pod test-webserver-33e3aa5f-61d5-4984-b95d-064f95548bb3 is Running (Ready = true)
Mar  2 22:15:52.784: INFO: Container started at 2022-03-02 22:15:30 +0000 UTC, pod became ready at 2022-03-02 22:15:51 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:15:52.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3466" for this suite.

• [SLOW TEST:24.375 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:15:52.859: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:15:53.145: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  2 22:15:58.156: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 22:15:58.157: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 22:15:58.298: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9686 /apis/apps/v1/namespaces/deployment-9686/deployments/test-cleanup-deployment 96335eb1-5412-4242-91e3-b55d2a6090d8 73379 1 2022-03-02 22:15:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-03-02 22:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046ba898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 22:15:58.315: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-9686 /apis/apps/v1/namespaces/deployment-9686/replicasets/test-cleanup-deployment-685c4f8568 8750cf2a-0655-4b34-9112-1dd7dfab4acd 73382 1 2022-03-02 22:15:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 96335eb1-5412-4242-91e3-b55d2a6090d8 0xc0039d7627 0xc0039d7628}] []  [{kube-controller-manager Update apps/v1 2022-03-02 22:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96335eb1-5412-4242-91e3-b55d2a6090d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039d76c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:15:58.315: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  2 22:15:58.315: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9686 /apis/apps/v1/namespaces/deployment-9686/replicasets/test-cleanup-controller df2399c8-60bb-49f9-806b-ad12ee656c65 73381 1 2022-03-02 22:15:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 96335eb1-5412-4242-91e3-b55d2a6090d8 0xc0039d7517 0xc0039d7518}] []  [{e2e.test Update apps/v1 2022-03-02 22:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-02 22:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"96335eb1-5412-4242-91e3-b55d2a6090d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0039d75b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:15:58.345: INFO: Pod "test-cleanup-controller-frqw5" is available:
&Pod{ObjectMeta:{test-cleanup-controller-frqw5 test-cleanup-controller- deployment-9686 /api/v1/namespaces/deployment-9686/pods/test-cleanup-controller-frqw5 9f3b67a7-3bd4-44eb-bf0d-67244cd17056 73364 0 2022-03-02 22:15:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0eeada1e7fadee02394464e478bbc6ac31527355250f74041c46231e310190b8 cni.projectcalico.org/podIP:172.30.228.17/32 cni.projectcalico.org/podIPs:172.30.228.17/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.17"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.17"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller df2399c8-60bb-49f9-806b-ad12ee656c65 0xc0039d7b07 0xc0039d7b08}] []  [{kube-controller-manager Update v1 2022-03-02 22:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df2399c8-60bb-49f9-806b-ad12ee656c65\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 22:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 22:15:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.228.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzrw8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzrw8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzrw8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:15:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:15:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:15:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:172.30.228.17,StartTime:2022-03-02 22:15:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:15:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://afe1faea7a9155db0d2636829adcc7f1a2773d9aabbded4f0eb0778388e15063,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.228.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:15:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9686" for this suite.

• [SLOW TEST:5.567 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":83,"skipped":1546,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:15:58.429: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:15:58.874: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  2 22:16:03.890: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 22:16:03.891: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  2 22:16:05.903: INFO: Creating deployment "test-rollover-deployment"
Mar  2 22:16:05.939: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  2 22:16:07.968: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  2 22:16:07.995: INFO: Ensure that both replica sets have 1 created replica
Mar  2 22:16:08.015: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  2 22:16:08.097: INFO: Updating deployment test-rollover-deployment
Mar  2 22:16:08.097: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  2 22:16:10.146: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  2 22:16:10.204: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  2 22:16:10.251: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:16:10.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856168, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856165, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:12.287: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:16:12.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856170, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856165, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:14.281: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:16:14.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856170, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856165, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:16.278: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:16:16.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856170, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856165, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:18.274: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:16:18.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856170, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856165, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:20.279: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:16:20.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856166, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856170, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856165, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:22.290: INFO: 
Mar  2 22:16:22.290: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 22:16:22.337: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-340 /apis/apps/v1/namespaces/deployment-340/deployments/test-rollover-deployment c949053b-be2f-4805-8946-95daec2b099a 73713 2 2022-03-02 22:16:05 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-02 22:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-02 22:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028a9318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-02 22:16:06 +0000 UTC,LastTransitionTime:2022-03-02 22:16:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2022-03-02 22:16:20 +0000 UTC,LastTransitionTime:2022-03-02 22:16:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 22:16:22.349: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-340 /apis/apps/v1/namespaces/deployment-340/replicasets/test-rollover-deployment-668db69979 bef9cf07-fed9-485e-947e-fdfef0345127 73701 2 2022-03-02 22:16:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c949053b-be2f-4805-8946-95daec2b099a 0xc0028a9797 0xc0028a9798}] []  [{kube-controller-manager Update apps/v1 2022-03-02 22:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c949053b-be2f-4805-8946-95daec2b099a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028a9828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:16:22.349: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  2 22:16:22.350: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-340 /apis/apps/v1/namespaces/deployment-340/replicasets/test-rollover-controller 0b5b3324-aed4-434a-91ce-0736bcc5236f 73711 2 2022-03-02 22:15:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c949053b-be2f-4805-8946-95daec2b099a 0xc0028a9687 0xc0028a9688}] []  [{e2e.test Update apps/v1 2022-03-02 22:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-02 22:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c949053b-be2f-4805-8946-95daec2b099a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0028a9728 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:16:22.350: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-340 /apis/apps/v1/namespaces/deployment-340/replicasets/test-rollover-deployment-78bc8b888c adc8c8f1-d073-4266-848a-8d7fe880ef6e 73616 2 2022-03-02 22:16:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c949053b-be2f-4805-8946-95daec2b099a 0xc0028a98b7 0xc0028a98b8}] []  [{kube-controller-manager Update apps/v1 2022-03-02 22:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c949053b-be2f-4805-8946-95daec2b099a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028a9948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:16:22.370: INFO: Pod "test-rollover-deployment-668db69979-wgs6v" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-wgs6v test-rollover-deployment-668db69979- deployment-340 /api/v1/namespaces/deployment-340/pods/test-rollover-deployment-668db69979-wgs6v 7e7be59a-f076-4a55-9a0e-9251bb5d6bc5 73654 0 2022-03-02 22:16:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/containerID:7e7abee9a9f3ff6f8273e4ce350ec7ec69076486e73848f5afeef09ee3a42a6e cni.projectcalico.org/podIP:172.30.228.11/32 cni.projectcalico.org/podIPs:172.30.228.11/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.11"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.11"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 bef9cf07-fed9-485e-947e-fdfef0345127 0xc0028a9e67 0xc0028a9e68}] []  [{kube-controller-manager Update v1 2022-03-02 22:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bef9cf07-fed9-485e-947e-fdfef0345127\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 22:16:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-03-02 22:16:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.228.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2022-03-02 22:16:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b72th,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b72th,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b72th,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b7tfk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:172.30.228.11,StartTime:2022-03-02 22:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 22:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://4db9e445456242bca8aa0016b6eb99b3438b2d3824515cd9ffdef1103fa3464c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.228.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:16:22.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-340" for this suite.

• [SLOW TEST:24.012 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":84,"skipped":1565,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:16:22.442: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 22:16:24.813: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:16:24.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6053" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:16:24.904: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:16:26.457: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:16:28.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856186, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856186, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856186, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856186, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:16:31.774: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:16:32.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7980" for this suite.
STEP: Destroying namespace "webhook-7980-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.756 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":86,"skipped":1669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:16:32.670: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Mar  2 22:16:33.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-5492 create -f -'
Mar  2 22:16:33.844: INFO: stderr: ""
Mar  2 22:16:33.844: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 22:16:34.855: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:16:34.855: INFO: Found 0 / 1
Mar  2 22:16:35.855: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:16:35.855: INFO: Found 0 / 1
Mar  2 22:16:36.867: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:16:36.867: INFO: Found 1 / 1
Mar  2 22:16:36.867: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  2 22:16:36.886: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:16:36.886: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 22:16:36.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-5492 patch pod agnhost-primary-n8j4z -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  2 22:16:37.125: INFO: stderr: ""
Mar  2 22:16:37.125: INFO: stdout: "pod/agnhost-primary-n8j4z patched\n"
STEP: checking annotations
Mar  2 22:16:37.138: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:16:37.139: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:16:37.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5492" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":87,"skipped":1726,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:16:37.183: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-7wpp
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 22:16:37.609: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7wpp" in namespace "subpath-9378" to be "Succeeded or Failed"
Mar  2 22:16:37.619: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Pending", Reason="", readiness=false. Elapsed: 9.639158ms
Mar  2 22:16:39.636: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026775534s
Mar  2 22:16:41.669: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 4.059466831s
Mar  2 22:16:43.682: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 6.072979084s
Mar  2 22:16:45.700: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 8.090781286s
Mar  2 22:16:47.717: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 10.107512643s
Mar  2 22:16:49.729: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 12.120231618s
Mar  2 22:16:51.741: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 14.132123125s
Mar  2 22:16:53.764: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 16.154753202s
Mar  2 22:16:55.790: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 18.180573996s
Mar  2 22:16:57.801: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 20.19207788s
Mar  2 22:16:59.877: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Running", Reason="", readiness=true. Elapsed: 22.267606488s
Mar  2 22:17:01.956: INFO: Pod "pod-subpath-test-configmap-7wpp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.347032396s
STEP: Saw pod success
Mar  2 22:17:01.956: INFO: Pod "pod-subpath-test-configmap-7wpp" satisfied condition "Succeeded or Failed"
Mar  2 22:17:01.979: INFO: Trying to get logs from node 10.138.244.159 pod pod-subpath-test-configmap-7wpp container test-container-subpath-configmap-7wpp: <nil>
STEP: delete the pod
Mar  2 22:17:02.210: INFO: Waiting for pod pod-subpath-test-configmap-7wpp to disappear
Mar  2 22:17:02.252: INFO: Pod pod-subpath-test-configmap-7wpp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7wpp
Mar  2 22:17:02.252: INFO: Deleting pod "pod-subpath-test-configmap-7wpp" in namespace "subpath-9378"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:17:02.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9378" for this suite.

• [SLOW TEST:25.164 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":88,"skipped":1744,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:17:02.349: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-f74775cb-4620-4037-8b47-5d77911f89dd
STEP: Creating a pod to test consume secrets
Mar  2 22:17:02.734: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-764019a8-1d63-4c66-b0d7-da97a3521b2e" in namespace "projected-3583" to be "Succeeded or Failed"
Mar  2 22:17:02.761: INFO: Pod "pod-projected-secrets-764019a8-1d63-4c66-b0d7-da97a3521b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 27.230135ms
Mar  2 22:17:04.825: INFO: Pod "pod-projected-secrets-764019a8-1d63-4c66-b0d7-da97a3521b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090630526s
Mar  2 22:17:06.853: INFO: Pod "pod-projected-secrets-764019a8-1d63-4c66-b0d7-da97a3521b2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.118424655s
STEP: Saw pod success
Mar  2 22:17:06.853: INFO: Pod "pod-projected-secrets-764019a8-1d63-4c66-b0d7-da97a3521b2e" satisfied condition "Succeeded or Failed"
Mar  2 22:17:06.880: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-secrets-764019a8-1d63-4c66-b0d7-da97a3521b2e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:17:06.978: INFO: Waiting for pod pod-projected-secrets-764019a8-1d63-4c66-b0d7-da97a3521b2e to disappear
Mar  2 22:17:06.996: INFO: Pod pod-projected-secrets-764019a8-1d63-4c66-b0d7-da97a3521b2e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:17:06.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3583" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":89,"skipped":1758,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:17:07.047: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7751
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7751
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7751
Mar  2 22:17:07.443: INFO: Found 0 stateful pods, waiting for 1
Mar  2 22:17:17.458: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  2 22:17:17.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-7751 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:17:18.005: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:17:18.005: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:17:18.005: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:17:18.018: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 22:17:28.029: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:17:28.029: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:17:28.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999994675s
Mar  2 22:17:29.092: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980762366s
Mar  2 22:17:30.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.970208518s
Mar  2 22:17:31.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.956029633s
Mar  2 22:17:32.131: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.943221817s
Mar  2 22:17:33.165: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.931117532s
Mar  2 22:17:34.183: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.897824739s
Mar  2 22:17:35.194: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.879319849s
Mar  2 22:17:36.211: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.86816809s
Mar  2 22:17:37.222: INFO: Verifying statefulset ss doesn't scale past 1 for another 851.835186ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7751
Mar  2 22:17:38.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-7751 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:17:38.712: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:17:38.712: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:17:38.712: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:17:38.729: INFO: Found 1 stateful pods, waiting for 3
Mar  2 22:17:48.743: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:17:48.743: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:17:48.743: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  2 22:17:48.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-7751 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:17:49.476: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:17:49.476: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:17:49.476: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:17:49.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-7751 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:17:49.964: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:17:49.964: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:17:49.964: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:17:49.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-7751 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:17:50.422: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:17:50.423: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:17:50.423: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:17:50.423: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:17:50.433: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  2 22:18:00.457: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:18:00.457: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:18:00.457: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:18:00.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999994712s
Mar  2 22:18:01.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988435777s
Mar  2 22:18:02.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972496384s
Mar  2 22:18:03.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960416935s
Mar  2 22:18:04.560: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940336905s
Mar  2 22:18:05.574: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.928840337s
Mar  2 22:18:06.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.915460748s
Mar  2 22:18:07.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.901093391s
Mar  2 22:18:08.623: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.886640127s
Mar  2 22:18:09.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 865.697654ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7751
Mar  2 22:18:10.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-7751 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:18:11.039: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:18:11.039: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:18:11.039: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:18:11.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-7751 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:18:11.474: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:18:11.474: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:18:11.474: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:18:11.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-7751 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:18:11.981: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:18:11.982: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:18:11.982: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:18:11.982: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 22:18:52.029: INFO: Deleting all statefulset in ns statefulset-7751
Mar  2 22:18:52.038: INFO: Scaling statefulset ss to 0
Mar  2 22:18:52.098: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:18:52.108: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:18:52.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7751" for this suite.

• [SLOW TEST:105.194 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":90,"skipped":1760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:18:52.242: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9408/configmap-test-30b46b5d-d429-49e1-bcad-f7f300b1a98c
STEP: Creating a pod to test consume configMaps
Mar  2 22:18:52.644: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2c0bf0e-19d2-43af-bbbc-101dff3853fc" in namespace "configmap-9408" to be "Succeeded or Failed"
Mar  2 22:18:52.664: INFO: Pod "pod-configmaps-f2c0bf0e-19d2-43af-bbbc-101dff3853fc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.873864ms
Mar  2 22:18:54.676: INFO: Pod "pod-configmaps-f2c0bf0e-19d2-43af-bbbc-101dff3853fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031461752s
Mar  2 22:18:56.686: INFO: Pod "pod-configmaps-f2c0bf0e-19d2-43af-bbbc-101dff3853fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041889431s
STEP: Saw pod success
Mar  2 22:18:56.687: INFO: Pod "pod-configmaps-f2c0bf0e-19d2-43af-bbbc-101dff3853fc" satisfied condition "Succeeded or Failed"
Mar  2 22:18:56.700: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-f2c0bf0e-19d2-43af-bbbc-101dff3853fc container env-test: <nil>
STEP: delete the pod
Mar  2 22:18:56.796: INFO: Waiting for pod pod-configmaps-f2c0bf0e-19d2-43af-bbbc-101dff3853fc to disappear
Mar  2 22:18:56.805: INFO: Pod pod-configmaps-f2c0bf0e-19d2-43af-bbbc-101dff3853fc no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:18:56.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9408" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":91,"skipped":1784,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:18:56.859: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:18:57.146: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-97529957-d474-4fad-a839-2a75d7b5712b" in namespace "security-context-test-3353" to be "Succeeded or Failed"
Mar  2 22:18:57.162: INFO: Pod "busybox-readonly-false-97529957-d474-4fad-a839-2a75d7b5712b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.915726ms
Mar  2 22:18:59.179: INFO: Pod "busybox-readonly-false-97529957-d474-4fad-a839-2a75d7b5712b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032932348s
Mar  2 22:19:01.190: INFO: Pod "busybox-readonly-false-97529957-d474-4fad-a839-2a75d7b5712b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043829175s
Mar  2 22:19:01.190: INFO: Pod "busybox-readonly-false-97529957-d474-4fad-a839-2a75d7b5712b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:19:01.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3353" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":92,"skipped":1805,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:19:01.229: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7892
Mar  2 22:19:03.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7892 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 22:19:04.024: INFO: rc: 7
Mar  2 22:19:04.091: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 22:19:04.103: INFO: Pod kube-proxy-mode-detector no longer exists
Mar  2 22:19:04.103: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7892 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-7892
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7892
I0302 22:19:04.189192      25 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7892, replica count: 3
I0302 22:19:07.239789      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:19:07.267: INFO: Creating new exec pod
Mar  2 22:19:12.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7892 exec execpod-affinity52t86 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Mar  2 22:19:12.730: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 22:19:12.730: INFO: stdout: ""
Mar  2 22:19:12.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7892 exec execpod-affinity52t86 -- /bin/sh -x -c nc -zv -t -w 2 172.21.171.138 80'
Mar  2 22:19:13.192: INFO: stderr: "+ nc -zv -t -w 2 172.21.171.138 80\nConnection to 172.21.171.138 80 port [tcp/http] succeeded!\n"
Mar  2 22:19:13.192: INFO: stdout: ""
Mar  2 22:19:13.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7892 exec execpod-affinity52t86 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.171.138:80/ ; done'
Mar  2 22:19:13.868: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n"
Mar  2 22:19:13.868: INFO: stdout: "\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt\naffinity-clusterip-timeout-td2kt"
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Received response from host: affinity-clusterip-timeout-td2kt
Mar  2 22:19:13.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7892 exec execpod-affinity52t86 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.171.138:80/'
Mar  2 22:19:14.309: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n"
Mar  2 22:19:14.309: INFO: stdout: "affinity-clusterip-timeout-td2kt"
Mar  2 22:19:34.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7892 exec execpod-affinity52t86 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.171.138:80/'
Mar  2 22:19:34.707: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n"
Mar  2 22:19:34.707: INFO: stdout: "affinity-clusterip-timeout-td2kt"
Mar  2 22:19:54.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7892 exec execpod-affinity52t86 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.171.138:80/'
Mar  2 22:19:55.186: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.171.138:80/\n"
Mar  2 22:19:55.186: INFO: stdout: "affinity-clusterip-timeout-9rrqg"
Mar  2 22:19:55.186: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7892, will wait for the garbage collector to delete the pods
Mar  2 22:19:55.302: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 24.362372ms
Mar  2 22:19:55.403: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.970317ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:09.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7892" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:67.982 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":93,"skipped":1818,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:09.211: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  2 22:20:09.554: INFO: Waiting up to 5m0s for pod "downward-api-5d953e6c-773f-4b43-b742-0a11cbebd239" in namespace "downward-api-543" to be "Succeeded or Failed"
Mar  2 22:20:09.568: INFO: Pod "downward-api-5d953e6c-773f-4b43-b742-0a11cbebd239": Phase="Pending", Reason="", readiness=false. Elapsed: 13.527927ms
Mar  2 22:20:11.585: INFO: Pod "downward-api-5d953e6c-773f-4b43-b742-0a11cbebd239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030546445s
Mar  2 22:20:13.597: INFO: Pod "downward-api-5d953e6c-773f-4b43-b742-0a11cbebd239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042865514s
STEP: Saw pod success
Mar  2 22:20:13.598: INFO: Pod "downward-api-5d953e6c-773f-4b43-b742-0a11cbebd239" satisfied condition "Succeeded or Failed"
Mar  2 22:20:13.607: INFO: Trying to get logs from node 10.138.244.159 pod downward-api-5d953e6c-773f-4b43-b742-0a11cbebd239 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:20:13.739: INFO: Waiting for pod downward-api-5d953e6c-773f-4b43-b742-0a11cbebd239 to disappear
Mar  2 22:20:13.768: INFO: Pod downward-api-5d953e6c-773f-4b43-b742-0a11cbebd239 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:13.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-543" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:13.874: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 22:20:14.108: INFO: Waiting up to 5m0s for pod "pod-00ff5666-cbae-4547-8bec-ea7ab0c079aa" in namespace "emptydir-7601" to be "Succeeded or Failed"
Mar  2 22:20:14.118: INFO: Pod "pod-00ff5666-cbae-4547-8bec-ea7ab0c079aa": Phase="Pending", Reason="", readiness=false. Elapsed: 9.212292ms
Mar  2 22:20:16.134: INFO: Pod "pod-00ff5666-cbae-4547-8bec-ea7ab0c079aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025900123s
Mar  2 22:20:18.146: INFO: Pod "pod-00ff5666-cbae-4547-8bec-ea7ab0c079aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037885522s
STEP: Saw pod success
Mar  2 22:20:18.146: INFO: Pod "pod-00ff5666-cbae-4547-8bec-ea7ab0c079aa" satisfied condition "Succeeded or Failed"
Mar  2 22:20:18.175: INFO: Trying to get logs from node 10.138.244.159 pod pod-00ff5666-cbae-4547-8bec-ea7ab0c079aa container test-container: <nil>
STEP: delete the pod
Mar  2 22:20:18.267: INFO: Waiting for pod pod-00ff5666-cbae-4547-8bec-ea7ab0c079aa to disappear
Mar  2 22:20:18.278: INFO: Pod pod-00ff5666-cbae-4547-8bec-ea7ab0c079aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:18.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7601" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1942,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:18.338: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Mar  2 22:20:18.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-4557 api-versions'
Mar  2 22:20:18.706: INFO: stderr: ""
Mar  2 22:20:18.706: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:18.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4557" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":96,"skipped":1944,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:18.750: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:18.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6020" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":97,"skipped":1950,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:19.046: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:20:20.053: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:20:22.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856420, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856420, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856420, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856420, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:20:25.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar  2 22:20:25.214: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:25.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9372" for this suite.
STEP: Destroying namespace "webhook-9372-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.752 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":98,"skipped":1950,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:25.798: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar  2 22:20:26.133: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar  2 22:20:26.194: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 22:20:26.194: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar  2 22:20:26.295: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 22:20:26.295: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar  2 22:20:26.480: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  2 22:20:26.480: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar  2 22:20:33.744: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:33.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4276" for this suite.

• [SLOW TEST:8.058 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":99,"skipped":1957,"failed":0}
SSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:33.859: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:34.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5775" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":100,"skipped":1960,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:34.531: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1561
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1561
I0302 22:20:34.909373      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1561, replica count: 2
Mar  2 22:20:37.960: INFO: Creating new exec pod
I0302 22:20:37.959927      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:20:43.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-1561 exec execpodfnhdb -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  2 22:20:43.497: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 22:20:43.497: INFO: stdout: ""
Mar  2 22:20:43.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-1561 exec execpodfnhdb -- /bin/sh -x -c nc -zv -t -w 2 172.21.228.137 80'
Mar  2 22:20:43.911: INFO: stderr: "+ nc -zv -t -w 2 172.21.228.137 80\nConnection to 172.21.228.137 80 port [tcp/http] succeeded!\n"
Mar  2 22:20:43.911: INFO: stdout: ""
Mar  2 22:20:43.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-1561 exec execpodfnhdb -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.162 30882'
Mar  2 22:20:44.316: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.162 30882\nConnection to 10.138.244.162 30882 port [tcp/30882] succeeded!\n"
Mar  2 22:20:44.316: INFO: stdout: ""
Mar  2 22:20:44.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-1561 exec execpodfnhdb -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.150 30882'
Mar  2 22:20:44.751: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.150 30882\nConnection to 10.138.244.150 30882 port [tcp/30882] succeeded!\n"
Mar  2 22:20:44.751: INFO: stdout: ""
Mar  2 22:20:44.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-1561 exec execpodfnhdb -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.211 30882'
Mar  2 22:20:45.205: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.211 30882\nConnection to 168.1.11.211 30882 port [tcp/30882] succeeded!\n"
Mar  2 22:20:45.205: INFO: stdout: ""
Mar  2 22:20:45.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-1561 exec execpodfnhdb -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.212 30882'
Mar  2 22:20:45.621: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.212 30882\nConnection to 168.1.11.212 30882 port [tcp/30882] succeeded!\n"
Mar  2 22:20:45.621: INFO: stdout: ""
Mar  2 22:20:45.621: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:20:45.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1561" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:11.281 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":101,"skipped":1973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:20:45.811: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-63675a4b-cbdb-4feb-9b2e-de9b4bcc8cff in namespace container-probe-367
Mar  2 22:20:50.085: INFO: Started pod test-webserver-63675a4b-cbdb-4feb-9b2e-de9b4bcc8cff in namespace container-probe-367
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:20:50.095: INFO: Initial restart count of pod test-webserver-63675a4b-cbdb-4feb-9b2e-de9b4bcc8cff is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:24:50.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-367" for this suite.

• [SLOW TEST:244.465 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":2001,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:24:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:24:55.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9622" for this suite.

• [SLOW TEST:5.109 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":103,"skipped":2004,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:24:55.387: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 22:24:55.638: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 22:25:55.831: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:25:55.860: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar  2 22:26:00.193: INFO: found a healthy node: 10.138.244.159
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:26:14.456: INFO: pods created so far: [1 1 1]
Mar  2 22:26:14.456: INFO: length of pods created so far: 3
Mar  2 22:26:32.514: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:26:39.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-960" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:26:39.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6916" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:104.574 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":104,"skipped":2023,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:26:39.963: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar  2 22:26:45.297: INFO: Successfully updated pod "annotationupdatec28ad25e-5642-4913-99df-8293265bde68"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:26:47.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2650" for this suite.

• [SLOW TEST:7.451 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":105,"skipped":2036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:26:47.421: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-0337aac4-aeb2-48ee-ab37-0a3b00b7ddae
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:26:47.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9954" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":106,"skipped":2068,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:26:47.690: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:26:47.942: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cace9b31-02cd-4d29-abb5-201829ef1c4e" in namespace "downward-api-3293" to be "Succeeded or Failed"
Mar  2 22:26:47.975: INFO: Pod "downwardapi-volume-cace9b31-02cd-4d29-abb5-201829ef1c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.913131ms
Mar  2 22:26:49.989: INFO: Pod "downwardapi-volume-cace9b31-02cd-4d29-abb5-201829ef1c4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046432078s
STEP: Saw pod success
Mar  2 22:26:49.989: INFO: Pod "downwardapi-volume-cace9b31-02cd-4d29-abb5-201829ef1c4e" satisfied condition "Succeeded or Failed"
Mar  2 22:26:50.006: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-cace9b31-02cd-4d29-abb5-201829ef1c4e container client-container: <nil>
STEP: delete the pod
Mar  2 22:26:50.101: INFO: Waiting for pod downwardapi-volume-cace9b31-02cd-4d29-abb5-201829ef1c4e to disappear
Mar  2 22:26:50.128: INFO: Pod downwardapi-volume-cace9b31-02cd-4d29-abb5-201829ef1c4e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:26:50.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3293" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":2079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:26:50.165: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-a3a36c84-42e6-4b4a-bcdd-58e5f222890c
STEP: Creating a pod to test consume secrets
Mar  2 22:26:50.419: INFO: Waiting up to 5m0s for pod "pod-secrets-94c01b0e-92fc-42a4-bb9a-6323cc0707fd" in namespace "secrets-327" to be "Succeeded or Failed"
Mar  2 22:26:50.430: INFO: Pod "pod-secrets-94c01b0e-92fc-42a4-bb9a-6323cc0707fd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.803563ms
Mar  2 22:26:52.444: INFO: Pod "pod-secrets-94c01b0e-92fc-42a4-bb9a-6323cc0707fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025560823s
Mar  2 22:26:54.460: INFO: Pod "pod-secrets-94c01b0e-92fc-42a4-bb9a-6323cc0707fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041368917s
STEP: Saw pod success
Mar  2 22:26:54.461: INFO: Pod "pod-secrets-94c01b0e-92fc-42a4-bb9a-6323cc0707fd" satisfied condition "Succeeded or Failed"
Mar  2 22:26:54.477: INFO: Trying to get logs from node 10.138.244.159 pod pod-secrets-94c01b0e-92fc-42a4-bb9a-6323cc0707fd container secret-env-test: <nil>
STEP: delete the pod
Mar  2 22:26:54.554: INFO: Waiting for pod pod-secrets-94c01b0e-92fc-42a4-bb9a-6323cc0707fd to disappear
Mar  2 22:26:54.568: INFO: Pod pod-secrets-94c01b0e-92fc-42a4-bb9a-6323cc0707fd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:26:54.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-327" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":108,"skipped":2103,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:26:54.615: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:26:55.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:26:57.511: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856815, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856815, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856815, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781856815, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:27:00.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:27:11.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4629" for this suite.
STEP: Destroying namespace "webhook-4629-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:16.920 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":109,"skipped":2103,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:27:11.537: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar  2 22:27:11.772: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5953 /api/v1/namespaces/dns-5953/pods/test-dns-nameservers 0cd65580-956e-4c04-a41f-8e4d845f6f34 80189 0 2022-03-02 22:27:11 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-03-02 22:27:11 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dllkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dllkx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dllkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:27:11.785: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 22:27:13.797: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 22:27:15.815: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar  2 22:27:15.815: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5953 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:27:15.815: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Verifying customized DNS server is configured on pod...
Mar  2 22:27:16.304: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5953 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:27:16.304: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:27:16.598: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:27:16.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5953" for this suite.

• [SLOW TEST:5.201 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":110,"skipped":2107,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:27:16.741: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9326
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9326
STEP: creating replication controller externalsvc in namespace services-9326
I0302 22:27:17.135994      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9326, replica count: 2
I0302 22:27:20.186833      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar  2 22:27:20.282: INFO: Creating new exec pod
Mar  2 22:27:22.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9326 exec execpodfbkpj -- /bin/sh -x -c nslookup clusterip-service.services-9326.svc.cluster.local'
Mar  2 22:27:22.848: INFO: stderr: "+ nslookup clusterip-service.services-9326.svc.cluster.local\n"
Mar  2 22:27:22.848: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-9326.svc.cluster.local\tcanonical name = externalsvc.services-9326.svc.cluster.local.\nName:\texternalsvc.services-9326.svc.cluster.local\nAddress: 172.21.157.172\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9326, will wait for the garbage collector to delete the pods
Mar  2 22:27:22.964: INFO: Deleting ReplicationController externalsvc took: 51.959187ms
Mar  2 22:27:23.164: INFO: Terminating ReplicationController externalsvc pods took: 200.434971ms
Mar  2 22:27:39.240: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:27:39.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9326" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.601 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":111,"skipped":2127,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:27:39.342: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 22:27:39.644: INFO: Waiting up to 5m0s for pod "pod-c7fc7848-8d15-4759-aa9f-b937896d6fc9" in namespace "emptydir-6214" to be "Succeeded or Failed"
Mar  2 22:27:39.656: INFO: Pod "pod-c7fc7848-8d15-4759-aa9f-b937896d6fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.660479ms
Mar  2 22:27:41.667: INFO: Pod "pod-c7fc7848-8d15-4759-aa9f-b937896d6fc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021985825s
STEP: Saw pod success
Mar  2 22:27:41.667: INFO: Pod "pod-c7fc7848-8d15-4759-aa9f-b937896d6fc9" satisfied condition "Succeeded or Failed"
Mar  2 22:27:41.677: INFO: Trying to get logs from node 10.138.244.159 pod pod-c7fc7848-8d15-4759-aa9f-b937896d6fc9 container test-container: <nil>
STEP: delete the pod
Mar  2 22:27:41.731: INFO: Waiting for pod pod-c7fc7848-8d15-4759-aa9f-b937896d6fc9 to disappear
Mar  2 22:27:41.744: INFO: Pod pod-c7fc7848-8d15-4759-aa9f-b937896d6fc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:27:41.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6214" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":112,"skipped":2128,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:27:41.794: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar  2 22:27:46.157: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1480 PodName:var-expansion-6bc66f53-68a9-4ba7-a850-4c49dfdfd073 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:27:46.157: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: test for file in mounted path
Mar  2 22:27:46.618: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1480 PodName:var-expansion-6bc66f53-68a9-4ba7-a850-4c49dfdfd073 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:27:46.618: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: updating the annotation value
Mar  2 22:27:47.561: INFO: Successfully updated pod "var-expansion-6bc66f53-68a9-4ba7-a850-4c49dfdfd073"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar  2 22:27:47.572: INFO: Deleting pod "var-expansion-6bc66f53-68a9-4ba7-a850-4c49dfdfd073" in namespace "var-expansion-1480"
Mar  2 22:27:47.597: INFO: Wait up to 5m0s for pod "var-expansion-6bc66f53-68a9-4ba7-a850-4c49dfdfd073" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:28:27.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1480" for this suite.

• [SLOW TEST:45.876 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":113,"skipped":2137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:28:27.676: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0302 22:28:38.119827      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0302 22:28:38.119923      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0302 22:28:38.119943      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 22:28:38.119: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:28:38.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7391" for this suite.

• [SLOW TEST:10.489 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":114,"skipped":2179,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:28:38.165: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  2 22:28:38.442: INFO: Waiting up to 5m0s for pod "downward-api-9bc29cb5-1c29-4be8-96b0-8c5a6a7d34fe" in namespace "downward-api-7182" to be "Succeeded or Failed"
Mar  2 22:28:38.453: INFO: Pod "downward-api-9bc29cb5-1c29-4be8-96b0-8c5a6a7d34fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.420259ms
Mar  2 22:28:40.484: INFO: Pod "downward-api-9bc29cb5-1c29-4be8-96b0-8c5a6a7d34fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041723557s
Mar  2 22:28:42.508: INFO: Pod "downward-api-9bc29cb5-1c29-4be8-96b0-8c5a6a7d34fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065446191s
STEP: Saw pod success
Mar  2 22:28:42.508: INFO: Pod "downward-api-9bc29cb5-1c29-4be8-96b0-8c5a6a7d34fe" satisfied condition "Succeeded or Failed"
Mar  2 22:28:42.531: INFO: Trying to get logs from node 10.138.244.159 pod downward-api-9bc29cb5-1c29-4be8-96b0-8c5a6a7d34fe container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:28:42.601: INFO: Waiting for pod downward-api-9bc29cb5-1c29-4be8-96b0-8c5a6a7d34fe to disappear
Mar  2 22:28:42.611: INFO: Pod downward-api-9bc29cb5-1c29-4be8-96b0-8c5a6a7d34fe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:28:42.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7182" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":115,"skipped":2184,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:28:42.678: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-de0ddcb4-057f-4df8-98f0-459674440d26
STEP: Creating a pod to test consume configMaps
Mar  2 22:28:43.029: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-85fefd84-4080-4633-a4c3-ce4ba3819b48" in namespace "projected-5560" to be "Succeeded or Failed"
Mar  2 22:28:43.043: INFO: Pod "pod-projected-configmaps-85fefd84-4080-4633-a4c3-ce4ba3819b48": Phase="Pending", Reason="", readiness=false. Elapsed: 13.17941ms
Mar  2 22:28:45.058: INFO: Pod "pod-projected-configmaps-85fefd84-4080-4633-a4c3-ce4ba3819b48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027988615s
Mar  2 22:28:47.068: INFO: Pod "pod-projected-configmaps-85fefd84-4080-4633-a4c3-ce4ba3819b48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038393882s
STEP: Saw pod success
Mar  2 22:28:47.069: INFO: Pod "pod-projected-configmaps-85fefd84-4080-4633-a4c3-ce4ba3819b48" satisfied condition "Succeeded or Failed"
Mar  2 22:28:47.078: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-configmaps-85fefd84-4080-4633-a4c3-ce4ba3819b48 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:28:47.141: INFO: Waiting for pod pod-projected-configmaps-85fefd84-4080-4633-a4c3-ce4ba3819b48 to disappear
Mar  2 22:28:47.155: INFO: Pod pod-projected-configmaps-85fefd84-4080-4633-a4c3-ce4ba3819b48 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:28:47.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5560" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":2184,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:28:47.213: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-a01d0ca8-a097-4aea-9cad-9605c2ae6734
STEP: Creating a pod to test consume secrets
Mar  2 22:28:47.675: INFO: Waiting up to 5m0s for pod "pod-secrets-d334a8f6-0721-4768-a1eb-f3732f1c9315" in namespace "secrets-147" to be "Succeeded or Failed"
Mar  2 22:28:47.693: INFO: Pod "pod-secrets-d334a8f6-0721-4768-a1eb-f3732f1c9315": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013685ms
Mar  2 22:28:49.705: INFO: Pod "pod-secrets-d334a8f6-0721-4768-a1eb-f3732f1c9315": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030641127s
Mar  2 22:28:51.716: INFO: Pod "pod-secrets-d334a8f6-0721-4768-a1eb-f3732f1c9315": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041455512s
STEP: Saw pod success
Mar  2 22:28:51.716: INFO: Pod "pod-secrets-d334a8f6-0721-4768-a1eb-f3732f1c9315" satisfied condition "Succeeded or Failed"
Mar  2 22:28:51.726: INFO: Trying to get logs from node 10.138.244.159 pod pod-secrets-d334a8f6-0721-4768-a1eb-f3732f1c9315 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:28:51.803: INFO: Waiting for pod pod-secrets-d334a8f6-0721-4768-a1eb-f3732f1c9315 to disappear
Mar  2 22:28:51.815: INFO: Pod pod-secrets-d334a8f6-0721-4768-a1eb-f3732f1c9315 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:28:51.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-147" for this suite.
STEP: Destroying namespace "secret-namespace-1120" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2187,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:28:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:28:54.287: INFO: Deleting pod "var-expansion-0e9d901d-b888-4b26-bb8b-8924c2b42712" in namespace "var-expansion-6533"
Mar  2 22:28:54.348: INFO: Wait up to 5m0s for pod "var-expansion-0e9d901d-b888-4b26-bb8b-8924c2b42712" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:29:06.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6533" for this suite.

• [SLOW TEST:14.509 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":118,"skipped":2199,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:29:06.433: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar  2 22:29:06.692: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 22:30:06.931: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:30:06.952: INFO: Starting informer...
STEP: Starting pods...
Mar  2 22:30:07.284: INFO: Pod1 is running on 10.138.244.159. Tainting Node
Mar  2 22:30:09.586: INFO: Pod2 is running on 10.138.244.159. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar  2 22:30:26.998: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  2 22:30:37.173: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:30:37.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6104" for this suite.

• [SLOW TEST:90.987 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":119,"skipped":2209,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:30:37.421: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  2 22:30:38.944: INFO: Waiting up to 5m0s for pod "downward-api-ff4402d1-0f69-4b7a-8bc4-4d2bbce67cc1" in namespace "downward-api-4556" to be "Succeeded or Failed"
Mar  2 22:30:38.956: INFO: Pod "downward-api-ff4402d1-0f69-4b7a-8bc4-4d2bbce67cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.698417ms
Mar  2 22:30:40.968: INFO: Pod "downward-api-ff4402d1-0f69-4b7a-8bc4-4d2bbce67cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023584592s
Mar  2 22:30:42.978: INFO: Pod "downward-api-ff4402d1-0f69-4b7a-8bc4-4d2bbce67cc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033484057s
STEP: Saw pod success
Mar  2 22:30:42.978: INFO: Pod "downward-api-ff4402d1-0f69-4b7a-8bc4-4d2bbce67cc1" satisfied condition "Succeeded or Failed"
Mar  2 22:30:42.992: INFO: Trying to get logs from node 10.138.244.159 pod downward-api-ff4402d1-0f69-4b7a-8bc4-4d2bbce67cc1 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:30:43.079: INFO: Waiting for pod downward-api-ff4402d1-0f69-4b7a-8bc4-4d2bbce67cc1 to disappear
Mar  2 22:30:43.088: INFO: Pod downward-api-ff4402d1-0f69-4b7a-8bc4-4d2bbce67cc1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:30:43.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4556" for this suite.

• [SLOW TEST:5.715 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2227,"failed":0}
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:30:43.137: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Mar  2 22:30:43.300: INFO: namespace kubectl-5126
Mar  2 22:30:43.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-5126 create -f -'
Mar  2 22:30:44.486: INFO: stderr: ""
Mar  2 22:30:44.486: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 22:30:45.500: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:30:45.500: INFO: Found 0 / 1
Mar  2 22:30:46.498: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:30:46.498: INFO: Found 0 / 1
Mar  2 22:30:47.498: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:30:47.498: INFO: Found 1 / 1
Mar  2 22:30:47.498: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 22:30:47.511: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 22:30:47.511: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 22:30:47.511: INFO: wait on agnhost-primary startup in kubectl-5126 
Mar  2 22:30:47.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-5126 logs agnhost-primary-jmxrt agnhost-primary'
Mar  2 22:30:47.723: INFO: stderr: ""
Mar  2 22:30:47.723: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar  2 22:30:47.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-5126 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  2 22:30:47.973: INFO: stderr: ""
Mar  2 22:30:47.973: INFO: stdout: "service/rm2 exposed\n"
Mar  2 22:30:48.012: INFO: Service rm2 in namespace kubectl-5126 found.
STEP: exposing service
Mar  2 22:30:50.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-5126 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  2 22:30:50.313: INFO: stderr: ""
Mar  2 22:30:50.319: INFO: stdout: "service/rm3 exposed\n"
Mar  2 22:30:50.331: INFO: Service rm3 in namespace kubectl-5126 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:30:52.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5126" for this suite.

• [SLOW TEST:9.261 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":121,"skipped":2227,"failed":0}
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:30:52.399: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-2894/secret-test-2b55f1dc-32db-4784-9e87-14f600829d52
STEP: Creating a pod to test consume secrets
Mar  2 22:30:52.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-d32d8395-f178-44c1-9bc2-f5168e4f1642" in namespace "secrets-2894" to be "Succeeded or Failed"
Mar  2 22:30:52.698: INFO: Pod "pod-configmaps-d32d8395-f178-44c1-9bc2-f5168e4f1642": Phase="Pending", Reason="", readiness=false. Elapsed: 25.576563ms
Mar  2 22:30:54.711: INFO: Pod "pod-configmaps-d32d8395-f178-44c1-9bc2-f5168e4f1642": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038897855s
Mar  2 22:30:56.722: INFO: Pod "pod-configmaps-d32d8395-f178-44c1-9bc2-f5168e4f1642": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049966232s
STEP: Saw pod success
Mar  2 22:30:56.723: INFO: Pod "pod-configmaps-d32d8395-f178-44c1-9bc2-f5168e4f1642" satisfied condition "Succeeded or Failed"
Mar  2 22:30:56.735: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-d32d8395-f178-44c1-9bc2-f5168e4f1642 container env-test: <nil>
STEP: delete the pod
Mar  2 22:30:56.798: INFO: Waiting for pod pod-configmaps-d32d8395-f178-44c1-9bc2-f5168e4f1642 to disappear
Mar  2 22:30:56.811: INFO: Pod pod-configmaps-d32d8395-f178-44c1-9bc2-f5168e4f1642 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:30:56.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2894" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2227,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:30:56.878: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-4aa030c2-0209-4c9c-b60c-b0d8fd97a00e
STEP: Creating a pod to test consume configMaps
Mar  2 22:30:57.160: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ccccab7-a0ab-4887-99da-4fa77aa31774" in namespace "projected-9248" to be "Succeeded or Failed"
Mar  2 22:30:57.191: INFO: Pod "pod-projected-configmaps-9ccccab7-a0ab-4887-99da-4fa77aa31774": Phase="Pending", Reason="", readiness=false. Elapsed: 31.22219ms
Mar  2 22:30:59.206: INFO: Pod "pod-projected-configmaps-9ccccab7-a0ab-4887-99da-4fa77aa31774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046282157s
Mar  2 22:31:01.218: INFO: Pod "pod-projected-configmaps-9ccccab7-a0ab-4887-99da-4fa77aa31774": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058826802s
STEP: Saw pod success
Mar  2 22:31:01.219: INFO: Pod "pod-projected-configmaps-9ccccab7-a0ab-4887-99da-4fa77aa31774" satisfied condition "Succeeded or Failed"
Mar  2 22:31:01.228: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-configmaps-9ccccab7-a0ab-4887-99da-4fa77aa31774 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:31:01.303: INFO: Waiting for pod pod-projected-configmaps-9ccccab7-a0ab-4887-99da-4fa77aa31774 to disappear
Mar  2 22:31:01.314: INFO: Pod pod-projected-configmaps-9ccccab7-a0ab-4887-99da-4fa77aa31774 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:01.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9248" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":2234,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:01.362: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:07.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4324" for this suite.

• [SLOW TEST:5.756 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":124,"skipped":2250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:07.122: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:11.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5118" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2281,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:11.575: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:31:11.823: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-305440c4-8f53-4fb1-b495-def16d5a35b6
STEP: Creating secret with name s-test-opt-upd-3db2ac71-431b-4809-82b6-54cf52f72649
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-305440c4-8f53-4fb1-b495-def16d5a35b6
STEP: Updating secret s-test-opt-upd-3db2ac71-431b-4809-82b6-54cf52f72649
STEP: Creating secret with name s-test-opt-create-5026c3c9-5b72-4936-aaff-9df044781a66
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:18.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9112" for this suite.

• [SLOW TEST:6.782 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:18.362: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:31:18.538: INFO: Creating deployment "test-recreate-deployment"
Mar  2 22:31:18.560: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  2 22:31:18.596: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  2 22:31:20.615: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  2 22:31:20.625: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  2 22:31:20.651: INFO: Updating deployment test-recreate-deployment
Mar  2 22:31:20.651: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 22:31:20.836: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6740 /apis/apps/v1/namespaces/deployment-6740/deployments/test-recreate-deployment 0df25baf-6eea-40ea-96d8-fa9c53f85be8 84149 2 2022-03-02 22:31:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-02 22:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-02 22:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0066f9fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-03-02 22:31:20 +0000 UTC,LastTransitionTime:2022-03-02 22:31:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2022-03-02 22:31:20 +0000 UTC,LastTransitionTime:2022-03-02 22:31:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 22:31:20.851: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-6740 /apis/apps/v1/namespaces/deployment-6740/replicasets/test-recreate-deployment-f79dd4667 bbd0c9af-e99f-4043-9d04-cdf955cbd70e 84146 1 2022-03-02 22:31:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0df25baf-6eea-40ea-96d8-fa9c53f85be8 0xc0007ffda0 0xc0007ffda1}] []  [{kube-controller-manager Update apps/v1 2022-03-02 22:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0df25baf-6eea-40ea-96d8-fa9c53f85be8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007ffe98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:31:20.851: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  2 22:31:20.851: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-6740 /apis/apps/v1/namespaces/deployment-6740/replicasets/test-recreate-deployment-786dd7c454 28b602d1-3b91-40d7-a999-762b4932c59c 84136 2 2022-03-02 22:31:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0df25baf-6eea-40ea-96d8-fa9c53f85be8 0xc0007ffb87 0xc0007ffb88}] []  [{kube-controller-manager Update apps/v1 2022-03-02 22:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0df25baf-6eea-40ea-96d8-fa9c53f85be8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007ffc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:31:20.862: INFO: Pod "test-recreate-deployment-f79dd4667-r7btk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-r7btk test-recreate-deployment-f79dd4667- deployment-6740 /api/v1/namespaces/deployment-6740/pods/test-recreate-deployment-f79dd4667-r7btk 268d0377-f1a2-458e-bc25-ecd68f36cc53 84150 0 2022-03-02 22:31:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 bbd0c9af-e99f-4043-9d04-cdf955cbd70e 0xc005966457 0xc005966458}] []  [{kube-controller-manager Update v1 2022-03-02 22:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbd0c9af-e99f-4043-9d04-cdf955cbd70e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-02 22:31:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-w25mx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-w25mx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-w25mx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nfcbz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:31:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:31:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 22:31:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:,StartTime:2022-03-02 22:31:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:20.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6740" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":127,"skipped":2314,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:20.916: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-a2374cb2-0bd7-47a0-9ac0-e20f88b8f058
STEP: Creating a pod to test consume configMaps
Mar  2 22:31:21.196: INFO: Waiting up to 5m0s for pod "pod-configmaps-eebf552b-56f1-4c3a-ac1d-72cee0aedc1a" in namespace "configmap-4081" to be "Succeeded or Failed"
Mar  2 22:31:21.221: INFO: Pod "pod-configmaps-eebf552b-56f1-4c3a-ac1d-72cee0aedc1a": Phase="Pending", Reason="", readiness=false. Elapsed: 25.392183ms
Mar  2 22:31:23.243: INFO: Pod "pod-configmaps-eebf552b-56f1-4c3a-ac1d-72cee0aedc1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047583413s
Mar  2 22:31:25.264: INFO: Pod "pod-configmaps-eebf552b-56f1-4c3a-ac1d-72cee0aedc1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068041242s
STEP: Saw pod success
Mar  2 22:31:25.264: INFO: Pod "pod-configmaps-eebf552b-56f1-4c3a-ac1d-72cee0aedc1a" satisfied condition "Succeeded or Failed"
Mar  2 22:31:25.275: INFO: Trying to get logs from node 10.138.244.162 pod pod-configmaps-eebf552b-56f1-4c3a-ac1d-72cee0aedc1a container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:31:25.622: INFO: Waiting for pod pod-configmaps-eebf552b-56f1-4c3a-ac1d-72cee0aedc1a to disappear
Mar  2 22:31:25.631: INFO: Pod pod-configmaps-eebf552b-56f1-4c3a-ac1d-72cee0aedc1a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:25.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4081" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":128,"skipped":2325,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:25.676: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  2 22:31:25.987: INFO: Waiting up to 5m0s for pod "pod-2d04b648-d7ef-4f32-b416-ad5a0c3326b1" in namespace "emptydir-8980" to be "Succeeded or Failed"
Mar  2 22:31:25.999: INFO: Pod "pod-2d04b648-d7ef-4f32-b416-ad5a0c3326b1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.712438ms
Mar  2 22:31:28.016: INFO: Pod "pod-2d04b648-d7ef-4f32-b416-ad5a0c3326b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.029203127s
Mar  2 22:31:30.028: INFO: Pod "pod-2d04b648-d7ef-4f32-b416-ad5a0c3326b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041373862s
STEP: Saw pod success
Mar  2 22:31:30.028: INFO: Pod "pod-2d04b648-d7ef-4f32-b416-ad5a0c3326b1" satisfied condition "Succeeded or Failed"
Mar  2 22:31:30.039: INFO: Trying to get logs from node 10.138.244.162 pod pod-2d04b648-d7ef-4f32-b416-ad5a0c3326b1 container test-container: <nil>
STEP: delete the pod
Mar  2 22:31:30.090: INFO: Waiting for pod pod-2d04b648-d7ef-4f32-b416-ad5a0c3326b1 to disappear
Mar  2 22:31:30.101: INFO: Pod pod-2d04b648-d7ef-4f32-b416-ad5a0c3326b1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:30.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8980" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":129,"skipped":2331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:30.139: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  2 22:31:38.540: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:38.540: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:38.826: INFO: Exec stderr: ""
Mar  2 22:31:38.826: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:38.826: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:39.122: INFO: Exec stderr: ""
Mar  2 22:31:39.122: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:39.122: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:39.415: INFO: Exec stderr: ""
Mar  2 22:31:39.415: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:39.415: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:39.717: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  2 22:31:39.717: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:39.717: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:39.998: INFO: Exec stderr: ""
Mar  2 22:31:40.000: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:40.000: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:40.264: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  2 22:31:40.264: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:40.264: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:40.542: INFO: Exec stderr: ""
Mar  2 22:31:40.544: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:40.544: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:40.792: INFO: Exec stderr: ""
Mar  2 22:31:40.792: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:40.793: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:41.028: INFO: Exec stderr: ""
Mar  2 22:31:41.028: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-108 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:31:41.028: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:31:41.249: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:41.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-108" for this suite.

• [SLOW TEST:11.154 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":130,"skipped":2353,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:41.295: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 22:31:54.742: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:54.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4101" for this suite.

• [SLOW TEST:13.535 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2354,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:54.830: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:31:54.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-4333 version'
Mar  2 22:31:55.129: INFO: stderr: ""
Mar  2 22:31:55.129: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.11\", GitCommit:\"27522a29febbcc4badac257763044d0d90c11abd\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T19:21:44Z\", GoVersion:\"go1.15.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.11+e880017\", GitCommit:\"f12ed970dad7592bafc187c70f6c2caa8fef8843\", GitTreeState:\"clean\", BuildDate:\"2021-12-22T05:47:56Z\", GoVersion:\"go1.15.14\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:31:55.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4333" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":132,"skipped":2369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:31:55.165: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:31:56.189: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:31:58.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857116, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857116, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857116, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857116, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:32:01.273: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:32:02.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-560" for this suite.
STEP: Destroying namespace "webhook-560-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.335 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":133,"skipped":2395,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:32:02.501: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  2 22:32:13.135: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0302 22:32:13.135243      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0302 22:32:13.135302      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0302 22:32:13.135327      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 22:32:13.135: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vjqg" in namespace "gc-5638"
Mar  2 22:32:13.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8kbk" in namespace "gc-5638"
Mar  2 22:32:13.227: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfxzs" in namespace "gc-5638"
Mar  2 22:32:13.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-n2gr8" in namespace "gc-5638"
Mar  2 22:32:13.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-pj8lt" in namespace "gc-5638"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:32:13.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5638" for this suite.

• [SLOW TEST:10.901 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":134,"skipped":2406,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:32:13.407: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3460
STEP: creating service affinity-clusterip in namespace services-3460
STEP: creating replication controller affinity-clusterip in namespace services-3460
I0302 22:32:13.714299      25 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-3460, replica count: 3
I0302 22:32:16.765040      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:32:16.786: INFO: Creating new exec pod
Mar  2 22:32:21.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-3460 exec execpod-affinitydwfz2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Mar  2 22:32:22.334: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  2 22:32:22.334: INFO: stdout: ""
Mar  2 22:32:22.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-3460 exec execpod-affinitydwfz2 -- /bin/sh -x -c nc -zv -t -w 2 172.21.153.144 80'
Mar  2 22:32:23.012: INFO: stderr: "+ nc -zv -t -w 2 172.21.153.144 80\nConnection to 172.21.153.144 80 port [tcp/http] succeeded!\n"
Mar  2 22:32:23.012: INFO: stdout: ""
Mar  2 22:32:23.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-3460 exec execpod-affinitydwfz2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.153.144:80/ ; done'
Mar  2 22:32:23.614: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.144:80/\n"
Mar  2 22:32:23.615: INFO: stdout: "\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj\naffinity-clusterip-l62cj"
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.615: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.616: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.616: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.616: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.616: INFO: Received response from host: affinity-clusterip-l62cj
Mar  2 22:32:23.616: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3460, will wait for the garbage collector to delete the pods
Mar  2 22:32:23.768: INFO: Deleting ReplicationController affinity-clusterip took: 53.774774ms
Mar  2 22:32:23.869: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.677957ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:32:39.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3460" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:25.884 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":135,"skipped":2416,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:32:39.296: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:32:39.547: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:32:40.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9334" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":136,"skipped":2424,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:32:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:32:40.706: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-cc0603d4-9aab-4fa2-9122-4775f72a9641
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:32:44.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7093" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2433,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:32:44.957: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:33:11.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8447" for this suite.

• [SLOW TEST:26.462 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":138,"skipped":2448,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:33:11.420: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:33:12.463: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:33:14.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857192, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857192, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857192, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857192, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:33:17.555: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:33:17.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8748" for this suite.
STEP: Destroying namespace "webhook-8748-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.637 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":139,"skipped":2466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:33:18.059: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar  2 22:33:18.362: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar  2 22:33:18.504: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:33:18.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6995" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":140,"skipped":2511,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:33:18.983: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:33:19.321: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:33:27.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8551" for this suite.

• [SLOW TEST:8.618 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":141,"skipped":2517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:33:27.606: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:33:27.998: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-566e24ae-4ccb-4b97-82e7-e0898ea5a319
STEP: Creating configMap with name cm-test-opt-upd-dc2a1ea5-84c4-44a2-82a7-3887f089c5d0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-566e24ae-4ccb-4b97-82e7-e0898ea5a319
STEP: Updating configmap cm-test-opt-upd-dc2a1ea5-84c4-44a2-82a7-3887f089c5d0
STEP: Creating configMap with name cm-test-opt-create-ff9529aa-d7af-454e-b4f7-c6d74f28021f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:33:36.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2268" for this suite.

• [SLOW TEST:9.081 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":142,"skipped":2553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:33:36.693: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:33:37.219: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-92d5d554-1448-4057-9f47-3e3b887ba93f
STEP: Creating secret with name s-test-opt-upd-8bfcce0a-2eae-469d-b8c0-e0ab01e8aba5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-92d5d554-1448-4057-9f47-3e3b887ba93f
STEP: Updating secret s-test-opt-upd-8bfcce0a-2eae-469d-b8c0-e0ab01e8aba5
STEP: Creating secret with name s-test-opt-create-bd4a8f1b-0529-4176-9d1e-cb3097c85aad
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:35:09.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7728" for this suite.

• [SLOW TEST:93.054 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":143,"skipped":2598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:35:09.757: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  2 22:35:11.375: INFO: Pod name wrapped-volume-race-c291fa54-7a17-4535-8154-c308eae88893: Found 0 pods out of 5
Mar  2 22:35:16.402: INFO: Pod name wrapped-volume-race-c291fa54-7a17-4535-8154-c308eae88893: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c291fa54-7a17-4535-8154-c308eae88893 in namespace emptydir-wrapper-2867, will wait for the garbage collector to delete the pods
Mar  2 22:35:16.648: INFO: Deleting ReplicationController wrapped-volume-race-c291fa54-7a17-4535-8154-c308eae88893 took: 56.806349ms
Mar  2 22:35:16.849: INFO: Terminating ReplicationController wrapped-volume-race-c291fa54-7a17-4535-8154-c308eae88893 pods took: 200.527121ms
STEP: Creating RC which spawns configmap-volume pods
Mar  2 22:35:27.816: INFO: Pod name wrapped-volume-race-6a2dde48-35ba-4538-8cf6-27ce9e07a474: Found 0 pods out of 5
Mar  2 22:35:32.847: INFO: Pod name wrapped-volume-race-6a2dde48-35ba-4538-8cf6-27ce9e07a474: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6a2dde48-35ba-4538-8cf6-27ce9e07a474 in namespace emptydir-wrapper-2867, will wait for the garbage collector to delete the pods
Mar  2 22:35:32.997: INFO: Deleting ReplicationController wrapped-volume-race-6a2dde48-35ba-4538-8cf6-27ce9e07a474 took: 23.393752ms
Mar  2 22:35:33.197: INFO: Terminating ReplicationController wrapped-volume-race-6a2dde48-35ba-4538-8cf6-27ce9e07a474 pods took: 200.431883ms
STEP: Creating RC which spawns configmap-volume pods
Mar  2 22:35:47.469: INFO: Pod name wrapped-volume-race-8af14365-ffda-4548-9931-37834ac6129b: Found 0 pods out of 5
Mar  2 22:35:52.538: INFO: Pod name wrapped-volume-race-8af14365-ffda-4548-9931-37834ac6129b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8af14365-ffda-4548-9931-37834ac6129b in namespace emptydir-wrapper-2867, will wait for the garbage collector to delete the pods
Mar  2 22:35:52.720: INFO: Deleting ReplicationController wrapped-volume-race-8af14365-ffda-4548-9931-37834ac6129b took: 44.57998ms
Mar  2 22:35:52.921: INFO: Terminating ReplicationController wrapped-volume-race-8af14365-ffda-4548-9931-37834ac6129b pods took: 200.418888ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:36:08.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2867" for this suite.

• [SLOW TEST:59.228 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":144,"skipped":2635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:36:08.986: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1354.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1354.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1354.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1354.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1354.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1354.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1354.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1354.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1354.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1354.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1354.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1354.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1354.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 133.143.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.143.133_udp@PTR;check="$$(dig +tcp +noall +answer +search 133.143.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.143.133_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1354.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1354.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1354.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1354.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1354.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1354.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1354.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1354.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1354.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1354.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1354.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1354.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1354.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 133.143.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.143.133_udp@PTR;check="$$(dig +tcp +noall +answer +search 133.143.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.143.133_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:36:13.509: INFO: Unable to read wheezy_udp@dns-test-service.dns-1354.svc.cluster.local from pod dns-1354/dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda: the server could not find the requested resource (get pods dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda)
Mar  2 22:36:13.526: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1354.svc.cluster.local from pod dns-1354/dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda: the server could not find the requested resource (get pods dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda)
Mar  2 22:36:13.546: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1354.svc.cluster.local from pod dns-1354/dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda: the server could not find the requested resource (get pods dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda)
Mar  2 22:36:13.690: INFO: Unable to read jessie_udp@dns-test-service.dns-1354.svc.cluster.local from pod dns-1354/dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda: the server could not find the requested resource (get pods dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda)
Mar  2 22:36:13.706: INFO: Unable to read jessie_tcp@dns-test-service.dns-1354.svc.cluster.local from pod dns-1354/dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda: the server could not find the requested resource (get pods dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda)
Mar  2 22:36:13.723: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1354.svc.cluster.local from pod dns-1354/dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda: the server could not find the requested resource (get pods dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda)
Mar  2 22:36:13.896: INFO: Lookups using dns-1354/dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda failed for: [wheezy_udp@dns-test-service.dns-1354.svc.cluster.local wheezy_tcp@dns-test-service.dns-1354.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1354.svc.cluster.local jessie_udp@dns-test-service.dns-1354.svc.cluster.local jessie_tcp@dns-test-service.dns-1354.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1354.svc.cluster.local]

Mar  2 22:36:19.441: INFO: DNS probes using dns-1354/dns-test-55e78080-28bc-48c2-9fa6-b8074a787fda succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:36:19.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1354" for this suite.

• [SLOW TEST:10.763 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":145,"skipped":2693,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:36:19.753: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 22:36:19.979: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 22:36:20.140: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 22:36:20.212: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.150 before test
Mar  2 22:36:20.289: INFO: calico-kube-controllers-64bc47d78c-r9h75 from calico-system started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 22:36:20.290: INFO: calico-node-sfwd8 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 22:36:20.290: INFO: calico-typha-747778ff7-7mfzl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 22:36:20.290: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-03-02 20:09:15 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  2 22:36:20.290: INFO: managed-storage-validation-webhooks-7d645c9954-9tz2t from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 22:36:20.290: INFO: managed-storage-validation-webhooks-7d645c9954-fnvqt from ibm-odf-validation-webhook started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 22:36:20.290: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-xx9p2 from ibm-system started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 22:36:20.290: INFO: ibm-file-plugin-58c5ccc6d4-mhk8g from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 22:36:20.290: INFO: ibm-keepalived-watcher-lzm26 from kube-system started at 2022-03-02 20:04:31 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 22:36:20.290: INFO: ibm-master-proxy-static-10.138.244.150 from kube-system started at 2022-03-02 20:04:27 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 22:36:20.290: INFO: 	Container pause ready: true, restart count 0
Mar  2 22:36:20.290: INFO: ibmcloud-block-storage-driver-n9lmx from kube-system started at 2022-03-02 20:04:35 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 22:36:20.290: INFO: ibmcloud-block-storage-plugin-67c5f49db6-wlj2j from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 22:36:20.290: INFO: vpn-7bf7499b46-dvdqm from kube-system started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container vpn ready: true, restart count 0
Mar  2 22:36:20.290: INFO: tuned-fqxdn from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container tuned ready: true, restart count 0
Mar  2 22:36:20.290: INFO: cluster-samples-operator-69fbcc775-4zjvg from openshift-cluster-samples-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 22:36:20.290: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 22:36:20.290: INFO: cluster-storage-operator-56787c5bcb-9f5hs from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 22:36:20.290: INFO: csi-snapshot-controller-69454d4b56-gwpsw from openshift-cluster-storage-operator started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 22:36:20.290: INFO: csi-snapshot-controller-operator-6977df7ddd-dkdjh from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 22:36:20.290: INFO: csi-snapshot-webhook-788b7d55dd-2rmtk from openshift-cluster-storage-operator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container webhook ready: true, restart count 0
Mar  2 22:36:20.290: INFO: console-78b4c56c55-ndmlz from openshift-console started at 2022-03-02 20:13:37 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container console ready: true, restart count 0
Mar  2 22:36:20.290: INFO: downloads-79b8c4c9f8-hqr26 from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container download-server ready: true, restart count 0
Mar  2 22:36:20.290: INFO: dns-default-jf6bp from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container dns ready: true, restart count 0
Mar  2 22:36:20.290: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 22:36:20.290: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.290: INFO: cluster-image-registry-operator-58888444b7-jsm7m from openshift-image-registry started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.290: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 22:36:20.291: INFO: image-registry-78c5f597d5-rfdtq from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container registry ready: true, restart count 0
Mar  2 22:36:20.291: INFO: node-ca-2d6rp from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 22:36:20.291: INFO: ingress-canary-wptt6 from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 22:36:20.291: INFO: router-default-678545d6db-sgrbg from openshift-ingress started at 2022-03-02 22:12:55 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container router ready: true, restart count 0
Mar  2 22:36:20.291: INFO: openshift-kube-proxy-9mnl6 from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: kube-storage-version-migrator-operator-68756f7898-kvfhq from openshift-kube-storage-version-migrator-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 22:36:20.291: INFO: migrator-76bc956454-5h92l from openshift-kube-storage-version-migrator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container migrator ready: true, restart count 0
Mar  2 22:36:20.291: INFO: certified-operators-4lfsq from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 22:36:20.291: INFO: community-operators-fj9gp from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 22:36:20.291: INFO: marketplace-operator-8689886944-xlcrw from openshift-marketplace started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 22:36:20.291: INFO: redhat-marketplace-tvwc6 from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 22:36:20.291: INFO: redhat-operators-87vss from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 22:36:20.291: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-03-02 22:30:17 +0000 UTC (5 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: cluster-monitoring-operator-668f4b779-pt5v7 from openshift-monitoring started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container cluster-monitoring-operator ready: true, restart count 1
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Mar  2 22:36:20.291: INFO: grafana-77dc549b6f-nn2b2 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container grafana ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: kube-state-metrics-7b6c6d96b-scdqx from openshift-monitoring started at 2022-03-02 22:30:09 +0000 UTC (3 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 22:36:20.291: INFO: node-exporter-dff24 from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 22:36:20.291: INFO: openshift-state-metrics-84c4bdd485-5d95q from openshift-monitoring started at 2022-03-02 22:30:10 +0000 UTC (3 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 22:36:20.291: INFO: prometheus-adapter-68858877cc-d2bhz from openshift-monitoring started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 22:36:20.291: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-03-02 22:30:27 +0000 UTC (7 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 22:36:20.291: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 22:36:20.291: INFO: prometheus-operator-7797d58ccd-8kdw9 from openshift-monitoring started at 2022-03-02 22:30:10 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 22:36:20.291: INFO: thanos-querier-544bf8c477-jdvqv from openshift-monitoring started at 2022-03-02 20:07:50 +0000 UTC (5 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 22:36:20.291: INFO: multus-admission-controller-2wfsf from openshift-multus started at 2022-03-02 20:06:01 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 22:36:20.291: INFO: multus-zgnwz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 22:36:20.291: INFO: network-metrics-daemon-qmb7k from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.291: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 22:36:20.291: INFO: network-check-source-758fcf9d76-dnnrz from openshift-network-diagnostics started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 22:36:20.291: INFO: network-check-target-rmfwb from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.291: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 22:36:20.292: INFO: network-operator-57496bd6cc-wt8z2 from openshift-network-operator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.292: INFO: 	Container network-operator ready: true, restart count 0
Mar  2 22:36:20.292: INFO: catalog-operator-777865977d-2fqfw from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.292: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 22:36:20.292: INFO: olm-operator-5cb6cff486-wld6q from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.292: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 22:36:20.292: INFO: packageserver-67967c9875-524hv from openshift-operator-lifecycle-manager started at 2022-03-02 20:07:33 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.292: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 22:36:20.292: INFO: packageserver-67967c9875-jm8b8 from openshift-operator-lifecycle-manager started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.292: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 22:36:20.292: INFO: service-ca-operator-5b668f5895-69p65 from openshift-service-ca-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.292: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 22:36:20.292: INFO: service-ca-b874796d6-4g8hj from openshift-service-ca started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.292: INFO: 	Container service-ca-controller ready: false, restart count 0
Mar  2 22:36:20.292: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-pl9fl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 22:36:20.292: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 22:36:20.292: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.159 before test
Mar  2 22:36:20.364: INFO: calico-node-fhc29 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.364: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 22:36:20.365: INFO: calico-typha-747778ff7-sgxvl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.365: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 22:36:20.365: INFO: ibm-keepalived-watcher-drf4g from kube-system started at 2022-03-02 20:04:20 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.365: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 22:36:20.365: INFO: ibm-master-proxy-static-10.138.244.159 from kube-system started at 2022-03-02 20:04:17 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.365: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 22:36:20.365: INFO: 	Container pause ready: true, restart count 0
Mar  2 22:36:20.365: INFO: ibmcloud-block-storage-driver-85npp from kube-system started at 2022-03-02 20:04:25 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.366: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 22:36:20.366: INFO: tuned-xv4r5 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.366: INFO: 	Container tuned ready: true, restart count 0
Mar  2 22:36:20.366: INFO: dns-default-224cd from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 22:36:20.366: INFO: 	Container dns ready: true, restart count 0
Mar  2 22:36:20.366: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 22:36:20.366: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.366: INFO: node-ca-bc4lw from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.367: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 22:36:20.367: INFO: registry-pvc-permissions-kstcw from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.367: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 22:36:20.367: INFO: ingress-canary-gg9t2 from openshift-ingress-canary started at 2022-03-02 22:30:37 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.367: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 22:36:20.367: INFO: openshift-kube-proxy-lxhnl from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.367: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 22:36:20.367: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.368: INFO: node-exporter-t8msj from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.368: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.368: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 22:36:20.368: INFO: multus-58kxw from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.368: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 22:36:20.368: INFO: multus-admission-controller-42x25 from openshift-multus started at 2022-03-02 22:30:47 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.368: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.368: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 22:36:20.369: INFO: network-metrics-daemon-4hdhq from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.369: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.369: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 22:36:20.369: INFO: network-check-target-c4pgw from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.369: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 22:36:20.369: INFO: sonobuoy from sonobuoy started at 2022-03-02 21:45:05 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.369: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 22:36:20.369: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-xczvl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.370: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 22:36:20.370: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 22:36:20.370: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.162 before test
Mar  2 22:36:20.427: INFO: calico-node-8mbgb from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 22:36:20.427: INFO: calico-typha-747778ff7-cxk8p from calico-system started at 2022-03-02 20:05:22 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 22:36:20.427: INFO: managed-storage-validation-webhooks-7d645c9954-hxlwh from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 22:36:20.427: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-zj24d from ibm-system started at 2022-03-02 22:12:55 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 22:36:20.427: INFO: ibm-keepalived-watcher-g4r68 from kube-system started at 2022-03-02 20:04:32 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 22:36:20.427: INFO: ibm-master-proxy-static-10.138.244.162 from kube-system started at 2022-03-02 20:04:29 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 22:36:20.427: INFO: 	Container pause ready: true, restart count 0
Mar  2 22:36:20.427: INFO: ibm-storage-metrics-agent-5686f759d8-6tq4n from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 22:36:20.427: INFO: ibm-storage-watcher-6488446f7b-nmfdr from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 22:36:20.427: INFO: ibmcloud-block-storage-driver-bz98p from kube-system started at 2022-03-02 20:04:36 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 22:36:20.427: INFO: cluster-node-tuning-operator-5cb4c7f989-xchxd from openshift-cluster-node-tuning-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 22:36:20.427: INFO: tuned-krv52 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container tuned ready: true, restart count 0
Mar  2 22:36:20.427: INFO: console-operator-58ff8dbf6b-bz5q9 from openshift-console-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 22:36:20.427: INFO: console-78b4c56c55-kxx8b from openshift-console started at 2022-03-02 20:13:09 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.427: INFO: 	Container console ready: true, restart count 0
Mar  2 22:36:20.427: INFO: downloads-79b8c4c9f8-djqwn from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container download-server ready: true, restart count 0
Mar  2 22:36:20.428: INFO: dns-operator-55f6c97874-bfkv2 from openshift-dns-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: dns-default-p6v58 from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container dns ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: node-ca-bgqw9 from openshift-image-registry started at 2022-03-02 20:07:31 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 22:36:20.428: INFO: ingress-canary-wsb4b from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 22:36:20.428: INFO: ingress-operator-bc8cf4dbb-pqp88 from openshift-ingress-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: router-default-678545d6db-zh9tr from openshift-ingress started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container router ready: true, restart count 0
Mar  2 22:36:20.428: INFO: openshift-kube-proxy-ks9sn from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: node-exporter-gtb4f from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 22:36:20.428: INFO: prometheus-adapter-68858877cc-cxjl2 from openshift-monitoring started at 2022-03-02 20:09:30 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 22:36:20.428: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-03-02 20:14:02 +0000 UTC (7 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 22:36:20.428: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 22:36:20.428: INFO: telemeter-client-765c47dd74-4s7d4 from openshift-monitoring started at 2022-03-02 20:06:40 +0000 UTC (3 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container reload ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 22:36:20.428: INFO: thanos-querier-544bf8c477-hzf5x from openshift-monitoring started at 2022-03-02 22:12:55 +0000 UTC (5 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 22:36:20.428: INFO: multus-admission-controller-ljkq4 from openshift-multus started at 2022-03-02 20:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 22:36:20.428: INFO: multus-bkzqz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 22:36:20.428: INFO: network-metrics-daemon-2bsdl from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 22:36:20.428: INFO: network-check-target-t499h from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 22:36:20.428: INFO: metrics-78d8646588-dvms5 from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container metrics ready: true, restart count 2
Mar  2 22:36:20.428: INFO: push-gateway-7b4b958bfb-dmwzc from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 22:36:20.428: INFO: sonobuoy-e2e-job-87017147060647d2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container e2e ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 22:36:20.428: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-szbl2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 22:36:20.428: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 22:36:20.428: INFO: tigera-operator-597f8644c9-8tp9s from tigera-operator started at 2022-03-02 22:12:54 +0000 UTC (1 container statuses recorded)
Mar  2 22:36:20.428: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node 10.138.244.150
STEP: verifying the node has the label node 10.138.244.159
STEP: verifying the node has the label node 10.138.244.162
Mar  2 22:36:20.892: INFO: Pod calico-kube-controllers-64bc47d78c-r9h75 requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.892: INFO: Pod calico-node-8mbgb requesting resource cpu=250m on Node 10.138.244.162
Mar  2 22:36:20.892: INFO: Pod calico-node-fhc29 requesting resource cpu=250m on Node 10.138.244.159
Mar  2 22:36:20.892: INFO: Pod calico-node-sfwd8 requesting resource cpu=250m on Node 10.138.244.150
Mar  2 22:36:20.892: INFO: Pod calico-typha-747778ff7-7mfzl requesting resource cpu=250m on Node 10.138.244.150
Mar  2 22:36:20.892: INFO: Pod calico-typha-747778ff7-cxk8p requesting resource cpu=250m on Node 10.138.244.162
Mar  2 22:36:20.892: INFO: Pod calico-typha-747778ff7-sgxvl requesting resource cpu=250m on Node 10.138.244.159
Mar  2 22:36:20.892: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.138.244.150
Mar  2 22:36:20.892: INFO: Pod managed-storage-validation-webhooks-7d645c9954-9tz2t requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.892: INFO: Pod managed-storage-validation-webhooks-7d645c9954-fnvqt requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.892: INFO: Pod managed-storage-validation-webhooks-7d645c9954-hxlwh requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.892: INFO: Pod ibm-cloud-provider-ip-168-1-52-154-569658dd7b-xx9p2 requesting resource cpu=5m on Node 10.138.244.150
Mar  2 22:36:20.892: INFO: Pod ibm-cloud-provider-ip-168-1-52-154-569658dd7b-zj24d requesting resource cpu=5m on Node 10.138.244.162
Mar  2 22:36:20.892: INFO: Pod ibm-file-plugin-58c5ccc6d4-mhk8g requesting resource cpu=50m on Node 10.138.244.150
Mar  2 22:36:20.892: INFO: Pod ibm-keepalived-watcher-drf4g requesting resource cpu=5m on Node 10.138.244.159
Mar  2 22:36:20.892: INFO: Pod ibm-keepalived-watcher-g4r68 requesting resource cpu=5m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod ibm-keepalived-watcher-lzm26 requesting resource cpu=5m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod ibm-master-proxy-static-10.138.244.150 requesting resource cpu=25m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod ibm-master-proxy-static-10.138.244.159 requesting resource cpu=25m on Node 10.138.244.159
Mar  2 22:36:20.893: INFO: Pod ibm-master-proxy-static-10.138.244.162 requesting resource cpu=25m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod ibm-storage-metrics-agent-5686f759d8-6tq4n requesting resource cpu=50m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod ibm-storage-watcher-6488446f7b-nmfdr requesting resource cpu=50m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod ibmcloud-block-storage-driver-85npp requesting resource cpu=50m on Node 10.138.244.159
Mar  2 22:36:20.893: INFO: Pod ibmcloud-block-storage-driver-bz98p requesting resource cpu=50m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod ibmcloud-block-storage-driver-n9lmx requesting resource cpu=50m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod ibmcloud-block-storage-plugin-67c5f49db6-wlj2j requesting resource cpu=50m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod vpn-7bf7499b46-dvdqm requesting resource cpu=5m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod cluster-node-tuning-operator-5cb4c7f989-xchxd requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod tuned-fqxdn requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod tuned-krv52 requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod tuned-xv4r5 requesting resource cpu=10m on Node 10.138.244.159
Mar  2 22:36:20.893: INFO: Pod cluster-samples-operator-69fbcc775-4zjvg requesting resource cpu=20m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod cluster-storage-operator-56787c5bcb-9f5hs requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod csi-snapshot-controller-69454d4b56-gwpsw requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod csi-snapshot-controller-operator-6977df7ddd-dkdjh requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod csi-snapshot-webhook-788b7d55dd-2rmtk requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.893: INFO: Pod console-operator-58ff8dbf6b-bz5q9 requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod console-78b4c56c55-kxx8b requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.893: INFO: Pod console-78b4c56c55-ndmlz requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod downloads-79b8c4c9f8-djqwn requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod downloads-79b8c4c9f8-hqr26 requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod dns-operator-55f6c97874-bfkv2 requesting resource cpu=20m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod dns-default-224cd requesting resource cpu=65m on Node 10.138.244.159
Mar  2 22:36:20.894: INFO: Pod dns-default-jf6bp requesting resource cpu=65m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod dns-default-p6v58 requesting resource cpu=65m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod cluster-image-registry-operator-58888444b7-jsm7m requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod image-registry-78c5f597d5-rfdtq requesting resource cpu=100m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod node-ca-2d6rp requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod node-ca-bc4lw requesting resource cpu=10m on Node 10.138.244.159
Mar  2 22:36:20.894: INFO: Pod node-ca-bgqw9 requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod ingress-canary-gg9t2 requesting resource cpu=10m on Node 10.138.244.159
Mar  2 22:36:20.894: INFO: Pod ingress-canary-wptt6 requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod ingress-canary-wsb4b requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod ingress-operator-bc8cf4dbb-pqp88 requesting resource cpu=20m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod router-default-678545d6db-sgrbg requesting resource cpu=100m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod router-default-678545d6db-zh9tr requesting resource cpu=100m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod openshift-kube-proxy-9mnl6 requesting resource cpu=110m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod openshift-kube-proxy-ks9sn requesting resource cpu=110m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod openshift-kube-proxy-lxhnl requesting resource cpu=110m on Node 10.138.244.159
Mar  2 22:36:20.894: INFO: Pod kube-storage-version-migrator-operator-68756f7898-kvfhq requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod migrator-76bc956454-5h92l requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod certified-operators-4lfsq requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod community-operators-fj9gp requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod marketplace-operator-8689886944-xlcrw requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod redhat-marketplace-tvwc6 requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod redhat-operators-87vss requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod cluster-monitoring-operator-668f4b779-pt5v7 requesting resource cpu=11m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod grafana-77dc549b6f-nn2b2 requesting resource cpu=5m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod kube-state-metrics-7b6c6d96b-scdqx requesting resource cpu=4m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod node-exporter-dff24 requesting resource cpu=9m on Node 10.138.244.150
Mar  2 22:36:20.894: INFO: Pod node-exporter-gtb4f requesting resource cpu=9m on Node 10.138.244.162
Mar  2 22:36:20.894: INFO: Pod node-exporter-t8msj requesting resource cpu=9m on Node 10.138.244.159
Mar  2 22:36:20.894: INFO: Pod openshift-state-metrics-84c4bdd485-5d95q requesting resource cpu=3m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod prometheus-adapter-68858877cc-cxjl2 requesting resource cpu=1m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod prometheus-adapter-68858877cc-d2bhz requesting resource cpu=1m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod prometheus-operator-7797d58ccd-8kdw9 requesting resource cpu=6m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod telemeter-client-765c47dd74-4s7d4 requesting resource cpu=3m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod thanos-querier-544bf8c477-hzf5x requesting resource cpu=9m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod thanos-querier-544bf8c477-jdvqv requesting resource cpu=9m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod multus-58kxw requesting resource cpu=10m on Node 10.138.244.159
Mar  2 22:36:20.895: INFO: Pod multus-admission-controller-2wfsf requesting resource cpu=20m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod multus-admission-controller-42x25 requesting resource cpu=20m on Node 10.138.244.159
Mar  2 22:36:20.895: INFO: Pod multus-admission-controller-ljkq4 requesting resource cpu=20m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod multus-bkzqz requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod multus-zgnwz requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod network-metrics-daemon-2bsdl requesting resource cpu=20m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod network-metrics-daemon-4hdhq requesting resource cpu=20m on Node 10.138.244.159
Mar  2 22:36:20.895: INFO: Pod network-metrics-daemon-qmb7k requesting resource cpu=20m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod network-check-source-758fcf9d76-dnnrz requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod network-check-target-c4pgw requesting resource cpu=10m on Node 10.138.244.159
Mar  2 22:36:20.895: INFO: Pod network-check-target-rmfwb requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod network-check-target-t499h requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod network-operator-57496bd6cc-wt8z2 requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod catalog-operator-777865977d-2fqfw requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod olm-operator-5cb6cff486-wld6q requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod packageserver-67967c9875-524hv requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod packageserver-67967c9875-jm8b8 requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod metrics-78d8646588-dvms5 requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod push-gateway-7b4b958bfb-dmwzc requesting resource cpu=10m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod service-ca-operator-5b668f5895-69p65 requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod service-ca-b874796d6-4g8hj requesting resource cpu=10m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.138.244.159
Mar  2 22:36:20.895: INFO: Pod sonobuoy-e2e-job-87017147060647d2 requesting resource cpu=0m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-pl9fl requesting resource cpu=0m on Node 10.138.244.150
Mar  2 22:36:20.895: INFO: Pod sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-szbl2 requesting resource cpu=0m on Node 10.138.244.162
Mar  2 22:36:20.895: INFO: Pod sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-xczvl requesting resource cpu=0m on Node 10.138.244.159
Mar  2 22:36:20.895: INFO: Pod tigera-operator-597f8644c9-8tp9s requesting resource cpu=100m on Node 10.138.244.162
STEP: Starting Pods to consume most of the cluster CPU.
Mar  2 22:36:20.895: INFO: Creating a pod which consumes cpu=1641m on Node 10.138.244.150
Mar  2 22:36:20.940: INFO: Creating a pod which consumes cpu=2139m on Node 10.138.244.159
Mar  2 22:36:21.020: INFO: Creating a pod which consumes cpu=1780m on Node 10.138.244.162
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13afe4e4-d69e-4e1c-948a-7ccb1a48f3ac.16d8b146b630fb4f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7937/filler-pod-13afe4e4-d69e-4e1c-948a-7ccb1a48f3ac to 10.138.244.162]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13afe4e4-d69e-4e1c-948a-7ccb1a48f3ac.16d8b147066a483f], Reason = [AddedInterface], Message = [Add eth0 [172.30.21.154/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13afe4e4-d69e-4e1c-948a-7ccb1a48f3ac.16d8b1470b1fac03], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13afe4e4-d69e-4e1c-948a-7ccb1a48f3ac.16d8b1471ae4e926], Reason = [Created], Message = [Created container filler-pod-13afe4e4-d69e-4e1c-948a-7ccb1a48f3ac]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13afe4e4-d69e-4e1c-948a-7ccb1a48f3ac.16d8b1471ffdbb0b], Reason = [Started], Message = [Started container filler-pod-13afe4e4-d69e-4e1c-948a-7ccb1a48f3ac]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b58680-84cb-4abd-b1e4-cf725b275bdd.16d8b146b26b8bb2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7937/filler-pod-18b58680-84cb-4abd-b1e4-cf725b275bdd to 10.138.244.159]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b58680-84cb-4abd-b1e4-cf725b275bdd.16d8b14702e7f500], Reason = [AddedInterface], Message = [Add eth0 [172.30.228.3/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b58680-84cb-4abd-b1e4-cf725b275bdd.16d8b1470a5295c5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b58680-84cb-4abd-b1e4-cf725b275bdd.16d8b147199a1a4c], Reason = [Created], Message = [Created container filler-pod-18b58680-84cb-4abd-b1e4-cf725b275bdd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b58680-84cb-4abd-b1e4-cf725b275bdd.16d8b1471c5797ff], Reason = [Started], Message = [Started container filler-pod-18b58680-84cb-4abd-b1e4-cf725b275bdd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c059505-a892-463d-864b-c4d2fd93d89e.16d8b146af5c68c2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7937/filler-pod-8c059505-a892-463d-864b-c4d2fd93d89e to 10.138.244.150]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c059505-a892-463d-864b-c4d2fd93d89e.16d8b1470a25ae9d], Reason = [AddedInterface], Message = [Add eth0 [172.30.43.114/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c059505-a892-463d-864b-c4d2fd93d89e.16d8b1470dd4e6dc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c059505-a892-463d-864b-c4d2fd93d89e.16d8b1471f66f6fe], Reason = [Created], Message = [Created container filler-pod-8c059505-a892-463d-864b-c4d2fd93d89e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c059505-a892-463d-864b-c4d2fd93d89e.16d8b14724350fcc], Reason = [Started], Message = [Started container filler-pod-8c059505-a892-463d-864b-c4d2fd93d89e]
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-7937.16d8b1466c54c93a], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16d8b147aaaf3697], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.138.244.150
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.138.244.159
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.138.244.162
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:36:26.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7937" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.759 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":146,"skipped":2704,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:36:26.512: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar  2 22:36:31.557: INFO: Successfully updated pod "annotationupdate1155828a-a95b-4142-92e8-f814b10023f1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:36:33.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2411" for this suite.

• [SLOW TEST:7.155 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:36:33.670: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 22:36:42.259: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 22:36:42.292: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 22:36:44.292: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 22:36:44.324: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 22:36:46.292: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 22:36:46.304: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 22:36:48.292: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 22:36:48.303: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:36:48.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5684" for this suite.

• [SLOW TEST:14.665 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:36:48.338: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-f202706f-3acf-42e5-b055-1e0fdd45ad7a in namespace container-probe-5143
Mar  2 22:36:50.635: INFO: Started pod busybox-f202706f-3acf-42e5-b055-1e0fdd45ad7a in namespace container-probe-5143
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:36:50.646: INFO: Initial restart count of pod busybox-f202706f-3acf-42e5-b055-1e0fdd45ad7a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:40:50.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5143" for this suite.

• [SLOW TEST:242.484 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":149,"skipped":2765,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:40:50.823: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5417
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5417
STEP: creating replication controller externalsvc in namespace services-5417
I0302 22:40:51.242899      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5417, replica count: 2
I0302 22:40:54.293576      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar  2 22:40:54.361: INFO: Creating new exec pod
Mar  2 22:40:58.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5417 exec execpod98klq -- /bin/sh -x -c nslookup nodeport-service.services-5417.svc.cluster.local'
Mar  2 22:40:59.189: INFO: stderr: "+ nslookup nodeport-service.services-5417.svc.cluster.local\n"
Mar  2 22:40:59.189: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-5417.svc.cluster.local\tcanonical name = externalsvc.services-5417.svc.cluster.local.\nName:\texternalsvc.services-5417.svc.cluster.local\nAddress: 172.21.242.204\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5417, will wait for the garbage collector to delete the pods
Mar  2 22:40:59.272: INFO: Deleting ReplicationController externalsvc took: 20.622558ms
Mar  2 22:40:59.372: INFO: Terminating ReplicationController externalsvc pods took: 100.300734ms
Mar  2 22:41:09.271: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:41:09.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5417" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:18.566 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":150,"skipped":2768,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:41:09.389: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:41:10.389: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:41:12.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857670, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857670, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857670, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781857670, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:41:15.515: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:41:15.552: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6550-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:41:16.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2203" for this suite.
STEP: Destroying namespace "webhook-2203-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.041 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":151,"skipped":2768,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:41:17.431: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 22:41:25.907: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:41:25.925: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:41:27.925: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:41:27.976: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:41:29.926: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:41:29.958: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:41:31.925: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:41:31.937: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:41:33.926: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:41:33.942: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:41:35.926: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:41:35.936: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:41:37.926: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:41:37.937: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:41:37.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3730" for this suite.

• [SLOW TEST:20.554 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2779,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:41:37.987: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:41:38.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12c7bcf0-e761-412a-8595-d3b6cb103137" in namespace "downward-api-2910" to be "Succeeded or Failed"
Mar  2 22:41:38.443: INFO: Pod "downwardapi-volume-12c7bcf0-e761-412a-8595-d3b6cb103137": Phase="Pending", Reason="", readiness=false. Elapsed: 19.081579ms
Mar  2 22:41:40.457: INFO: Pod "downwardapi-volume-12c7bcf0-e761-412a-8595-d3b6cb103137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033570857s
Mar  2 22:41:42.471: INFO: Pod "downwardapi-volume-12c7bcf0-e761-412a-8595-d3b6cb103137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047670739s
STEP: Saw pod success
Mar  2 22:41:42.472: INFO: Pod "downwardapi-volume-12c7bcf0-e761-412a-8595-d3b6cb103137" satisfied condition "Succeeded or Failed"
Mar  2 22:41:42.496: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-12c7bcf0-e761-412a-8595-d3b6cb103137 container client-container: <nil>
STEP: delete the pod
Mar  2 22:41:42.595: INFO: Waiting for pod downwardapi-volume-12c7bcf0-e761-412a-8595-d3b6cb103137 to disappear
Mar  2 22:41:42.605: INFO: Pod downwardapi-volume-12c7bcf0-e761-412a-8595-d3b6cb103137 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:41:42.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2910" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2781,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:41:42.664: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:41:43.017: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"77cf3c77-10a3-414f-ad4c-16c3237cfd9b", Controller:(*bool)(0xc001f9ebc2), BlockOwnerDeletion:(*bool)(0xc001f9ebc3)}}
Mar  2 22:41:43.041: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ff9ce8d0-28a5-4252-9ad1-3ab93cecf833", Controller:(*bool)(0xc0011a9bb2), BlockOwnerDeletion:(*bool)(0xc0011a9bb3)}}
Mar  2 22:41:43.059: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2ceeb4a9-3aa9-4c48-b07d-a3c5307de3d6", Controller:(*bool)(0xc001f9ee56), BlockOwnerDeletion:(*bool)(0xc001f9ee57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:41:48.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5387" for this suite.

• [SLOW TEST:5.501 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":154,"skipped":2788,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:41:48.166: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0302 22:41:54.527760      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0302 22:41:54.527797      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0302 22:41:54.527812      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 22:41:54.527: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:41:54.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1574" for this suite.

• [SLOW TEST:6.417 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":155,"skipped":2804,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:41:54.584: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-d8d536e8-1268-4fba-891c-d40d27daac73
STEP: Creating a pod to test consume configMaps
Mar  2 22:41:54.896: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-19276deb-65fe-4934-9969-83d76a80d023" in namespace "projected-492" to be "Succeeded or Failed"
Mar  2 22:41:54.910: INFO: Pod "pod-projected-configmaps-19276deb-65fe-4934-9969-83d76a80d023": Phase="Pending", Reason="", readiness=false. Elapsed: 13.729921ms
Mar  2 22:41:56.921: INFO: Pod "pod-projected-configmaps-19276deb-65fe-4934-9969-83d76a80d023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024856753s
Mar  2 22:41:58.934: INFO: Pod "pod-projected-configmaps-19276deb-65fe-4934-9969-83d76a80d023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037919791s
STEP: Saw pod success
Mar  2 22:41:58.935: INFO: Pod "pod-projected-configmaps-19276deb-65fe-4934-9969-83d76a80d023" satisfied condition "Succeeded or Failed"
Mar  2 22:41:58.947: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-configmaps-19276deb-65fe-4934-9969-83d76a80d023 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:41:59.020: INFO: Waiting for pod pod-projected-configmaps-19276deb-65fe-4934-9969-83d76a80d023 to disappear
Mar  2 22:41:59.056: INFO: Pod pod-projected-configmaps-19276deb-65fe-4934-9969-83d76a80d023 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:41:59.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-492" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":156,"skipped":2808,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:41:59.099: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-3a19cf0f-19bf-4845-9197-5608aab1db13
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:41:59.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-923" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":157,"skipped":2809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:41:59.351: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:41:59.511: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:42:01.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9457" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:42:01.938: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-7e61a385-341a-4ef6-8737-60a023e76adc
STEP: Creating a pod to test consume configMaps
Mar  2 22:42:02.307: INFO: Waiting up to 5m0s for pod "pod-configmaps-0264c527-cf7b-4659-8fc1-5ac2c0beaa15" in namespace "configmap-3051" to be "Succeeded or Failed"
Mar  2 22:42:02.330: INFO: Pod "pod-configmaps-0264c527-cf7b-4659-8fc1-5ac2c0beaa15": Phase="Pending", Reason="", readiness=false. Elapsed: 22.426288ms
Mar  2 22:42:04.340: INFO: Pod "pod-configmaps-0264c527-cf7b-4659-8fc1-5ac2c0beaa15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03192428s
Mar  2 22:42:06.353: INFO: Pod "pod-configmaps-0264c527-cf7b-4659-8fc1-5ac2c0beaa15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04534188s
STEP: Saw pod success
Mar  2 22:42:06.353: INFO: Pod "pod-configmaps-0264c527-cf7b-4659-8fc1-5ac2c0beaa15" satisfied condition "Succeeded or Failed"
Mar  2 22:42:06.362: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-0264c527-cf7b-4659-8fc1-5ac2c0beaa15 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:42:06.461: INFO: Waiting for pod pod-configmaps-0264c527-cf7b-4659-8fc1-5ac2c0beaa15 to disappear
Mar  2 22:42:06.471: INFO: Pod pod-configmaps-0264c527-cf7b-4659-8fc1-5ac2c0beaa15 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:42:06.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3051" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2887,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:42:06.513: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:42:06.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6ce4c22-f3fa-480b-bc1d-f22f671b107d" in namespace "projected-898" to be "Succeeded or Failed"
Mar  2 22:42:06.856: INFO: Pod "downwardapi-volume-b6ce4c22-f3fa-480b-bc1d-f22f671b107d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.146977ms
Mar  2 22:42:08.867: INFO: Pod "downwardapi-volume-b6ce4c22-f3fa-480b-bc1d-f22f671b107d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020938627s
Mar  2 22:42:10.879: INFO: Pod "downwardapi-volume-b6ce4c22-f3fa-480b-bc1d-f22f671b107d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032971288s
STEP: Saw pod success
Mar  2 22:42:10.880: INFO: Pod "downwardapi-volume-b6ce4c22-f3fa-480b-bc1d-f22f671b107d" satisfied condition "Succeeded or Failed"
Mar  2 22:42:10.972: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-b6ce4c22-f3fa-480b-bc1d-f22f671b107d container client-container: <nil>
STEP: delete the pod
Mar  2 22:42:11.043: INFO: Waiting for pod downwardapi-volume-b6ce4c22-f3fa-480b-bc1d-f22f671b107d to disappear
Mar  2 22:42:11.059: INFO: Pod downwardapi-volume-b6ce4c22-f3fa-480b-bc1d-f22f671b107d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:42:11.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-898" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2892,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:42:11.119: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Mar  2 22:42:11.389: INFO: Waiting up to 5m0s for pod "client-containers-4aafc4af-6648-4a1b-836b-3c3852902e22" in namespace "containers-7123" to be "Succeeded or Failed"
Mar  2 22:42:11.402: INFO: Pod "client-containers-4aafc4af-6648-4a1b-836b-3c3852902e22": Phase="Pending", Reason="", readiness=false. Elapsed: 12.851465ms
Mar  2 22:42:13.413: INFO: Pod "client-containers-4aafc4af-6648-4a1b-836b-3c3852902e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024012085s
Mar  2 22:42:15.425: INFO: Pod "client-containers-4aafc4af-6648-4a1b-836b-3c3852902e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036119s
STEP: Saw pod success
Mar  2 22:42:15.426: INFO: Pod "client-containers-4aafc4af-6648-4a1b-836b-3c3852902e22" satisfied condition "Succeeded or Failed"
Mar  2 22:42:15.439: INFO: Trying to get logs from node 10.138.244.159 pod client-containers-4aafc4af-6648-4a1b-836b-3c3852902e22 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:42:15.536: INFO: Waiting for pod client-containers-4aafc4af-6648-4a1b-836b-3c3852902e22 to disappear
Mar  2 22:42:15.550: INFO: Pod client-containers-4aafc4af-6648-4a1b-836b-3c3852902e22 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:42:15.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7123" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2895,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:42:15.593: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7501.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7501.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7501.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7501.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7501.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7501.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7501.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7501.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7501.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7501.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:42:19.929: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local from pod dns-7501/dns-test-c953d079-49f3-4796-bced-6809e2198d08: the server could not find the requested resource (get pods dns-test-c953d079-49f3-4796-bced-6809e2198d08)
Mar  2 22:42:19.970: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7501.svc.cluster.local from pod dns-7501/dns-test-c953d079-49f3-4796-bced-6809e2198d08: the server could not find the requested resource (get pods dns-test-c953d079-49f3-4796-bced-6809e2198d08)
Mar  2 22:42:20.004: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7501.svc.cluster.local from pod dns-7501/dns-test-c953d079-49f3-4796-bced-6809e2198d08: the server could not find the requested resource (get pods dns-test-c953d079-49f3-4796-bced-6809e2198d08)
Mar  2 22:42:20.075: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local from pod dns-7501/dns-test-c953d079-49f3-4796-bced-6809e2198d08: the server could not find the requested resource (get pods dns-test-c953d079-49f3-4796-bced-6809e2198d08)
Mar  2 22:42:20.119: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7501.svc.cluster.local from pod dns-7501/dns-test-c953d079-49f3-4796-bced-6809e2198d08: the server could not find the requested resource (get pods dns-test-c953d079-49f3-4796-bced-6809e2198d08)
Mar  2 22:42:20.225: INFO: Lookups using dns-7501/dns-test-c953d079-49f3-4796-bced-6809e2198d08 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7501.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7501.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7501.svc.cluster.local jessie_udp@dns-test-service-2.dns-7501.svc.cluster.local]

Mar  2 22:42:25.513: INFO: DNS probes using dns-7501/dns-test-c953d079-49f3-4796-bced-6809e2198d08 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:42:25.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7501" for this suite.

• [SLOW TEST:10.066 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":162,"skipped":2905,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:42:25.664: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar  2 22:42:30.480: INFO: Successfully updated pod "adopt-release-d9pwh"
STEP: Checking that the Job readopts the Pod
Mar  2 22:42:30.481: INFO: Waiting up to 15m0s for pod "adopt-release-d9pwh" in namespace "job-3115" to be "adopted"
Mar  2 22:42:30.511: INFO: Pod "adopt-release-d9pwh": Phase="Running", Reason="", readiness=true. Elapsed: 29.527514ms
Mar  2 22:42:32.525: INFO: Pod "adopt-release-d9pwh": Phase="Running", Reason="", readiness=true. Elapsed: 2.043498055s
Mar  2 22:42:32.525: INFO: Pod "adopt-release-d9pwh" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar  2 22:42:33.082: INFO: Successfully updated pod "adopt-release-d9pwh"
STEP: Checking that the Job releases the Pod
Mar  2 22:42:33.082: INFO: Waiting up to 15m0s for pod "adopt-release-d9pwh" in namespace "job-3115" to be "released"
Mar  2 22:42:33.092: INFO: Pod "adopt-release-d9pwh": Phase="Running", Reason="", readiness=true. Elapsed: 9.77665ms
Mar  2 22:42:35.109: INFO: Pod "adopt-release-d9pwh": Phase="Running", Reason="", readiness=true. Elapsed: 2.027295847s
Mar  2 22:42:35.109: INFO: Pod "adopt-release-d9pwh" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:42:35.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3115" for this suite.

• [SLOW TEST:9.481 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":163,"skipped":2916,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:42:35.157: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  2 22:42:35.333: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:42:47.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1459" for this suite.

• [SLOW TEST:11.902 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":2961,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:42:47.060: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-55dd15f2-2c38-43ee-8aef-7af25e540885
STEP: Creating a pod to test consume secrets
Mar  2 22:42:47.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3528d38a-ffa1-4588-8fb4-24335bd45f03" in namespace "projected-4729" to be "Succeeded or Failed"
Mar  2 22:42:47.491: INFO: Pod "pod-projected-secrets-3528d38a-ffa1-4588-8fb4-24335bd45f03": Phase="Pending", Reason="", readiness=false. Elapsed: 9.538207ms
Mar  2 22:42:49.507: INFO: Pod "pod-projected-secrets-3528d38a-ffa1-4588-8fb4-24335bd45f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02543502s
Mar  2 22:42:51.519: INFO: Pod "pod-projected-secrets-3528d38a-ffa1-4588-8fb4-24335bd45f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037456332s
STEP: Saw pod success
Mar  2 22:42:51.519: INFO: Pod "pod-projected-secrets-3528d38a-ffa1-4588-8fb4-24335bd45f03" satisfied condition "Succeeded or Failed"
Mar  2 22:42:51.530: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-secrets-3528d38a-ffa1-4588-8fb4-24335bd45f03 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:42:51.618: INFO: Waiting for pod pod-projected-secrets-3528d38a-ffa1-4588-8fb4-24335bd45f03 to disappear
Mar  2 22:42:51.685: INFO: Pod pod-projected-secrets-3528d38a-ffa1-4588-8fb4-24335bd45f03 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:42:51.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4729" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":2967,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:42:51.727: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8793.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8793.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8793.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8793.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:42:56.046: INFO: DNS probes using dns-test-8f3bf687-a548-48b7-8395-8766c926d3e4 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8793.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8793.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8793.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8793.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:43:00.290: INFO: DNS probes using dns-test-21762b53-9180-419c-a639-b61f34e7f545 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8793.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8793.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8793.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8793.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:43:04.523: INFO: DNS probes using dns-test-babddaae-ba33-44f6-857a-ac8086a0feb6 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:43:04.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8793" for this suite.

• [SLOW TEST:12.920 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":166,"skipped":2977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:43:04.652: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-ea75635a-c43d-495e-82ef-cf7864367ba6
STEP: Creating a pod to test consume configMaps
Mar  2 22:43:04.907: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7f43e94-8536-481e-bccf-69cb2fe6bd8d" in namespace "configmap-5882" to be "Succeeded or Failed"
Mar  2 22:43:04.916: INFO: Pod "pod-configmaps-e7f43e94-8536-481e-bccf-69cb2fe6bd8d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.738576ms
Mar  2 22:43:06.929: INFO: Pod "pod-configmaps-e7f43e94-8536-481e-bccf-69cb2fe6bd8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021908682s
Mar  2 22:43:08.944: INFO: Pod "pod-configmaps-e7f43e94-8536-481e-bccf-69cb2fe6bd8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036582999s
STEP: Saw pod success
Mar  2 22:43:08.944: INFO: Pod "pod-configmaps-e7f43e94-8536-481e-bccf-69cb2fe6bd8d" satisfied condition "Succeeded or Failed"
Mar  2 22:43:08.955: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-e7f43e94-8536-481e-bccf-69cb2fe6bd8d container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:43:09.011: INFO: Waiting for pod pod-configmaps-e7f43e94-8536-481e-bccf-69cb2fe6bd8d to disappear
Mar  2 22:43:09.026: INFO: Pod pod-configmaps-e7f43e94-8536-481e-bccf-69cb2fe6bd8d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:43:09.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5882" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":167,"skipped":3005,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:43:09.078: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:43:09.310: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e" in namespace "security-context-test-7624" to be "Succeeded or Failed"
Mar  2 22:43:09.327: INFO: Pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.355331ms
Mar  2 22:43:11.337: INFO: Pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027332573s
Mar  2 22:43:13.349: INFO: Pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039432926s
Mar  2 22:43:15.363: INFO: Pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053079726s
Mar  2 22:43:17.395: INFO: Pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084830457s
Mar  2 22:43:19.406: INFO: Pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096113197s
Mar  2 22:43:21.418: INFO: Pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.107837737s
Mar  2 22:43:21.418: INFO: Pod "alpine-nnp-false-fcea3644-954f-4034-abc4-412ce41e925e" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:43:21.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7624" for this suite.

• [SLOW TEST:12.410 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":3017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:43:21.490: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:43:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1770" for this suite.

• [SLOW TEST:17.428 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":169,"skipped":3070,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:43:38.918: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-371/configmap-test-594df773-872b-40d6-b730-e0ee4a152ae4
STEP: Creating a pod to test consume configMaps
Mar  2 22:43:39.180: INFO: Waiting up to 5m0s for pod "pod-configmaps-eecbaec9-e76b-4121-a1d9-28ea1d290f64" in namespace "configmap-371" to be "Succeeded or Failed"
Mar  2 22:43:39.208: INFO: Pod "pod-configmaps-eecbaec9-e76b-4121-a1d9-28ea1d290f64": Phase="Pending", Reason="", readiness=false. Elapsed: 28.330127ms
Mar  2 22:43:41.225: INFO: Pod "pod-configmaps-eecbaec9-e76b-4121-a1d9-28ea1d290f64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04500033s
Mar  2 22:43:43.243: INFO: Pod "pod-configmaps-eecbaec9-e76b-4121-a1d9-28ea1d290f64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063348976s
STEP: Saw pod success
Mar  2 22:43:43.243: INFO: Pod "pod-configmaps-eecbaec9-e76b-4121-a1d9-28ea1d290f64" satisfied condition "Succeeded or Failed"
Mar  2 22:43:43.253: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-eecbaec9-e76b-4121-a1d9-28ea1d290f64 container env-test: <nil>
STEP: delete the pod
Mar  2 22:43:43.320: INFO: Waiting for pod pod-configmaps-eecbaec9-e76b-4121-a1d9-28ea1d290f64 to disappear
Mar  2 22:43:43.334: INFO: Pod pod-configmaps-eecbaec9-e76b-4121-a1d9-28ea1d290f64 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:43:43.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-371" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":3078,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:43:43.367: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e5a1aa4f-d689-43c1-a1b3-569c1c4c1559
STEP: Creating a pod to test consume secrets
Mar  2 22:43:44.672: INFO: Waiting up to 5m0s for pod "pod-secrets-531099d9-2d55-4629-8881-97905383a580" in namespace "secrets-8976" to be "Succeeded or Failed"
Mar  2 22:43:44.694: INFO: Pod "pod-secrets-531099d9-2d55-4629-8881-97905383a580": Phase="Pending", Reason="", readiness=false. Elapsed: 21.549687ms
Mar  2 22:43:46.706: INFO: Pod "pod-secrets-531099d9-2d55-4629-8881-97905383a580": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034387307s
STEP: Saw pod success
Mar  2 22:43:46.707: INFO: Pod "pod-secrets-531099d9-2d55-4629-8881-97905383a580" satisfied condition "Succeeded or Failed"
Mar  2 22:43:46.716: INFO: Trying to get logs from node 10.138.244.159 pod pod-secrets-531099d9-2d55-4629-8881-97905383a580 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:43:46.808: INFO: Waiting for pod pod-secrets-531099d9-2d55-4629-8881-97905383a580 to disappear
Mar  2 22:43:46.818: INFO: Pod pod-secrets-531099d9-2d55-4629-8881-97905383a580 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:43:46.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8976" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":3087,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:43:46.861: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  2 22:43:55.311: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:43:55.326: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 22:43:57.326: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:43:57.337: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 22:43:59.326: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:43:59.348: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 22:44:01.326: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:44:01.337: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:44:01.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4874" for this suite.

• [SLOW TEST:14.537 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":3092,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:44:01.399: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Mar  2 22:44:01.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 create -f -'
Mar  2 22:44:02.484: INFO: stderr: ""
Mar  2 22:44:02.485: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 22:44:02.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 22:44:02.705: INFO: stderr: ""
Mar  2 22:44:02.705: INFO: stdout: "update-demo-nautilus-7jpld update-demo-nautilus-pv29p "
Mar  2 22:44:02.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods update-demo-nautilus-7jpld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 22:44:02.921: INFO: stderr: ""
Mar  2 22:44:02.921: INFO: stdout: ""
Mar  2 22:44:02.921: INFO: update-demo-nautilus-7jpld is created but not running
Mar  2 22:44:07.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 22:44:08.108: INFO: stderr: ""
Mar  2 22:44:08.108: INFO: stdout: "update-demo-nautilus-7jpld update-demo-nautilus-pv29p "
Mar  2 22:44:08.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods update-demo-nautilus-7jpld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 22:44:08.273: INFO: stderr: ""
Mar  2 22:44:08.273: INFO: stdout: ""
Mar  2 22:44:08.273: INFO: update-demo-nautilus-7jpld is created but not running
Mar  2 22:44:13.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 22:44:13.487: INFO: stderr: ""
Mar  2 22:44:13.487: INFO: stdout: "update-demo-nautilus-7jpld update-demo-nautilus-pv29p "
Mar  2 22:44:13.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods update-demo-nautilus-7jpld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 22:44:13.651: INFO: stderr: ""
Mar  2 22:44:13.651: INFO: stdout: "true"
Mar  2 22:44:13.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods update-demo-nautilus-7jpld -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 22:44:13.819: INFO: stderr: ""
Mar  2 22:44:13.819: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 22:44:13.819: INFO: validating pod update-demo-nautilus-7jpld
Mar  2 22:44:13.883: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 22:44:13.883: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 22:44:13.883: INFO: update-demo-nautilus-7jpld is verified up and running
Mar  2 22:44:13.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods update-demo-nautilus-pv29p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 22:44:14.030: INFO: stderr: ""
Mar  2 22:44:14.030: INFO: stdout: "true"
Mar  2 22:44:14.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods update-demo-nautilus-pv29p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 22:44:14.190: INFO: stderr: ""
Mar  2 22:44:14.190: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 22:44:14.190: INFO: validating pod update-demo-nautilus-pv29p
Mar  2 22:44:14.248: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 22:44:14.248: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 22:44:14.248: INFO: update-demo-nautilus-pv29p is verified up and running
STEP: using delete to clean up resources
Mar  2 22:44:14.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 delete --grace-period=0 --force -f -'
Mar  2 22:44:14.598: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:44:14.598: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 22:44:14.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get rc,svc -l name=update-demo --no-headers'
Mar  2 22:44:14.811: INFO: stderr: "No resources found in kubectl-8652 namespace.\n"
Mar  2 22:44:14.811: INFO: stdout: ""
Mar  2 22:44:14.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-8652 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 22:44:14.947: INFO: stderr: ""
Mar  2 22:44:14.947: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:44:14.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8652" for this suite.

• [SLOW TEST:13.611 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":173,"skipped":3092,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:44:15.010: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-59sgr in namespace proxy-2543
I0302 22:44:15.424774      25 runners.go:190] Created replication controller with name: proxy-service-59sgr, namespace: proxy-2543, replica count: 1
I0302 22:44:16.475462      25 runners.go:190] proxy-service-59sgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:44:17.475678      25 runners.go:190] proxy-service-59sgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:44:18.476183      25 runners.go:190] proxy-service-59sgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 22:44:19.477643      25 runners.go:190] proxy-service-59sgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 22:44:20.478724      25 runners.go:190] proxy-service-59sgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 22:44:21.479017      25 runners.go:190] proxy-service-59sgr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:44:21.503: INFO: setup took 6.170135164s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  2 22:44:21.535: INFO: (0) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 31.203337ms)
Mar  2 22:44:21.536: INFO: (0) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 32.705971ms)
Mar  2 22:44:21.537: INFO: (0) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 32.921425ms)
Mar  2 22:44:21.537: INFO: (0) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 33.434701ms)
Mar  2 22:44:21.546: INFO: (0) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 41.948754ms)
Mar  2 22:44:21.550: INFO: (0) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 46.057415ms)
Mar  2 22:44:21.550: INFO: (0) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 46.033678ms)
Mar  2 22:44:21.550: INFO: (0) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 46.278641ms)
Mar  2 22:44:21.555: INFO: (0) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 50.926693ms)
Mar  2 22:44:21.557: INFO: (0) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 53.390473ms)
Mar  2 22:44:21.558: INFO: (0) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 53.843103ms)
Mar  2 22:44:21.558: INFO: (0) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 54.246454ms)
Mar  2 22:44:21.562: INFO: (0) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 57.839367ms)
Mar  2 22:44:21.562: INFO: (0) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 58.194831ms)
Mar  2 22:44:21.597: INFO: (0) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 93.444294ms)
Mar  2 22:44:21.597: INFO: (0) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 93.102267ms)
Mar  2 22:44:21.615: INFO: (1) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 17.906398ms)
Mar  2 22:44:21.617: INFO: (1) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 19.153509ms)
Mar  2 22:44:21.619: INFO: (1) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 21.572222ms)
Mar  2 22:44:21.624: INFO: (1) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 26.160454ms)
Mar  2 22:44:21.625: INFO: (1) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 26.861793ms)
Mar  2 22:44:21.625: INFO: (1) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 27.214397ms)
Mar  2 22:44:21.625: INFO: (1) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 27.106183ms)
Mar  2 22:44:21.625: INFO: (1) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 27.198318ms)
Mar  2 22:44:21.626: INFO: (1) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 27.590524ms)
Mar  2 22:44:21.626: INFO: (1) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 27.993052ms)
Mar  2 22:44:21.627: INFO: (1) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 28.707985ms)
Mar  2 22:44:21.638: INFO: (1) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 39.771413ms)
Mar  2 22:44:21.638: INFO: (1) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 39.81716ms)
Mar  2 22:44:21.639: INFO: (1) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 40.500687ms)
Mar  2 22:44:21.639: INFO: (1) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 40.39511ms)
Mar  2 22:44:21.639: INFO: (1) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 40.470601ms)
Mar  2 22:44:21.657: INFO: (2) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 17.912857ms)
Mar  2 22:44:21.663: INFO: (2) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 24.046053ms)
Mar  2 22:44:21.663: INFO: (2) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 24.374589ms)
Mar  2 22:44:21.663: INFO: (2) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 23.62488ms)
Mar  2 22:44:21.664: INFO: (2) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 23.919578ms)
Mar  2 22:44:21.668: INFO: (2) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 28.245396ms)
Mar  2 22:44:21.669: INFO: (2) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 28.891458ms)
Mar  2 22:44:21.669: INFO: (2) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 29.482355ms)
Mar  2 22:44:21.669: INFO: (2) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 29.293397ms)
Mar  2 22:44:21.669: INFO: (2) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 29.839062ms)
Mar  2 22:44:21.670: INFO: (2) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 31.172066ms)
Mar  2 22:44:21.671: INFO: (2) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 31.077974ms)
Mar  2 22:44:21.671: INFO: (2) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 31.522572ms)
Mar  2 22:44:21.671: INFO: (2) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 32.050574ms)
Mar  2 22:44:21.674: INFO: (2) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 34.005236ms)
Mar  2 22:44:21.675: INFO: (2) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 34.751819ms)
Mar  2 22:44:21.732: INFO: (3) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 57.265945ms)
Mar  2 22:44:21.732: INFO: (3) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 56.999874ms)
Mar  2 22:44:21.732: INFO: (3) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 57.101123ms)
Mar  2 22:44:21.732: INFO: (3) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 56.389227ms)
Mar  2 22:44:21.732: INFO: (3) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 56.662067ms)
Mar  2 22:44:21.735: INFO: (3) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 59.837659ms)
Mar  2 22:44:21.735: INFO: (3) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 59.49487ms)
Mar  2 22:44:21.735: INFO: (3) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 59.566925ms)
Mar  2 22:44:21.736: INFO: (3) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 60.56056ms)
Mar  2 22:44:21.736: INFO: (3) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 60.667379ms)
Mar  2 22:44:21.741: INFO: (3) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 66.548457ms)
Mar  2 22:44:21.742: INFO: (3) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 67.011513ms)
Mar  2 22:44:21.748: INFO: (3) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 72.593732ms)
Mar  2 22:44:21.748: INFO: (3) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 73.007555ms)
Mar  2 22:44:21.748: INFO: (3) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 72.337003ms)
Mar  2 22:44:21.748: INFO: (3) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 73.171962ms)
Mar  2 22:44:21.769: INFO: (4) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 20.389381ms)
Mar  2 22:44:21.799: INFO: (4) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 49.731699ms)
Mar  2 22:44:21.799: INFO: (4) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 48.940136ms)
Mar  2 22:44:21.799: INFO: (4) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 49.216372ms)
Mar  2 22:44:21.799: INFO: (4) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 49.670678ms)
Mar  2 22:44:21.801: INFO: (4) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 48.885103ms)
Mar  2 22:44:21.802: INFO: (4) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 50.03384ms)
Mar  2 22:44:21.802: INFO: (4) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 50.533712ms)
Mar  2 22:44:21.802: INFO: (4) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 51.954311ms)
Mar  2 22:44:21.802: INFO: (4) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 50.893173ms)
Mar  2 22:44:21.805: INFO: (4) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 53.821209ms)
Mar  2 22:44:21.814: INFO: (4) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 63.851812ms)
Mar  2 22:44:21.815: INFO: (4) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 65.767795ms)
Mar  2 22:44:21.815: INFO: (4) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 63.55658ms)
Mar  2 22:44:21.816: INFO: (4) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 63.758267ms)
Mar  2 22:44:21.817: INFO: (4) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 63.937845ms)
Mar  2 22:44:21.832: INFO: (5) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 15.62639ms)
Mar  2 22:44:21.838: INFO: (5) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 20.100467ms)
Mar  2 22:44:21.838: INFO: (5) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 20.581616ms)
Mar  2 22:44:21.838: INFO: (5) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 20.557277ms)
Mar  2 22:44:21.848: INFO: (5) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 30.932822ms)
Mar  2 22:44:21.849: INFO: (5) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 32.611177ms)
Mar  2 22:44:21.850: INFO: (5) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 32.00893ms)
Mar  2 22:44:21.850: INFO: (5) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 32.373914ms)
Mar  2 22:44:21.850: INFO: (5) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 32.876391ms)
Mar  2 22:44:21.851: INFO: (5) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 33.353873ms)
Mar  2 22:44:21.857: INFO: (5) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 39.222467ms)
Mar  2 22:44:21.863: INFO: (5) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 46.000615ms)
Mar  2 22:44:21.863: INFO: (5) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 45.340159ms)
Mar  2 22:44:21.864: INFO: (5) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 45.822877ms)
Mar  2 22:44:21.864: INFO: (5) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 46.006564ms)
Mar  2 22:44:21.864: INFO: (5) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 46.329972ms)
Mar  2 22:44:21.882: INFO: (6) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 17.713714ms)
Mar  2 22:44:21.884: INFO: (6) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 19.136765ms)
Mar  2 22:44:21.885: INFO: (6) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 19.500848ms)
Mar  2 22:44:21.892: INFO: (6) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 26.355744ms)
Mar  2 22:44:21.893: INFO: (6) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 26.784263ms)
Mar  2 22:44:21.893: INFO: (6) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 27.497579ms)
Mar  2 22:44:21.897: INFO: (6) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 31.649455ms)
Mar  2 22:44:21.897: INFO: (6) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 31.450942ms)
Mar  2 22:44:21.897: INFO: (6) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 32.248771ms)
Mar  2 22:44:21.898: INFO: (6) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 31.74196ms)
Mar  2 22:44:21.898: INFO: (6) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 32.815401ms)
Mar  2 22:44:21.901: INFO: (6) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 34.999569ms)
Mar  2 22:44:21.903: INFO: (6) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 38.132825ms)
Mar  2 22:44:21.904: INFO: (6) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 38.506551ms)
Mar  2 22:44:21.910: INFO: (6) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 44.971713ms)
Mar  2 22:44:21.911: INFO: (6) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 45.214585ms)
Mar  2 22:44:21.928: INFO: (7) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 16.972199ms)
Mar  2 22:44:21.931: INFO: (7) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 20.001712ms)
Mar  2 22:44:21.932: INFO: (7) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 19.755101ms)
Mar  2 22:44:21.943: INFO: (7) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 30.941147ms)
Mar  2 22:44:21.943: INFO: (7) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 31.331682ms)
Mar  2 22:44:21.943: INFO: (7) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 31.292628ms)
Mar  2 22:44:21.943: INFO: (7) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 31.434655ms)
Mar  2 22:44:21.943: INFO: (7) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 31.239651ms)
Mar  2 22:44:21.944: INFO: (7) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 32.003774ms)
Mar  2 22:44:21.944: INFO: (7) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 32.112277ms)
Mar  2 22:44:21.944: INFO: (7) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 32.513176ms)
Mar  2 22:44:21.946: INFO: (7) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 34.016464ms)
Mar  2 22:44:21.946: INFO: (7) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 34.126162ms)
Mar  2 22:44:21.947: INFO: (7) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 35.41721ms)
Mar  2 22:44:21.952: INFO: (7) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 39.787654ms)
Mar  2 22:44:21.952: INFO: (7) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 40.306985ms)
Mar  2 22:44:21.970: INFO: (8) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 17.60567ms)
Mar  2 22:44:21.973: INFO: (8) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 18.901707ms)
Mar  2 22:44:21.974: INFO: (8) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 19.453797ms)
Mar  2 22:44:21.976: INFO: (8) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 22.602332ms)
Mar  2 22:44:21.976: INFO: (8) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 22.317749ms)
Mar  2 22:44:21.977: INFO: (8) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 23.489153ms)
Mar  2 22:44:21.978: INFO: (8) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 24.155811ms)
Mar  2 22:44:21.979: INFO: (8) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 26.315091ms)
Mar  2 22:44:21.979: INFO: (8) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 24.708174ms)
Mar  2 22:44:21.979: INFO: (8) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 26.196584ms)
Mar  2 22:44:21.979: INFO: (8) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 22.521757ms)
Mar  2 22:44:21.981: INFO: (8) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 27.228029ms)
Mar  2 22:44:21.982: INFO: (8) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 28.949271ms)
Mar  2 22:44:21.988: INFO: (8) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 34.861719ms)
Mar  2 22:44:21.988: INFO: (8) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 33.779859ms)
Mar  2 22:44:21.988: INFO: (8) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 33.7827ms)
Mar  2 22:44:22.007: INFO: (9) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 18.817878ms)
Mar  2 22:44:22.014: INFO: (9) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 25.254385ms)
Mar  2 22:44:22.018: INFO: (9) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 27.870522ms)
Mar  2 22:44:22.018: INFO: (9) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 29.124315ms)
Mar  2 22:44:22.018: INFO: (9) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 28.644615ms)
Mar  2 22:44:22.020: INFO: (9) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 30.572293ms)
Mar  2 22:44:22.021: INFO: (9) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 31.161711ms)
Mar  2 22:44:22.021: INFO: (9) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 31.783313ms)
Mar  2 22:44:22.021: INFO: (9) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 31.505547ms)
Mar  2 22:44:22.021: INFO: (9) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 31.309072ms)
Mar  2 22:44:22.026: INFO: (9) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 36.583747ms)
Mar  2 22:44:22.026: INFO: (9) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 36.034413ms)
Mar  2 22:44:22.026: INFO: (9) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 36.845682ms)
Mar  2 22:44:22.029: INFO: (9) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 38.955908ms)
Mar  2 22:44:22.031: INFO: (9) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 41.021049ms)
Mar  2 22:44:22.031: INFO: (9) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 41.088906ms)
Mar  2 22:44:22.051: INFO: (10) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 19.257554ms)
Mar  2 22:44:22.061: INFO: (10) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 25.661159ms)
Mar  2 22:44:22.062: INFO: (10) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 29.411548ms)
Mar  2 22:44:22.062: INFO: (10) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 30.113318ms)
Mar  2 22:44:22.062: INFO: (10) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 28.537111ms)
Mar  2 22:44:22.062: INFO: (10) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 29.842588ms)
Mar  2 22:44:22.063: INFO: (10) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 27.781277ms)
Mar  2 22:44:22.062: INFO: (10) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 27.285483ms)
Mar  2 22:44:22.068: INFO: (10) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 31.830295ms)
Mar  2 22:44:22.068: INFO: (10) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 31.759551ms)
Mar  2 22:44:22.069: INFO: (10) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 35.549298ms)
Mar  2 22:44:22.070: INFO: (10) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 36.904376ms)
Mar  2 22:44:22.070: INFO: (10) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 35.31522ms)
Mar  2 22:44:22.070: INFO: (10) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 36.475049ms)
Mar  2 22:44:22.070: INFO: (10) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 34.158502ms)
Mar  2 22:44:22.070: INFO: (10) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 36.150748ms)
Mar  2 22:44:22.090: INFO: (11) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 19.701304ms)
Mar  2 22:44:22.093: INFO: (11) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 21.901846ms)
Mar  2 22:44:22.093: INFO: (11) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 21.326773ms)
Mar  2 22:44:22.093: INFO: (11) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 22.213488ms)
Mar  2 22:44:22.109: INFO: (11) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 38.03322ms)
Mar  2 22:44:22.109: INFO: (11) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 37.839448ms)
Mar  2 22:44:22.109: INFO: (11) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 38.319ms)
Mar  2 22:44:22.110: INFO: (11) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 38.685528ms)
Mar  2 22:44:22.110: INFO: (11) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 38.134575ms)
Mar  2 22:44:22.110: INFO: (11) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 38.42094ms)
Mar  2 22:44:22.110: INFO: (11) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 38.740226ms)
Mar  2 22:44:22.110: INFO: (11) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 39.195092ms)
Mar  2 22:44:22.118: INFO: (11) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 46.657822ms)
Mar  2 22:44:22.120: INFO: (11) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 48.365311ms)
Mar  2 22:44:22.120: INFO: (11) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 48.374062ms)
Mar  2 22:44:22.120: INFO: (11) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 48.109323ms)
Mar  2 22:44:22.179: INFO: (12) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 58.26914ms)
Mar  2 22:44:22.179: INFO: (12) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 58.441304ms)
Mar  2 22:44:22.179: INFO: (12) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 58.605719ms)
Mar  2 22:44:22.179: INFO: (12) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 58.166156ms)
Mar  2 22:44:22.180: INFO: (12) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 58.35024ms)
Mar  2 22:44:22.180: INFO: (12) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 59.464228ms)
Mar  2 22:44:22.180: INFO: (12) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 58.055325ms)
Mar  2 22:44:22.182: INFO: (12) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 61.266313ms)
Mar  2 22:44:22.182: INFO: (12) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 61.402544ms)
Mar  2 22:44:22.182: INFO: (12) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 61.00879ms)
Mar  2 22:44:22.229: INFO: (12) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 108.098796ms)
Mar  2 22:44:22.230: INFO: (12) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 108.592058ms)
Mar  2 22:44:22.229: INFO: (12) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 108.22717ms)
Mar  2 22:44:22.230: INFO: (12) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 109.248423ms)
Mar  2 22:44:22.230: INFO: (12) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 110.236832ms)
Mar  2 22:44:22.230: INFO: (12) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 108.762832ms)
Mar  2 22:44:22.252: INFO: (13) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 20.984683ms)
Mar  2 22:44:22.291: INFO: (13) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 59.067421ms)
Mar  2 22:44:22.291: INFO: (13) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 58.90634ms)
Mar  2 22:44:22.292: INFO: (13) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 61.01001ms)
Mar  2 22:44:22.292: INFO: (13) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 61.316643ms)
Mar  2 22:44:22.293: INFO: (13) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 60.652441ms)
Mar  2 22:44:22.293: INFO: (13) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 61.516608ms)
Mar  2 22:44:22.293: INFO: (13) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 60.846407ms)
Mar  2 22:44:22.293: INFO: (13) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 60.767507ms)
Mar  2 22:44:22.293: INFO: (13) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 61.378479ms)
Mar  2 22:44:22.293: INFO: (13) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 61.564512ms)
Mar  2 22:44:22.293: INFO: (13) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 61.160269ms)
Mar  2 22:44:22.293: INFO: (13) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 61.814067ms)
Mar  2 22:44:22.295: INFO: (13) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 63.440701ms)
Mar  2 22:44:22.299: INFO: (13) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 67.575027ms)
Mar  2 22:44:22.299: INFO: (13) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 67.354774ms)
Mar  2 22:44:22.326: INFO: (14) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 26.276091ms)
Mar  2 22:44:22.360: INFO: (14) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 58.250115ms)
Mar  2 22:44:22.360: INFO: (14) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 58.590761ms)
Mar  2 22:44:22.360: INFO: (14) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 59.344569ms)
Mar  2 22:44:22.361: INFO: (14) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 60.442053ms)
Mar  2 22:44:22.363: INFO: (14) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 60.736667ms)
Mar  2 22:44:22.364: INFO: (14) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 63.50708ms)
Mar  2 22:44:22.364: INFO: (14) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 62.294058ms)
Mar  2 22:44:22.364: INFO: (14) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 62.695516ms)
Mar  2 22:44:22.365: INFO: (14) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 63.76376ms)
Mar  2 22:44:22.378: INFO: (14) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 76.793842ms)
Mar  2 22:44:22.378: INFO: (14) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 76.34368ms)
Mar  2 22:44:22.378: INFO: (14) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 76.710357ms)
Mar  2 22:44:22.379: INFO: (14) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 77.428531ms)
Mar  2 22:44:22.379: INFO: (14) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 77.824057ms)
Mar  2 22:44:22.378: INFO: (14) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 77.243712ms)
Mar  2 22:44:22.423: INFO: (15) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 44.090075ms)
Mar  2 22:44:22.425: INFO: (15) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 45.125433ms)
Mar  2 22:44:22.425: INFO: (15) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 45.125773ms)
Mar  2 22:44:22.425: INFO: (15) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 45.209215ms)
Mar  2 22:44:22.427: INFO: (15) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 45.978121ms)
Mar  2 22:44:22.427: INFO: (15) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 46.876878ms)
Mar  2 22:44:22.427: INFO: (15) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 46.058948ms)
Mar  2 22:44:22.427: INFO: (15) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 46.476866ms)
Mar  2 22:44:22.427: INFO: (15) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 46.848813ms)
Mar  2 22:44:22.427: INFO: (15) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 47.044524ms)
Mar  2 22:44:22.427: INFO: (15) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 46.381761ms)
Mar  2 22:44:22.438: INFO: (15) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 57.498464ms)
Mar  2 22:44:22.441: INFO: (15) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 61.065917ms)
Mar  2 22:44:22.442: INFO: (15) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 61.395967ms)
Mar  2 22:44:22.442: INFO: (15) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 62.220222ms)
Mar  2 22:44:22.444: INFO: (15) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 63.002589ms)
Mar  2 22:44:22.475: INFO: (16) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 30.224092ms)
Mar  2 22:44:22.475: INFO: (16) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 29.261887ms)
Mar  2 22:44:22.475: INFO: (16) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 28.153356ms)
Mar  2 22:44:22.475: INFO: (16) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 28.376905ms)
Mar  2 22:44:22.475: INFO: (16) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 28.192831ms)
Mar  2 22:44:22.475: INFO: (16) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 28.978521ms)
Mar  2 22:44:22.475: INFO: (16) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 28.60281ms)
Mar  2 22:44:22.476: INFO: (16) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 30.882407ms)
Mar  2 22:44:22.475: INFO: (16) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 30.32564ms)
Mar  2 22:44:22.476: INFO: (16) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 30.049716ms)
Mar  2 22:44:22.477: INFO: (16) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 30.334762ms)
Mar  2 22:44:22.482: INFO: (16) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 35.326879ms)
Mar  2 22:44:22.482: INFO: (16) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 35.471368ms)
Mar  2 22:44:22.482: INFO: (16) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 35.305305ms)
Mar  2 22:44:22.486: INFO: (16) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 39.383638ms)
Mar  2 22:44:22.487: INFO: (16) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 40.204872ms)
Mar  2 22:44:22.531: INFO: (17) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 44.091ms)
Mar  2 22:44:22.540: INFO: (17) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 52.143347ms)
Mar  2 22:44:22.540: INFO: (17) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 51.736618ms)
Mar  2 22:44:22.540: INFO: (17) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 52.392787ms)
Mar  2 22:44:22.542: INFO: (17) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 53.712045ms)
Mar  2 22:44:22.577: INFO: (17) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 88.359844ms)
Mar  2 22:44:22.577: INFO: (17) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 88.073948ms)
Mar  2 22:44:22.577: INFO: (17) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 89.331505ms)
Mar  2 22:44:22.577: INFO: (17) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 87.929534ms)
Mar  2 22:44:22.577: INFO: (17) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 87.834139ms)
Mar  2 22:44:22.578: INFO: (17) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 89.706979ms)
Mar  2 22:44:22.579: INFO: (17) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 91.491055ms)
Mar  2 22:44:22.579: INFO: (17) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 91.472887ms)
Mar  2 22:44:22.579: INFO: (17) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 90.376547ms)
Mar  2 22:44:22.579: INFO: (17) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 89.919732ms)
Mar  2 22:44:22.600: INFO: (17) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 111.305888ms)
Mar  2 22:44:22.626: INFO: (18) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 25.872786ms)
Mar  2 22:44:22.627: INFO: (18) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 26.809491ms)
Mar  2 22:44:22.627: INFO: (18) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 25.978241ms)
Mar  2 22:44:22.631: INFO: (18) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 28.925765ms)
Mar  2 22:44:22.632: INFO: (18) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 29.693415ms)
Mar  2 22:44:22.631: INFO: (18) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 29.058009ms)
Mar  2 22:44:22.631: INFO: (18) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 29.243587ms)
Mar  2 22:44:22.632: INFO: (18) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 30.266858ms)
Mar  2 22:44:22.632: INFO: (18) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 31.687849ms)
Mar  2 22:44:22.633: INFO: (18) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 31.825211ms)
Mar  2 22:44:22.633: INFO: (18) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 31.734332ms)
Mar  2 22:44:22.638: INFO: (18) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 37.540434ms)
Mar  2 22:44:22.656: INFO: (18) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 55.385591ms)
Mar  2 22:44:22.675: INFO: (18) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 74.063053ms)
Mar  2 22:44:22.675: INFO: (18) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 74.235566ms)
Mar  2 22:44:22.676: INFO: (18) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 73.633038ms)
Mar  2 22:44:22.729: INFO: (19) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">... (200; 51.936324ms)
Mar  2 22:44:22.729: INFO: (19) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 50.798019ms)
Mar  2 22:44:22.730: INFO: (19) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:460/proxy/: tls baz (200; 51.642459ms)
Mar  2 22:44:22.730: INFO: (19) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:160/proxy/: foo (200; 52.002959ms)
Mar  2 22:44:22.730: INFO: (19) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:462/proxy/: tls qux (200; 51.810913ms)
Mar  2 22:44:22.730: INFO: (19) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:1080/proxy/rewriteme">test<... (200; 52.095813ms)
Mar  2 22:44:22.736: INFO: (19) /api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/https:proxy-service-59sgr-rdn9b:443/proxy/tlsrewritem... (200; 58.356988ms)
Mar  2 22:44:22.737: INFO: (19) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/: <a href="/api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b/proxy/rewriteme">test</a> (200; 60.104408ms)
Mar  2 22:44:22.740: INFO: (19) /api/v1/namespaces/proxy-2543/pods/http:proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 61.634611ms)
Mar  2 22:44:22.740: INFO: (19) /api/v1/namespaces/proxy-2543/pods/proxy-service-59sgr-rdn9b:162/proxy/: bar (200; 64.14186ms)
Mar  2 22:44:22.740: INFO: (19) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname1/proxy/: tls baz (200; 63.560647ms)
Mar  2 22:44:22.740: INFO: (19) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname1/proxy/: foo (200; 62.525091ms)
Mar  2 22:44:22.740: INFO: (19) /api/v1/namespaces/proxy-2543/services/https:proxy-service-59sgr:tlsportname2/proxy/: tls qux (200; 62.450511ms)
Mar  2 22:44:22.740: INFO: (19) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname1/proxy/: foo (200; 62.726427ms)
Mar  2 22:44:22.747: INFO: (19) /api/v1/namespaces/proxy-2543/services/http:proxy-service-59sgr:portname2/proxy/: bar (200; 70.635081ms)
Mar  2 22:44:22.747: INFO: (19) /api/v1/namespaces/proxy-2543/services/proxy-service-59sgr:portname2/proxy/: bar (200; 70.161621ms)
STEP: deleting ReplicationController proxy-service-59sgr in namespace proxy-2543, will wait for the garbage collector to delete the pods
Mar  2 22:44:22.880: INFO: Deleting ReplicationController proxy-service-59sgr took: 73.657204ms
Mar  2 22:44:23.081: INFO: Terminating ReplicationController proxy-service-59sgr pods took: 200.250059ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:44:37.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2543" for this suite.

• [SLOW TEST:22.133 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":174,"skipped":3105,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:44:37.148: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  2 22:44:37.320: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  2 22:45:06.833: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:45:16.169: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:45:48.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6752" for this suite.

• [SLOW TEST:71.184 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":175,"skipped":3117,"failed":0}
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:45:48.332: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:46:04.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1935" for this suite.

• [SLOW TEST:16.654 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":176,"skipped":3117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:46:04.988: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 22:46:05.238: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 22:47:05.446: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:05.466: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:47:05.732: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar  2 22:47:05.745: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:05.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3428" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:05.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2562" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.101 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":177,"skipped":3148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:06.089: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:47:06.347: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ae122ed-2e3c-4f17-872e-48bc4911703f" in namespace "projected-8038" to be "Succeeded or Failed"
Mar  2 22:47:06.402: INFO: Pod "downwardapi-volume-4ae122ed-2e3c-4f17-872e-48bc4911703f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.476371ms
Mar  2 22:47:08.414: INFO: Pod "downwardapi-volume-4ae122ed-2e3c-4f17-872e-48bc4911703f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066264356s
Mar  2 22:47:10.424: INFO: Pod "downwardapi-volume-4ae122ed-2e3c-4f17-872e-48bc4911703f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077244109s
STEP: Saw pod success
Mar  2 22:47:10.425: INFO: Pod "downwardapi-volume-4ae122ed-2e3c-4f17-872e-48bc4911703f" satisfied condition "Succeeded or Failed"
Mar  2 22:47:10.435: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-4ae122ed-2e3c-4f17-872e-48bc4911703f container client-container: <nil>
STEP: delete the pod
Mar  2 22:47:10.587: INFO: Waiting for pod downwardapi-volume-4ae122ed-2e3c-4f17-872e-48bc4911703f to disappear
Mar  2 22:47:10.601: INFO: Pod downwardapi-volume-4ae122ed-2e3c-4f17-872e-48bc4911703f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:10.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8038" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":3170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:10.652: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  2 22:47:10.863: INFO: Waiting up to 5m0s for pod "downward-api-7c50a949-988b-472b-8421-14f3c0628de1" in namespace "downward-api-1162" to be "Succeeded or Failed"
Mar  2 22:47:10.876: INFO: Pod "downward-api-7c50a949-988b-472b-8421-14f3c0628de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.371373ms
Mar  2 22:47:12.893: INFO: Pod "downward-api-7c50a949-988b-472b-8421-14f3c0628de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029555941s
Mar  2 22:47:14.909: INFO: Pod "downward-api-7c50a949-988b-472b-8421-14f3c0628de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045537122s
STEP: Saw pod success
Mar  2 22:47:14.909: INFO: Pod "downward-api-7c50a949-988b-472b-8421-14f3c0628de1" satisfied condition "Succeeded or Failed"
Mar  2 22:47:14.919: INFO: Trying to get logs from node 10.138.244.159 pod downward-api-7c50a949-988b-472b-8421-14f3c0628de1 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:47:15.021: INFO: Waiting for pod downward-api-7c50a949-988b-472b-8421-14f3c0628de1 to disappear
Mar  2 22:47:15.030: INFO: Pod downward-api-7c50a949-988b-472b-8421-14f3c0628de1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:15.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1162" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:15.061: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-9779
STEP: creating replication controller nodeport-test in namespace services-9779
I0302 22:47:15.324016      25 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9779, replica count: 2
Mar  2 22:47:18.376: INFO: Creating new exec pod
I0302 22:47:18.376160      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:47:23.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9779 exec execpodh7qqz -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar  2 22:47:24.031: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  2 22:47:24.031: INFO: stdout: ""
Mar  2 22:47:24.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9779 exec execpodh7qqz -- /bin/sh -x -c nc -zv -t -w 2 172.21.33.246 80'
Mar  2 22:47:24.491: INFO: stderr: "+ nc -zv -t -w 2 172.21.33.246 80\nConnection to 172.21.33.246 80 port [tcp/http] succeeded!\n"
Mar  2 22:47:24.491: INFO: stdout: ""
Mar  2 22:47:24.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9779 exec execpodh7qqz -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.150 31719'
Mar  2 22:47:24.975: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.150 31719\nConnection to 10.138.244.150 31719 port [tcp/31719] succeeded!\n"
Mar  2 22:47:24.975: INFO: stdout: ""
Mar  2 22:47:24.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9779 exec execpodh7qqz -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.162 31719'
Mar  2 22:47:25.419: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.162 31719\nConnection to 10.138.244.162 31719 port [tcp/31719] succeeded!\n"
Mar  2 22:47:25.419: INFO: stdout: ""
Mar  2 22:47:25.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9779 exec execpodh7qqz -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.212 31719'
Mar  2 22:47:25.883: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.212 31719\nConnection to 168.1.11.212 31719 port [tcp/31719] succeeded!\n"
Mar  2 22:47:25.884: INFO: stdout: ""
Mar  2 22:47:25.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-9779 exec execpodh7qqz -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.211 31719'
Mar  2 22:47:26.297: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.211 31719\nConnection to 168.1.11.211 31719 port [tcp/31719] succeeded!\n"
Mar  2 22:47:26.297: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:26.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9779" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:11.298 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":180,"skipped":3276,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:26.360: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar  2 22:47:31.371: INFO: Successfully updated pod "labelsupdate145b6089-c2ef-450b-ac57-9b32110fa0a3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:33.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3531" for this suite.

• [SLOW TEST:7.126 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:33.486: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Mar  2 22:47:33.757: INFO: Waiting up to 5m0s for pod "var-expansion-f1b9dd4d-7e1c-422e-a379-a4b150c1f89b" in namespace "var-expansion-7116" to be "Succeeded or Failed"
Mar  2 22:47:33.779: INFO: Pod "var-expansion-f1b9dd4d-7e1c-422e-a379-a4b150c1f89b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.496188ms
Mar  2 22:47:35.792: INFO: Pod "var-expansion-f1b9dd4d-7e1c-422e-a379-a4b150c1f89b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035162916s
Mar  2 22:47:37.804: INFO: Pod "var-expansion-f1b9dd4d-7e1c-422e-a379-a4b150c1f89b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047686023s
STEP: Saw pod success
Mar  2 22:47:37.805: INFO: Pod "var-expansion-f1b9dd4d-7e1c-422e-a379-a4b150c1f89b" satisfied condition "Succeeded or Failed"
Mar  2 22:47:37.813: INFO: Trying to get logs from node 10.138.244.159 pod var-expansion-f1b9dd4d-7e1c-422e-a379-a4b150c1f89b container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:47:37.880: INFO: Waiting for pod var-expansion-f1b9dd4d-7e1c-422e-a379-a4b150c1f89b to disappear
Mar  2 22:47:37.895: INFO: Pod var-expansion-f1b9dd4d-7e1c-422e-a379-a4b150c1f89b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:37.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7116" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":182,"skipped":3301,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:37.927: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  2 22:47:38.143: INFO: Waiting up to 5m0s for pod "pod-28ecb751-5620-42c9-aa04-6c6ba4e77ec5" in namespace "emptydir-270" to be "Succeeded or Failed"
Mar  2 22:47:38.169: INFO: Pod "pod-28ecb751-5620-42c9-aa04-6c6ba4e77ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 25.872904ms
Mar  2 22:47:40.203: INFO: Pod "pod-28ecb751-5620-42c9-aa04-6c6ba4e77ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060318497s
Mar  2 22:47:42.226: INFO: Pod "pod-28ecb751-5620-42c9-aa04-6c6ba4e77ec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082933029s
STEP: Saw pod success
Mar  2 22:47:42.226: INFO: Pod "pod-28ecb751-5620-42c9-aa04-6c6ba4e77ec5" satisfied condition "Succeeded or Failed"
Mar  2 22:47:42.242: INFO: Trying to get logs from node 10.138.244.159 pod pod-28ecb751-5620-42c9-aa04-6c6ba4e77ec5 container test-container: <nil>
STEP: delete the pod
Mar  2 22:47:42.312: INFO: Waiting for pod pod-28ecb751-5620-42c9-aa04-6c6ba4e77ec5 to disappear
Mar  2 22:47:42.321: INFO: Pod pod-28ecb751-5620-42c9-aa04-6c6ba4e77ec5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:42.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-270" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3302,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:42.358: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:47:42.640: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db90682c-c7f7-4200-9580-cc3c532fe46f" in namespace "downward-api-7503" to be "Succeeded or Failed"
Mar  2 22:47:42.653: INFO: Pod "downwardapi-volume-db90682c-c7f7-4200-9580-cc3c532fe46f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.998491ms
Mar  2 22:47:44.664: INFO: Pod "downwardapi-volume-db90682c-c7f7-4200-9580-cc3c532fe46f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024184512s
Mar  2 22:47:46.676: INFO: Pod "downwardapi-volume-db90682c-c7f7-4200-9580-cc3c532fe46f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03569537s
STEP: Saw pod success
Mar  2 22:47:46.676: INFO: Pod "downwardapi-volume-db90682c-c7f7-4200-9580-cc3c532fe46f" satisfied condition "Succeeded or Failed"
Mar  2 22:47:46.700: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-db90682c-c7f7-4200-9580-cc3c532fe46f container client-container: <nil>
STEP: delete the pod
Mar  2 22:47:46.766: INFO: Waiting for pod downwardapi-volume-db90682c-c7f7-4200-9580-cc3c532fe46f to disappear
Mar  2 22:47:46.791: INFO: Pod downwardapi-volume-db90682c-c7f7-4200-9580-cc3c532fe46f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:46.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7503" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":184,"skipped":3315,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:46.828: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1964.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1964.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1964.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1964.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1964.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1964.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:47:51.398: INFO: DNS probes using dns-1964/dns-test-70fdc8fe-4683-4677-9bd4-e86c27eb5200 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:51.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1964" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":185,"skipped":3320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:51.539: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-26fa0c46-d78d-4401-884b-707ddf1d1182
STEP: Creating secret with name secret-projected-all-test-volume-69ba446e-ed05-4a09-af42-51994341bd6c
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  2 22:47:51.815: INFO: Waiting up to 5m0s for pod "projected-volume-3a30adbe-dbf0-4157-b8b5-aa97910aa297" in namespace "projected-8626" to be "Succeeded or Failed"
Mar  2 22:47:51.851: INFO: Pod "projected-volume-3a30adbe-dbf0-4157-b8b5-aa97910aa297": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015701ms
Mar  2 22:47:53.865: INFO: Pod "projected-volume-3a30adbe-dbf0-4157-b8b5-aa97910aa297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049575104s
Mar  2 22:47:55.883: INFO: Pod "projected-volume-3a30adbe-dbf0-4157-b8b5-aa97910aa297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068111495s
STEP: Saw pod success
Mar  2 22:47:55.883: INFO: Pod "projected-volume-3a30adbe-dbf0-4157-b8b5-aa97910aa297" satisfied condition "Succeeded or Failed"
Mar  2 22:47:55.895: INFO: Trying to get logs from node 10.138.244.159 pod projected-volume-3a30adbe-dbf0-4157-b8b5-aa97910aa297 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  2 22:47:55.957: INFO: Waiting for pod projected-volume-3a30adbe-dbf0-4157-b8b5-aa97910aa297 to disappear
Mar  2 22:47:55.966: INFO: Pod projected-volume-3a30adbe-dbf0-4157-b8b5-aa97910aa297 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:47:55.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8626" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3366,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:47:56.024: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Mar  2 22:47:56.314: INFO: Waiting up to 5m0s for pod "client-containers-f1b72837-0221-44ce-9961-853e9f5101b2" in namespace "containers-3343" to be "Succeeded or Failed"
Mar  2 22:47:56.325: INFO: Pod "client-containers-f1b72837-0221-44ce-9961-853e9f5101b2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.043189ms
Mar  2 22:47:58.336: INFO: Pod "client-containers-f1b72837-0221-44ce-9961-853e9f5101b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022182005s
Mar  2 22:48:00.358: INFO: Pod "client-containers-f1b72837-0221-44ce-9961-853e9f5101b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043740327s
STEP: Saw pod success
Mar  2 22:48:00.358: INFO: Pod "client-containers-f1b72837-0221-44ce-9961-853e9f5101b2" satisfied condition "Succeeded or Failed"
Mar  2 22:48:00.387: INFO: Trying to get logs from node 10.138.244.159 pod client-containers-f1b72837-0221-44ce-9961-853e9f5101b2 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 22:48:00.463: INFO: Waiting for pod client-containers-f1b72837-0221-44ce-9961-853e9f5101b2 to disappear
Mar  2 22:48:00.472: INFO: Pod client-containers-f1b72837-0221-44ce-9961-853e9f5101b2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:00.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3343" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3379,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:00.505: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  2 22:48:00.918: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2341 /api/v1/namespaces/watch-2341/configmaps/e2e-watch-test-resource-version 41c8bd0a-59ce-40f9-addc-b27dbcb7064e 96718 0 2022-03-02 22:48:00 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-03-02 22:48:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 22:48:00.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2341 /api/v1/namespaces/watch-2341/configmaps/e2e-watch-test-resource-version 41c8bd0a-59ce-40f9-addc-b27dbcb7064e 96720 0 2022-03-02 22:48:00 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-03-02 22:48:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:00.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2341" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":188,"skipped":3389,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:00.958: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:05.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1406" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3402,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:05.430: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Mar  2 22:48:05.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-6047 create -f -'
Mar  2 22:48:06.669: INFO: stderr: ""
Mar  2 22:48:06.669: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar  2 22:48:06.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-6047 diff -f -'
Mar  2 22:48:07.632: INFO: rc: 1
Mar  2 22:48:07.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-6047 delete -f -'
Mar  2 22:48:07.858: INFO: stderr: ""
Mar  2 22:48:07.858: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:07.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6047" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":190,"skipped":3403,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:07.898: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:48:08.100: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-327fe4e9-95ab-4f85-95ce-f0b7aa8a5534
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-327fe4e9-95ab-4f85-95ce-f0b7aa8a5534
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:12.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1299" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:12.371: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-439112d3-f1d9-4e2d-af66-97fc93536601
STEP: Creating a pod to test consume secrets
Mar  2 22:48:12.632: INFO: Waiting up to 5m0s for pod "pod-secrets-1b8e3091-274b-4aed-a6d7-75d70e30fedd" in namespace "secrets-5168" to be "Succeeded or Failed"
Mar  2 22:48:12.644: INFO: Pod "pod-secrets-1b8e3091-274b-4aed-a6d7-75d70e30fedd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.471959ms
Mar  2 22:48:14.664: INFO: Pod "pod-secrets-1b8e3091-274b-4aed-a6d7-75d70e30fedd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031828753s
Mar  2 22:48:16.691: INFO: Pod "pod-secrets-1b8e3091-274b-4aed-a6d7-75d70e30fedd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058867639s
STEP: Saw pod success
Mar  2 22:48:16.691: INFO: Pod "pod-secrets-1b8e3091-274b-4aed-a6d7-75d70e30fedd" satisfied condition "Succeeded or Failed"
Mar  2 22:48:16.700: INFO: Trying to get logs from node 10.138.244.159 pod pod-secrets-1b8e3091-274b-4aed-a6d7-75d70e30fedd container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:48:16.765: INFO: Waiting for pod pod-secrets-1b8e3091-274b-4aed-a6d7-75d70e30fedd to disappear
Mar  2 22:48:16.776: INFO: Pod pod-secrets-1b8e3091-274b-4aed-a6d7-75d70e30fedd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:16.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5168" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":192,"skipped":3454,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:16.815: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 22:48:17.119: INFO: Waiting up to 5m0s for pod "pod-6b1a0f82-5cfa-4ccf-b307-48518123a4ba" in namespace "emptydir-2987" to be "Succeeded or Failed"
Mar  2 22:48:17.140: INFO: Pod "pod-6b1a0f82-5cfa-4ccf-b307-48518123a4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016125ms
Mar  2 22:48:19.152: INFO: Pod "pod-6b1a0f82-5cfa-4ccf-b307-48518123a4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032339664s
Mar  2 22:48:21.165: INFO: Pod "pod-6b1a0f82-5cfa-4ccf-b307-48518123a4ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045542979s
STEP: Saw pod success
Mar  2 22:48:21.165: INFO: Pod "pod-6b1a0f82-5cfa-4ccf-b307-48518123a4ba" satisfied condition "Succeeded or Failed"
Mar  2 22:48:21.179: INFO: Trying to get logs from node 10.138.244.159 pod pod-6b1a0f82-5cfa-4ccf-b307-48518123a4ba container test-container: <nil>
STEP: delete the pod
Mar  2 22:48:21.288: INFO: Waiting for pod pod-6b1a0f82-5cfa-4ccf-b307-48518123a4ba to disappear
Mar  2 22:48:21.302: INFO: Pod pod-6b1a0f82-5cfa-4ccf-b307-48518123a4ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:21.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2987" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3455,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:21.342: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5546
STEP: creating service affinity-nodeport in namespace services-5546
STEP: creating replication controller affinity-nodeport in namespace services-5546
I0302 22:48:21.628550      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-5546, replica count: 3
I0302 22:48:24.679062      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:48:27.679517      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:48:27.774: INFO: Creating new exec pod
Mar  2 22:48:32.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5546 exec execpod-affinity6746b -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Mar  2 22:48:33.371: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  2 22:48:33.371: INFO: stdout: ""
Mar  2 22:48:33.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5546 exec execpod-affinity6746b -- /bin/sh -x -c nc -zv -t -w 2 172.21.135.113 80'
Mar  2 22:48:33.796: INFO: stderr: "+ nc -zv -t -w 2 172.21.135.113 80\nConnection to 172.21.135.113 80 port [tcp/http] succeeded!\n"
Mar  2 22:48:33.796: INFO: stdout: ""
Mar  2 22:48:33.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5546 exec execpod-affinity6746b -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.159 31719'
Mar  2 22:48:34.225: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.159 31719\nConnection to 10.138.244.159 31719 port [tcp/31719] succeeded!\n"
Mar  2 22:48:34.225: INFO: stdout: ""
Mar  2 22:48:34.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5546 exec execpod-affinity6746b -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.150 31719'
Mar  2 22:48:34.714: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.150 31719\nConnection to 10.138.244.150 31719 port [tcp/31719] succeeded!\n"
Mar  2 22:48:34.714: INFO: stdout: ""
Mar  2 22:48:34.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5546 exec execpod-affinity6746b -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.218 31719'
Mar  2 22:48:35.204: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.218 31719\nConnection to 168.1.11.218 31719 port [tcp/31719] succeeded!\n"
Mar  2 22:48:35.204: INFO: stdout: ""
Mar  2 22:48:35.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5546 exec execpod-affinity6746b -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.212 31719'
Mar  2 22:48:35.721: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.212 31719\nConnection to 168.1.11.212 31719 port [tcp/31719] succeeded!\n"
Mar  2 22:48:35.721: INFO: stdout: ""
Mar  2 22:48:35.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5546 exec execpod-affinity6746b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.244.150:31719/ ; done'
Mar  2 22:48:36.375: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31719/\n"
Mar  2 22:48:36.375: INFO: stdout: "\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl\naffinity-nodeport-rqmzl"
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.375: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.376: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.376: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.376: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.376: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.376: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.376: INFO: Received response from host: affinity-nodeport-rqmzl
Mar  2 22:48:36.376: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5546, will wait for the garbage collector to delete the pods
Mar  2 22:48:36.553: INFO: Deleting ReplicationController affinity-nodeport took: 20.927107ms
Mar  2 22:48:36.662: INFO: Terminating ReplicationController affinity-nodeport pods took: 108.975205ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:49.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5546" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:27.939 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":194,"skipped":3476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:49.282: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:52.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2108" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":195,"skipped":3511,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:52.695: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 22:48:58.793: INFO: Successfully updated pod "pod-update-7c9d7791-4ecf-4bdd-814a-e7303f040726"
STEP: verifying the updated pod is in kubernetes
Mar  2 22:48:58.833: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:48:58.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4365" for this suite.

• [SLOW TEST:6.203 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3512,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:48:58.899: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:48:59.186: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-abc10454-79c7-4a3e-a1dd-d84ad6cf899f
STEP: Creating configMap with name cm-test-opt-upd-8c455928-3eca-404e-b0a5-76c2c18a82cc
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-abc10454-79c7-4a3e-a1dd-d84ad6cf899f
STEP: Updating configmap cm-test-opt-upd-8c455928-3eca-404e-b0a5-76c2c18a82cc
STEP: Creating configMap with name cm-test-opt-create-2a7eac87-d371-44fc-9b55-82a795875ba0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:50:11.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4998" for this suite.

• [SLOW TEST:72.183 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":197,"skipped":3518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:50:11.082: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-b80feae8-41a7-4bae-a7bc-44caa2a603c9
STEP: Creating a pod to test consume secrets
Mar  2 22:50:11.377: INFO: Waiting up to 5m0s for pod "pod-secrets-ac92a4ae-51c4-4c88-ae78-ac86b3147b15" in namespace "secrets-3437" to be "Succeeded or Failed"
Mar  2 22:50:11.393: INFO: Pod "pod-secrets-ac92a4ae-51c4-4c88-ae78-ac86b3147b15": Phase="Pending", Reason="", readiness=false. Elapsed: 15.255112ms
Mar  2 22:50:13.404: INFO: Pod "pod-secrets-ac92a4ae-51c4-4c88-ae78-ac86b3147b15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026794288s
Mar  2 22:50:15.415: INFO: Pod "pod-secrets-ac92a4ae-51c4-4c88-ae78-ac86b3147b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037560919s
STEP: Saw pod success
Mar  2 22:50:15.415: INFO: Pod "pod-secrets-ac92a4ae-51c4-4c88-ae78-ac86b3147b15" satisfied condition "Succeeded or Failed"
Mar  2 22:50:15.425: INFO: Trying to get logs from node 10.138.244.159 pod pod-secrets-ac92a4ae-51c4-4c88-ae78-ac86b3147b15 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:50:15.486: INFO: Waiting for pod pod-secrets-ac92a4ae-51c4-4c88-ae78-ac86b3147b15 to disappear
Mar  2 22:50:15.498: INFO: Pod pod-secrets-ac92a4ae-51c4-4c88-ae78-ac86b3147b15 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:50:15.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3437" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:50:15.534: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:50:26.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-479" for this suite.

• [SLOW TEST:11.436 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":199,"skipped":3566,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:50:26.971: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:50:31.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8197" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":200,"skipped":3568,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:50:31.482: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Mar  2 22:50:31.648: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  2 22:50:31.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 create -f -'
Mar  2 22:50:32.455: INFO: stderr: ""
Mar  2 22:50:32.455: INFO: stdout: "service/agnhost-replica created\n"
Mar  2 22:50:32.455: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  2 22:50:32.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 create -f -'
Mar  2 22:50:33.210: INFO: stderr: ""
Mar  2 22:50:33.210: INFO: stdout: "service/agnhost-primary created\n"
Mar  2 22:50:33.210: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  2 22:50:33.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 create -f -'
Mar  2 22:50:34.096: INFO: stderr: ""
Mar  2 22:50:34.096: INFO: stdout: "service/frontend created\n"
Mar  2 22:50:34.096: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  2 22:50:34.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 create -f -'
Mar  2 22:50:34.818: INFO: stderr: ""
Mar  2 22:50:34.818: INFO: stdout: "deployment.apps/frontend created\n"
Mar  2 22:50:34.818: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 22:50:34.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 create -f -'
Mar  2 22:50:35.664: INFO: stderr: ""
Mar  2 22:50:35.664: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  2 22:50:35.665: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 22:50:35.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 create -f -'
Mar  2 22:50:36.638: INFO: stderr: ""
Mar  2 22:50:36.638: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar  2 22:50:36.638: INFO: Waiting for all frontend pods to be Running.
Mar  2 22:50:41.689: INFO: Waiting for frontend to serve content.
Mar  2 22:50:41.729: INFO: Trying to add a new entry to the guestbook.
Mar  2 22:50:41.773: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  2 22:50:41.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 delete --grace-period=0 --force -f -'
Mar  2 22:50:42.061: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:50:42.061: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 22:50:42.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 delete --grace-period=0 --force -f -'
Mar  2 22:50:42.294: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:50:42.294: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 22:50:42.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 delete --grace-period=0 --force -f -'
Mar  2 22:50:42.553: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:50:42.553: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 22:50:42.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 delete --grace-period=0 --force -f -'
Mar  2 22:50:42.840: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:50:42.840: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 22:50:42.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 delete --grace-period=0 --force -f -'
Mar  2 22:50:43.084: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:50:43.084: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 22:50:43.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1148 delete --grace-period=0 --force -f -'
Mar  2 22:50:43.361: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:50:43.361: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:50:43.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1148" for this suite.

• [SLOW TEST:11.961 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":201,"skipped":3570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:50:43.445: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:50:44.772: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:50:46.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858244, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858244, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858244, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858244, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:50:49.909: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:51:02.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5573" for this suite.
STEP: Destroying namespace "webhook-5573-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:19.383 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":202,"skipped":3598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:51:02.828: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:51:03.575: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:51:05.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858263, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858263, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858263, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858263, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:51:08.676: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:51:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7452-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:51:10.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6613" for this suite.
STEP: Destroying namespace "webhook-6613-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.899 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":203,"skipped":3637,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:51:10.731: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Mar  2 22:51:11.571: INFO: created pod pod-service-account-defaultsa
Mar  2 22:51:11.571: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  2 22:51:11.638: INFO: created pod pod-service-account-mountsa
Mar  2 22:51:11.638: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  2 22:51:11.750: INFO: created pod pod-service-account-nomountsa
Mar  2 22:51:11.750: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  2 22:51:11.909: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  2 22:51:11.909: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  2 22:51:11.945: INFO: created pod pod-service-account-mountsa-mountspec
Mar  2 22:51:11.945: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  2 22:51:11.978: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  2 22:51:11.979: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  2 22:51:12.034: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  2 22:51:12.034: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  2 22:51:12.094: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  2 22:51:12.094: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  2 22:51:12.145: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  2 22:51:12.145: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:51:12.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3659" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":204,"skipped":3641,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:51:12.224: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 22:51:26.862: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:51:26.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7080" for this suite.

• [SLOW TEST:14.715 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":205,"skipped":3642,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:51:26.939: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:51:27.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ca42065-480e-4228-8196-23c37fcf9317" in namespace "downward-api-4779" to be "Succeeded or Failed"
Mar  2 22:51:27.241: INFO: Pod "downwardapi-volume-2ca42065-480e-4228-8196-23c37fcf9317": Phase="Pending", Reason="", readiness=false. Elapsed: 17.121839ms
Mar  2 22:51:29.253: INFO: Pod "downwardapi-volume-2ca42065-480e-4228-8196-23c37fcf9317": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028909921s
Mar  2 22:51:31.278: INFO: Pod "downwardapi-volume-2ca42065-480e-4228-8196-23c37fcf9317": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054056035s
STEP: Saw pod success
Mar  2 22:51:31.278: INFO: Pod "downwardapi-volume-2ca42065-480e-4228-8196-23c37fcf9317" satisfied condition "Succeeded or Failed"
Mar  2 22:51:31.294: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-2ca42065-480e-4228-8196-23c37fcf9317 container client-container: <nil>
STEP: delete the pod
Mar  2 22:51:31.367: INFO: Waiting for pod downwardapi-volume-2ca42065-480e-4228-8196-23c37fcf9317 to disappear
Mar  2 22:51:31.378: INFO: Pod downwardapi-volume-2ca42065-480e-4228-8196-23c37fcf9317 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:51:31.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4779" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3647,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:51:31.422: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Mar  2 22:51:32.706: INFO: Waiting up to 5m0s for pod "var-expansion-dea294b2-c641-45be-b458-d8004a52c977" in namespace "var-expansion-8013" to be "Succeeded or Failed"
Mar  2 22:51:32.752: INFO: Pod "var-expansion-dea294b2-c641-45be-b458-d8004a52c977": Phase="Pending", Reason="", readiness=false. Elapsed: 44.198377ms
Mar  2 22:51:34.771: INFO: Pod "var-expansion-dea294b2-c641-45be-b458-d8004a52c977": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063170479s
Mar  2 22:51:36.789: INFO: Pod "var-expansion-dea294b2-c641-45be-b458-d8004a52c977": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081419584s
STEP: Saw pod success
Mar  2 22:51:36.789: INFO: Pod "var-expansion-dea294b2-c641-45be-b458-d8004a52c977" satisfied condition "Succeeded or Failed"
Mar  2 22:51:36.800: INFO: Trying to get logs from node 10.138.244.159 pod var-expansion-dea294b2-c641-45be-b458-d8004a52c977 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:51:36.876: INFO: Waiting for pod var-expansion-dea294b2-c641-45be-b458-d8004a52c977 to disappear
Mar  2 22:51:36.888: INFO: Pod var-expansion-dea294b2-c641-45be-b458-d8004a52c977 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:51:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8013" for this suite.

• [SLOW TEST:5.540 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":207,"skipped":3648,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:51:36.963: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 22:51:37.222: INFO: Waiting up to 5m0s for pod "pod-0b8af1f8-d89f-416b-a2c7-b21b590104fa" in namespace "emptydir-4330" to be "Succeeded or Failed"
Mar  2 22:51:37.239: INFO: Pod "pod-0b8af1f8-d89f-416b-a2c7-b21b590104fa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.457353ms
Mar  2 22:51:39.251: INFO: Pod "pod-0b8af1f8-d89f-416b-a2c7-b21b590104fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028325153s
Mar  2 22:51:41.264: INFO: Pod "pod-0b8af1f8-d89f-416b-a2c7-b21b590104fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041840028s
STEP: Saw pod success
Mar  2 22:51:41.265: INFO: Pod "pod-0b8af1f8-d89f-416b-a2c7-b21b590104fa" satisfied condition "Succeeded or Failed"
Mar  2 22:51:41.281: INFO: Trying to get logs from node 10.138.244.159 pod pod-0b8af1f8-d89f-416b-a2c7-b21b590104fa container test-container: <nil>
STEP: delete the pod
Mar  2 22:51:41.385: INFO: Waiting for pod pod-0b8af1f8-d89f-416b-a2c7-b21b590104fa to disappear
Mar  2 22:51:41.398: INFO: Pod pod-0b8af1f8-d89f-416b-a2c7-b21b590104fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:51:41.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4330" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3651,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:51:41.440: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:51:41.675: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 22:51:50.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-2786 --namespace=crd-publish-openapi-2786 create -f -'
Mar  2 22:51:51.849: INFO: stderr: ""
Mar  2 22:51:51.849: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 22:51:51.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-2786 --namespace=crd-publish-openapi-2786 delete e2e-test-crd-publish-openapi-5794-crds test-cr'
Mar  2 22:51:52.041: INFO: stderr: ""
Mar  2 22:51:52.041: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  2 22:51:52.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-2786 --namespace=crd-publish-openapi-2786 apply -f -'
Mar  2 22:51:52.830: INFO: stderr: ""
Mar  2 22:51:52.830: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 22:51:52.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-2786 --namespace=crd-publish-openapi-2786 delete e2e-test-crd-publish-openapi-5794-crds test-cr'
Mar  2 22:51:53.011: INFO: stderr: ""
Mar  2 22:51:53.011: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar  2 22:51:53.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-2786 explain e2e-test-crd-publish-openapi-5794-crds'
Mar  2 22:51:53.555: INFO: stderr: ""
Mar  2 22:51:53.555: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:52:02.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2786" for this suite.

• [SLOW TEST:21.132 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":209,"skipped":3656,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:52:02.573: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-73bb8b97-0f7d-467e-872d-579eae0249a3 in namespace container-probe-1004
Mar  2 22:52:06.901: INFO: Started pod liveness-73bb8b97-0f7d-467e-872d-579eae0249a3 in namespace container-probe-1004
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:52:06.911: INFO: Initial restart count of pod liveness-73bb8b97-0f7d-467e-872d-579eae0249a3 is 0
Mar  2 22:52:23.056: INFO: Restart count of pod container-probe-1004/liveness-73bb8b97-0f7d-467e-872d-579eae0249a3 is now 1 (16.144716145s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:52:23.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1004" for this suite.

• [SLOW TEST:20.557 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:52:23.134: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:52:23.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22a61d82-7cb7-4c01-a652-498b093c7019" in namespace "projected-8555" to be "Succeeded or Failed"
Mar  2 22:52:23.358: INFO: Pod "downwardapi-volume-22a61d82-7cb7-4c01-a652-498b093c7019": Phase="Pending", Reason="", readiness=false. Elapsed: 15.264332ms
Mar  2 22:52:25.369: INFO: Pod "downwardapi-volume-22a61d82-7cb7-4c01-a652-498b093c7019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026598249s
Mar  2 22:52:27.386: INFO: Pod "downwardapi-volume-22a61d82-7cb7-4c01-a652-498b093c7019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043050897s
STEP: Saw pod success
Mar  2 22:52:27.386: INFO: Pod "downwardapi-volume-22a61d82-7cb7-4c01-a652-498b093c7019" satisfied condition "Succeeded or Failed"
Mar  2 22:52:27.395: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-22a61d82-7cb7-4c01-a652-498b093c7019 container client-container: <nil>
STEP: delete the pod
Mar  2 22:52:27.517: INFO: Waiting for pod downwardapi-volume-22a61d82-7cb7-4c01-a652-498b093c7019 to disappear
Mar  2 22:52:27.540: INFO: Pod downwardapi-volume-22a61d82-7cb7-4c01-a652-498b093c7019 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:52:27.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8555" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":211,"skipped":3704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:52:27.606: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 22:52:27.919: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 22:53:28.122: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Mar  2 22:53:28.253: INFO: Created pod: pod0-sched-preemption-low-priority
Mar  2 22:53:28.358: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar  2 22:53:28.465: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:53:50.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2270" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:83.287 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":212,"skipped":3731,"failed":0}
SSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:53:50.893: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:53:51.230: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6585
I0302 22:53:51.301186      25 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6585, replica count: 1
I0302 22:53:52.401567      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:53:53.407294      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:53:54.407710      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:53:54.544: INFO: Created: latency-svc-hw2c8
Mar  2 22:53:54.569: INFO: Got endpoints: latency-svc-hw2c8 [61.473741ms]
Mar  2 22:53:54.603: INFO: Created: latency-svc-fn4r7
Mar  2 22:53:54.635: INFO: Got endpoints: latency-svc-fn4r7 [65.777435ms]
Mar  2 22:53:54.647: INFO: Created: latency-svc-r45nr
Mar  2 22:53:54.675: INFO: Created: latency-svc-9rf77
Mar  2 22:53:54.701: INFO: Got endpoints: latency-svc-r45nr [130.833618ms]
Mar  2 22:53:54.705: INFO: Got endpoints: latency-svc-9rf77 [134.421805ms]
Mar  2 22:53:54.707: INFO: Created: latency-svc-l5p4b
Mar  2 22:53:54.727: INFO: Got endpoints: latency-svc-l5p4b [156.112774ms]
Mar  2 22:53:54.730: INFO: Created: latency-svc-mkx8f
Mar  2 22:53:54.756: INFO: Got endpoints: latency-svc-mkx8f [185.278941ms]
Mar  2 22:53:54.790: INFO: Created: latency-svc-z28xn
Mar  2 22:53:54.791: INFO: Got endpoints: latency-svc-z28xn [219.345623ms]
Mar  2 22:53:54.795: INFO: Created: latency-svc-kk9pl
Mar  2 22:53:54.796: INFO: Created: latency-svc-4p2mc
Mar  2 22:53:54.796: INFO: Got endpoints: latency-svc-kk9pl [224.679714ms]
Mar  2 22:53:54.818: INFO: Got endpoints: latency-svc-4p2mc [246.785546ms]
Mar  2 22:53:54.820: INFO: Created: latency-svc-8plcc
Mar  2 22:53:54.843: INFO: Created: latency-svc-dqbdq
Mar  2 22:53:54.845: INFO: Got endpoints: latency-svc-8plcc [273.083566ms]
Mar  2 22:53:54.859: INFO: Created: latency-svc-xxqvt
Mar  2 22:53:54.862: INFO: Got endpoints: latency-svc-dqbdq [290.388762ms]
Mar  2 22:53:54.877: INFO: Got endpoints: latency-svc-xxqvt [307.694063ms]
Mar  2 22:53:54.879: INFO: Created: latency-svc-8r6wn
Mar  2 22:53:54.896: INFO: Created: latency-svc-hpvsg
Mar  2 22:53:54.898: INFO: Got endpoints: latency-svc-8r6wn [326.744755ms]
Mar  2 22:53:54.899: INFO: Created: latency-svc-lhdcp
Mar  2 22:53:54.933: INFO: Got endpoints: latency-svc-hpvsg [361.037244ms]
Mar  2 22:53:54.961: INFO: Got endpoints: latency-svc-lhdcp [389.081891ms]
Mar  2 22:53:54.961: INFO: Created: latency-svc-b6c69
Mar  2 22:53:54.968: INFO: Created: latency-svc-hcxq9
Mar  2 22:53:54.972: INFO: Got endpoints: latency-svc-b6c69 [400.881986ms]
Mar  2 22:53:54.984: INFO: Got endpoints: latency-svc-hcxq9 [348.751898ms]
Mar  2 22:53:54.988: INFO: Created: latency-svc-smblq
Mar  2 22:53:55.042: INFO: Got endpoints: latency-svc-smblq [341.218988ms]
Mar  2 22:53:55.044: INFO: Created: latency-svc-4vkk5
Mar  2 22:53:55.058: INFO: Created: latency-svc-blk22
Mar  2 22:53:55.077: INFO: Got endpoints: latency-svc-4vkk5 [371.576471ms]
Mar  2 22:53:55.085: INFO: Got endpoints: latency-svc-blk22 [358.175655ms]
Mar  2 22:53:55.086: INFO: Created: latency-svc-gs4rk
Mar  2 22:53:55.105: INFO: Got endpoints: latency-svc-gs4rk [348.54565ms]
Mar  2 22:53:55.114: INFO: Created: latency-svc-26lmn
Mar  2 22:53:55.134: INFO: Created: latency-svc-snhzb
Mar  2 22:53:55.135: INFO: Got endpoints: latency-svc-26lmn [344.149664ms]
Mar  2 22:53:55.151: INFO: Got endpoints: latency-svc-snhzb [354.855182ms]
Mar  2 22:53:55.165: INFO: Created: latency-svc-dhjpg
Mar  2 22:53:55.167: INFO: Created: latency-svc-pr86j
Mar  2 22:53:55.204: INFO: Got endpoints: latency-svc-dhjpg [385.422077ms]
Mar  2 22:53:55.210: INFO: Got endpoints: latency-svc-pr86j [365.203382ms]
Mar  2 22:53:55.212: INFO: Created: latency-svc-r8d9m
Mar  2 22:53:55.226: INFO: Created: latency-svc-5glrj
Mar  2 22:53:55.232: INFO: Got endpoints: latency-svc-r8d9m [369.048671ms]
Mar  2 22:53:55.239: INFO: Created: latency-svc-rdxqw
Mar  2 22:53:55.255: INFO: Got endpoints: latency-svc-5glrj [377.923209ms]
Mar  2 22:53:55.275: INFO: Got endpoints: latency-svc-rdxqw [376.321194ms]
Mar  2 22:53:55.276: INFO: Created: latency-svc-dscqz
Mar  2 22:53:55.280: INFO: Created: latency-svc-pjrxr
Mar  2 22:53:55.332: INFO: Created: latency-svc-hsftv
Mar  2 22:53:55.333: INFO: Got endpoints: latency-svc-pjrxr [371.850717ms]
Mar  2 22:53:55.333: INFO: Got endpoints: latency-svc-dscqz [400.066515ms]
Mar  2 22:53:55.341: INFO: Created: latency-svc-vjpxt
Mar  2 22:53:55.351: INFO: Created: latency-svc-stlp9
Mar  2 22:53:55.351: INFO: Got endpoints: latency-svc-hsftv [379.17739ms]
Mar  2 22:53:55.361: INFO: Got endpoints: latency-svc-vjpxt [376.466052ms]
Mar  2 22:53:55.374: INFO: Got endpoints: latency-svc-stlp9 [331.419413ms]
Mar  2 22:53:55.375: INFO: Created: latency-svc-8dv8g
Mar  2 22:53:55.399: INFO: Created: latency-svc-pkrqw
Mar  2 22:53:55.399: INFO: Got endpoints: latency-svc-8dv8g [66.095403ms]
Mar  2 22:53:55.403: INFO: Created: latency-svc-2cfwk
Mar  2 22:53:55.444: INFO: Got endpoints: latency-svc-pkrqw [367.66176ms]
Mar  2 22:53:55.445: INFO: Got endpoints: latency-svc-2cfwk [359.844958ms]
Mar  2 22:53:55.450: INFO: Created: latency-svc-2mhb2
Mar  2 22:53:55.465: INFO: Got endpoints: latency-svc-2mhb2 [360.056625ms]
Mar  2 22:53:55.469: INFO: Created: latency-svc-kpj68
Mar  2 22:53:55.488: INFO: Got endpoints: latency-svc-kpj68 [353.361835ms]
Mar  2 22:53:55.490: INFO: Created: latency-svc-zmbnh
Mar  2 22:53:55.496: INFO: Created: latency-svc-lblsd
Mar  2 22:53:55.510: INFO: Got endpoints: latency-svc-zmbnh [359.255944ms]
Mar  2 22:53:55.512: INFO: Created: latency-svc-lb8c7
Mar  2 22:53:55.513: INFO: Got endpoints: latency-svc-lblsd [308.839488ms]
Mar  2 22:53:55.525: INFO: Got endpoints: latency-svc-lb8c7 [314.215932ms]
Mar  2 22:53:55.576: INFO: Created: latency-svc-k25vl
Mar  2 22:53:55.616: INFO: Got endpoints: latency-svc-k25vl [384.583884ms]
Mar  2 22:53:55.618: INFO: Created: latency-svc-7llbr
Mar  2 22:53:55.638: INFO: Got endpoints: latency-svc-7llbr [382.81594ms]
Mar  2 22:53:55.650: INFO: Created: latency-svc-kj99g
Mar  2 22:53:55.672: INFO: Created: latency-svc-mxmrf
Mar  2 22:53:55.673: INFO: Got endpoints: latency-svc-kj99g [398.548643ms]
Mar  2 22:53:55.686: INFO: Got endpoints: latency-svc-mxmrf [353.122129ms]
Mar  2 22:53:55.690: INFO: Created: latency-svc-822bt
Mar  2 22:53:55.728: INFO: Got endpoints: latency-svc-822bt [376.412629ms]
Mar  2 22:53:55.739: INFO: Created: latency-svc-8v67r
Mar  2 22:53:55.780: INFO: Created: latency-svc-btkt8
Mar  2 22:53:55.781: INFO: Got endpoints: latency-svc-8v67r [419.674603ms]
Mar  2 22:53:55.806: INFO: Got endpoints: latency-svc-btkt8 [432.19632ms]
Mar  2 22:53:55.810: INFO: Created: latency-svc-nbk2v
Mar  2 22:53:55.839: INFO: Got endpoints: latency-svc-nbk2v [439.603012ms]
Mar  2 22:53:55.850: INFO: Created: latency-svc-8j7tq
Mar  2 22:53:55.892: INFO: Got endpoints: latency-svc-8j7tq [446.672476ms]
Mar  2 22:53:55.892: INFO: Created: latency-svc-hjjxh
Mar  2 22:53:55.926: INFO: Got endpoints: latency-svc-hjjxh [480.595909ms]
Mar  2 22:53:55.927: INFO: Created: latency-svc-9vz8x
Mar  2 22:53:55.928: INFO: Created: latency-svc-pwkkw
Mar  2 22:53:55.942: INFO: Got endpoints: latency-svc-9vz8x [476.651224ms]
Mar  2 22:53:55.943: INFO: Got endpoints: latency-svc-pwkkw [455.134158ms]
Mar  2 22:53:55.957: INFO: Created: latency-svc-hfvxl
Mar  2 22:53:55.972: INFO: Created: latency-svc-2mnmd
Mar  2 22:53:55.976: INFO: Got endpoints: latency-svc-hfvxl [465.506947ms]
Mar  2 22:53:55.993: INFO: Created: latency-svc-kk5qd
Mar  2 22:53:55.994: INFO: Got endpoints: latency-svc-2mnmd [480.74212ms]
Mar  2 22:53:56.039: INFO: Got endpoints: latency-svc-kk5qd [514.280663ms]
Mar  2 22:53:56.041: INFO: Created: latency-svc-dcdm5
Mar  2 22:53:56.068: INFO: Created: latency-svc-bz8s8
Mar  2 22:53:56.076: INFO: Got endpoints: latency-svc-dcdm5 [459.537586ms]
Mar  2 22:53:56.085: INFO: Created: latency-svc-6wf42
Mar  2 22:53:56.091: INFO: Got endpoints: latency-svc-bz8s8 [453.25603ms]
Mar  2 22:53:56.097: INFO: Got endpoints: latency-svc-6wf42 [423.892242ms]
Mar  2 22:53:56.103: INFO: Created: latency-svc-m6vqz
Mar  2 22:53:56.129: INFO: Got endpoints: latency-svc-m6vqz [440.769592ms]
Mar  2 22:53:56.131: INFO: Created: latency-svc-gcnjl
Mar  2 22:53:56.165: INFO: Got endpoints: latency-svc-gcnjl [436.882272ms]
Mar  2 22:53:56.168: INFO: Created: latency-svc-2hndg
Mar  2 22:53:56.191: INFO: Created: latency-svc-tj66z
Mar  2 22:53:56.192: INFO: Got endpoints: latency-svc-2hndg [410.859325ms]
Mar  2 22:53:56.230: INFO: Got endpoints: latency-svc-tj66z [423.953375ms]
Mar  2 22:53:56.235: INFO: Created: latency-svc-8bp8l
Mar  2 22:53:56.247: INFO: Got endpoints: latency-svc-8bp8l [408.109719ms]
Mar  2 22:53:56.248: INFO: Created: latency-svc-ljkv9
Mar  2 22:53:56.278: INFO: Got endpoints: latency-svc-ljkv9 [386.141088ms]
Mar  2 22:53:56.279: INFO: Created: latency-svc-4l8qh
Mar  2 22:53:56.300: INFO: Created: latency-svc-5wtt5
Mar  2 22:53:56.301: INFO: Got endpoints: latency-svc-4l8qh [374.608735ms]
Mar  2 22:53:56.321: INFO: Got endpoints: latency-svc-5wtt5 [379.411747ms]
Mar  2 22:53:56.326: INFO: Created: latency-svc-zsks4
Mar  2 22:53:56.380: INFO: Got endpoints: latency-svc-zsks4 [436.241467ms]
Mar  2 22:53:56.384: INFO: Created: latency-svc-fpwk4
Mar  2 22:53:56.422: INFO: Got endpoints: latency-svc-fpwk4 [446.294672ms]
Mar  2 22:53:56.425: INFO: Created: latency-svc-xk5l4
Mar  2 22:53:56.446: INFO: Got endpoints: latency-svc-xk5l4 [451.848057ms]
Mar  2 22:53:56.455: INFO: Created: latency-svc-dzbql
Mar  2 22:53:56.459: INFO: Got endpoints: latency-svc-dzbql [419.758424ms]
Mar  2 22:53:56.466: INFO: Created: latency-svc-flflr
Mar  2 22:53:56.482: INFO: Got endpoints: latency-svc-flflr [405.790538ms]
Mar  2 22:53:56.517: INFO: Created: latency-svc-bw2z9
Mar  2 22:53:56.555: INFO: Got endpoints: latency-svc-bw2z9 [463.782098ms]
Mar  2 22:53:56.578: INFO: Created: latency-svc-ppmtp
Mar  2 22:53:56.595: INFO: Got endpoints: latency-svc-ppmtp [497.421374ms]
Mar  2 22:53:56.601: INFO: Created: latency-svc-vbbjk
Mar  2 22:53:56.629: INFO: Got endpoints: latency-svc-vbbjk [499.758872ms]
Mar  2 22:53:56.629: INFO: Created: latency-svc-7m4lj
Mar  2 22:53:56.644: INFO: Got endpoints: latency-svc-7m4lj [478.52223ms]
Mar  2 22:53:56.658: INFO: Created: latency-svc-s8lbb
Mar  2 22:53:56.676: INFO: Got endpoints: latency-svc-s8lbb [484.17461ms]
Mar  2 22:53:56.730: INFO: Created: latency-svc-5f6xx
Mar  2 22:53:56.776: INFO: Got endpoints: latency-svc-5f6xx [545.616982ms]
Mar  2 22:53:56.784: INFO: Created: latency-svc-r45hh
Mar  2 22:53:56.829: INFO: Got endpoints: latency-svc-r45hh [582.1887ms]
Mar  2 22:53:56.842: INFO: Created: latency-svc-km7gn
Mar  2 22:53:56.865: INFO: Got endpoints: latency-svc-km7gn [587.101237ms]
Mar  2 22:53:56.890: INFO: Created: latency-svc-r8fnr
Mar  2 22:53:56.900: INFO: Created: latency-svc-s65dw
Mar  2 22:53:56.910: INFO: Got endpoints: latency-svc-r8fnr [609.05693ms]
Mar  2 22:53:56.919: INFO: Created: latency-svc-jqhwq
Mar  2 22:53:56.919: INFO: Got endpoints: latency-svc-s65dw [597.197253ms]
Mar  2 22:53:56.939: INFO: Created: latency-svc-fjpfw
Mar  2 22:53:56.940: INFO: Got endpoints: latency-svc-jqhwq [559.745491ms]
Mar  2 22:53:56.946: INFO: Created: latency-svc-vgzxk
Mar  2 22:53:56.956: INFO: Got endpoints: latency-svc-fjpfw [527.941748ms]
Mar  2 22:53:56.963: INFO: Created: latency-svc-nppjl
Mar  2 22:53:56.965: INFO: Got endpoints: latency-svc-vgzxk [510.643928ms]
Mar  2 22:53:56.985: INFO: Got endpoints: latency-svc-nppjl [525.335981ms]
Mar  2 22:53:56.987: INFO: Created: latency-svc-9xspq
Mar  2 22:53:57.012: INFO: Got endpoints: latency-svc-9xspq [529.676953ms]
Mar  2 22:53:57.013: INFO: Created: latency-svc-bd562
Mar  2 22:53:57.028: INFO: Created: latency-svc-ndw2z
Mar  2 22:53:57.056: INFO: Created: latency-svc-lb6bz
Mar  2 22:53:57.057: INFO: Got endpoints: latency-svc-ndw2z [456.160269ms]
Mar  2 22:53:57.058: INFO: Got endpoints: latency-svc-bd562 [501.985673ms]
Mar  2 22:53:57.075: INFO: Created: latency-svc-g7n5k
Mar  2 22:53:57.076: INFO: Got endpoints: latency-svc-lb6bz [447.336191ms]
Mar  2 22:53:57.084: INFO: Created: latency-svc-pltv6
Mar  2 22:53:57.091: INFO: Got endpoints: latency-svc-g7n5k [446.55848ms]
Mar  2 22:53:57.112: INFO: Created: latency-svc-jhbs4
Mar  2 22:53:57.112: INFO: Got endpoints: latency-svc-pltv6 [433.669908ms]
Mar  2 22:53:57.134: INFO: Created: latency-svc-qx4vr
Mar  2 22:53:57.135: INFO: Got endpoints: latency-svc-jhbs4 [358.718229ms]
Mar  2 22:53:57.156: INFO: Got endpoints: latency-svc-qx4vr [326.718837ms]
Mar  2 22:53:57.157: INFO: Created: latency-svc-9vtnn
Mar  2 22:53:57.165: INFO: Created: latency-svc-gclmx
Mar  2 22:53:57.168: INFO: Got endpoints: latency-svc-9vtnn [302.672505ms]
Mar  2 22:53:57.174: INFO: Created: latency-svc-hcfm5
Mar  2 22:53:57.181: INFO: Got endpoints: latency-svc-gclmx [270.638482ms]
Mar  2 22:53:57.191: INFO: Got endpoints: latency-svc-hcfm5 [271.710958ms]
Mar  2 22:53:57.192: INFO: Created: latency-svc-n8424
Mar  2 22:53:57.201: INFO: Created: latency-svc-xkfn6
Mar  2 22:53:57.206: INFO: Got endpoints: latency-svc-n8424 [266.107645ms]
Mar  2 22:53:57.213: INFO: Got endpoints: latency-svc-xkfn6 [256.595793ms]
Mar  2 22:53:57.218: INFO: Created: latency-svc-sbjb2
Mar  2 22:53:57.235: INFO: Got endpoints: latency-svc-sbjb2 [269.63128ms]
Mar  2 22:53:57.235: INFO: Created: latency-svc-666j9
Mar  2 22:53:57.247: INFO: Created: latency-svc-r5jvg
Mar  2 22:53:57.249: INFO: Got endpoints: latency-svc-666j9 [263.612993ms]
Mar  2 22:53:57.271: INFO: Created: latency-svc-jldrf
Mar  2 22:53:57.273: INFO: Got endpoints: latency-svc-r5jvg [261.244688ms]
Mar  2 22:53:57.281: INFO: Created: latency-svc-z2vd5
Mar  2 22:53:57.283: INFO: Got endpoints: latency-svc-jldrf [224.929981ms]
Mar  2 22:53:57.311: INFO: Got endpoints: latency-svc-z2vd5 [254.265369ms]
Mar  2 22:53:57.312: INFO: Created: latency-svc-2rk79
Mar  2 22:53:57.331: INFO: Got endpoints: latency-svc-2rk79 [253.285304ms]
Mar  2 22:53:57.340: INFO: Created: latency-svc-g7fq2
Mar  2 22:53:57.357: INFO: Created: latency-svc-cl6gm
Mar  2 22:53:57.358: INFO: Got endpoints: latency-svc-g7fq2 [267.08664ms]
Mar  2 22:53:57.365: INFO: Created: latency-svc-8kbpt
Mar  2 22:53:57.372: INFO: Got endpoints: latency-svc-cl6gm [259.534354ms]
Mar  2 22:53:57.381: INFO: Created: latency-svc-lfwc8
Mar  2 22:53:57.385: INFO: Got endpoints: latency-svc-8kbpt [250.282869ms]
Mar  2 22:53:57.402: INFO: Created: latency-svc-jws9f
Mar  2 22:53:57.402: INFO: Got endpoints: latency-svc-lfwc8 [246.22807ms]
Mar  2 22:53:57.420: INFO: Got endpoints: latency-svc-jws9f [251.621394ms]
Mar  2 22:53:57.422: INFO: Created: latency-svc-dzl5w
Mar  2 22:53:57.439: INFO: Created: latency-svc-74sfj
Mar  2 22:53:57.451: INFO: Got endpoints: latency-svc-dzl5w [269.836284ms]
Mar  2 22:53:57.463: INFO: Got endpoints: latency-svc-74sfj [272.187524ms]
Mar  2 22:53:57.468: INFO: Created: latency-svc-ljppl
Mar  2 22:53:57.482: INFO: Got endpoints: latency-svc-ljppl [275.884229ms]
Mar  2 22:53:57.484: INFO: Created: latency-svc-8sx8n
Mar  2 22:53:57.500: INFO: Created: latency-svc-jsx54
Mar  2 22:53:57.500: INFO: Got endpoints: latency-svc-8sx8n [286.84016ms]
Mar  2 22:53:57.527: INFO: Got endpoints: latency-svc-jsx54 [291.845249ms]
Mar  2 22:53:57.536: INFO: Created: latency-svc-9cfbf
Mar  2 22:53:57.564: INFO: Created: latency-svc-fjh8w
Mar  2 22:53:57.568: INFO: Got endpoints: latency-svc-9cfbf [318.533521ms]
Mar  2 22:53:57.610: INFO: Created: latency-svc-fxzpx
Mar  2 22:53:57.619: INFO: Got endpoints: latency-svc-fjh8w [345.579521ms]
Mar  2 22:53:57.622: INFO: Created: latency-svc-9x2jr
Mar  2 22:53:57.626: INFO: Got endpoints: latency-svc-fxzpx [343.290064ms]
Mar  2 22:53:57.641: INFO: Got endpoints: latency-svc-9x2jr [329.904408ms]
Mar  2 22:53:57.643: INFO: Created: latency-svc-nwdqs
Mar  2 22:53:57.663: INFO: Created: latency-svc-znckq
Mar  2 22:53:57.666: INFO: Got endpoints: latency-svc-nwdqs [335.220084ms]
Mar  2 22:53:57.680: INFO: Created: latency-svc-5cpn9
Mar  2 22:53:57.681: INFO: Got endpoints: latency-svc-znckq [322.448339ms]
Mar  2 22:53:57.705: INFO: Got endpoints: latency-svc-5cpn9 [332.260658ms]
Mar  2 22:53:57.709: INFO: Created: latency-svc-rbvrc
Mar  2 22:53:57.710: INFO: Created: latency-svc-8hwml
Mar  2 22:53:57.710: INFO: Got endpoints: latency-svc-rbvrc [324.679475ms]
Mar  2 22:53:57.731: INFO: Got endpoints: latency-svc-8hwml [328.599129ms]
Mar  2 22:53:57.735: INFO: Created: latency-svc-wx65f
Mar  2 22:53:57.759: INFO: Created: latency-svc-5c78w
Mar  2 22:53:57.766: INFO: Got endpoints: latency-svc-wx65f [346.017123ms]
Mar  2 22:53:57.779: INFO: Got endpoints: latency-svc-5c78w [328.795479ms]
Mar  2 22:53:57.784: INFO: Created: latency-svc-z7wbr
Mar  2 22:53:57.786: INFO: Created: latency-svc-skpbh
Mar  2 22:53:57.789: INFO: Got endpoints: latency-svc-z7wbr [325.433509ms]
Mar  2 22:53:57.801: INFO: Got endpoints: latency-svc-skpbh [318.165104ms]
Mar  2 22:53:57.801: INFO: Created: latency-svc-8l8nc
Mar  2 22:53:57.810: INFO: Created: latency-svc-r7mcf
Mar  2 22:53:57.819: INFO: Got endpoints: latency-svc-8l8nc [319.325777ms]
Mar  2 22:53:57.830: INFO: Got endpoints: latency-svc-r7mcf [302.996517ms]
Mar  2 22:53:57.832: INFO: Created: latency-svc-mrxsb
Mar  2 22:53:57.845: INFO: Got endpoints: latency-svc-mrxsb [276.768878ms]
Mar  2 22:53:57.853: INFO: Created: latency-svc-r6dsw
Mar  2 22:53:57.868: INFO: Created: latency-svc-6dqrd
Mar  2 22:53:57.869: INFO: Got endpoints: latency-svc-r6dsw [249.532217ms]
Mar  2 22:53:57.884: INFO: Created: latency-svc-lgwdm
Mar  2 22:53:57.891: INFO: Got endpoints: latency-svc-6dqrd [265.560198ms]
Mar  2 22:53:57.901: INFO: Created: latency-svc-25xbz
Mar  2 22:53:57.907: INFO: Got endpoints: latency-svc-lgwdm [265.065247ms]
Mar  2 22:53:57.915: INFO: Created: latency-svc-wt4rp
Mar  2 22:53:57.916: INFO: Got endpoints: latency-svc-25xbz [249.562623ms]
Mar  2 22:53:57.922: INFO: Created: latency-svc-228dw
Mar  2 22:53:57.929: INFO: Got endpoints: latency-svc-wt4rp [247.358829ms]
Mar  2 22:53:57.937: INFO: Created: latency-svc-nksgl
Mar  2 22:53:57.937: INFO: Got endpoints: latency-svc-228dw [232.440422ms]
Mar  2 22:53:57.951: INFO: Created: latency-svc-6qgr4
Mar  2 22:53:57.952: INFO: Got endpoints: latency-svc-nksgl [241.360584ms]
Mar  2 22:53:57.965: INFO: Got endpoints: latency-svc-6qgr4 [222.465767ms]
Mar  2 22:53:57.967: INFO: Created: latency-svc-kcvjp
Mar  2 22:53:57.984: INFO: Created: latency-svc-gqwlf
Mar  2 22:53:57.984: INFO: Got endpoints: latency-svc-kcvjp [218.097014ms]
Mar  2 22:53:58.007: INFO: Got endpoints: latency-svc-gqwlf [227.034349ms]
Mar  2 22:53:58.008: INFO: Created: latency-svc-ft8f2
Mar  2 22:53:58.012: INFO: Created: latency-svc-r8pgk
Mar  2 22:53:58.024: INFO: Got endpoints: latency-svc-ft8f2 [235.24593ms]
Mar  2 22:53:58.032: INFO: Got endpoints: latency-svc-r8pgk [230.96358ms]
Mar  2 22:53:58.037: INFO: Created: latency-svc-5z862
Mar  2 22:53:58.047: INFO: Created: latency-svc-2fvcx
Mar  2 22:53:58.047: INFO: Got endpoints: latency-svc-5z862 [228.146414ms]
Mar  2 22:53:58.059: INFO: Got endpoints: latency-svc-2fvcx [229.500409ms]
Mar  2 22:53:58.062: INFO: Created: latency-svc-t67t5
Mar  2 22:53:58.073: INFO: Created: latency-svc-dx897
Mar  2 22:53:58.075: INFO: Got endpoints: latency-svc-t67t5 [230.479381ms]
Mar  2 22:53:58.084: INFO: Created: latency-svc-5wgrg
Mar  2 22:53:58.088: INFO: Got endpoints: latency-svc-dx897 [218.57134ms]
Mar  2 22:53:58.094: INFO: Created: latency-svc-lb6sq
Mar  2 22:53:58.104: INFO: Got endpoints: latency-svc-5wgrg [210.607436ms]
Mar  2 22:53:58.106: INFO: Got endpoints: latency-svc-lb6sq [199.516422ms]
Mar  2 22:53:58.115: INFO: Created: latency-svc-97r6n
Mar  2 22:53:58.124: INFO: Created: latency-svc-ql7kk
Mar  2 22:53:58.130: INFO: Got endpoints: latency-svc-97r6n [214.186887ms]
Mar  2 22:53:58.145: INFO: Got endpoints: latency-svc-ql7kk [216.693461ms]
Mar  2 22:53:58.147: INFO: Created: latency-svc-fqkhl
Mar  2 22:53:58.156: INFO: Created: latency-svc-vcxsn
Mar  2 22:53:58.162: INFO: Got endpoints: latency-svc-fqkhl [224.808959ms]
Mar  2 22:53:58.173: INFO: Created: latency-svc-4rwhd
Mar  2 22:53:58.177: INFO: Got endpoints: latency-svc-vcxsn [225.283778ms]
Mar  2 22:53:58.187: INFO: Got endpoints: latency-svc-4rwhd [221.05763ms]
Mar  2 22:53:58.191: INFO: Created: latency-svc-qzv72
Mar  2 22:53:58.198: INFO: Created: latency-svc-fklzq
Mar  2 22:53:58.208: INFO: Got endpoints: latency-svc-qzv72 [223.531338ms]
Mar  2 22:53:58.210: INFO: Created: latency-svc-ldfsf
Mar  2 22:53:58.220: INFO: Got endpoints: latency-svc-fklzq [213.137168ms]
Mar  2 22:53:58.232: INFO: Created: latency-svc-7ddl2
Mar  2 22:53:58.254: INFO: Created: latency-svc-h8r9j
Mar  2 22:53:58.254: INFO: Got endpoints: latency-svc-ldfsf [229.651738ms]
Mar  2 22:53:58.263: INFO: Got endpoints: latency-svc-7ddl2 [230.155193ms]
Mar  2 22:53:58.271: INFO: Created: latency-svc-v5nmc
Mar  2 22:53:58.271: INFO: Got endpoints: latency-svc-h8r9j [223.814372ms]
Mar  2 22:53:58.284: INFO: Created: latency-svc-4fjjk
Mar  2 22:53:58.285: INFO: Got endpoints: latency-svc-v5nmc [224.987872ms]
Mar  2 22:53:58.296: INFO: Got endpoints: latency-svc-4fjjk [220.95289ms]
Mar  2 22:53:58.298: INFO: Created: latency-svc-bmm2c
Mar  2 22:53:58.312: INFO: Created: latency-svc-rwr9l
Mar  2 22:53:58.317: INFO: Got endpoints: latency-svc-bmm2c [228.921666ms]
Mar  2 22:53:58.326: INFO: Got endpoints: latency-svc-rwr9l [221.962496ms]
Mar  2 22:53:58.328: INFO: Created: latency-svc-ljs5n
Mar  2 22:53:58.339: INFO: Created: latency-svc-h6nxx
Mar  2 22:53:58.343: INFO: Got endpoints: latency-svc-ljs5n [236.333515ms]
Mar  2 22:53:58.360: INFO: Created: latency-svc-s56sk
Mar  2 22:53:58.363: INFO: Got endpoints: latency-svc-h6nxx [232.725573ms]
Mar  2 22:53:58.366: INFO: Created: latency-svc-5phgm
Mar  2 22:53:58.374: INFO: Got endpoints: latency-svc-s56sk [227.872586ms]
Mar  2 22:53:58.384: INFO: Got endpoints: latency-svc-5phgm [221.893318ms]
Mar  2 22:53:58.390: INFO: Created: latency-svc-h9dz6
Mar  2 22:53:58.400: INFO: Got endpoints: latency-svc-h9dz6 [221.093402ms]
Mar  2 22:53:58.401: INFO: Created: latency-svc-rp7wk
Mar  2 22:53:58.413: INFO: Got endpoints: latency-svc-rp7wk [226.007456ms]
Mar  2 22:53:58.416: INFO: Created: latency-svc-5c2c5
Mar  2 22:53:58.434: INFO: Created: latency-svc-hp46d
Mar  2 22:53:58.436: INFO: Got endpoints: latency-svc-5c2c5 [227.999494ms]
Mar  2 22:53:58.444: INFO: Created: latency-svc-sxg2p
Mar  2 22:53:58.452: INFO: Got endpoints: latency-svc-hp46d [231.858648ms]
Mar  2 22:53:58.487: INFO: Got endpoints: latency-svc-sxg2p [232.721205ms]
Mar  2 22:53:58.490: INFO: Created: latency-svc-vmvqx
Mar  2 22:53:58.512: INFO: Created: latency-svc-tfz7d
Mar  2 22:53:58.513: INFO: Got endpoints: latency-svc-vmvqx [248.55735ms]
Mar  2 22:53:58.521: INFO: Created: latency-svc-8prw4
Mar  2 22:53:58.536: INFO: Got endpoints: latency-svc-8prw4 [251.558554ms]
Mar  2 22:53:58.537: INFO: Got endpoints: latency-svc-tfz7d [265.222852ms]
Mar  2 22:53:58.547: INFO: Created: latency-svc-drwr4
Mar  2 22:53:58.556: INFO: Created: latency-svc-qsgln
Mar  2 22:53:58.565: INFO: Got endpoints: latency-svc-drwr4 [268.702652ms]
Mar  2 22:53:58.601: INFO: Got endpoints: latency-svc-qsgln [284.260621ms]
Mar  2 22:53:58.612: INFO: Created: latency-svc-5w9qx
Mar  2 22:53:58.627: INFO: Got endpoints: latency-svc-5w9qx [301.424193ms]
Mar  2 22:53:58.634: INFO: Created: latency-svc-mgnhw
Mar  2 22:53:58.645: INFO: Got endpoints: latency-svc-mgnhw [302.163881ms]
Mar  2 22:53:58.651: INFO: Created: latency-svc-bgxwf
Mar  2 22:53:58.674: INFO: Got endpoints: latency-svc-bgxwf [311.394374ms]
Mar  2 22:53:58.689: INFO: Created: latency-svc-tkpxw
Mar  2 22:53:58.713: INFO: Got endpoints: latency-svc-tkpxw [338.860897ms]
Mar  2 22:53:58.730: INFO: Created: latency-svc-t29hq
Mar  2 22:53:58.748: INFO: Created: latency-svc-nw4q4
Mar  2 22:53:58.753: INFO: Got endpoints: latency-svc-t29hq [368.359923ms]
Mar  2 22:53:58.807: INFO: Got endpoints: latency-svc-nw4q4 [406.357063ms]
Mar  2 22:53:58.808: INFO: Created: latency-svc-7fcvs
Mar  2 22:53:58.830: INFO: Created: latency-svc-7s6h6
Mar  2 22:53:58.830: INFO: Got endpoints: latency-svc-7fcvs [417.212082ms]
Mar  2 22:53:58.880: INFO: Got endpoints: latency-svc-7s6h6 [443.849631ms]
Mar  2 22:53:58.884: INFO: Created: latency-svc-hxkrk
Mar  2 22:53:58.889: INFO: Created: latency-svc-wptzl
Mar  2 22:53:58.907: INFO: Got endpoints: latency-svc-hxkrk [454.593219ms]
Mar  2 22:53:58.919: INFO: Created: latency-svc-56clk
Mar  2 22:53:58.919: INFO: Got endpoints: latency-svc-wptzl [431.724737ms]
Mar  2 22:53:58.938: INFO: Got endpoints: latency-svc-56clk [425.141436ms]
Mar  2 22:53:58.937: INFO: Created: latency-svc-tx6gk
Mar  2 22:53:58.967: INFO: Got endpoints: latency-svc-tx6gk [430.521222ms]
Mar  2 22:53:58.974: INFO: Created: latency-svc-vdjws
Mar  2 22:53:58.996: INFO: Got endpoints: latency-svc-vdjws [458.529418ms]
Mar  2 22:53:58.999: INFO: Created: latency-svc-c9nkg
Mar  2 22:53:59.036: INFO: Got endpoints: latency-svc-c9nkg [470.503424ms]
Mar  2 22:53:59.037: INFO: Created: latency-svc-x8v2b
Mar  2 22:53:59.037: INFO: Created: latency-svc-xbwsd
Mar  2 22:53:59.041: INFO: Got endpoints: latency-svc-x8v2b [440.48825ms]
Mar  2 22:53:59.049: INFO: Got endpoints: latency-svc-xbwsd [421.766452ms]
Mar  2 22:53:59.055: INFO: Created: latency-svc-cxtc4
Mar  2 22:53:59.071: INFO: Got endpoints: latency-svc-cxtc4 [425.587901ms]
Mar  2 22:53:59.072: INFO: Created: latency-svc-2btj5
Mar  2 22:53:59.092: INFO: Got endpoints: latency-svc-2btj5 [417.004306ms]
Mar  2 22:53:59.100: INFO: Created: latency-svc-bcts6
Mar  2 22:53:59.118: INFO: Created: latency-svc-srch5
Mar  2 22:53:59.128: INFO: Got endpoints: latency-svc-bcts6 [414.862859ms]
Mar  2 22:53:59.134: INFO: Got endpoints: latency-svc-srch5 [380.901263ms]
Mar  2 22:53:59.135: INFO: Created: latency-svc-v6vxm
Mar  2 22:53:59.151: INFO: Got endpoints: latency-svc-v6vxm [343.760686ms]
Mar  2 22:53:59.152: INFO: Created: latency-svc-m7x55
Mar  2 22:53:59.172: INFO: Got endpoints: latency-svc-m7x55 [341.738274ms]
Mar  2 22:53:59.177: INFO: Created: latency-svc-9w9mh
Mar  2 22:53:59.223: INFO: Got endpoints: latency-svc-9w9mh [343.018125ms]
Mar  2 22:53:59.226: INFO: Created: latency-svc-5phmh
Mar  2 22:53:59.240: INFO: Got endpoints: latency-svc-5phmh [333.623191ms]
Mar  2 22:53:59.241: INFO: Latencies: [65.777435ms 66.095403ms 130.833618ms 134.421805ms 156.112774ms 185.278941ms 199.516422ms 210.607436ms 213.137168ms 214.186887ms 216.693461ms 218.097014ms 218.57134ms 219.345623ms 220.95289ms 221.05763ms 221.093402ms 221.893318ms 221.962496ms 222.465767ms 223.531338ms 223.814372ms 224.679714ms 224.808959ms 224.929981ms 224.987872ms 225.283778ms 226.007456ms 227.034349ms 227.872586ms 227.999494ms 228.146414ms 228.921666ms 229.500409ms 229.651738ms 230.155193ms 230.479381ms 230.96358ms 231.858648ms 232.440422ms 232.721205ms 232.725573ms 235.24593ms 236.333515ms 241.360584ms 246.22807ms 246.785546ms 247.358829ms 248.55735ms 249.532217ms 249.562623ms 250.282869ms 251.558554ms 251.621394ms 253.285304ms 254.265369ms 256.595793ms 259.534354ms 261.244688ms 263.612993ms 265.065247ms 265.222852ms 265.560198ms 266.107645ms 267.08664ms 268.702652ms 269.63128ms 269.836284ms 270.638482ms 271.710958ms 272.187524ms 273.083566ms 275.884229ms 276.768878ms 284.260621ms 286.84016ms 290.388762ms 291.845249ms 301.424193ms 302.163881ms 302.672505ms 302.996517ms 307.694063ms 308.839488ms 311.394374ms 314.215932ms 318.165104ms 318.533521ms 319.325777ms 322.448339ms 324.679475ms 325.433509ms 326.718837ms 326.744755ms 328.599129ms 328.795479ms 329.904408ms 331.419413ms 332.260658ms 333.623191ms 335.220084ms 338.860897ms 341.218988ms 341.738274ms 343.018125ms 343.290064ms 343.760686ms 344.149664ms 345.579521ms 346.017123ms 348.54565ms 348.751898ms 353.122129ms 353.361835ms 354.855182ms 358.175655ms 358.718229ms 359.255944ms 359.844958ms 360.056625ms 361.037244ms 365.203382ms 367.66176ms 368.359923ms 369.048671ms 371.576471ms 371.850717ms 374.608735ms 376.321194ms 376.412629ms 376.466052ms 377.923209ms 379.17739ms 379.411747ms 380.901263ms 382.81594ms 384.583884ms 385.422077ms 386.141088ms 389.081891ms 398.548643ms 400.066515ms 400.881986ms 405.790538ms 406.357063ms 408.109719ms 410.859325ms 414.862859ms 417.004306ms 417.212082ms 419.674603ms 419.758424ms 421.766452ms 423.892242ms 423.953375ms 425.141436ms 425.587901ms 430.521222ms 431.724737ms 432.19632ms 433.669908ms 436.241467ms 436.882272ms 439.603012ms 440.48825ms 440.769592ms 443.849631ms 446.294672ms 446.55848ms 446.672476ms 447.336191ms 451.848057ms 453.25603ms 454.593219ms 455.134158ms 456.160269ms 458.529418ms 459.537586ms 463.782098ms 465.506947ms 470.503424ms 476.651224ms 478.52223ms 480.595909ms 480.74212ms 484.17461ms 497.421374ms 499.758872ms 501.985673ms 510.643928ms 514.280663ms 525.335981ms 527.941748ms 529.676953ms 545.616982ms 559.745491ms 582.1887ms 587.101237ms 597.197253ms 609.05693ms]
Mar  2 22:53:59.241: INFO: 50 %ile: 335.220084ms
Mar  2 22:53:59.241: INFO: 90 %ile: 470.503424ms
Mar  2 22:53:59.241: INFO: 99 %ile: 597.197253ms
Mar  2 22:53:59.242: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:53:59.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6585" for this suite.

• [SLOW TEST:8.388 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":213,"skipped":3738,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:53:59.284: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:54:00.038: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:54:02.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858440, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858440, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858440, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858440, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:54:05.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:54:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-149-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:54:06.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3743" for this suite.
STEP: Destroying namespace "webhook-3743-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.613 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":214,"skipped":3749,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:54:06.898: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:54:07.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6799" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3784,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:54:07.396: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar  2 22:54:07.669: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:54:14.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8539" for this suite.

• [SLOW TEST:6.907 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":216,"skipped":3809,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:54:14.303: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:54:14.515: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 22:54:24.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-296 --namespace=crd-publish-openapi-296 create -f -'
Mar  2 22:54:25.961: INFO: stderr: ""
Mar  2 22:54:25.961: INFO: stdout: "e2e-test-crd-publish-openapi-8167-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 22:54:25.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-296 --namespace=crd-publish-openapi-296 delete e2e-test-crd-publish-openapi-8167-crds test-cr'
Mar  2 22:54:26.154: INFO: stderr: ""
Mar  2 22:54:26.154: INFO: stdout: "e2e-test-crd-publish-openapi-8167-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  2 22:54:26.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-296 --namespace=crd-publish-openapi-296 apply -f -'
Mar  2 22:54:26.757: INFO: stderr: ""
Mar  2 22:54:26.757: INFO: stdout: "e2e-test-crd-publish-openapi-8167-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 22:54:26.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-296 --namespace=crd-publish-openapi-296 delete e2e-test-crd-publish-openapi-8167-crds test-cr'
Mar  2 22:54:26.979: INFO: stderr: ""
Mar  2 22:54:26.979: INFO: stdout: "e2e-test-crd-publish-openapi-8167-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  2 22:54:26.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-296 explain e2e-test-crd-publish-openapi-8167-crds'
Mar  2 22:54:27.699: INFO: stderr: ""
Mar  2 22:54:27.699: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8167-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:54:36.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-296" for this suite.

• [SLOW TEST:22.703 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":217,"skipped":3817,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:54:37.012: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6740
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Mar  2 22:54:37.358: INFO: Found 0 stateful pods, waiting for 3
Mar  2 22:54:47.370: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:54:47.371: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:54:47.371: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:54:47.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-6740 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:54:47.847: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:54:47.847: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:54:47.847: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  2 22:54:57.953: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  2 22:55:08.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-6740 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:55:08.458: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:55:08.458: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:55:08.458: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:55:18.548: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:55:18.548: INFO: Waiting for Pod statefulset-6740/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:55:18.548: INFO: Waiting for Pod statefulset-6740/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:55:28.573: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:55:28.573: INFO: Waiting for Pod statefulset-6740/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:55:28.573: INFO: Waiting for Pod statefulset-6740/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:55:38.576: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:55:38.576: INFO: Waiting for Pod statefulset-6740/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:55:38.576: INFO: Waiting for Pod statefulset-6740/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:55:48.569: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:55:48.569: INFO: Waiting for Pod statefulset-6740/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:55:48.569: INFO: Waiting for Pod statefulset-6740/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:55:58.572: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:55:58.572: INFO: Waiting for Pod statefulset-6740/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:56:08.572: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:56:08.573: INFO: Waiting for Pod statefulset-6740/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:56:18.569: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:56:28.579: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  2 22:56:38.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-6740 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:56:39.026: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:56:39.026: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:56:39.026: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:56:49.107: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  2 22:56:59.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-6740 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:56:59.618: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:56:59.618: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:56:59.618: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:57:09.678: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:57:09.678: INFO: Waiting for Pod statefulset-6740/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 22:57:09.678: INFO: Waiting for Pod statefulset-6740/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 22:57:19.700: INFO: Waiting for StatefulSet statefulset-6740/ss2 to complete update
Mar  2 22:57:19.700: INFO: Waiting for Pod statefulset-6740/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 22:57:29.699: INFO: Deleting all statefulset in ns statefulset-6740
Mar  2 22:57:29.710: INFO: Scaling statefulset ss2 to 0
Mar  2 22:57:59.767: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:57:59.775: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:57:59.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6740" for this suite.

• [SLOW TEST:202.859 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":218,"skipped":3835,"failed":0}
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:57:59.873: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:58:00.071: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Creating first CR 
Mar  2 22:58:00.767: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T22:58:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T22:58:00Z]] name:name1 resourceVersion:105351 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1eaf7c3f-a2b0-4a46-85f7-30e4b023bee8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar  2 22:58:10.791: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T22:58:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T22:58:10Z]] name:name2 resourceVersion:105512 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21659431-5a49-4b89-9340-8ff0374eb138] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar  2 22:58:20.818: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T22:58:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T22:58:20Z]] name:name1 resourceVersion:105558 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1eaf7c3f-a2b0-4a46-85f7-30e4b023bee8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar  2 22:58:30.841: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T22:58:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T22:58:30Z]] name:name2 resourceVersion:105637 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21659431-5a49-4b89-9340-8ff0374eb138] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar  2 22:58:40.870: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T22:58:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T22:58:20Z]] name:name1 resourceVersion:105742 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1eaf7c3f-a2b0-4a46-85f7-30e4b023bee8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar  2 22:58:50.941: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T22:58:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T22:58:30Z]] name:name2 resourceVersion:105790 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21659431-5a49-4b89-9340-8ff0374eb138] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:59:01.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5643" for this suite.

• [SLOW TEST:61.642 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":219,"skipped":3835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:59:01.516: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Mar  2 22:59:01.768: INFO: created test-podtemplate-1
Mar  2 22:59:01.812: INFO: created test-podtemplate-2
Mar  2 22:59:01.845: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar  2 22:59:01.881: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar  2 22:59:01.972: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:59:01.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7800" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":220,"skipped":3861,"failed":0}
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:59:02.049: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  2 22:59:02.325: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Mar  2 22:59:03.140: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  2 22:59:05.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:07.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:09.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:11.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:13.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:15.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:17.408: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:19.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:21.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:23.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:25.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:27.404: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:29.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858743, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:59:33.917: INFO: Waited 2.479100032s for the sample-apiserver to be ready to handle requests.
I0302 22:59:35.084056      25 request.go:655] Throttling request took 1.00068039s, request: GET:https://172.21.0.1:443/apis/k8s.cni.cncf.io/v1?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:59:36.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8898" for this suite.

• [SLOW TEST:34.784 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":221,"skipped":3863,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:59:36.835: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar  2 22:59:41.242: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8067 PodName:pod-sharedvolume-c1816e67-253e-4f2d-bbff-983703498683 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 22:59:41.242: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 22:59:41.532: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:59:41.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8067" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":222,"skipped":3889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:59:41.581: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Mar  2 22:59:41.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  2 22:59:41.986: INFO: stderr: ""
Mar  2 22:59:41.986: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Mar  2 22:59:41.986: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  2 22:59:41.986: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7102" to be "running and ready, or succeeded"
Mar  2 22:59:41.998: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.53678ms
Mar  2 22:59:44.010: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.024592411s
Mar  2 22:59:44.010: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  2 22:59:44.010: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar  2 22:59:44.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 logs logs-generator logs-generator'
Mar  2 22:59:44.271: INFO: stderr: ""
Mar  2 22:59:44.272: INFO: stdout: "I0302 22:59:43.607214       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/j48n 459\nI0302 22:59:43.806671       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/9v2 320\nI0302 22:59:44.006679       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/p954 506\nI0302 22:59:44.206576       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/fdl2 294\n"
Mar  2 22:59:46.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 logs logs-generator logs-generator'
Mar  2 22:59:46.670: INFO: stderr: ""
Mar  2 22:59:46.670: INFO: stdout: "I0302 22:59:43.607214       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/j48n 459\nI0302 22:59:43.806671       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/9v2 320\nI0302 22:59:44.006679       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/p954 506\nI0302 22:59:44.206576       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/fdl2 294\nI0302 22:59:44.406698       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/n4ts 240\nI0302 22:59:44.606615       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/mkk4 309\nI0302 22:59:44.806611       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/cb9 351\nI0302 22:59:45.006534       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/5wnm 561\nI0302 22:59:45.206611       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/9tg 386\nI0302 22:59:45.406614       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/dmp 255\nI0302 22:59:45.606703       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/5v2 538\nI0302 22:59:45.807769       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/77t 484\nI0302 22:59:46.006672       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/msj7 417\nI0302 22:59:46.206693       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/vnx 297\nI0302 22:59:46.406631       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/xhk7 245\nI0302 22:59:46.606519       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/8zng 457\n"
STEP: limiting log lines
Mar  2 22:59:46.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 logs logs-generator logs-generator --tail=1'
Mar  2 22:59:46.925: INFO: stderr: ""
Mar  2 22:59:46.925: INFO: stdout: "I0302 22:59:46.806671       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/mz6 360\n"
Mar  2 22:59:46.925: INFO: got output "I0302 22:59:46.806671       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/mz6 360\n"
STEP: limiting log bytes
Mar  2 22:59:46.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 logs logs-generator logs-generator --limit-bytes=1'
Mar  2 22:59:47.098: INFO: stderr: ""
Mar  2 22:59:47.098: INFO: stdout: "I"
Mar  2 22:59:47.098: INFO: got output "I"
STEP: exposing timestamps
Mar  2 22:59:47.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  2 22:59:47.365: INFO: stderr: ""
Mar  2 22:59:47.365: INFO: stdout: "2022-03-02T16:59:47.206742641-06:00 I0302 22:59:47.206608       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/qxv4 457\n"
Mar  2 22:59:47.365: INFO: got output "2022-03-02T16:59:47.206742641-06:00 I0302 22:59:47.206608       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/qxv4 457\n"
STEP: restricting to a time range
Mar  2 22:59:49.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 logs logs-generator logs-generator --since=1s'
Mar  2 22:59:50.078: INFO: stderr: ""
Mar  2 22:59:50.078: INFO: stdout: "I0302 22:59:49.206922       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/k8p 518\nI0302 22:59:49.406523       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/dck 588\nI0302 22:59:49.606542       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/bchq 576\nI0302 22:59:49.806601       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/rvjr 333\nI0302 22:59:50.006540       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/97xn 292\n"
Mar  2 22:59:50.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 logs logs-generator logs-generator --since=24h'
Mar  2 22:59:50.294: INFO: stderr: ""
Mar  2 22:59:50.294: INFO: stdout: "I0302 22:59:43.607214       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/j48n 459\nI0302 22:59:43.806671       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/9v2 320\nI0302 22:59:44.006679       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/p954 506\nI0302 22:59:44.206576       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/fdl2 294\nI0302 22:59:44.406698       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/n4ts 240\nI0302 22:59:44.606615       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/mkk4 309\nI0302 22:59:44.806611       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/cb9 351\nI0302 22:59:45.006534       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/5wnm 561\nI0302 22:59:45.206611       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/9tg 386\nI0302 22:59:45.406614       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/dmp 255\nI0302 22:59:45.606703       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/5v2 538\nI0302 22:59:45.807769       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/77t 484\nI0302 22:59:46.006672       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/msj7 417\nI0302 22:59:46.206693       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/vnx 297\nI0302 22:59:46.406631       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/xhk7 245\nI0302 22:59:46.606519       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/8zng 457\nI0302 22:59:46.806671       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/mz6 360\nI0302 22:59:47.006670       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/8rg9 325\nI0302 22:59:47.206608       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/qxv4 457\nI0302 22:59:47.406602       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/fcc 402\nI0302 22:59:47.606587       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/z9x 564\nI0302 22:59:47.806682       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/9fk 366\nI0302 22:59:48.006475       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/5vm 449\nI0302 22:59:48.207047       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/2mmp 467\nI0302 22:59:48.406665       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/5d5 299\nI0302 22:59:48.606524       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/x4cq 496\nI0302 22:59:48.806612       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/6nvd 203\nI0302 22:59:49.007641       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/khh 513\nI0302 22:59:49.206922       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/k8p 518\nI0302 22:59:49.406523       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/dck 588\nI0302 22:59:49.606542       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/bchq 576\nI0302 22:59:49.806601       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/rvjr 333\nI0302 22:59:50.006540       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/97xn 292\nI0302 22:59:50.206652       1 logs_generator.go:76] 33 GET /api/v1/namespaces/kube-system/pods/tzqc 216\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Mar  2 22:59:50.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-7102 delete pod logs-generator'
Mar  2 22:59:56.995: INFO: stderr: ""
Mar  2 22:59:56.995: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 22:59:56.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7102" for this suite.

• [SLOW TEST:15.476 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":223,"skipped":3912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 22:59:57.058: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 22:59:57.327: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  2 22:59:57.353: INFO: Number of nodes with available pods: 0
Mar  2 22:59:57.353: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  2 22:59:57.427: INFO: Number of nodes with available pods: 0
Mar  2 22:59:57.427: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 22:59:58.438: INFO: Number of nodes with available pods: 0
Mar  2 22:59:58.438: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 22:59:59.448: INFO: Number of nodes with available pods: 0
Mar  2 22:59:59.449: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:00.439: INFO: Number of nodes with available pods: 1
Mar  2 23:00:00.439: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  2 23:00:00.533: INFO: Number of nodes with available pods: 1
Mar  2 23:00:00.533: INFO: Number of running nodes: 0, number of available pods: 1
Mar  2 23:00:01.549: INFO: Number of nodes with available pods: 0
Mar  2 23:00:01.549: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  2 23:00:01.587: INFO: Number of nodes with available pods: 0
Mar  2 23:00:01.587: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:02.610: INFO: Number of nodes with available pods: 0
Mar  2 23:00:02.611: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:03.600: INFO: Number of nodes with available pods: 0
Mar  2 23:00:03.600: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:04.636: INFO: Number of nodes with available pods: 0
Mar  2 23:00:04.636: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:05.601: INFO: Number of nodes with available pods: 0
Mar  2 23:00:05.601: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:06.629: INFO: Number of nodes with available pods: 0
Mar  2 23:00:06.629: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:07.601: INFO: Number of nodes with available pods: 0
Mar  2 23:00:07.601: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:08.600: INFO: Number of nodes with available pods: 0
Mar  2 23:00:08.601: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:09.628: INFO: Number of nodes with available pods: 0
Mar  2 23:00:09.628: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:00:10.632: INFO: Number of nodes with available pods: 1
Mar  2 23:00:10.632: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6877, will wait for the garbage collector to delete the pods
Mar  2 23:00:10.794: INFO: Deleting DaemonSet.extensions daemon-set took: 36.680512ms
Mar  2 23:00:10.995: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.482295ms
Mar  2 23:00:17.412: INFO: Number of nodes with available pods: 0
Mar  2 23:00:17.424: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 23:00:17.435: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6877/daemonsets","resourceVersion":"106745"},"items":null}

Mar  2 23:00:17.443: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6877/pods","resourceVersion":"106745"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:00:17.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6877" for this suite.

• [SLOW TEST:20.535 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":224,"skipped":3946,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:00:17.602: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-1da6f1ba-cfbc-4124-807c-4fd161823d24
STEP: Creating a pod to test consume secrets
Mar  2 23:00:18.125: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f270a9ad-2054-462d-8e2a-de7023d9406c" in namespace "projected-7253" to be "Succeeded or Failed"
Mar  2 23:00:18.155: INFO: Pod "pod-projected-secrets-f270a9ad-2054-462d-8e2a-de7023d9406c": Phase="Pending", Reason="", readiness=false. Elapsed: 29.778009ms
Mar  2 23:00:20.185: INFO: Pod "pod-projected-secrets-f270a9ad-2054-462d-8e2a-de7023d9406c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.060053049s
STEP: Saw pod success
Mar  2 23:00:20.185: INFO: Pod "pod-projected-secrets-f270a9ad-2054-462d-8e2a-de7023d9406c" satisfied condition "Succeeded or Failed"
Mar  2 23:00:20.200: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-secrets-f270a9ad-2054-462d-8e2a-de7023d9406c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:00:20.304: INFO: Waiting for pod pod-projected-secrets-f270a9ad-2054-462d-8e2a-de7023d9406c to disappear
Mar  2 23:00:20.314: INFO: Pod pod-projected-secrets-f270a9ad-2054-462d-8e2a-de7023d9406c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:00:20.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7253" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":225,"skipped":4012,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:00:20.381: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:00:20.641: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:00:21.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8718" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":226,"skipped":4015,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:00:22.042: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  2 23:00:22.491: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9751 /api/v1/namespaces/watch-9751/configmaps/e2e-watch-test-watch-closed d2f1a255-da00-449a-8c4a-9b6301b38dde 106897 0 2022-03-02 23:00:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-02 23:00:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:00:22.491: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9751 /api/v1/namespaces/watch-9751/configmaps/e2e-watch-test-watch-closed d2f1a255-da00-449a-8c4a-9b6301b38dde 106902 0 2022-03-02 23:00:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-02 23:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  2 23:00:22.623: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9751 /api/v1/namespaces/watch-9751/configmaps/e2e-watch-test-watch-closed d2f1a255-da00-449a-8c4a-9b6301b38dde 106903 0 2022-03-02 23:00:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-02 23:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:00:22.624: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9751 /api/v1/namespaces/watch-9751/configmaps/e2e-watch-test-watch-closed d2f1a255-da00-449a-8c4a-9b6301b38dde 106907 0 2022-03-02 23:00:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-02 23:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:00:22.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9751" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":227,"skipped":4023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:00:22.673: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:00:23.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3804" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":228,"skipped":4067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:00:23.091: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:00:23.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7211" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":229,"skipped":4108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:00:23.398: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-8757e3ea-4901-4bfa-85fa-f71911619d6f
STEP: Creating a pod to test consume secrets
Mar  2 23:00:23.688: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-958fb14a-e575-4c74-9b5b-a3d4796a728c" in namespace "projected-1916" to be "Succeeded or Failed"
Mar  2 23:00:23.735: INFO: Pod "pod-projected-secrets-958fb14a-e575-4c74-9b5b-a3d4796a728c": Phase="Pending", Reason="", readiness=false. Elapsed: 46.945514ms
Mar  2 23:00:25.772: INFO: Pod "pod-projected-secrets-958fb14a-e575-4c74-9b5b-a3d4796a728c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083432244s
Mar  2 23:00:27.794: INFO: Pod "pod-projected-secrets-958fb14a-e575-4c74-9b5b-a3d4796a728c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.105793829s
STEP: Saw pod success
Mar  2 23:00:27.795: INFO: Pod "pod-projected-secrets-958fb14a-e575-4c74-9b5b-a3d4796a728c" satisfied condition "Succeeded or Failed"
Mar  2 23:00:27.822: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-secrets-958fb14a-e575-4c74-9b5b-a3d4796a728c container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:00:27.892: INFO: Waiting for pod pod-projected-secrets-958fb14a-e575-4c74-9b5b-a3d4796a728c to disappear
Mar  2 23:00:27.967: INFO: Pod pod-projected-secrets-958fb14a-e575-4c74-9b5b-a3d4796a728c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:00:27.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1916" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":230,"skipped":4134,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:00:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:00:28.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3072" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":231,"skipped":4135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:00:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-29a85bcf-6b7a-46eb-850e-85f5a2e0aa93 in namespace container-probe-5996
Mar  2 23:00:32.813: INFO: Started pod liveness-29a85bcf-6b7a-46eb-850e-85f5a2e0aa93 in namespace container-probe-5996
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 23:00:32.823: INFO: Initial restart count of pod liveness-29a85bcf-6b7a-46eb-850e-85f5a2e0aa93 is 0
Mar  2 23:00:42.921: INFO: Restart count of pod container-probe-5996/liveness-29a85bcf-6b7a-46eb-850e-85f5a2e0aa93 is now 1 (10.097873085s elapsed)
Mar  2 23:01:03.070: INFO: Restart count of pod container-probe-5996/liveness-29a85bcf-6b7a-46eb-850e-85f5a2e0aa93 is now 2 (30.247119798s elapsed)
Mar  2 23:01:23.190: INFO: Restart count of pod container-probe-5996/liveness-29a85bcf-6b7a-46eb-850e-85f5a2e0aa93 is now 3 (50.366820502s elapsed)
Mar  2 23:01:43.344: INFO: Restart count of pod container-probe-5996/liveness-29a85bcf-6b7a-46eb-850e-85f5a2e0aa93 is now 4 (1m10.521569783s elapsed)
Mar  2 23:02:45.917: INFO: Restart count of pod container-probe-5996/liveness-29a85bcf-6b7a-46eb-850e-85f5a2e0aa93 is now 5 (2m13.094409967s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:02:45.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5996" for this suite.

• [SLOW TEST:137.556 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4190,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:02:46.031: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:02:46.238: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  2 23:02:47.484: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:02:48.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5555" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":233,"skipped":4191,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:02:48.601: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 23:02:50.879: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:02:50.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1929" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":4200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:02:51.012: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0302 23:02:52.581128      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0302 23:02:52.581197      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0302 23:02:52.581224      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 23:02:52.581: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:02:52.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4741" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":235,"skipped":4232,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:02:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 23:02:53.646: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 23:02:55.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858973, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858973, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858973, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781858973, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 23:02:58.745: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:02:58.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6205" for this suite.
STEP: Destroying namespace "webhook-6205-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.545 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":236,"skipped":4242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:02:59.200: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:03:03.540: INFO: Waiting up to 5m0s for pod "client-envvars-bf8e9a21-93cd-45c4-a825-b3d49f731b1c" in namespace "pods-725" to be "Succeeded or Failed"
Mar  2 23:03:03.550: INFO: Pod "client-envvars-bf8e9a21-93cd-45c4-a825-b3d49f731b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.903899ms
Mar  2 23:03:05.564: INFO: Pod "client-envvars-bf8e9a21-93cd-45c4-a825-b3d49f731b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023970583s
Mar  2 23:03:07.575: INFO: Pod "client-envvars-bf8e9a21-93cd-45c4-a825-b3d49f731b1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035128113s
STEP: Saw pod success
Mar  2 23:03:07.575: INFO: Pod "client-envvars-bf8e9a21-93cd-45c4-a825-b3d49f731b1c" satisfied condition "Succeeded or Failed"
Mar  2 23:03:07.586: INFO: Trying to get logs from node 10.138.244.159 pod client-envvars-bf8e9a21-93cd-45c4-a825-b3d49f731b1c container env3cont: <nil>
STEP: delete the pod
Mar  2 23:03:07.744: INFO: Waiting for pod client-envvars-bf8e9a21-93cd-45c4-a825-b3d49f731b1c to disappear
Mar  2 23:03:07.770: INFO: Pod client-envvars-bf8e9a21-93cd-45c4-a825-b3d49f731b1c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:03:07.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-725" for this suite.

• [SLOW TEST:8.610 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4284,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:03:07.811: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar  2 23:03:08.058: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:03:12.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3047" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":238,"skipped":4287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:03:12.088: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-a30608a8-c840-4e38-94bc-56ad611bbefd
Mar  2 23:03:12.354: INFO: Pod name my-hostname-basic-a30608a8-c840-4e38-94bc-56ad611bbefd: Found 0 pods out of 1
Mar  2 23:03:17.366: INFO: Pod name my-hostname-basic-a30608a8-c840-4e38-94bc-56ad611bbefd: Found 1 pods out of 1
Mar  2 23:03:17.366: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a30608a8-c840-4e38-94bc-56ad611bbefd" are running
Mar  2 23:03:17.380: INFO: Pod "my-hostname-basic-a30608a8-c840-4e38-94bc-56ad611bbefd-ql7kh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 23:03:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 23:03:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 23:03:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 23:03:12 +0000 UTC Reason: Message:}])
Mar  2 23:03:17.382: INFO: Trying to dial the pod
Mar  2 23:03:22.484: INFO: Controller my-hostname-basic-a30608a8-c840-4e38-94bc-56ad611bbefd: Got expected result from replica 1 [my-hostname-basic-a30608a8-c840-4e38-94bc-56ad611bbefd-ql7kh]: "my-hostname-basic-a30608a8-c840-4e38-94bc-56ad611bbefd-ql7kh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:03:22.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8" for this suite.

• [SLOW TEST:10.597 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":239,"skipped":4311,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:03:22.687: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  2 23:03:22.897: INFO: Waiting up to 5m0s for pod "pod-4e826e50-64b0-4833-aba7-697a9a04486e" in namespace "emptydir-3206" to be "Succeeded or Failed"
Mar  2 23:03:22.909: INFO: Pod "pod-4e826e50-64b0-4833-aba7-697a9a04486e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.328667ms
Mar  2 23:03:24.922: INFO: Pod "pod-4e826e50-64b0-4833-aba7-697a9a04486e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024900433s
Mar  2 23:03:26.935: INFO: Pod "pod-4e826e50-64b0-4833-aba7-697a9a04486e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037436145s
STEP: Saw pod success
Mar  2 23:03:26.935: INFO: Pod "pod-4e826e50-64b0-4833-aba7-697a9a04486e" satisfied condition "Succeeded or Failed"
Mar  2 23:03:26.945: INFO: Trying to get logs from node 10.138.244.159 pod pod-4e826e50-64b0-4833-aba7-697a9a04486e container test-container: <nil>
STEP: delete the pod
Mar  2 23:03:27.014: INFO: Waiting for pod pod-4e826e50-64b0-4833-aba7-697a9a04486e to disappear
Mar  2 23:03:27.024: INFO: Pod pod-4e826e50-64b0-4833-aba7-697a9a04486e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:03:27.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3206" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4322,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:03:27.065: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:03:27.207: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 23:03:36.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-686 --namespace=crd-publish-openapi-686 create -f -'
Mar  2 23:03:37.813: INFO: stderr: ""
Mar  2 23:03:37.813: INFO: stdout: "e2e-test-crd-publish-openapi-4013-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 23:03:37.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-686 --namespace=crd-publish-openapi-686 delete e2e-test-crd-publish-openapi-4013-crds test-cr'
Mar  2 23:03:38.014: INFO: stderr: ""
Mar  2 23:03:38.014: INFO: stdout: "e2e-test-crd-publish-openapi-4013-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  2 23:03:38.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-686 --namespace=crd-publish-openapi-686 apply -f -'
Mar  2 23:03:38.581: INFO: stderr: ""
Mar  2 23:03:38.581: INFO: stdout: "e2e-test-crd-publish-openapi-4013-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 23:03:38.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-686 --namespace=crd-publish-openapi-686 delete e2e-test-crd-publish-openapi-4013-crds test-cr'
Mar  2 23:03:38.773: INFO: stderr: ""
Mar  2 23:03:38.773: INFO: stdout: "e2e-test-crd-publish-openapi-4013-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  2 23:03:38.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=crd-publish-openapi-686 explain e2e-test-crd-publish-openapi-4013-crds'
Mar  2 23:03:39.537: INFO: stderr: ""
Mar  2 23:03:39.537: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4013-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:03:48.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-686" for this suite.

• [SLOW TEST:21.442 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":241,"skipped":4323,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:03:48.508: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 23:03:48.733: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 23:03:48.772: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 23:03:48.795: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.150 before test
Mar  2 23:03:48.864: INFO: calico-kube-controllers-64bc47d78c-r9h75 from calico-system started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 23:03:48.864: INFO: calico-node-sfwd8 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 23:03:48.864: INFO: calico-typha-747778ff7-7mfzl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 23:03:48.864: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-03-02 20:09:15 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  2 23:03:48.864: INFO: managed-storage-validation-webhooks-7d645c9954-9tz2t from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 23:03:48.864: INFO: managed-storage-validation-webhooks-7d645c9954-fnvqt from ibm-odf-validation-webhook started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 23:03:48.864: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-xx9p2 from ibm-system started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 23:03:48.864: INFO: ibm-file-plugin-58c5ccc6d4-mhk8g from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 23:03:48.864: INFO: ibm-keepalived-watcher-lzm26 from kube-system started at 2022-03-02 20:04:31 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 23:03:48.864: INFO: ibm-master-proxy-static-10.138.244.150 from kube-system started at 2022-03-02 20:04:27 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 23:03:48.864: INFO: 	Container pause ready: true, restart count 0
Mar  2 23:03:48.864: INFO: ibmcloud-block-storage-driver-n9lmx from kube-system started at 2022-03-02 20:04:35 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 23:03:48.864: INFO: ibmcloud-block-storage-plugin-67c5f49db6-wlj2j from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 23:03:48.864: INFO: vpn-7bf7499b46-dvdqm from kube-system started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container vpn ready: true, restart count 0
Mar  2 23:03:48.864: INFO: tuned-fqxdn from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:03:48.864: INFO: cluster-samples-operator-69fbcc775-4zjvg from openshift-cluster-samples-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 23:03:48.864: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 23:03:48.864: INFO: cluster-storage-operator-56787c5bcb-9f5hs from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 23:03:48.864: INFO: csi-snapshot-controller-69454d4b56-gwpsw from openshift-cluster-storage-operator started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 23:03:48.864: INFO: csi-snapshot-controller-operator-6977df7ddd-dkdjh from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 23:03:48.864: INFO: csi-snapshot-webhook-788b7d55dd-2rmtk from openshift-cluster-storage-operator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.864: INFO: 	Container webhook ready: true, restart count 0
Mar  2 23:03:48.865: INFO: console-78b4c56c55-ndmlz from openshift-console started at 2022-03-02 20:13:37 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container console ready: true, restart count 0
Mar  2 23:03:48.865: INFO: downloads-79b8c4c9f8-hqr26 from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container download-server ready: true, restart count 0
Mar  2 23:03:48.865: INFO: dns-default-jf6bp from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: cluster-image-registry-operator-58888444b7-jsm7m from openshift-image-registry started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 23:03:48.865: INFO: image-registry-78c5f597d5-rfdtq from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container registry ready: true, restart count 0
Mar  2 23:03:48.865: INFO: node-ca-2d6rp from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:03:48.865: INFO: ingress-canary-wptt6 from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 23:03:48.865: INFO: router-default-678545d6db-sgrbg from openshift-ingress started at 2022-03-02 22:12:55 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container router ready: true, restart count 0
Mar  2 23:03:48.865: INFO: openshift-kube-proxy-9mnl6 from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: kube-storage-version-migrator-operator-68756f7898-kvfhq from openshift-kube-storage-version-migrator-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 23:03:48.865: INFO: migrator-76bc956454-5h92l from openshift-kube-storage-version-migrator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container migrator ready: true, restart count 0
Mar  2 23:03:48.865: INFO: certified-operators-4lfsq from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 23:03:48.865: INFO: community-operators-fj9gp from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 23:03:48.865: INFO: marketplace-operator-8689886944-xlcrw from openshift-marketplace started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 23:03:48.865: INFO: redhat-marketplace-tvwc6 from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 23:03:48.865: INFO: redhat-operators-87vss from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 23:03:48.865: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-03-02 22:30:17 +0000 UTC (5 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: cluster-monitoring-operator-668f4b779-pt5v7 from openshift-monitoring started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container cluster-monitoring-operator ready: true, restart count 1
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Mar  2 23:03:48.865: INFO: grafana-77dc549b6f-nn2b2 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container grafana ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: kube-state-metrics-7b6c6d96b-scdqx from openshift-monitoring started at 2022-03-02 22:30:09 +0000 UTC (3 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 23:03:48.865: INFO: node-exporter-dff24 from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:03:48.865: INFO: openshift-state-metrics-84c4bdd485-5d95q from openshift-monitoring started at 2022-03-02 22:30:10 +0000 UTC (3 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 23:03:48.865: INFO: prometheus-adapter-68858877cc-d2bhz from openshift-monitoring started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 23:03:48.865: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-03-02 22:30:27 +0000 UTC (7 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 23:03:48.865: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 23:03:48.865: INFO: prometheus-operator-7797d58ccd-8kdw9 from openshift-monitoring started at 2022-03-02 22:30:10 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 23:03:48.865: INFO: thanos-querier-544bf8c477-jdvqv from openshift-monitoring started at 2022-03-02 20:07:50 +0000 UTC (5 container statuses recorded)
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 23:03:48.865: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 23:03:48.866: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:03:48.866: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 23:03:48.866: INFO: multus-admission-controller-2wfsf from openshift-multus started at 2022-03-02 20:06:01 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.866: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 23:03:48.866: INFO: multus-zgnwz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:03:48.866: INFO: network-metrics-daemon-qmb7k from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.866: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 23:03:48.866: INFO: network-check-source-758fcf9d76-dnnrz from openshift-network-diagnostics started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 23:03:48.866: INFO: network-check-target-rmfwb from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 23:03:48.866: INFO: network-operator-57496bd6cc-wt8z2 from openshift-network-operator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container network-operator ready: true, restart count 0
Mar  2 23:03:48.866: INFO: catalog-operator-777865977d-2fqfw from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 23:03:48.866: INFO: olm-operator-5cb6cff486-wld6q from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 23:03:48.866: INFO: packageserver-67967c9875-524hv from openshift-operator-lifecycle-manager started at 2022-03-02 20:07:33 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 23:03:48.866: INFO: packageserver-67967c9875-jm8b8 from openshift-operator-lifecycle-manager started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 23:03:48.866: INFO: service-ca-operator-5b668f5895-69p65 from openshift-service-ca-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 23:03:48.866: INFO: service-ca-b874796d6-4g8hj from openshift-service-ca started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container service-ca-controller ready: false, restart count 0
Mar  2 23:03:48.866: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-pl9fl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.866: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 23:03:48.866: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 23:03:48.866: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.159 before test
Mar  2 23:03:48.915: INFO: calico-node-fhc29 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.915: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 23:03:48.915: INFO: calico-typha-747778ff7-sgxvl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.916: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 23:03:48.916: INFO: ibm-keepalived-watcher-drf4g from kube-system started at 2022-03-02 20:04:20 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.916: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 23:03:48.916: INFO: ibm-master-proxy-static-10.138.244.159 from kube-system started at 2022-03-02 20:04:17 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.916: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 23:03:48.916: INFO: 	Container pause ready: true, restart count 0
Mar  2 23:03:48.916: INFO: ibmcloud-block-storage-driver-85npp from kube-system started at 2022-03-02 20:04:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.917: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 23:03:48.917: INFO: tuned-xv4r5 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.917: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:03:48.917: INFO: dns-default-224cd from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 23:03:48.917: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:03:48.917: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:03:48.917: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.918: INFO: node-ca-bc4lw from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.918: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:03:48.918: INFO: registry-pvc-permissions-kstcw from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.918: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 23:03:48.918: INFO: ingress-canary-gg9t2 from openshift-ingress-canary started at 2022-03-02 22:30:37 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.918: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 23:03:48.918: INFO: openshift-kube-proxy-lxhnl from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.919: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 23:03:48.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.919: INFO: node-exporter-t8msj from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.919: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:03:48.919: INFO: multus-58kxw from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.920: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:03:48.920: INFO: multus-admission-controller-42x25 from openshift-multus started at 2022-03-02 22:30:47 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.920: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 23:03:48.920: INFO: network-metrics-daemon-4hdhq from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.920: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 23:03:48.920: INFO: network-check-target-c4pgw from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.920: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 23:03:48.920: INFO: sonobuoy from sonobuoy started at 2022-03-02 21:45:05 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.920: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 23:03:48.920: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-xczvl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 23:03:48.920: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 23:03:48.920: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.162 before test
Mar  2 23:03:48.970: INFO: calico-node-8mbgb from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 23:03:48.970: INFO: calico-typha-747778ff7-cxk8p from calico-system started at 2022-03-02 20:05:22 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 23:03:48.970: INFO: managed-storage-validation-webhooks-7d645c9954-hxlwh from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 23:03:48.970: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-zj24d from ibm-system started at 2022-03-02 22:12:55 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 23:03:48.970: INFO: ibm-keepalived-watcher-g4r68 from kube-system started at 2022-03-02 20:04:32 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 23:03:48.970: INFO: ibm-master-proxy-static-10.138.244.162 from kube-system started at 2022-03-02 20:04:29 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 23:03:48.970: INFO: 	Container pause ready: true, restart count 0
Mar  2 23:03:48.970: INFO: ibm-storage-metrics-agent-5686f759d8-6tq4n from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 23:03:48.970: INFO: ibm-storage-watcher-6488446f7b-nmfdr from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 23:03:48.970: INFO: ibmcloud-block-storage-driver-bz98p from kube-system started at 2022-03-02 20:04:36 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 23:03:48.970: INFO: cluster-node-tuning-operator-5cb4c7f989-xchxd from openshift-cluster-node-tuning-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 23:03:48.970: INFO: tuned-krv52 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:03:48.970: INFO: console-operator-58ff8dbf6b-bz5q9 from openshift-console-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 23:03:48.970: INFO: console-78b4c56c55-kxx8b from openshift-console started at 2022-03-02 20:13:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container console ready: true, restart count 0
Mar  2 23:03:48.970: INFO: downloads-79b8c4c9f8-djqwn from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container download-server ready: true, restart count 0
Mar  2 23:03:48.970: INFO: dns-operator-55f6c97874-bfkv2 from openshift-dns-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 23:03:48.970: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.970: INFO: dns-default-p6v58 from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:03:48.970: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:03:48.970: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.970: INFO: node-ca-bgqw9 from openshift-image-registry started at 2022-03-02 20:07:31 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:03:48.970: INFO: ingress-canary-wsb4b from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 23:03:48.970: INFO: ingress-operator-bc8cf4dbb-pqp88 from openshift-ingress-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.970: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 23:03:48.970: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: router-default-678545d6db-zh9tr from openshift-ingress started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.971: INFO: 	Container router ready: true, restart count 0
Mar  2 23:03:48.971: INFO: openshift-kube-proxy-ks9sn from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.971: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 23:03:48.971: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: node-exporter-gtb4f from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.971: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:03:48.971: INFO: prometheus-adapter-68858877cc-cxjl2 from openshift-monitoring started at 2022-03-02 20:09:30 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.971: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 23:03:48.971: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-03-02 20:14:02 +0000 UTC (7 container statuses recorded)
Mar  2 23:03:48.971: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 23:03:48.971: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 23:03:48.971: INFO: telemeter-client-765c47dd74-4s7d4 from openshift-monitoring started at 2022-03-02 20:06:40 +0000 UTC (3 container statuses recorded)
Mar  2 23:03:48.971: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.971: INFO: 	Container reload ready: true, restart count 0
Mar  2 23:03:48.973: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 23:03:48.973: INFO: thanos-querier-544bf8c477-hzf5x from openshift-monitoring started at 2022-03-02 22:12:55 +0000 UTC (5 container statuses recorded)
Mar  2 23:03:48.973: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.973: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 23:03:48.973: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 23:03:48.973: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:03:48.973: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 23:03:48.973: INFO: multus-admission-controller-ljkq4 from openshift-multus started at 2022-03-02 20:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.973: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.973: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 23:03:48.973: INFO: multus-bkzqz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.973: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:03:48.973: INFO: network-metrics-daemon-2bsdl from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.973: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:03:48.973: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 23:03:48.973: INFO: network-check-target-t499h from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.973: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 23:03:48.973: INFO: metrics-78d8646588-dvms5 from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.973: INFO: 	Container metrics ready: true, restart count 2
Mar  2 23:03:48.973: INFO: push-gateway-7b4b958bfb-dmwzc from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.973: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 23:03:48.973: INFO: sonobuoy-e2e-job-87017147060647d2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.974: INFO: 	Container e2e ready: true, restart count 0
Mar  2 23:03:48.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 23:03:48.974: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-szbl2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 23:03:48.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 23:03:48.974: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 23:03:48.974: INFO: tigera-operator-597f8644c9-8tp9s from tigera-operator started at 2022-03-02 22:12:54 +0000 UTC (1 container statuses recorded)
Mar  2 23:03:48.974: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b388fbd5-6af7-4d42-8dbc-5a648e134e60 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b388fbd5-6af7-4d42-8dbc-5a648e134e60 off the node 10.138.244.159
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b388fbd5-6af7-4d42-8dbc-5a648e134e60
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:03:57.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8615" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:8.927 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":242,"skipped":4327,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:03:57.439: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-5ztr
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 23:03:57.813: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-5ztr" in namespace "subpath-1948" to be "Succeeded or Failed"
Mar  2 23:03:57.831: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Pending", Reason="", readiness=false. Elapsed: 17.835948ms
Mar  2 23:03:59.845: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031745944s
Mar  2 23:04:01.857: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 4.043719881s
Mar  2 23:04:03.868: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 6.05446125s
Mar  2 23:04:05.879: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 8.066143093s
Mar  2 23:04:07.890: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 10.077091365s
Mar  2 23:04:09.904: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 12.090720838s
Mar  2 23:04:11.917: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 14.103494733s
Mar  2 23:04:13.936: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 16.123158431s
Mar  2 23:04:15.948: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 18.134408549s
Mar  2 23:04:17.963: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 20.149553883s
Mar  2 23:04:19.983: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Running", Reason="", readiness=true. Elapsed: 22.170358384s
Mar  2 23:04:21.999: INFO: Pod "pod-subpath-test-downwardapi-5ztr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.185772182s
STEP: Saw pod success
Mar  2 23:04:21.999: INFO: Pod "pod-subpath-test-downwardapi-5ztr" satisfied condition "Succeeded or Failed"
Mar  2 23:04:22.010: INFO: Trying to get logs from node 10.138.244.159 pod pod-subpath-test-downwardapi-5ztr container test-container-subpath-downwardapi-5ztr: <nil>
STEP: delete the pod
Mar  2 23:04:22.132: INFO: Waiting for pod pod-subpath-test-downwardapi-5ztr to disappear
Mar  2 23:04:22.146: INFO: Pod pod-subpath-test-downwardapi-5ztr no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-5ztr
Mar  2 23:04:22.146: INFO: Deleting pod "pod-subpath-test-downwardapi-5ztr" in namespace "subpath-1948"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:04:22.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1948" for this suite.

• [SLOW TEST:24.770 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":243,"skipped":4337,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:04:22.213: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:04:38.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7838" for this suite.

• [SLOW TEST:16.740 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":244,"skipped":4341,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:04:38.953: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 23:04:39.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-2734 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar  2 23:04:39.456: INFO: stderr: ""
Mar  2 23:04:39.456: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar  2 23:04:39.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-2734 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Mar  2 23:04:40.198: INFO: stderr: ""
Mar  2 23:04:40.198: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Mar  2 23:04:40.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-2734 delete pods e2e-test-httpd-pod'
Mar  2 23:04:47.014: INFO: stderr: ""
Mar  2 23:04:47.014: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:04:47.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2734" for this suite.

• [SLOW TEST:8.096 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":245,"skipped":4354,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:04:47.049: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  2 23:04:47.284: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-a 4acfbfde-ffe8-4253-adc2-5e38761af5de 110150 0 2022-03-02 23:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:04:47.285: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-a 4acfbfde-ffe8-4253-adc2-5e38761af5de 110150 0 2022-03-02 23:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  2 23:04:57.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-a 4acfbfde-ffe8-4253-adc2-5e38761af5de 110236 0 2022-03-02 23:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 23:04:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:04:57.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-a 4acfbfde-ffe8-4253-adc2-5e38761af5de 110236 0 2022-03-02 23:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 23:04:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  2 23:05:07.343: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-a 4acfbfde-ffe8-4253-adc2-5e38761af5de 110282 0 2022-03-02 23:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 23:04:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:05:07.343: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-a 4acfbfde-ffe8-4253-adc2-5e38761af5de 110282 0 2022-03-02 23:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 23:04:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  2 23:05:17.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-a 4acfbfde-ffe8-4253-adc2-5e38761af5de 110328 0 2022-03-02 23:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 23:04:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:05:17.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-a 4acfbfde-ffe8-4253-adc2-5e38761af5de 110328 0 2022-03-02 23:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 23:04:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  2 23:05:27.425: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-b 18c9ac3e-f01c-4b94-91b0-d4141c77e2a7 110377 0 2022-03-02 23:05:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-02 23:05:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:05:27.425: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-b 18c9ac3e-f01c-4b94-91b0-d4141c77e2a7 110377 0 2022-03-02 23:05:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-02 23:05:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  2 23:05:37.502: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-b 18c9ac3e-f01c-4b94-91b0-d4141c77e2a7 110423 0 2022-03-02 23:05:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-02 23:05:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:05:37.502: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2937 /api/v1/namespaces/watch-2937/configmaps/e2e-watch-test-configmap-b 18c9ac3e-f01c-4b94-91b0-d4141c77e2a7 110423 0 2022-03-02 23:05:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-02 23:05:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:05:47.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2937" for this suite.

• [SLOW TEST:60.507 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":246,"skipped":4357,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:05:47.557: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3561
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Mar  2 23:05:47.879: INFO: Found 0 stateful pods, waiting for 3
Mar  2 23:05:57.891: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:05:57.891: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:05:57.891: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  2 23:05:57.966: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  2 23:06:08.072: INFO: Updating stateful set ss2
Mar  2 23:06:08.095: INFO: Waiting for Pod statefulset-3561/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar  2 23:06:18.256: INFO: Found 2 stateful pods, waiting for 3
Mar  2 23:06:28.267: INFO: Found 2 stateful pods, waiting for 3
Mar  2 23:06:38.278: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:06:38.278: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:06:38.278: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 23:06:48.289: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:06:48.289: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:06:48.289: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  2 23:06:48.367: INFO: Updating stateful set ss2
Mar  2 23:06:48.407: INFO: Waiting for Pod statefulset-3561/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 23:06:58.503: INFO: Updating stateful set ss2
Mar  2 23:06:58.534: INFO: Waiting for StatefulSet statefulset-3561/ss2 to complete update
Mar  2 23:06:58.534: INFO: Waiting for Pod statefulset-3561/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 23:07:08.558: INFO: Deleting all statefulset in ns statefulset-3561
Mar  2 23:07:08.569: INFO: Scaling statefulset ss2 to 0
Mar  2 23:07:28.638: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:07:28.649: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:07:28.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3561" for this suite.

• [SLOW TEST:101.191 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":247,"skipped":4363,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:07:28.748: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Mar  2 23:07:30.135: INFO: Waiting up to 5m0s for pod "var-expansion-73706352-6bb7-4ae4-84bf-066cd813db12" in namespace "var-expansion-8018" to be "Succeeded or Failed"
Mar  2 23:07:30.149: INFO: Pod "var-expansion-73706352-6bb7-4ae4-84bf-066cd813db12": Phase="Pending", Reason="", readiness=false. Elapsed: 12.76145ms
Mar  2 23:07:32.160: INFO: Pod "var-expansion-73706352-6bb7-4ae4-84bf-066cd813db12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024048073s
Mar  2 23:07:34.184: INFO: Pod "var-expansion-73706352-6bb7-4ae4-84bf-066cd813db12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047745106s
STEP: Saw pod success
Mar  2 23:07:34.184: INFO: Pod "var-expansion-73706352-6bb7-4ae4-84bf-066cd813db12" satisfied condition "Succeeded or Failed"
Mar  2 23:07:34.200: INFO: Trying to get logs from node 10.138.244.159 pod var-expansion-73706352-6bb7-4ae4-84bf-066cd813db12 container dapi-container: <nil>
STEP: delete the pod
Mar  2 23:07:34.347: INFO: Waiting for pod var-expansion-73706352-6bb7-4ae4-84bf-066cd813db12 to disappear
Mar  2 23:07:34.356: INFO: Pod var-expansion-73706352-6bb7-4ae4-84bf-066cd813db12 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:07:34.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8018" for this suite.

• [SLOW TEST:5.642 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4369,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:07:34.391: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7663
Mar  2 23:07:36.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 23:07:37.338: INFO: rc: 7
Mar  2 23:07:37.370: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 23:07:37.382: INFO: Pod kube-proxy-mode-detector no longer exists
Mar  2 23:07:37.382: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-7663
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7663
I0302 23:07:37.454697      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7663, replica count: 3
I0302 23:07:40.505364      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:07:43.506121      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 23:07:43.553: INFO: Creating new exec pod
Mar  2 23:07:46.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Mar  2 23:07:47.074: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 23:07:47.074: INFO: stdout: ""
Mar  2 23:07:47.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c nc -zv -t -w 2 172.21.83.87 80'
Mar  2 23:07:47.523: INFO: stderr: "+ nc -zv -t -w 2 172.21.83.87 80\nConnection to 172.21.83.87 80 port [tcp/http] succeeded!\n"
Mar  2 23:07:47.523: INFO: stdout: ""
Mar  2 23:07:47.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.162 31130'
Mar  2 23:07:47.980: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.162 31130\nConnection to 10.138.244.162 31130 port [tcp/31130] succeeded!\n"
Mar  2 23:07:47.980: INFO: stdout: ""
Mar  2 23:07:47.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.150 31130'
Mar  2 23:07:48.485: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.150 31130\nConnection to 10.138.244.150 31130 port [tcp/31130] succeeded!\n"
Mar  2 23:07:48.485: INFO: stdout: ""
Mar  2 23:07:48.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.211 31130'
Mar  2 23:07:49.012: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.211 31130\nConnection to 168.1.11.211 31130 port [tcp/31130] succeeded!\n"
Mar  2 23:07:49.012: INFO: stdout: ""
Mar  2 23:07:49.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.212 31130'
Mar  2 23:07:49.531: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.212 31130\nConnection to 168.1.11.212 31130 port [tcp/31130] succeeded!\n"
Mar  2 23:07:49.531: INFO: stdout: ""
Mar  2 23:07:49.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.244.150:31130/ ; done'
Mar  2 23:07:50.175: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n"
Mar  2 23:07:50.175: INFO: stdout: "\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c\naffinity-nodeport-timeout-hss2c"
Mar  2 23:07:50.175: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.175: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.175: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.175: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Received response from host: affinity-nodeport-timeout-hss2c
Mar  2 23:07:50.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.138.244.150:31130/'
Mar  2 23:07:50.650: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n"
Mar  2 23:07:50.650: INFO: stdout: "affinity-nodeport-timeout-hss2c"
Mar  2 23:08:10.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.138.244.150:31130/'
Mar  2 23:08:11.073: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n"
Mar  2 23:08:11.073: INFO: stdout: "affinity-nodeport-timeout-hss2c"
Mar  2 23:08:31.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.138.244.150:31130/'
Mar  2 23:08:31.527: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n"
Mar  2 23:08:31.527: INFO: stdout: "affinity-nodeport-timeout-hss2c"
Mar  2 23:08:51.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7663 exec execpod-affinity4f7hv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.138.244.150:31130/'
Mar  2 23:08:51.978: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.138.244.150:31130/\n"
Mar  2 23:08:51.978: INFO: stdout: "affinity-nodeport-timeout-sdfj5"
Mar  2 23:08:51.978: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7663, will wait for the garbage collector to delete the pods
Mar  2 23:08:52.109: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 32.027367ms
Mar  2 23:08:52.309: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.305153ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:09:07.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7663" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:93.123 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":249,"skipped":4372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:09:07.525: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-dtvz
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 23:09:07.812: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dtvz" in namespace "subpath-4732" to be "Succeeded or Failed"
Mar  2 23:09:07.825: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Pending", Reason="", readiness=false. Elapsed: 13.509948ms
Mar  2 23:09:09.837: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024876243s
Mar  2 23:09:11.849: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 4.037386567s
Mar  2 23:09:13.860: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 6.048048427s
Mar  2 23:09:15.872: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 8.060177958s
Mar  2 23:09:17.885: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 10.073656885s
Mar  2 23:09:19.899: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 12.087085518s
Mar  2 23:09:21.910: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 14.097949376s
Mar  2 23:09:23.952: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 16.140131545s
Mar  2 23:09:25.969: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 18.157553982s
Mar  2 23:09:27.984: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 20.172045429s
Mar  2 23:09:30.029: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Running", Reason="", readiness=true. Elapsed: 22.216781743s
Mar  2 23:09:32.066: INFO: Pod "pod-subpath-test-configmap-dtvz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.253934613s
STEP: Saw pod success
Mar  2 23:09:32.066: INFO: Pod "pod-subpath-test-configmap-dtvz" satisfied condition "Succeeded or Failed"
Mar  2 23:09:32.082: INFO: Trying to get logs from node 10.138.244.159 pod pod-subpath-test-configmap-dtvz container test-container-subpath-configmap-dtvz: <nil>
STEP: delete the pod
Mar  2 23:09:32.187: INFO: Waiting for pod pod-subpath-test-configmap-dtvz to disappear
Mar  2 23:09:32.197: INFO: Pod pod-subpath-test-configmap-dtvz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dtvz
Mar  2 23:09:32.197: INFO: Deleting pod "pod-subpath-test-configmap-dtvz" in namespace "subpath-4732"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:09:32.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4732" for this suite.

• [SLOW TEST:24.727 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":250,"skipped":4398,"failed":0}
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:09:32.257: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Mar  2 23:09:32.593: INFO: Waiting up to 5m0s for pod "client-containers-1642f5fc-266e-4cbd-ba03-6bb4fd5afde7" in namespace "containers-2522" to be "Succeeded or Failed"
Mar  2 23:09:32.611: INFO: Pod "client-containers-1642f5fc-266e-4cbd-ba03-6bb4fd5afde7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.366425ms
Mar  2 23:09:34.626: INFO: Pod "client-containers-1642f5fc-266e-4cbd-ba03-6bb4fd5afde7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032567433s
Mar  2 23:09:36.637: INFO: Pod "client-containers-1642f5fc-266e-4cbd-ba03-6bb4fd5afde7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043320173s
STEP: Saw pod success
Mar  2 23:09:36.637: INFO: Pod "client-containers-1642f5fc-266e-4cbd-ba03-6bb4fd5afde7" satisfied condition "Succeeded or Failed"
Mar  2 23:09:36.652: INFO: Trying to get logs from node 10.138.244.159 pod client-containers-1642f5fc-266e-4cbd-ba03-6bb4fd5afde7 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 23:09:36.770: INFO: Waiting for pod client-containers-1642f5fc-266e-4cbd-ba03-6bb4fd5afde7 to disappear
Mar  2 23:09:36.784: INFO: Pod client-containers-1642f5fc-266e-4cbd-ba03-6bb4fd5afde7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:09:36.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2522" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":251,"skipped":4398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:09:36.841: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0302 23:09:38.154370      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0302 23:09:38.154428      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0302 23:09:38.154454      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 23:09:38.154: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:09:38.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7544" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":252,"skipped":4478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:09:38.194: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:09:45.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-748" for this suite.

• [SLOW TEST:7.268 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":253,"skipped":4517,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:09:45.464: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:09:45.842: INFO: Waiting up to 5m0s for pod "downwardapi-volume-055d6c24-3b94-4429-b64e-85074b8dc10b" in namespace "projected-8445" to be "Succeeded or Failed"
Mar  2 23:09:45.875: INFO: Pod "downwardapi-volume-055d6c24-3b94-4429-b64e-85074b8dc10b": Phase="Pending", Reason="", readiness=false. Elapsed: 33.532555ms
Mar  2 23:09:47.889: INFO: Pod "downwardapi-volume-055d6c24-3b94-4429-b64e-85074b8dc10b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047199375s
Mar  2 23:09:49.909: INFO: Pod "downwardapi-volume-055d6c24-3b94-4429-b64e-85074b8dc10b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067001975s
STEP: Saw pod success
Mar  2 23:09:49.909: INFO: Pod "downwardapi-volume-055d6c24-3b94-4429-b64e-85074b8dc10b" satisfied condition "Succeeded or Failed"
Mar  2 23:09:49.931: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-055d6c24-3b94-4429-b64e-85074b8dc10b container client-container: <nil>
STEP: delete the pod
Mar  2 23:09:50.016: INFO: Waiting for pod downwardapi-volume-055d6c24-3b94-4429-b64e-85074b8dc10b to disappear
Mar  2 23:09:50.031: INFO: Pod downwardapi-volume-055d6c24-3b94-4429-b64e-85074b8dc10b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:09:50.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8445" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":254,"skipped":4528,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:09:50.088: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9091.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9091.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9091.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9091.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9091.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9091.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 23:09:54.611: INFO: DNS probes using dns-9091/dns-test-c9a4edf2-dd66-4cc9-97ba-65b331ff7525 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:09:54.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9091" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":255,"skipped":4530,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:09:54.690: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4634
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 23:09:54.964: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 23:09:55.226: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 23:09:57.263: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 23:09:59.240: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:10:01.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:10:03.236: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:10:05.238: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:10:07.239: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:10:09.237: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:10:11.238: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 23:10:11.281: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 23:10:13.303: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 23:10:15.292: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 23:10:15.316: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 23:10:17.388: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 23:10:17.388: INFO: Breadth first check of 172.30.43.112 on host 10.138.244.150...
Mar  2 23:10:17.399: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.228.25:9080/dial?request=hostname&protocol=udp&host=172.30.43.112&port=8081&tries=1'] Namespace:pod-network-test-4634 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:10:17.399: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:10:17.659: INFO: Waiting for responses: map[]
Mar  2 23:10:17.659: INFO: reached 172.30.43.112 after 0/1 tries
Mar  2 23:10:17.659: INFO: Breadth first check of 172.30.228.20 on host 10.138.244.159...
Mar  2 23:10:17.671: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.228.25:9080/dial?request=hostname&protocol=udp&host=172.30.228.20&port=8081&tries=1'] Namespace:pod-network-test-4634 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:10:17.671: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:10:18.058: INFO: Waiting for responses: map[]
Mar  2 23:10:18.058: INFO: reached 172.30.228.20 after 0/1 tries
Mar  2 23:10:18.058: INFO: Breadth first check of 172.30.21.175 on host 10.138.244.162...
Mar  2 23:10:18.068: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.228.25:9080/dial?request=hostname&protocol=udp&host=172.30.21.175&port=8081&tries=1'] Namespace:pod-network-test-4634 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:10:18.068: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:10:18.347: INFO: Waiting for responses: map[]
Mar  2 23:10:18.348: INFO: reached 172.30.21.175 after 0/1 tries
Mar  2 23:10:18.348: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:10:18.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4634" for this suite.

• [SLOW TEST:23.722 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4531,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:10:18.417: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  2 23:10:18.607: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:10:27.574: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:10:59.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3063" for this suite.

• [SLOW TEST:41.113 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":257,"skipped":4538,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:10:59.529: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:11:00.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4640" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":258,"skipped":4543,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:11:00.188: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  2 23:11:00.452: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9020 /api/v1/namespaces/watch-9020/configmaps/e2e-watch-test-label-changed 9e21dda4-ef36-4e9f-b8e1-2c9f0e11321b 113637 0 2022-03-02 23:11:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 23:11:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:11:00.453: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9020 /api/v1/namespaces/watch-9020/configmaps/e2e-watch-test-label-changed 9e21dda4-ef36-4e9f-b8e1-2c9f0e11321b 113641 0 2022-03-02 23:11:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 23:11:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:11:00.453: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9020 /api/v1/namespaces/watch-9020/configmaps/e2e-watch-test-label-changed 9e21dda4-ef36-4e9f-b8e1-2c9f0e11321b 113644 0 2022-03-02 23:11:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 23:11:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  2 23:11:10.602: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9020 /api/v1/namespaces/watch-9020/configmaps/e2e-watch-test-label-changed 9e21dda4-ef36-4e9f-b8e1-2c9f0e11321b 113742 0 2022-03-02 23:11:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 23:11:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:11:10.603: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9020 /api/v1/namespaces/watch-9020/configmaps/e2e-watch-test-label-changed 9e21dda4-ef36-4e9f-b8e1-2c9f0e11321b 113743 0 2022-03-02 23:11:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 23:11:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 23:11:10.603: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9020 /api/v1/namespaces/watch-9020/configmaps/e2e-watch-test-label-changed 9e21dda4-ef36-4e9f-b8e1-2c9f0e11321b 113744 0 2022-03-02 23:11:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 23:11:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:11:10.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9020" for this suite.

• [SLOW TEST:10.513 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":259,"skipped":4561,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:11:10.703: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5063, will wait for the garbage collector to delete the pods
Mar  2 23:11:15.055: INFO: Deleting Job.batch foo took: 21.375101ms
Mar  2 23:11:15.155: INFO: Terminating Job.batch foo pods took: 100.275766ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:11:57.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5063" for this suite.

• [SLOW TEST:46.503 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":260,"skipped":4568,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:11:57.207: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Mar  2 23:11:58.502: INFO: Waiting up to 5m0s for pod "test-pod-07346e6d-aad4-4fa6-9a3f-26ec294880bf" in namespace "svcaccounts-8663" to be "Succeeded or Failed"
Mar  2 23:11:58.528: INFO: Pod "test-pod-07346e6d-aad4-4fa6-9a3f-26ec294880bf": Phase="Pending", Reason="", readiness=false. Elapsed: 25.128772ms
Mar  2 23:12:00.540: INFO: Pod "test-pod-07346e6d-aad4-4fa6-9a3f-26ec294880bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037697135s
Mar  2 23:12:02.552: INFO: Pod "test-pod-07346e6d-aad4-4fa6-9a3f-26ec294880bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049461691s
STEP: Saw pod success
Mar  2 23:12:02.552: INFO: Pod "test-pod-07346e6d-aad4-4fa6-9a3f-26ec294880bf" satisfied condition "Succeeded or Failed"
Mar  2 23:12:02.576: INFO: Trying to get logs from node 10.138.244.159 pod test-pod-07346e6d-aad4-4fa6-9a3f-26ec294880bf container agnhost-container: <nil>
STEP: delete the pod
Mar  2 23:12:02.664: INFO: Waiting for pod test-pod-07346e6d-aad4-4fa6-9a3f-26ec294880bf to disappear
Mar  2 23:12:02.699: INFO: Pod test-pod-07346e6d-aad4-4fa6-9a3f-26ec294880bf no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:12:02.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8663" for this suite.

• [SLOW TEST:5.573 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":261,"skipped":4572,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:12:02.780: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Mar  2 23:12:05.057: INFO: Pod pod-hostip-0d1edf85-28d7-48be-92d1-790a4269ef76 has hostIP: 10.138.244.159
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:12:05.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-57" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4578,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:12:05.108: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 23:12:05.430: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 23:12:05.501: INFO: waiting for watch events with expected annotations
Mar  2 23:12:05.501: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:12:05.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7052" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":263,"skipped":4590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:12:05.742: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:12:06.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57646bbd-4412-48d6-bb9e-6199f9803499" in namespace "projected-4783" to be "Succeeded or Failed"
Mar  2 23:12:06.114: INFO: Pod "downwardapi-volume-57646bbd-4412-48d6-bb9e-6199f9803499": Phase="Pending", Reason="", readiness=false. Elapsed: 44.676388ms
Mar  2 23:12:08.125: INFO: Pod "downwardapi-volume-57646bbd-4412-48d6-bb9e-6199f9803499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056018978s
STEP: Saw pod success
Mar  2 23:12:08.125: INFO: Pod "downwardapi-volume-57646bbd-4412-48d6-bb9e-6199f9803499" satisfied condition "Succeeded or Failed"
Mar  2 23:12:08.136: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-57646bbd-4412-48d6-bb9e-6199f9803499 container client-container: <nil>
STEP: delete the pod
Mar  2 23:12:08.220: INFO: Waiting for pod downwardapi-volume-57646bbd-4412-48d6-bb9e-6199f9803499 to disappear
Mar  2 23:12:08.230: INFO: Pod downwardapi-volume-57646bbd-4412-48d6-bb9e-6199f9803499 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:12:08.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4783" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":264,"skipped":4613,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:12:08.273: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar  2 23:12:08.509: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:12:08.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3726" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":265,"skipped":4618,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:12:08.631: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:12:22.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1657" for this suite.

• [SLOW TEST:13.695 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":266,"skipped":4622,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:12:22.327: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-36d7aa20-c509-4d37-96ab-40d2625c5e0a
STEP: Creating a pod to test consume secrets
Mar  2 23:12:22.657: INFO: Waiting up to 5m0s for pod "pod-secrets-b414c984-9475-4213-aa63-19d273876eea" in namespace "secrets-6324" to be "Succeeded or Failed"
Mar  2 23:12:22.668: INFO: Pod "pod-secrets-b414c984-9475-4213-aa63-19d273876eea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.493997ms
Mar  2 23:12:24.679: INFO: Pod "pod-secrets-b414c984-9475-4213-aa63-19d273876eea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022408143s
Mar  2 23:12:26.690: INFO: Pod "pod-secrets-b414c984-9475-4213-aa63-19d273876eea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033037924s
STEP: Saw pod success
Mar  2 23:12:26.690: INFO: Pod "pod-secrets-b414c984-9475-4213-aa63-19d273876eea" satisfied condition "Succeeded or Failed"
Mar  2 23:12:26.701: INFO: Trying to get logs from node 10.138.244.159 pod pod-secrets-b414c984-9475-4213-aa63-19d273876eea container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:12:26.826: INFO: Waiting for pod pod-secrets-b414c984-9475-4213-aa63-19d273876eea to disappear
Mar  2 23:12:26.868: INFO: Pod pod-secrets-b414c984-9475-4213-aa63-19d273876eea no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:12:26.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6324" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4631,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:12:26.904: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-2588
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2588 to expose endpoints map[]
Mar  2 23:12:27.201: INFO: successfully validated that service endpoint-test2 in namespace services-2588 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2588
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2588 to expose endpoints map[pod1:[80]]
Mar  2 23:12:29.303: INFO: successfully validated that service endpoint-test2 in namespace services-2588 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2588
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2588 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  2 23:12:32.393: INFO: successfully validated that service endpoint-test2 in namespace services-2588 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2588
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2588 to expose endpoints map[pod2:[80]]
Mar  2 23:12:32.504: INFO: successfully validated that service endpoint-test2 in namespace services-2588 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2588
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2588 to expose endpoints map[]
Mar  2 23:12:32.614: INFO: successfully validated that service endpoint-test2 in namespace services-2588 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:12:32.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2588" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.894 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":268,"skipped":4640,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:12:32.801: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 23:12:33.315: INFO: Number of nodes with available pods: 0
Mar  2 23:12:33.315: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:12:34.362: INFO: Number of nodes with available pods: 0
Mar  2 23:12:34.362: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:12:35.352: INFO: Number of nodes with available pods: 0
Mar  2 23:12:35.352: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:12:36.396: INFO: Number of nodes with available pods: 3
Mar  2 23:12:36.396: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  2 23:12:36.501: INFO: Number of nodes with available pods: 2
Mar  2 23:12:36.501: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:37.527: INFO: Number of nodes with available pods: 2
Mar  2 23:12:37.527: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:38.569: INFO: Number of nodes with available pods: 2
Mar  2 23:12:38.569: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:39.558: INFO: Number of nodes with available pods: 2
Mar  2 23:12:39.558: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:40.543: INFO: Number of nodes with available pods: 2
Mar  2 23:12:40.543: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:41.551: INFO: Number of nodes with available pods: 2
Mar  2 23:12:41.551: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:42.525: INFO: Number of nodes with available pods: 2
Mar  2 23:12:42.525: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:43.538: INFO: Number of nodes with available pods: 2
Mar  2 23:12:43.538: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:44.531: INFO: Number of nodes with available pods: 2
Mar  2 23:12:44.531: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:45.531: INFO: Number of nodes with available pods: 2
Mar  2 23:12:45.531: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:46.530: INFO: Number of nodes with available pods: 2
Mar  2 23:12:46.530: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:47.536: INFO: Number of nodes with available pods: 2
Mar  2 23:12:47.536: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:48.545: INFO: Number of nodes with available pods: 2
Mar  2 23:12:48.545: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:49.537: INFO: Number of nodes with available pods: 2
Mar  2 23:12:49.537: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:50.565: INFO: Number of nodes with available pods: 2
Mar  2 23:12:50.565: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:51.533: INFO: Number of nodes with available pods: 2
Mar  2 23:12:51.533: INFO: Node 10.138.244.162 is running more than one daemon pod
Mar  2 23:12:52.530: INFO: Number of nodes with available pods: 3
Mar  2 23:12:52.530: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7855, will wait for the garbage collector to delete the pods
Mar  2 23:12:52.638: INFO: Deleting DaemonSet.extensions daemon-set took: 30.977943ms
Mar  2 23:12:52.838: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.368891ms
Mar  2 23:13:07.059: INFO: Number of nodes with available pods: 0
Mar  2 23:13:07.059: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 23:13:07.073: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7855/daemonsets","resourceVersion":"115258"},"items":null}

Mar  2 23:13:07.086: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7855/pods","resourceVersion":"115259"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:13:07.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7855" for this suite.

• [SLOW TEST:34.435 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":269,"skipped":4649,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:13:07.236: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  2 23:13:07.504: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:13:16.681: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:13:48.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-297" for this suite.

• [SLOW TEST:41.346 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":270,"skipped":4661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:13:48.592: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Mar  2 23:13:48.773: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:14:34.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9571" for this suite.

• [SLOW TEST:45.602 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":271,"skipped":4707,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:14:34.196: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2793
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-2793
Mar  2 23:14:34.412: INFO: Found 0 stateful pods, waiting for 1
Mar  2 23:14:44.425: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 23:14:44.495: INFO: Deleting all statefulset in ns statefulset-2793
Mar  2 23:14:44.504: INFO: Scaling statefulset ss to 0
Mar  2 23:14:54.592: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:14:54.605: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:14:54.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2793" for this suite.

• [SLOW TEST:20.494 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":272,"skipped":4722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:14:54.691: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 23:14:54.910: INFO: Waiting up to 5m0s for pod "pod-ae40fc21-a0b6-45d7-8cfb-a74fc8913170" in namespace "emptydir-1854" to be "Succeeded or Failed"
Mar  2 23:14:54.927: INFO: Pod "pod-ae40fc21-a0b6-45d7-8cfb-a74fc8913170": Phase="Pending", Reason="", readiness=false. Elapsed: 17.474816ms
Mar  2 23:14:56.941: INFO: Pod "pod-ae40fc21-a0b6-45d7-8cfb-a74fc8913170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031557765s
Mar  2 23:14:58.960: INFO: Pod "pod-ae40fc21-a0b6-45d7-8cfb-a74fc8913170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050349374s
STEP: Saw pod success
Mar  2 23:14:58.960: INFO: Pod "pod-ae40fc21-a0b6-45d7-8cfb-a74fc8913170" satisfied condition "Succeeded or Failed"
Mar  2 23:14:58.971: INFO: Trying to get logs from node 10.138.244.159 pod pod-ae40fc21-a0b6-45d7-8cfb-a74fc8913170 container test-container: <nil>
STEP: delete the pod
Mar  2 23:14:59.137: INFO: Waiting for pod pod-ae40fc21-a0b6-45d7-8cfb-a74fc8913170 to disappear
Mar  2 23:14:59.150: INFO: Pod pod-ae40fc21-a0b6-45d7-8cfb-a74fc8913170 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:14:59.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1854" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:14:59.195: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:14:59.464: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-9b9d098c-8637-41af-8603-fc5692a4b923" in namespace "security-context-test-7786" to be "Succeeded or Failed"
Mar  2 23:14:59.476: INFO: Pod "busybox-privileged-false-9b9d098c-8637-41af-8603-fc5692a4b923": Phase="Pending", Reason="", readiness=false. Elapsed: 11.748343ms
Mar  2 23:15:01.535: INFO: Pod "busybox-privileged-false-9b9d098c-8637-41af-8603-fc5692a4b923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070280464s
Mar  2 23:15:03.548: INFO: Pod "busybox-privileged-false-9b9d098c-8637-41af-8603-fc5692a4b923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083423648s
Mar  2 23:15:03.548: INFO: Pod "busybox-privileged-false-9b9d098c-8637-41af-8603-fc5692a4b923" satisfied condition "Succeeded or Failed"
Mar  2 23:15:03.584: INFO: Got logs for pod "busybox-privileged-false-9b9d098c-8637-41af-8603-fc5692a4b923": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:15:03.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7786" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:15:03.618: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 23:15:04.016: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar  2 23:15:04.034: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 23:15:04.115: INFO: waiting for watch events with expected annotations
Mar  2 23:15:04.115: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:15:04.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-373" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":275,"skipped":4831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:15:04.493: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1046
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 23:15:04.697: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 23:15:04.984: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 23:15:07.018: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 23:15:08.995: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:15:11.000: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:15:13.054: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:15:14.999: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:15:17.072: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:15:18.997: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:15:21.002: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:15:22.997: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:15:24.996: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 23:15:25.037: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 23:15:25.060: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 23:15:27.175: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 23:15:27.175: INFO: Breadth first check of 172.30.43.118 on host 10.138.244.150...
Mar  2 23:15:27.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.228.11:9080/dial?request=hostname&protocol=http&host=172.30.43.118&port=8080&tries=1'] Namespace:pod-network-test-1046 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:15:27.185: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:15:27.509: INFO: Waiting for responses: map[]
Mar  2 23:15:27.509: INFO: reached 172.30.43.118 after 0/1 tries
Mar  2 23:15:27.509: INFO: Breadth first check of 172.30.228.31 on host 10.138.244.159...
Mar  2 23:15:27.523: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.228.11:9080/dial?request=hostname&protocol=http&host=172.30.228.31&port=8080&tries=1'] Namespace:pod-network-test-1046 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:15:27.523: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:15:27.831: INFO: Waiting for responses: map[]
Mar  2 23:15:27.831: INFO: reached 172.30.228.31 after 0/1 tries
Mar  2 23:15:27.831: INFO: Breadth first check of 172.30.21.176 on host 10.138.244.162...
Mar  2 23:15:27.858: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.228.11:9080/dial?request=hostname&protocol=http&host=172.30.21.176&port=8080&tries=1'] Namespace:pod-network-test-1046 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:15:27.858: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:15:28.082: INFO: Waiting for responses: map[]
Mar  2 23:15:28.082: INFO: reached 172.30.21.176 after 0/1 tries
Mar  2 23:15:28.083: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:15:28.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1046" for this suite.

• [SLOW TEST:23.624 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4855,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:15:28.118: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9787
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9787
STEP: Creating statefulset with conflicting port in namespace statefulset-9787
STEP: Waiting until pod test-pod will start running in namespace statefulset-9787
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9787
Mar  2 23:15:32.554: INFO: Observed stateful pod in namespace: statefulset-9787, name: ss-0, uid: ffa9a097-7ec2-489d-8fc4-d290a96ff764, status phase: Pending. Waiting for statefulset controller to delete.
Mar  2 23:15:32.623: INFO: Observed stateful pod in namespace: statefulset-9787, name: ss-0, uid: ffa9a097-7ec2-489d-8fc4-d290a96ff764, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 23:15:32.644: INFO: Observed stateful pod in namespace: statefulset-9787, name: ss-0, uid: ffa9a097-7ec2-489d-8fc4-d290a96ff764, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 23:15:32.654: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9787
STEP: Removing pod with conflicting port in namespace statefulset-9787
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9787 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 23:15:36.727: INFO: Deleting all statefulset in ns statefulset-9787
Mar  2 23:15:36.736: INFO: Scaling statefulset ss to 0
Mar  2 23:15:56.808: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:15:56.817: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:15:56.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9787" for this suite.

• [SLOW TEST:28.833 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":277,"skipped":4857,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:15:56.952: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-wj65
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 23:15:57.384: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wj65" in namespace "subpath-942" to be "Succeeded or Failed"
Mar  2 23:15:57.408: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Pending", Reason="", readiness=false. Elapsed: 24.833185ms
Mar  2 23:15:59.425: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 2.041139093s
Mar  2 23:16:01.441: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 4.057085831s
Mar  2 23:16:03.455: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 6.071438758s
Mar  2 23:16:05.469: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 8.085699522s
Mar  2 23:16:07.482: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 10.098517502s
Mar  2 23:16:09.501: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 12.117560765s
Mar  2 23:16:11.529: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 14.145240683s
Mar  2 23:16:13.543: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 16.159037121s
Mar  2 23:16:15.554: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 18.170386069s
Mar  2 23:16:17.567: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Running", Reason="", readiness=true. Elapsed: 20.183024046s
Mar  2 23:16:19.579: INFO: Pod "pod-subpath-test-secret-wj65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.19538159s
STEP: Saw pod success
Mar  2 23:16:19.579: INFO: Pod "pod-subpath-test-secret-wj65" satisfied condition "Succeeded or Failed"
Mar  2 23:16:19.591: INFO: Trying to get logs from node 10.138.244.159 pod pod-subpath-test-secret-wj65 container test-container-subpath-secret-wj65: <nil>
STEP: delete the pod
Mar  2 23:16:19.684: INFO: Waiting for pod pod-subpath-test-secret-wj65 to disappear
Mar  2 23:16:19.693: INFO: Pod pod-subpath-test-secret-wj65 no longer exists
STEP: Deleting pod pod-subpath-test-secret-wj65
Mar  2 23:16:19.693: INFO: Deleting pod "pod-subpath-test-secret-wj65" in namespace "subpath-942"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:16:19.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-942" for this suite.

• [SLOW TEST:22.786 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":278,"skipped":4883,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:16:19.738: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-99cbd15e-d431-49d3-8558-f21bd3e53b07
STEP: Creating a pod to test consume configMaps
Mar  2 23:16:20.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0bf8831f-6d16-4ca2-8f01-d5082e49ff66" in namespace "projected-3757" to be "Succeeded or Failed"
Mar  2 23:16:20.085: INFO: Pod "pod-projected-configmaps-0bf8831f-6d16-4ca2-8f01-d5082e49ff66": Phase="Pending", Reason="", readiness=false. Elapsed: 10.421366ms
Mar  2 23:16:22.104: INFO: Pod "pod-projected-configmaps-0bf8831f-6d16-4ca2-8f01-d5082e49ff66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029020139s
Mar  2 23:16:24.122: INFO: Pod "pod-projected-configmaps-0bf8831f-6d16-4ca2-8f01-d5082e49ff66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04713685s
STEP: Saw pod success
Mar  2 23:16:24.122: INFO: Pod "pod-projected-configmaps-0bf8831f-6d16-4ca2-8f01-d5082e49ff66" satisfied condition "Succeeded or Failed"
Mar  2 23:16:24.135: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-configmaps-0bf8831f-6d16-4ca2-8f01-d5082e49ff66 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 23:16:24.283: INFO: Waiting for pod pod-projected-configmaps-0bf8831f-6d16-4ca2-8f01-d5082e49ff66 to disappear
Mar  2 23:16:24.292: INFO: Pod pod-projected-configmaps-0bf8831f-6d16-4ca2-8f01-d5082e49ff66 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:16:24.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3757" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":279,"skipped":4887,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:16:24.410: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:16:26.755: INFO: Deleting pod "var-expansion-af7fcc24-2ee9-449a-b116-d1ede7eafdc7" in namespace "var-expansion-7757"
Mar  2 23:16:26.772: INFO: Wait up to 5m0s for pod "var-expansion-af7fcc24-2ee9-449a-b116-d1ede7eafdc7" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:16:28.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7757" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":280,"skipped":4902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:16:28.847: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 23:16:29.207: INFO: Waiting up to 5m0s for pod "pod-e9448c67-a8c4-453a-945e-23571bd8dd2f" in namespace "emptydir-2345" to be "Succeeded or Failed"
Mar  2 23:16:29.244: INFO: Pod "pod-e9448c67-a8c4-453a-945e-23571bd8dd2f": Phase="Pending", Reason="", readiness=false. Elapsed: 37.334863ms
Mar  2 23:16:31.255: INFO: Pod "pod-e9448c67-a8c4-453a-945e-23571bd8dd2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047973151s
Mar  2 23:16:33.266: INFO: Pod "pod-e9448c67-a8c4-453a-945e-23571bd8dd2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059443275s
STEP: Saw pod success
Mar  2 23:16:33.266: INFO: Pod "pod-e9448c67-a8c4-453a-945e-23571bd8dd2f" satisfied condition "Succeeded or Failed"
Mar  2 23:16:33.278: INFO: Trying to get logs from node 10.138.244.159 pod pod-e9448c67-a8c4-453a-945e-23571bd8dd2f container test-container: <nil>
STEP: delete the pod
Mar  2 23:16:33.441: INFO: Waiting for pod pod-e9448c67-a8c4-453a-945e-23571bd8dd2f to disappear
Mar  2 23:16:33.451: INFO: Pod pod-e9448c67-a8c4-453a-945e-23571bd8dd2f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:16:33.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2345" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4934,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:16:33.490: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7092
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7092
I0302 23:16:33.883829      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7092, replica count: 2
Mar  2 23:16:36.934: INFO: Creating new exec pod
I0302 23:16:36.934211      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 23:16:42.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7092 exec execpodtmgl8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  2 23:16:43.020: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 23:16:43.020: INFO: stdout: ""
Mar  2 23:16:43.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-7092 exec execpodtmgl8 -- /bin/sh -x -c nc -zv -t -w 2 172.21.112.221 80'
Mar  2 23:16:43.540: INFO: stderr: "+ nc -zv -t -w 2 172.21.112.221 80\nConnection to 172.21.112.221 80 port [tcp/http] succeeded!\n"
Mar  2 23:16:43.540: INFO: stdout: ""
Mar  2 23:16:43.540: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:16:43.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7092" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:10.173 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":282,"skipped":4937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:16:43.666: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7650
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 23:16:43.889: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 23:16:44.152: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 23:16:46.173: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 23:16:48.172: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:16:50.162: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:16:52.165: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:16:54.164: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:16:56.168: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:16:58.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 23:17:00.170: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 23:17:00.190: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 23:17:02.210: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 23:17:04.203: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 23:17:04.226: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 23:17:08.368: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 23:17:08.368: INFO: Going to poll 172.30.43.120 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 23:17:08.381: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.43.120 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7650 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:17:08.381: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:17:09.661: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 23:17:09.661: INFO: Going to poll 172.30.228.9 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 23:17:09.674: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.228.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7650 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:17:09.674: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:17:10.901: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 23:17:10.901: INFO: Going to poll 172.30.21.184 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 23:17:10.911: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.21.184 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7650 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 23:17:10.911: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
Mar  2 23:17:12.175: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:17:12.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7650" for this suite.

• [SLOW TEST:28.573 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4966,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:17:12.239: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:17:12.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da76789e-b458-4b45-a86f-79aa88c402dc" in namespace "downward-api-1820" to be "Succeeded or Failed"
Mar  2 23:17:12.483: INFO: Pod "downwardapi-volume-da76789e-b458-4b45-a86f-79aa88c402dc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.391109ms
Mar  2 23:17:14.496: INFO: Pod "downwardapi-volume-da76789e-b458-4b45-a86f-79aa88c402dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023222875s
Mar  2 23:17:16.528: INFO: Pod "downwardapi-volume-da76789e-b458-4b45-a86f-79aa88c402dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05569367s
STEP: Saw pod success
Mar  2 23:17:16.528: INFO: Pod "downwardapi-volume-da76789e-b458-4b45-a86f-79aa88c402dc" satisfied condition "Succeeded or Failed"
Mar  2 23:17:16.558: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-da76789e-b458-4b45-a86f-79aa88c402dc container client-container: <nil>
STEP: delete the pod
Mar  2 23:17:16.689: INFO: Waiting for pod downwardapi-volume-da76789e-b458-4b45-a86f-79aa88c402dc to disappear
Mar  2 23:17:16.701: INFO: Pod downwardapi-volume-da76789e-b458-4b45-a86f-79aa88c402dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:17:16.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1820" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":284,"skipped":4971,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:17:16.751: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-3bf40145-e41a-4e3d-96f7-d5e9a05b4ca6
STEP: Creating a pod to test consume configMaps
Mar  2 23:17:17.033: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2a73ba6-0078-4057-9393-6ea4061b9ef6" in namespace "configmap-8297" to be "Succeeded or Failed"
Mar  2 23:17:17.045: INFO: Pod "pod-configmaps-e2a73ba6-0078-4057-9393-6ea4061b9ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043153ms
Mar  2 23:17:19.060: INFO: Pod "pod-configmaps-e2a73ba6-0078-4057-9393-6ea4061b9ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02683822s
Mar  2 23:17:21.071: INFO: Pod "pod-configmaps-e2a73ba6-0078-4057-9393-6ea4061b9ef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0381067s
STEP: Saw pod success
Mar  2 23:17:21.071: INFO: Pod "pod-configmaps-e2a73ba6-0078-4057-9393-6ea4061b9ef6" satisfied condition "Succeeded or Failed"
Mar  2 23:17:21.082: INFO: Trying to get logs from node 10.138.244.159 pod pod-configmaps-e2a73ba6-0078-4057-9393-6ea4061b9ef6 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 23:17:21.149: INFO: Waiting for pod pod-configmaps-e2a73ba6-0078-4057-9393-6ea4061b9ef6 to disappear
Mar  2 23:17:21.164: INFO: Pod pod-configmaps-e2a73ba6-0078-4057-9393-6ea4061b9ef6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:17:21.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8297" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4993,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:17:21.227: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-pm7m
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 23:17:21.475: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pm7m" in namespace "subpath-1523" to be "Succeeded or Failed"
Mar  2 23:17:21.489: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Pending", Reason="", readiness=false. Elapsed: 14.368298ms
Mar  2 23:17:23.499: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024067017s
Mar  2 23:17:25.512: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 4.037517762s
Mar  2 23:17:27.525: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 6.050229984s
Mar  2 23:17:29.541: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 8.065921911s
Mar  2 23:17:31.552: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 10.077877753s
Mar  2 23:17:33.569: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 12.093969155s
Mar  2 23:17:35.582: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 14.107013853s
Mar  2 23:17:37.593: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 16.118637222s
Mar  2 23:17:39.607: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 18.132117517s
Mar  2 23:17:41.622: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 20.147673736s
Mar  2 23:17:43.634: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Running", Reason="", readiness=true. Elapsed: 22.159362458s
Mar  2 23:17:45.647: INFO: Pod "pod-subpath-test-projected-pm7m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.172240172s
STEP: Saw pod success
Mar  2 23:17:45.647: INFO: Pod "pod-subpath-test-projected-pm7m" satisfied condition "Succeeded or Failed"
Mar  2 23:17:45.661: INFO: Trying to get logs from node 10.138.244.159 pod pod-subpath-test-projected-pm7m container test-container-subpath-projected-pm7m: <nil>
STEP: delete the pod
Mar  2 23:17:45.757: INFO: Waiting for pod pod-subpath-test-projected-pm7m to disappear
Mar  2 23:17:45.766: INFO: Pod pod-subpath-test-projected-pm7m no longer exists
STEP: Deleting pod pod-subpath-test-projected-pm7m
Mar  2 23:17:45.766: INFO: Deleting pod "pod-subpath-test-projected-pm7m" in namespace "subpath-1523"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:17:45.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1523" for this suite.

• [SLOW TEST:24.606 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":286,"skipped":5015,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:17:45.833: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:17:46.050: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  2 23:17:46.096: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 23:17:51.110: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 23:17:51.110: INFO: Creating deployment "test-rolling-update-deployment"
Mar  2 23:17:51.126: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  2 23:17:51.152: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  2 23:17:53.175: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  2 23:17:53.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781859871, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781859871, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781859871, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781859871, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:17:55.198: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 23:17:55.235: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5863 /apis/apps/v1/namespaces/deployment-5863/deployments/test-rolling-update-deployment 2f60724a-8b0a-4145-8465-75169cae8e06 118833 1 2022-03-02 23:17:51 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-03-02 23:17:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-02 23:17:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003530398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-02 23:17:51 +0000 UTC,LastTransitionTime:2022-03-02 23:17:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2022-03-02 23:17:53 +0000 UTC,LastTransitionTime:2022-03-02 23:17:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 23:17:55.247: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-5863 /apis/apps/v1/namespaces/deployment-5863/replicasets/test-rolling-update-deployment-6b6bf9df46 6f00a9a3-f1a6-4dd1-9b48-1f8cf99b56c6 118821 1 2022-03-02 23:17:51 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 2f60724a-8b0a-4145-8465-75169cae8e06 0xc003530877 0xc003530878}] []  [{kube-controller-manager Update apps/v1 2022-03-02 23:17:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f60724a-8b0a-4145-8465-75169cae8e06\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003530908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 23:17:55.247: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  2 23:17:55.247: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5863 /apis/apps/v1/namespaces/deployment-5863/replicasets/test-rolling-update-controller 4ba7d080-5adc-4bca-b980-cd2fe08a2660 118830 2 2022-03-02 23:17:46 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 2f60724a-8b0a-4145-8465-75169cae8e06 0xc003530767 0xc003530768}] []  [{e2e.test Update apps/v1 2022-03-02 23:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-02 23:17:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f60724a-8b0a-4145-8465-75169cae8e06\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003530808 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 23:17:55.261: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-7wnxd" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-7wnxd test-rolling-update-deployment-6b6bf9df46- deployment-5863 /api/v1/namespaces/deployment-5863/pods/test-rolling-update-deployment-6b6bf9df46-7wnxd 3291a7d5-96dd-4c46-bccd-76e537ed38ce 118820 0 2022-03-02 23:17:51 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/containerID:48c26b5d639048ebc95ef77b12866a4cc0ef3efe830f17d89226cf9d04c3a0c5 cni.projectcalico.org/podIP:172.30.228.53/32 cni.projectcalico.org/podIPs:172.30.228.53/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.228.53"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.228.53"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 6f00a9a3-f1a6-4dd1-9b48-1f8cf99b56c6 0xc00af11de7 0xc00af11de8}] []  [{kube-controller-manager Update v1 2022-03-02 23:17:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f00a9a3-f1a6-4dd1-9b48-1f8cf99b56c6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-03-02 23:17:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2022-03-02 23:17:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2022-03-02 23:17:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.228.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24h6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24h6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24h6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.138.244.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nd4mk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 23:17:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 23:17:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 23:17:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 23:17:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.244.159,PodIP:172.30.228.53,StartTime:2022-03-02 23:17:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 23:17:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://9d54625fdd01e306845204d9dd5c47d3fcdd20136a7dfea36fdf28eb86317f0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.228.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:17:55.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5863" for this suite.

• [SLOW TEST:9.461 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":287,"skipped":5018,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:17:55.294: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-b779b101-f16c-4dad-b7ea-71c5855f16a8 in namespace container-probe-9568
Mar  2 23:17:59.622: INFO: Started pod busybox-b779b101-f16c-4dad-b7ea-71c5855f16a8 in namespace container-probe-9568
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 23:17:59.631: INFO: Initial restart count of pod busybox-b779b101-f16c-4dad-b7ea-71c5855f16a8 is 0
Mar  2 23:18:48.021: INFO: Restart count of pod container-probe-9568/busybox-b779b101-f16c-4dad-b7ea-71c5855f16a8 is now 1 (48.389790894s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:18:48.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9568" for this suite.

• [SLOW TEST:52.790 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":5035,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:18:48.087: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Mar  2 23:18:48.308: INFO: created test-event-1
Mar  2 23:18:48.323: INFO: created test-event-2
Mar  2 23:18:48.356: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar  2 23:18:48.371: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar  2 23:18:48.512: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:18:48.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3184" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":289,"skipped":5039,"failed":0}

------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:18:48.561: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar  2 23:18:48.715: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:18:54.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7513" for this suite.

• [SLOW TEST:5.806 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":290,"skipped":5039,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:18:54.368: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5926
STEP: creating service affinity-nodeport-transition in namespace services-5926
STEP: creating replication controller affinity-nodeport-transition in namespace services-5926
I0302 23:18:54.699453      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-5926, replica count: 3
I0302 23:18:57.752274      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 23:18:57.812: INFO: Creating new exec pod
Mar  2 23:19:02.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5926 exec execpod-affinityfjhs8 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Mar  2 23:19:03.415: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  2 23:19:03.416: INFO: stdout: ""
Mar  2 23:19:03.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5926 exec execpod-affinityfjhs8 -- /bin/sh -x -c nc -zv -t -w 2 172.21.246.33 80'
Mar  2 23:19:03.887: INFO: stderr: "+ nc -zv -t -w 2 172.21.246.33 80\nConnection to 172.21.246.33 80 port [tcp/http] succeeded!\n"
Mar  2 23:19:03.887: INFO: stdout: ""
Mar  2 23:19:03.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5926 exec execpod-affinityfjhs8 -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.159 32199'
Mar  2 23:19:04.309: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.159 32199\nConnection to 10.138.244.159 32199 port [tcp/32199] succeeded!\n"
Mar  2 23:19:04.309: INFO: stdout: ""
Mar  2 23:19:04.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5926 exec execpod-affinityfjhs8 -- /bin/sh -x -c nc -zv -t -w 2 10.138.244.162 32199'
Mar  2 23:19:04.746: INFO: stderr: "+ nc -zv -t -w 2 10.138.244.162 32199\nConnection to 10.138.244.162 32199 port [tcp/32199] succeeded!\n"
Mar  2 23:19:04.747: INFO: stdout: ""
Mar  2 23:19:04.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5926 exec execpod-affinityfjhs8 -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.218 32199'
Mar  2 23:19:05.189: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.218 32199\nConnection to 168.1.11.218 32199 port [tcp/32199] succeeded!\n"
Mar  2 23:19:05.189: INFO: stdout: ""
Mar  2 23:19:05.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5926 exec execpod-affinityfjhs8 -- /bin/sh -x -c nc -zv -t -w 2 168.1.11.211 32199'
Mar  2 23:19:05.620: INFO: stderr: "+ nc -zv -t -w 2 168.1.11.211 32199\nConnection to 168.1.11.211 32199 port [tcp/32199] succeeded!\n"
Mar  2 23:19:05.620: INFO: stdout: ""
Mar  2 23:19:05.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5926 exec execpod-affinityfjhs8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.244.150:32199/ ; done'
Mar  2 23:19:06.284: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n"
Mar  2 23:19:06.284: INFO: stdout: "\naffinity-nodeport-transition-mf8gt\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-fnr9p\naffinity-nodeport-transition-mf8gt\naffinity-nodeport-transition-mf8gt\naffinity-nodeport-transition-fnr9p\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-fnr9p\naffinity-nodeport-transition-fnr9p\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-fnr9p\naffinity-nodeport-transition-fnr9p\naffinity-nodeport-transition-fnr9p\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-mf8gt"
Mar  2 23:19:06.285: INFO: Received response from host: affinity-nodeport-transition-mf8gt
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-fnr9p
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-mf8gt
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-mf8gt
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-fnr9p
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-fnr9p
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-fnr9p
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-fnr9p
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-fnr9p
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-fnr9p
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:06.289: INFO: Received response from host: affinity-nodeport-transition-mf8gt
Mar  2 23:19:06.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=services-5926 exec execpod-affinityfjhs8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.244.150:32199/ ; done'
Mar  2 23:19:07.012: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.244.150:32199/\n"
Mar  2 23:19:07.012: INFO: stdout: "\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm\naffinity-nodeport-transition-lc7mm"
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Received response from host: affinity-nodeport-transition-lc7mm
Mar  2 23:19:07.012: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5926, will wait for the garbage collector to delete the pods
Mar  2 23:19:07.159: INFO: Deleting ReplicationController affinity-nodeport-transition took: 22.025938ms
Mar  2 23:19:07.259: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.315005ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:19:19.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5926" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:24.943 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":291,"skipped":5081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:19:19.313: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 23:19:20.207: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 23:19:22.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781859960, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781859960, loc:(*time.Location)(0x7984060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781859960, loc:(*time.Location)(0x7984060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781859960, loc:(*time.Location)(0x7984060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 23:19:25.309: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:19:26.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6279" for this suite.
STEP: Destroying namespace "webhook-6279-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.158 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":292,"skipped":5135,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:19:26.472: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-5e3715c8-f7c0-4f83-8df4-3361c29af93e
STEP: Creating a pod to test consume configMaps
Mar  2 23:19:26.791: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ecebd196-0e81-4706-a51b-eb1e764e554a" in namespace "projected-5029" to be "Succeeded or Failed"
Mar  2 23:19:26.803: INFO: Pod "pod-projected-configmaps-ecebd196-0e81-4706-a51b-eb1e764e554a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.497698ms
Mar  2 23:19:28.815: INFO: Pod "pod-projected-configmaps-ecebd196-0e81-4706-a51b-eb1e764e554a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023866017s
STEP: Saw pod success
Mar  2 23:19:28.815: INFO: Pod "pod-projected-configmaps-ecebd196-0e81-4706-a51b-eb1e764e554a" satisfied condition "Succeeded or Failed"
Mar  2 23:19:28.830: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-configmaps-ecebd196-0e81-4706-a51b-eb1e764e554a container agnhost-container: <nil>
STEP: delete the pod
Mar  2 23:19:28.945: INFO: Waiting for pod pod-projected-configmaps-ecebd196-0e81-4706-a51b-eb1e764e554a to disappear
Mar  2 23:19:28.976: INFO: Pod pod-projected-configmaps-ecebd196-0e81-4706-a51b-eb1e764e554a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:19:28.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5029" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5138,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:19:29.024: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:19:29.512: INFO: Create a RollingUpdate DaemonSet
Mar  2 23:19:29.538: INFO: Check that daemon pods launch on every node of the cluster
Mar  2 23:19:29.611: INFO: Number of nodes with available pods: 0
Mar  2 23:19:29.611: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:19:30.640: INFO: Number of nodes with available pods: 0
Mar  2 23:19:30.640: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:19:31.713: INFO: Number of nodes with available pods: 1
Mar  2 23:19:31.713: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:19:32.641: INFO: Number of nodes with available pods: 2
Mar  2 23:19:32.641: INFO: Node 10.138.244.150 is running more than one daemon pod
Mar  2 23:19:33.672: INFO: Number of nodes with available pods: 3
Mar  2 23:19:33.672: INFO: Number of running nodes: 3, number of available pods: 3
Mar  2 23:19:33.672: INFO: Update the DaemonSet to trigger a rollout
Mar  2 23:19:33.744: INFO: Updating DaemonSet daemon-set
Mar  2 23:19:38.832: INFO: Roll back the DaemonSet before rollout is complete
Mar  2 23:19:38.863: INFO: Updating DaemonSet daemon-set
Mar  2 23:19:38.863: INFO: Make sure DaemonSet rollback is complete
Mar  2 23:19:38.875: INFO: Wrong image for pod: daemon-set-vtnkj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 23:19:38.876: INFO: Pod daemon-set-vtnkj is not available
Mar  2 23:19:39.910: INFO: Wrong image for pod: daemon-set-vtnkj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 23:19:39.910: INFO: Pod daemon-set-vtnkj is not available
Mar  2 23:19:40.911: INFO: Pod daemon-set-kc996 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4010, will wait for the garbage collector to delete the pods
Mar  2 23:19:41.054: INFO: Deleting DaemonSet.extensions daemon-set took: 31.142046ms
Mar  2 23:19:41.155: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.099792ms
Mar  2 23:19:47.364: INFO: Number of nodes with available pods: 0
Mar  2 23:19:47.365: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 23:19:47.375: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4010/daemonsets","resourceVersion":"120432"},"items":null}

Mar  2 23:19:47.385: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4010/pods","resourceVersion":"120432"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:19:47.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4010" for this suite.

• [SLOW TEST:18.454 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":294,"skipped":5152,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:19:47.478: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:19:47.670: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05f8bb69-a2e5-4fd7-85d7-a112661b9b70" in namespace "downward-api-3536" to be "Succeeded or Failed"
Mar  2 23:19:47.684: INFO: Pod "downwardapi-volume-05f8bb69-a2e5-4fd7-85d7-a112661b9b70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.130392ms
Mar  2 23:19:49.698: INFO: Pod "downwardapi-volume-05f8bb69-a2e5-4fd7-85d7-a112661b9b70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028063084s
Mar  2 23:19:51.712: INFO: Pod "downwardapi-volume-05f8bb69-a2e5-4fd7-85d7-a112661b9b70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042125179s
STEP: Saw pod success
Mar  2 23:19:51.712: INFO: Pod "downwardapi-volume-05f8bb69-a2e5-4fd7-85d7-a112661b9b70" satisfied condition "Succeeded or Failed"
Mar  2 23:19:51.721: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-05f8bb69-a2e5-4fd7-85d7-a112661b9b70 container client-container: <nil>
STEP: delete the pod
Mar  2 23:19:51.818: INFO: Waiting for pod downwardapi-volume-05f8bb69-a2e5-4fd7-85d7-a112661b9b70 to disappear
Mar  2 23:19:51.831: INFO: Pod downwardapi-volume-05f8bb69-a2e5-4fd7-85d7-a112661b9b70 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:19:51.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3536" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":5162,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:19:51.871: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1934
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-1934
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1934
Mar  2 23:19:52.304: INFO: Found 0 stateful pods, waiting for 1
Mar  2 23:20:02.325: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  2 23:20:02.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 23:20:02.807: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 23:20:02.807: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 23:20:02.807: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 23:20:02.848: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 23:20:12.859: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 23:20:12.859: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:20:12.912: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:12.912: INFO: ss-0  10.138.244.159  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:12.912: INFO: 
Mar  2 23:20:12.912: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  2 23:20:13.923: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986042333s
Mar  2 23:20:14.934: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975192731s
Mar  2 23:20:15.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.964165729s
Mar  2 23:20:16.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.949660773s
Mar  2 23:20:17.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937592936s
Mar  2 23:20:18.986: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.922789004s
Mar  2 23:20:20.005: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.910934674s
Mar  2 23:20:21.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.892832336s
Mar  2 23:20:22.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 864.258596ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1934
Mar  2 23:20:23.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:20:23.615: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 23:20:23.615: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 23:20:23.615: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 23:20:23.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:20:24.128: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 23:20:24.128: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 23:20:24.128: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 23:20:24.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:20:24.747: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 23:20:24.747: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 23:20:24.747: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 23:20:24.772: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:20:24.772: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:20:24.772: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  2 23:20:24.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 23:20:25.191: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 23:20:25.191: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 23:20:25.191: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 23:20:25.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 23:20:25.671: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 23:20:25.671: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 23:20:25.671: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 23:20:25.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 23:20:26.209: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 23:20:26.209: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 23:20:26.209: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 23:20:26.209: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:20:26.219: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  2 23:20:36.248: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 23:20:36.248: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 23:20:36.248: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 23:20:36.308: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:36.308: INFO: ss-0  10.138.244.159  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:36.308: INFO: ss-1  10.138.244.162  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:36.308: INFO: ss-2  10.138.244.150  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:36.309: INFO: 
Mar  2 23:20:36.309: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:37.320: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:37.320: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:37.321: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:37.321: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:37.321: INFO: 
Mar  2 23:20:37.321: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:38.354: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:38.354: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:38.355: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:38.356: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:38.357: INFO: 
Mar  2 23:20:38.357: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:39.368: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:39.368: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:39.368: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:39.368: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:39.368: INFO: 
Mar  2 23:20:39.369: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:40.381: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:40.381: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:40.381: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:40.381: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:40.381: INFO: 
Mar  2 23:20:40.381: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:41.394: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:41.394: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:41.395: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:41.395: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:41.395: INFO: 
Mar  2 23:20:41.395: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:42.417: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:42.417: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:42.417: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:42.418: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:42.418: INFO: 
Mar  2 23:20:42.418: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:43.430: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:43.430: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:43.431: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:43.431: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:43.431: INFO: 
Mar  2 23:20:43.431: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:44.444: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:44.444: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:44.444: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:44.444: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:44.445: INFO: 
Mar  2 23:20:44.445: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 23:20:45.458: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  2 23:20:45.458: INFO: ss-0  10.138.244.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:19:52 +0000 UTC  }]
Mar  2 23:20:45.458: INFO: ss-1  10.138.244.162  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:45.458: INFO: ss-2  10.138.244.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 23:20:12 +0000 UTC  }]
Mar  2 23:20:45.458: INFO: 
Mar  2 23:20:45.458: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1934
Mar  2 23:20:46.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:20:46.813: INFO: rc: 1
Mar  2 23:20:46.813: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  2 23:20:56.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:20:56.977: INFO: rc: 1
Mar  2 23:20:56.977: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:21:06.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:21:07.155: INFO: rc: 1
Mar  2 23:21:07.155: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:21:17.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:21:17.323: INFO: rc: 1
Mar  2 23:21:17.323: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:21:27.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:21:27.459: INFO: rc: 1
Mar  2 23:21:27.459: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:21:37.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:21:37.640: INFO: rc: 1
Mar  2 23:21:37.640: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:21:47.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:21:47.842: INFO: rc: 1
Mar  2 23:21:47.842: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:21:57.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:21:57.987: INFO: rc: 1
Mar  2 23:21:57.987: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:22:07.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:22:08.167: INFO: rc: 1
Mar  2 23:22:08.168: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:22:18.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:22:18.346: INFO: rc: 1
Mar  2 23:22:18.346: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:22:28.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:22:28.541: INFO: rc: 1
Mar  2 23:22:28.541: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:22:38.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:22:38.739: INFO: rc: 1
Mar  2 23:22:38.739: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:22:48.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:22:48.911: INFO: rc: 1
Mar  2 23:22:48.911: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:22:58.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:22:59.070: INFO: rc: 1
Mar  2 23:22:59.070: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:23:09.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:23:09.343: INFO: rc: 1
Mar  2 23:23:09.343: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:23:19.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:23:19.553: INFO: rc: 1
Mar  2 23:23:19.553: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:23:29.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:23:29.712: INFO: rc: 1
Mar  2 23:23:29.712: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:23:39.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:23:39.882: INFO: rc: 1
Mar  2 23:23:39.882: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:23:49.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:23:50.048: INFO: rc: 1
Mar  2 23:23:50.048: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:24:00.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:24:00.297: INFO: rc: 1
Mar  2 23:24:00.298: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:24:10.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:24:10.510: INFO: rc: 1
Mar  2 23:24:10.510: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:24:20.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:24:20.673: INFO: rc: 1
Mar  2 23:24:20.673: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:24:30.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:24:30.889: INFO: rc: 1
Mar  2 23:24:30.889: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:24:40.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:24:41.073: INFO: rc: 1
Mar  2 23:24:41.073: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:24:51.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:24:51.248: INFO: rc: 1
Mar  2 23:24:51.248: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:25:01.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:25:01.443: INFO: rc: 1
Mar  2 23:25:01.443: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:25:11.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:25:11.624: INFO: rc: 1
Mar  2 23:25:11.624: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:25:21.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:25:21.781: INFO: rc: 1
Mar  2 23:25:21.781: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:25:31.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:25:32.007: INFO: rc: 1
Mar  2 23:25:32.007: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:25:42.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:25:42.186: INFO: rc: 1
Mar  2 23:25:42.186: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 23:25:52.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=statefulset-1934 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 23:25:52.377: INFO: rc: 1
Mar  2 23:25:52.377: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Mar  2 23:25:52.377: INFO: Scaling statefulset ss to 0
Mar  2 23:25:52.414: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 23:25:52.424: INFO: Deleting all statefulset in ns statefulset-1934
Mar  2 23:25:52.443: INFO: Scaling statefulset ss to 0
Mar  2 23:25:52.493: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:25:52.503: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:25:52.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1934" for this suite.

• [SLOW TEST:360.749 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":296,"skipped":5166,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:25:52.620: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  2 23:25:52.875: INFO: Waiting up to 5m0s for pod "pod-eff4be2f-3af2-479f-b36e-069da5dac059" in namespace "emptydir-203" to be "Succeeded or Failed"
Mar  2 23:25:52.886: INFO: Pod "pod-eff4be2f-3af2-479f-b36e-069da5dac059": Phase="Pending", Reason="", readiness=false. Elapsed: 11.086103ms
Mar  2 23:25:54.897: INFO: Pod "pod-eff4be2f-3af2-479f-b36e-069da5dac059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022169859s
Mar  2 23:25:56.927: INFO: Pod "pod-eff4be2f-3af2-479f-b36e-069da5dac059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051693302s
STEP: Saw pod success
Mar  2 23:25:56.927: INFO: Pod "pod-eff4be2f-3af2-479f-b36e-069da5dac059" satisfied condition "Succeeded or Failed"
Mar  2 23:25:56.944: INFO: Trying to get logs from node 10.138.244.159 pod pod-eff4be2f-3af2-479f-b36e-069da5dac059 container test-container: <nil>
STEP: delete the pod
Mar  2 23:25:57.071: INFO: Waiting for pod pod-eff4be2f-3af2-479f-b36e-069da5dac059 to disappear
Mar  2 23:25:57.083: INFO: Pod pod-eff4be2f-3af2-479f-b36e-069da5dac059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:25:57.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-203" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5166,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:25:57.147: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:25:57.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c2a38cf-283f-4093-8568-a54dea7d4137" in namespace "projected-4876" to be "Succeeded or Failed"
Mar  2 23:25:57.444: INFO: Pod "downwardapi-volume-6c2a38cf-283f-4093-8568-a54dea7d4137": Phase="Pending", Reason="", readiness=false. Elapsed: 65.132858ms
Mar  2 23:25:59.457: INFO: Pod "downwardapi-volume-6c2a38cf-283f-4093-8568-a54dea7d4137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078533947s
Mar  2 23:26:01.470: INFO: Pod "downwardapi-volume-6c2a38cf-283f-4093-8568-a54dea7d4137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091298738s
STEP: Saw pod success
Mar  2 23:26:01.471: INFO: Pod "downwardapi-volume-6c2a38cf-283f-4093-8568-a54dea7d4137" satisfied condition "Succeeded or Failed"
Mar  2 23:26:01.481: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-6c2a38cf-283f-4093-8568-a54dea7d4137 container client-container: <nil>
STEP: delete the pod
Mar  2 23:26:01.570: INFO: Waiting for pod downwardapi-volume-6c2a38cf-283f-4093-8568-a54dea7d4137 to disappear
Mar  2 23:26:01.626: INFO: Pod downwardapi-volume-6c2a38cf-283f-4093-8568-a54dea7d4137 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:01.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4876" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":5172,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:01.784: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:26:02.009: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:06.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7640" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":5178,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:06.262: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:26:06.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21bb5012-c8f6-4681-94d3-ea052455e520" in namespace "projected-4332" to be "Succeeded or Failed"
Mar  2 23:26:06.578: INFO: Pod "downwardapi-volume-21bb5012-c8f6-4681-94d3-ea052455e520": Phase="Pending", Reason="", readiness=false. Elapsed: 10.89992ms
Mar  2 23:26:08.591: INFO: Pod "downwardapi-volume-21bb5012-c8f6-4681-94d3-ea052455e520": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023192317s
STEP: Saw pod success
Mar  2 23:26:08.591: INFO: Pod "downwardapi-volume-21bb5012-c8f6-4681-94d3-ea052455e520" satisfied condition "Succeeded or Failed"
Mar  2 23:26:08.600: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-21bb5012-c8f6-4681-94d3-ea052455e520 container client-container: <nil>
STEP: delete the pod
Mar  2 23:26:08.671: INFO: Waiting for pod downwardapi-volume-21bb5012-c8f6-4681-94d3-ea052455e520 to disappear
Mar  2 23:26:08.683: INFO: Pod downwardapi-volume-21bb5012-c8f6-4681-94d3-ea052455e520 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:08.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4332" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":5181,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:08.726: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 23:26:08.952: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 23:26:08.997: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 23:26:09.049: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.150 before test
Mar  2 23:26:09.101: INFO: calico-kube-controllers-64bc47d78c-r9h75 from calico-system started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.101: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 23:26:09.101: INFO: calico-node-sfwd8 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.101: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 23:26:09.101: INFO: calico-typha-747778ff7-7mfzl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.101: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 23:26:09.101: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-03-02 20:09:15 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.101: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  2 23:26:09.101: INFO: managed-storage-validation-webhooks-7d645c9954-9tz2t from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.101: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 23:26:09.101: INFO: managed-storage-validation-webhooks-7d645c9954-fnvqt from ibm-odf-validation-webhook started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.101: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 23:26:09.102: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-xx9p2 from ibm-system started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 23:26:09.102: INFO: ibm-file-plugin-58c5ccc6d4-mhk8g from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 23:26:09.102: INFO: ibm-keepalived-watcher-lzm26 from kube-system started at 2022-03-02 20:04:31 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 23:26:09.102: INFO: ibm-master-proxy-static-10.138.244.150 from kube-system started at 2022-03-02 20:04:27 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 23:26:09.102: INFO: 	Container pause ready: true, restart count 0
Mar  2 23:26:09.102: INFO: ibmcloud-block-storage-driver-n9lmx from kube-system started at 2022-03-02 20:04:35 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 23:26:09.102: INFO: ibmcloud-block-storage-plugin-67c5f49db6-wlj2j from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 23:26:09.102: INFO: vpn-7bf7499b46-dvdqm from kube-system started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container vpn ready: true, restart count 0
Mar  2 23:26:09.102: INFO: tuned-fqxdn from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:26:09.102: INFO: cluster-samples-operator-69fbcc775-4zjvg from openshift-cluster-samples-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 23:26:09.102: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 23:26:09.102: INFO: cluster-storage-operator-56787c5bcb-9f5hs from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.102: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 23:26:09.102: INFO: csi-snapshot-controller-69454d4b56-gwpsw from openshift-cluster-storage-operator started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 23:26:09.103: INFO: csi-snapshot-controller-operator-6977df7ddd-dkdjh from openshift-cluster-storage-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 23:26:09.103: INFO: csi-snapshot-webhook-788b7d55dd-2rmtk from openshift-cluster-storage-operator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container webhook ready: true, restart count 0
Mar  2 23:26:09.103: INFO: console-78b4c56c55-ndmlz from openshift-console started at 2022-03-02 20:13:37 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container console ready: true, restart count 0
Mar  2 23:26:09.103: INFO: downloads-79b8c4c9f8-hqr26 from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container download-server ready: true, restart count 0
Mar  2 23:26:09.103: INFO: dns-default-jf6bp from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:26:09.103: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:26:09.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.103: INFO: cluster-image-registry-operator-58888444b7-jsm7m from openshift-image-registry started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 23:26:09.103: INFO: image-registry-78c5f597d5-rfdtq from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container registry ready: true, restart count 0
Mar  2 23:26:09.103: INFO: node-ca-2d6rp from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.103: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:26:09.103: INFO: ingress-canary-wptt6 from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 23:26:09.104: INFO: router-default-678545d6db-sgrbg from openshift-ingress started at 2022-03-02 22:12:55 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container router ready: true, restart count 0
Mar  2 23:26:09.104: INFO: openshift-kube-proxy-9mnl6 from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 23:26:09.104: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.104: INFO: kube-storage-version-migrator-operator-68756f7898-kvfhq from openshift-kube-storage-version-migrator-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 23:26:09.104: INFO: migrator-76bc956454-5h92l from openshift-kube-storage-version-migrator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container migrator ready: true, restart count 0
Mar  2 23:26:09.104: INFO: certified-operators-4lfsq from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 23:26:09.104: INFO: community-operators-fj9gp from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 23:26:09.104: INFO: marketplace-operator-8689886944-xlcrw from openshift-marketplace started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 23:26:09.104: INFO: redhat-marketplace-tvwc6 from openshift-marketplace started at 2022-03-02 20:07:35 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 23:26:09.104: INFO: redhat-operators-87vss from openshift-marketplace started at 2022-03-02 20:07:32 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 23:26:09.104: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 23:26:09.104: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:26:09.105: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-03-02 22:30:17 +0000 UTC (5 container statuses recorded)
Mar  2 23:26:09.105: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:26:09.105: INFO: cluster-monitoring-operator-668f4b779-pt5v7 from openshift-monitoring started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.105: INFO: 	Container cluster-monitoring-operator ready: true, restart count 1
Mar  2 23:26:09.105: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Mar  2 23:26:09.105: INFO: grafana-77dc549b6f-nn2b2 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.105: INFO: 	Container grafana ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 23:26:09.105: INFO: kube-state-metrics-7b6c6d96b-scdqx from openshift-monitoring started at 2022-03-02 22:30:09 +0000 UTC (3 container statuses recorded)
Mar  2 23:26:09.105: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 23:26:09.105: INFO: node-exporter-dff24 from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.105: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:26:09.105: INFO: openshift-state-metrics-84c4bdd485-5d95q from openshift-monitoring started at 2022-03-02 22:30:10 +0000 UTC (3 container statuses recorded)
Mar  2 23:26:09.105: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 23:26:09.105: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 23:26:09.106: INFO: prometheus-adapter-68858877cc-d2bhz from openshift-monitoring started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.106: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 23:26:09.106: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-03-02 22:30:27 +0000 UTC (7 container statuses recorded)
Mar  2 23:26:09.106: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 23:26:09.106: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 23:26:09.106: INFO: prometheus-operator-7797d58ccd-8kdw9 from openshift-monitoring started at 2022-03-02 22:30:10 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 23:26:09.106: INFO: thanos-querier-544bf8c477-jdvqv from openshift-monitoring started at 2022-03-02 20:07:50 +0000 UTC (5 container statuses recorded)
Mar  2 23:26:09.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 23:26:09.106: INFO: multus-admission-controller-2wfsf from openshift-multus started at 2022-03-02 20:06:01 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.106: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 23:26:09.106: INFO: multus-zgnwz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.106: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:26:09.106: INFO: network-metrics-daemon-qmb7k from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.107: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 23:26:09.107: INFO: network-check-source-758fcf9d76-dnnrz from openshift-network-diagnostics started at 2022-03-02 22:30:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 23:26:09.107: INFO: network-check-target-rmfwb from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 23:26:09.107: INFO: network-operator-57496bd6cc-wt8z2 from openshift-network-operator started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container network-operator ready: true, restart count 0
Mar  2 23:26:09.107: INFO: catalog-operator-777865977d-2fqfw from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 23:26:09.107: INFO: olm-operator-5cb6cff486-wld6q from openshift-operator-lifecycle-manager started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 23:26:09.107: INFO: packageserver-67967c9875-524hv from openshift-operator-lifecycle-manager started at 2022-03-02 20:07:33 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 23:26:09.107: INFO: packageserver-67967c9875-jm8b8 from openshift-operator-lifecycle-manager started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 23:26:09.107: INFO: service-ca-operator-5b668f5895-69p65 from openshift-service-ca-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 23:26:09.107: INFO: service-ca-b874796d6-4g8hj from openshift-service-ca started at 2022-03-02 22:30:10 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container service-ca-controller ready: false, restart count 0
Mar  2 23:26:09.107: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-pl9fl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.107: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 23:26:09.107: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 23:26:09.107: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.159 before test
Mar  2 23:26:09.146: INFO: calico-node-fhc29 from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 23:26:09.146: INFO: calico-typha-747778ff7-sgxvl from calico-system started at 2022-03-02 20:05:29 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 23:26:09.146: INFO: ibm-keepalived-watcher-drf4g from kube-system started at 2022-03-02 20:04:20 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 23:26:09.146: INFO: ibm-master-proxy-static-10.138.244.159 from kube-system started at 2022-03-02 20:04:17 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 23:26:09.146: INFO: 	Container pause ready: true, restart count 0
Mar  2 23:26:09.146: INFO: ibmcloud-block-storage-driver-85npp from kube-system started at 2022-03-02 20:04:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 23:26:09.146: INFO: tuned-xv4r5 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:26:09.146: INFO: dns-default-224cd from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:26:09.146: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:26:09.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.146: INFO: node-ca-bc4lw from openshift-image-registry started at 2022-03-02 20:07:30 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:26:09.146: INFO: registry-pvc-permissions-kstcw from openshift-image-registry started at 2022-03-02 20:09:45 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.146: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 23:26:09.146: INFO: ingress-canary-gg9t2 from openshift-ingress-canary started at 2022-03-02 22:30:37 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.147: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 23:26:09.147: INFO: openshift-kube-proxy-lxhnl from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.147: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 23:26:09.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.147: INFO: node-exporter-t8msj from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.147: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:26:09.147: INFO: multus-58kxw from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.147: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:26:09.147: INFO: multus-admission-controller-42x25 from openshift-multus started at 2022-03-02 22:30:47 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.147: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 23:26:09.147: INFO: network-metrics-daemon-4hdhq from openshift-multus started at 2022-03-02 20:05:01 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.147: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 23:26:09.147: INFO: network-check-target-c4pgw from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.147: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 23:26:09.148: INFO: pod-logs-websocket-f62b0e73-edbd-4b8e-b95e-1e92f229e528 from pods-7640 started at 2022-03-02 23:26:02 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.148: INFO: 	Container main ready: true, restart count 0
Mar  2 23:26:09.148: INFO: sonobuoy from sonobuoy started at 2022-03-02 21:45:05 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.148: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 23:26:09.148: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-xczvl from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 23:26:09.148: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 23:26:09.148: INFO: 
Logging pods the apiserver thinks is on node 10.138.244.162 before test
Mar  2 23:26:09.199: INFO: calico-node-8mbgb from calico-system started at 2022-03-02 20:05:23 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.199: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 23:26:09.199: INFO: calico-typha-747778ff7-cxk8p from calico-system started at 2022-03-02 20:05:22 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.199: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 23:26:09.199: INFO: managed-storage-validation-webhooks-7d645c9954-hxlwh from ibm-odf-validation-webhook started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.199: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 23:26:09.199: INFO: ibm-cloud-provider-ip-168-1-52-154-569658dd7b-zj24d from ibm-system started at 2022-03-02 22:12:55 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.199: INFO: 	Container ibm-cloud-provider-ip-168-1-52-154 ready: true, restart count 0
Mar  2 23:26:09.199: INFO: ibm-keepalived-watcher-g4r68 from kube-system started at 2022-03-02 20:04:32 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.199: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 23:26:09.199: INFO: ibm-master-proxy-static-10.138.244.162 from kube-system started at 2022-03-02 20:04:29 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.199: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 23:26:09.199: INFO: 	Container pause ready: true, restart count 0
Mar  2 23:26:09.199: INFO: ibm-storage-metrics-agent-5686f759d8-6tq4n from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.199: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 23:26:09.199: INFO: ibm-storage-watcher-6488446f7b-nmfdr from kube-system started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.199: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 23:26:09.199: INFO: ibmcloud-block-storage-driver-bz98p from kube-system started at 2022-03-02 20:04:36 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.200: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 23:26:09.200: INFO: cluster-node-tuning-operator-5cb4c7f989-xchxd from openshift-cluster-node-tuning-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.200: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 23:26:09.200: INFO: tuned-krv52 from openshift-cluster-node-tuning-operator started at 2022-03-02 20:07:26 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.200: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:26:09.200: INFO: console-operator-58ff8dbf6b-bz5q9 from openshift-console-operator started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.200: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 23:26:09.200: INFO: console-78b4c56c55-kxx8b from openshift-console started at 2022-03-02 20:13:09 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.200: INFO: 	Container console ready: true, restart count 0
Mar  2 23:26:09.200: INFO: downloads-79b8c4c9f8-djqwn from openshift-console started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.200: INFO: 	Container download-server ready: true, restart count 0
Mar  2 23:26:09.200: INFO: dns-operator-55f6c97874-bfkv2 from openshift-dns-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.200: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 23:26:09.200: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.200: INFO: dns-default-p6v58 from openshift-dns started at 2022-03-02 20:07:33 +0000 UTC (3 container statuses recorded)
Mar  2 23:26:09.200: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:26:09.200: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:26:09.200: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.200: INFO: node-ca-bgqw9 from openshift-image-registry started at 2022-03-02 20:07:31 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.201: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:26:09.201: INFO: ingress-canary-wsb4b from openshift-ingress-canary started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.201: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Mar  2 23:26:09.201: INFO: ingress-operator-bc8cf4dbb-pqp88 from openshift-ingress-operator started at 2022-03-02 20:06:11 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.201: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 23:26:09.201: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.201: INFO: router-default-678545d6db-zh9tr from openshift-ingress started at 2022-03-02 20:07:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.201: INFO: 	Container router ready: true, restart count 0
Mar  2 23:26:09.201: INFO: openshift-kube-proxy-ks9sn from openshift-kube-proxy started at 2022-03-02 20:05:02 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.201: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 23:26:09.201: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.201: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-03-02 20:07:49 +0000 UTC (5 container statuses recorded)
Mar  2 23:26:09.201: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:26:09.201: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:26:09.201: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:26:09.201: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.201: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:26:09.201: INFO: node-exporter-gtb4f from openshift-monitoring started at 2022-03-02 20:06:35 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.201: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.201: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:26:09.201: INFO: prometheus-adapter-68858877cc-cxjl2 from openshift-monitoring started at 2022-03-02 20:09:30 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.201: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 23:26:09.201: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-03-02 20:14:02 +0000 UTC (7 container statuses recorded)
Mar  2 23:26:09.202: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 23:26:09.202: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 23:26:09.202: INFO: telemeter-client-765c47dd74-4s7d4 from openshift-monitoring started at 2022-03-02 20:06:40 +0000 UTC (3 container statuses recorded)
Mar  2 23:26:09.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container reload ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 23:26:09.202: INFO: thanos-querier-544bf8c477-hzf5x from openshift-monitoring started at 2022-03-02 22:12:55 +0000 UTC (5 container statuses recorded)
Mar  2 23:26:09.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 23:26:09.202: INFO: multus-admission-controller-ljkq4 from openshift-multus started at 2022-03-02 20:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.202: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 23:26:09.203: INFO: multus-bkzqz from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.203: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:26:09.203: INFO: network-metrics-daemon-2bsdl from openshift-multus started at 2022-03-02 20:05:00 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.203: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:26:09.203: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 23:26:09.203: INFO: network-check-target-t499h from openshift-network-diagnostics started at 2022-03-02 20:05:03 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.203: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 23:26:09.203: INFO: metrics-78d8646588-dvms5 from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.203: INFO: 	Container metrics ready: true, restart count 2
Mar  2 23:26:09.203: INFO: push-gateway-7b4b958bfb-dmwzc from openshift-roks-metrics started at 2022-03-02 20:06:11 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.203: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 23:26:09.203: INFO: sonobuoy-e2e-job-87017147060647d2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.203: INFO: 	Container e2e ready: true, restart count 0
Mar  2 23:26:09.203: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 23:26:09.203: INFO: sonobuoy-systemd-logs-daemon-set-7bd31679b4a240cc-szbl2 from sonobuoy started at 2022-03-02 21:45:20 +0000 UTC (2 container statuses recorded)
Mar  2 23:26:09.203: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 23:26:09.203: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 23:26:09.203: INFO: tigera-operator-597f8644c9-8tp9s from tigera-operator started at 2022-03-02 22:12:54 +0000 UTC (1 container statuses recorded)
Mar  2 23:26:09.203: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-7538.16d8b3fe5843f67d], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16d8b3fe7944071b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:10.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7538" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":301,"skipped":5187,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:10.394: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:26:10.725: INFO: Creating ReplicaSet my-hostname-basic-ab51c1b0-bb96-4f24-ba09-3334fe4eaebd
Mar  2 23:26:10.757: INFO: Pod name my-hostname-basic-ab51c1b0-bb96-4f24-ba09-3334fe4eaebd: Found 0 pods out of 1
Mar  2 23:26:15.769: INFO: Pod name my-hostname-basic-ab51c1b0-bb96-4f24-ba09-3334fe4eaebd: Found 1 pods out of 1
Mar  2 23:26:15.769: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ab51c1b0-bb96-4f24-ba09-3334fe4eaebd" is running
Mar  2 23:26:15.784: INFO: Pod "my-hostname-basic-ab51c1b0-bb96-4f24-ba09-3334fe4eaebd-5swwz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 23:26:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 23:26:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 23:26:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 23:26:10 +0000 UTC Reason: Message:}])
Mar  2 23:26:15.785: INFO: Trying to dial the pod
Mar  2 23:26:20.837: INFO: Controller my-hostname-basic-ab51c1b0-bb96-4f24-ba09-3334fe4eaebd: Got expected result from replica 1 [my-hostname-basic-ab51c1b0-bb96-4f24-ba09-3334fe4eaebd-5swwz]: "my-hostname-basic-ab51c1b0-bb96-4f24-ba09-3334fe4eaebd-5swwz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:20.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5817" for this suite.

• [SLOW TEST:10.512 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":302,"skipped":5200,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:20.911: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:26:21.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66a0ef15-5e54-44f2-9dd9-d7727f052607" in namespace "projected-9834" to be "Succeeded or Failed"
Mar  2 23:26:21.173: INFO: Pod "downwardapi-volume-66a0ef15-5e54-44f2-9dd9-d7727f052607": Phase="Pending", Reason="", readiness=false. Elapsed: 19.882566ms
Mar  2 23:26:23.183: INFO: Pod "downwardapi-volume-66a0ef15-5e54-44f2-9dd9-d7727f052607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030492908s
Mar  2 23:26:25.196: INFO: Pod "downwardapi-volume-66a0ef15-5e54-44f2-9dd9-d7727f052607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043077707s
STEP: Saw pod success
Mar  2 23:26:25.196: INFO: Pod "downwardapi-volume-66a0ef15-5e54-44f2-9dd9-d7727f052607" satisfied condition "Succeeded or Failed"
Mar  2 23:26:25.207: INFO: Trying to get logs from node 10.138.244.159 pod downwardapi-volume-66a0ef15-5e54-44f2-9dd9-d7727f052607 container client-container: <nil>
STEP: delete the pod
Mar  2 23:26:25.279: INFO: Waiting for pod downwardapi-volume-66a0ef15-5e54-44f2-9dd9-d7727f052607 to disappear
Mar  2 23:26:25.296: INFO: Pod downwardapi-volume-66a0ef15-5e54-44f2-9dd9-d7727f052607 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:25.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9834" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:25.343: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:26:25.521: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bca09442-30b7-4611-a68c-b1c3b42c1a49
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-bca09442-30b7-4611-a68c-b1c3b42c1a49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:29.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3723" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5231,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:29.911: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:38.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6939" for this suite.

• [SLOW TEST:8.355 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":305,"skipped":5247,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:38.268: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:42.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7109" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5253,"failed":0}
S
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:42.680: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 23:26:43.257: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  2 23:26:43.289: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  2 23:26:43.289: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.289: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  2 23:26:43.289: INFO: Checking APIGroup: apps
Mar  2 23:26:43.299: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  2 23:26:43.299: INFO: Versions found [{apps/v1 v1}]
Mar  2 23:26:43.299: INFO: apps/v1 matches apps/v1
Mar  2 23:26:43.299: INFO: Checking APIGroup: events.k8s.io
Mar  2 23:26:43.303: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  2 23:26:43.303: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.303: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  2 23:26:43.303: INFO: Checking APIGroup: authentication.k8s.io
Mar  2 23:26:43.307: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  2 23:26:43.307: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.307: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  2 23:26:43.307: INFO: Checking APIGroup: authorization.k8s.io
Mar  2 23:26:43.311: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  2 23:26:43.311: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.311: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  2 23:26:43.311: INFO: Checking APIGroup: autoscaling
Mar  2 23:26:43.314: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar  2 23:26:43.314: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar  2 23:26:43.314: INFO: autoscaling/v1 matches autoscaling/v1
Mar  2 23:26:43.314: INFO: Checking APIGroup: batch
Mar  2 23:26:43.319: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  2 23:26:43.319: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar  2 23:26:43.319: INFO: batch/v1 matches batch/v1
Mar  2 23:26:43.319: INFO: Checking APIGroup: certificates.k8s.io
Mar  2 23:26:43.324: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  2 23:26:43.324: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.324: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  2 23:26:43.324: INFO: Checking APIGroup: networking.k8s.io
Mar  2 23:26:43.330: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  2 23:26:43.330: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.330: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  2 23:26:43.330: INFO: Checking APIGroup: extensions
Mar  2 23:26:43.348: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Mar  2 23:26:43.348: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Mar  2 23:26:43.348: INFO: extensions/v1beta1 matches extensions/v1beta1
Mar  2 23:26:43.348: INFO: Checking APIGroup: policy
Mar  2 23:26:43.352: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Mar  2 23:26:43.352: INFO: Versions found [{policy/v1beta1 v1beta1}]
Mar  2 23:26:43.352: INFO: policy/v1beta1 matches policy/v1beta1
Mar  2 23:26:43.352: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  2 23:26:43.356: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  2 23:26:43.356: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.356: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  2 23:26:43.356: INFO: Checking APIGroup: storage.k8s.io
Mar  2 23:26:43.361: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  2 23:26:43.361: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.361: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  2 23:26:43.361: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  2 23:26:43.369: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  2 23:26:43.369: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.369: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  2 23:26:43.369: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  2 23:26:43.374: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  2 23:26:43.374: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.374: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  2 23:26:43.374: INFO: Checking APIGroup: scheduling.k8s.io
Mar  2 23:26:43.382: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  2 23:26:43.382: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.382: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  2 23:26:43.382: INFO: Checking APIGroup: coordination.k8s.io
Mar  2 23:26:43.386: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  2 23:26:43.386: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.386: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  2 23:26:43.386: INFO: Checking APIGroup: node.k8s.io
Mar  2 23:26:43.390: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  2 23:26:43.391: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.391: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  2 23:26:43.391: INFO: Checking APIGroup: discovery.k8s.io
Mar  2 23:26:43.396: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Mar  2 23:26:43.396: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.396: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Mar  2 23:26:43.396: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  2 23:26:43.399: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Mar  2 23:26:43.400: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1} {flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Mar  2 23:26:43.400: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Mar  2 23:26:43.400: INFO: Checking APIGroup: apps.openshift.io
Mar  2 23:26:43.403: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Mar  2 23:26:43.403: INFO: Versions found [{apps.openshift.io/v1 v1}]
Mar  2 23:26:43.403: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Mar  2 23:26:43.403: INFO: Checking APIGroup: authorization.openshift.io
Mar  2 23:26:43.407: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Mar  2 23:26:43.407: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Mar  2 23:26:43.407: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Mar  2 23:26:43.407: INFO: Checking APIGroup: build.openshift.io
Mar  2 23:26:43.411: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Mar  2 23:26:43.411: INFO: Versions found [{build.openshift.io/v1 v1}]
Mar  2 23:26:43.411: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Mar  2 23:26:43.411: INFO: Checking APIGroup: image.openshift.io
Mar  2 23:26:43.415: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Mar  2 23:26:43.415: INFO: Versions found [{image.openshift.io/v1 v1}]
Mar  2 23:26:43.415: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Mar  2 23:26:43.415: INFO: Checking APIGroup: oauth.openshift.io
Mar  2 23:26:43.418: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Mar  2 23:26:43.418: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Mar  2 23:26:43.418: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Mar  2 23:26:43.418: INFO: Checking APIGroup: project.openshift.io
Mar  2 23:26:43.422: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Mar  2 23:26:43.422: INFO: Versions found [{project.openshift.io/v1 v1}]
Mar  2 23:26:43.422: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Mar  2 23:26:43.422: INFO: Checking APIGroup: quota.openshift.io
Mar  2 23:26:43.427: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Mar  2 23:26:43.427: INFO: Versions found [{quota.openshift.io/v1 v1}]
Mar  2 23:26:43.427: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Mar  2 23:26:43.427: INFO: Checking APIGroup: route.openshift.io
Mar  2 23:26:43.431: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Mar  2 23:26:43.431: INFO: Versions found [{route.openshift.io/v1 v1}]
Mar  2 23:26:43.431: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Mar  2 23:26:43.431: INFO: Checking APIGroup: security.openshift.io
Mar  2 23:26:43.436: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Mar  2 23:26:43.436: INFO: Versions found [{security.openshift.io/v1 v1}]
Mar  2 23:26:43.436: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Mar  2 23:26:43.436: INFO: Checking APIGroup: template.openshift.io
Mar  2 23:26:43.440: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Mar  2 23:26:43.440: INFO: Versions found [{template.openshift.io/v1 v1}]
Mar  2 23:26:43.440: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Mar  2 23:26:43.440: INFO: Checking APIGroup: user.openshift.io
Mar  2 23:26:43.444: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Mar  2 23:26:43.444: INFO: Versions found [{user.openshift.io/v1 v1}]
Mar  2 23:26:43.444: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Mar  2 23:26:43.444: INFO: Checking APIGroup: packages.operators.coreos.com
Mar  2 23:26:43.448: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Mar  2 23:26:43.448: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Mar  2 23:26:43.448: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Mar  2 23:26:43.448: INFO: Checking APIGroup: config.openshift.io
Mar  2 23:26:43.452: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Mar  2 23:26:43.452: INFO: Versions found [{config.openshift.io/v1 v1}]
Mar  2 23:26:43.453: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Mar  2 23:26:43.453: INFO: Checking APIGroup: operator.openshift.io
Mar  2 23:26:43.457: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Mar  2 23:26:43.457: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Mar  2 23:26:43.457: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Mar  2 23:26:43.457: INFO: Checking APIGroup: cloudcredential.openshift.io
Mar  2 23:26:43.461: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Mar  2 23:26:43.461: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Mar  2 23:26:43.462: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Mar  2 23:26:43.462: INFO: Checking APIGroup: console.openshift.io
Mar  2 23:26:43.465: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Mar  2 23:26:43.465: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Mar  2 23:26:43.465: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Mar  2 23:26:43.465: INFO: Checking APIGroup: crd.projectcalico.org
Mar  2 23:26:43.469: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  2 23:26:43.469: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  2 23:26:43.469: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar  2 23:26:43.469: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Mar  2 23:26:43.480: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Mar  2 23:26:43.480: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Mar  2 23:26:43.480: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Mar  2 23:26:43.480: INFO: Checking APIGroup: ingress.operator.openshift.io
Mar  2 23:26:43.484: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Mar  2 23:26:43.484: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Mar  2 23:26:43.484: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Mar  2 23:26:43.484: INFO: Checking APIGroup: k8s.cni.cncf.io
Mar  2 23:26:43.488: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Mar  2 23:26:43.488: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Mar  2 23:26:43.488: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Mar  2 23:26:43.488: INFO: Checking APIGroup: machineconfiguration.openshift.io
Mar  2 23:26:43.492: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Mar  2 23:26:43.492: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Mar  2 23:26:43.492: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Mar  2 23:26:43.492: INFO: Checking APIGroup: monitoring.coreos.com
Mar  2 23:26:43.496: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Mar  2 23:26:43.496: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Mar  2 23:26:43.496: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Mar  2 23:26:43.496: INFO: Checking APIGroup: network.operator.openshift.io
Mar  2 23:26:43.501: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Mar  2 23:26:43.501: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Mar  2 23:26:43.501: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Mar  2 23:26:43.501: INFO: Checking APIGroup: operator.tigera.io
Mar  2 23:26:43.505: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Mar  2 23:26:43.505: INFO: Versions found [{operator.tigera.io/v1 v1}]
Mar  2 23:26:43.505: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Mar  2 23:26:43.505: INFO: Checking APIGroup: operators.coreos.com
Mar  2 23:26:43.509: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Mar  2 23:26:43.509: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Mar  2 23:26:43.509: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Mar  2 23:26:43.509: INFO: Checking APIGroup: samples.operator.openshift.io
Mar  2 23:26:43.513: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Mar  2 23:26:43.513: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Mar  2 23:26:43.513: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Mar  2 23:26:43.513: INFO: Checking APIGroup: security.internal.openshift.io
Mar  2 23:26:43.518: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Mar  2 23:26:43.518: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Mar  2 23:26:43.518: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Mar  2 23:26:43.518: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar  2 23:26:43.523: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar  2 23:26:43.523: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.523: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar  2 23:26:43.523: INFO: Checking APIGroup: tuned.openshift.io
Mar  2 23:26:43.526: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Mar  2 23:26:43.526: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Mar  2 23:26:43.527: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Mar  2 23:26:43.527: INFO: Checking APIGroup: controlplane.operator.openshift.io
Mar  2 23:26:43.530: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Mar  2 23:26:43.530: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Mar  2 23:26:43.530: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Mar  2 23:26:43.530: INFO: Checking APIGroup: ibm.com
Mar  2 23:26:43.534: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Mar  2 23:26:43.534: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Mar  2 23:26:43.534: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Mar  2 23:26:43.534: INFO: Checking APIGroup: metal3.io
Mar  2 23:26:43.538: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Mar  2 23:26:43.538: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Mar  2 23:26:43.538: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Mar  2 23:26:43.538: INFO: Checking APIGroup: migration.k8s.io
Mar  2 23:26:43.542: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Mar  2 23:26:43.542: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Mar  2 23:26:43.542: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Mar  2 23:26:43.542: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Mar  2 23:26:43.546: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Mar  2 23:26:43.546: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Mar  2 23:26:43.546: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Mar  2 23:26:43.546: INFO: Checking APIGroup: helm.openshift.io
Mar  2 23:26:43.558: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Mar  2 23:26:43.558: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Mar  2 23:26:43.558: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Mar  2 23:26:43.558: INFO: Checking APIGroup: metrics.k8s.io
Mar  2 23:26:43.609: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar  2 23:26:43.609: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar  2 23:26:43.609: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:26:43.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-882" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":307,"skipped":5254,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:26:43.753: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Mar  2 23:26:43.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 create -f -'
Mar  2 23:26:45.038: INFO: stderr: ""
Mar  2 23:26:45.038: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 23:26:45.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 23:26:45.205: INFO: stderr: ""
Mar  2 23:26:45.205: INFO: stdout: "update-demo-nautilus-fqr97 update-demo-nautilus-ltttl "
Mar  2 23:26:45.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:26:45.378: INFO: stderr: ""
Mar  2 23:26:45.378: INFO: stdout: ""
Mar  2 23:26:45.378: INFO: update-demo-nautilus-fqr97 is created but not running
Mar  2 23:26:50.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 23:26:50.579: INFO: stderr: ""
Mar  2 23:26:50.579: INFO: stdout: "update-demo-nautilus-fqr97 update-demo-nautilus-ltttl "
Mar  2 23:26:50.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:26:50.734: INFO: stderr: ""
Mar  2 23:26:50.734: INFO: stdout: "true"
Mar  2 23:26:50.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 23:26:50.923: INFO: stderr: ""
Mar  2 23:26:50.923: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:26:50.923: INFO: validating pod update-demo-nautilus-fqr97
Mar  2 23:26:50.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:26:50.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:26:50.951: INFO: update-demo-nautilus-fqr97 is verified up and running
Mar  2 23:26:50.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-ltttl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:26:51.204: INFO: stderr: ""
Mar  2 23:26:51.204: INFO: stdout: ""
Mar  2 23:26:51.204: INFO: update-demo-nautilus-ltttl is created but not running
Mar  2 23:26:56.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 23:26:56.365: INFO: stderr: ""
Mar  2 23:26:56.365: INFO: stdout: "update-demo-nautilus-fqr97 update-demo-nautilus-ltttl "
Mar  2 23:26:56.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:26:56.554: INFO: stderr: ""
Mar  2 23:26:56.554: INFO: stdout: "true"
Mar  2 23:26:56.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 23:26:56.772: INFO: stderr: ""
Mar  2 23:26:56.772: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:26:56.772: INFO: validating pod update-demo-nautilus-fqr97
Mar  2 23:26:56.842: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:26:56.842: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:26:56.842: INFO: update-demo-nautilus-fqr97 is verified up and running
Mar  2 23:26:56.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-ltttl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:26:57.054: INFO: stderr: ""
Mar  2 23:26:57.054: INFO: stdout: "true"
Mar  2 23:26:57.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-ltttl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 23:26:57.216: INFO: stderr: ""
Mar  2 23:26:57.216: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:26:57.216: INFO: validating pod update-demo-nautilus-ltttl
Mar  2 23:26:57.257: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:26:57.257: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:26:57.257: INFO: update-demo-nautilus-ltttl is verified up and running
STEP: scaling down the replication controller
Mar  2 23:26:57.279: INFO: scanned /root for discovery docs: <nil>
Mar  2 23:26:57.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  2 23:26:58.537: INFO: stderr: ""
Mar  2 23:26:58.537: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 23:26:58.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 23:26:58.717: INFO: stderr: ""
Mar  2 23:26:58.717: INFO: stdout: "update-demo-nautilus-fqr97 update-demo-nautilus-ltttl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  2 23:27:03.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 23:27:03.921: INFO: stderr: ""
Mar  2 23:27:03.921: INFO: stdout: "update-demo-nautilus-fqr97 "
Mar  2 23:27:03.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:27:04.084: INFO: stderr: ""
Mar  2 23:27:04.084: INFO: stdout: "true"
Mar  2 23:27:04.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 23:27:04.249: INFO: stderr: ""
Mar  2 23:27:04.249: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:27:04.249: INFO: validating pod update-demo-nautilus-fqr97
Mar  2 23:27:04.268: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:27:04.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:27:04.268: INFO: update-demo-nautilus-fqr97 is verified up and running
STEP: scaling up the replication controller
Mar  2 23:27:04.277: INFO: scanned /root for discovery docs: <nil>
Mar  2 23:27:04.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  2 23:27:05.521: INFO: stderr: ""
Mar  2 23:27:05.521: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 23:27:05.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 23:27:05.703: INFO: stderr: ""
Mar  2 23:27:05.703: INFO: stdout: "update-demo-nautilus-fqr97 update-demo-nautilus-kw9qf "
Mar  2 23:27:05.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:27:05.931: INFO: stderr: ""
Mar  2 23:27:05.931: INFO: stdout: "true"
Mar  2 23:27:05.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 23:27:06.145: INFO: stderr: ""
Mar  2 23:27:06.145: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:27:06.145: INFO: validating pod update-demo-nautilus-fqr97
Mar  2 23:27:06.164: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:27:06.164: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:27:06.164: INFO: update-demo-nautilus-fqr97 is verified up and running
Mar  2 23:27:06.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-kw9qf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:27:06.339: INFO: stderr: ""
Mar  2 23:27:06.339: INFO: stdout: ""
Mar  2 23:27:06.339: INFO: update-demo-nautilus-kw9qf is created but not running
Mar  2 23:27:11.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 23:27:11.502: INFO: stderr: ""
Mar  2 23:27:11.502: INFO: stdout: "update-demo-nautilus-fqr97 update-demo-nautilus-kw9qf "
Mar  2 23:27:11.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:27:11.705: INFO: stderr: ""
Mar  2 23:27:11.705: INFO: stdout: "true"
Mar  2 23:27:11.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-fqr97 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 23:27:11.935: INFO: stderr: ""
Mar  2 23:27:11.935: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:27:11.935: INFO: validating pod update-demo-nautilus-fqr97
Mar  2 23:27:12.003: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:27:12.003: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:27:12.003: INFO: update-demo-nautilus-fqr97 is verified up and running
Mar  2 23:27:12.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-kw9qf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 23:27:12.190: INFO: stderr: ""
Mar  2 23:27:12.190: INFO: stdout: "true"
Mar  2 23:27:12.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods update-demo-nautilus-kw9qf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 23:27:12.352: INFO: stderr: ""
Mar  2 23:27:12.352: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:27:12.352: INFO: validating pod update-demo-nautilus-kw9qf
Mar  2 23:27:12.376: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:27:12.376: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:27:12.376: INFO: update-demo-nautilus-kw9qf is verified up and running
STEP: using delete to clean up resources
Mar  2 23:27:12.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 delete --grace-period=0 --force -f -'
Mar  2 23:27:12.551: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:27:12.551: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 23:27:12.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get rc,svc -l name=update-demo --no-headers'
Mar  2 23:27:12.767: INFO: stderr: "No resources found in kubectl-1850 namespace.\n"
Mar  2 23:27:12.767: INFO: stdout: ""
Mar  2 23:27:12.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-1850 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 23:27:12.932: INFO: stderr: ""
Mar  2 23:27:12.932: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:27:12.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1850" for this suite.

• [SLOW TEST:29.226 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":308,"skipped":5261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:27:12.979: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar  2 23:27:13.219: INFO: PodSpec: initContainers in spec.initContainers
Mar  2 23:28:00.408: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c75233fb-ab96-4e89-9bbe-23d28c966654", GenerateName:"", Namespace:"init-container-5899", SelfLink:"/api/v1/namespaces/init-container-5899/pods/pod-init-c75233fb-ab96-4e89-9bbe-23d28c966654", UID:"cc5169f2-7540-4bea-b189-d878c7ba253b", ResourceVersion:"124452", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63781860433, loc:(*time.Location)(0x7984060)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"219362094"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"f494ca8d6877af2cc2dbfb535b5f1affa76e9f9b2c05c8fbc841a5aedb60fa0e", "cni.projectcalico.org/podIP":"172.30.228.7/32", "cni.projectcalico.org/podIPs":"172.30.228.7/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.30.228.7\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.30.228.7\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005ff2820), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005ff2840)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005ff2860), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005ff2880)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005ff28a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005ff28c0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005ff28e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005ff2900)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7qlxb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002ccf680), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qlxb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002e78960), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qlxb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002e78a20), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qlxb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002e78900), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004a039d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.138.244.159", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002b82000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004a03a90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004a03ab0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004a03acc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004a03ad0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002208bb0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781860433, loc:(*time.Location)(0x7984060)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781860433, loc:(*time.Location)(0x7984060)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781860433, loc:(*time.Location)(0x7984060)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781860433, loc:(*time.Location)(0x7984060)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.138.244.159", PodIP:"172.30.228.7", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.228.7"}}, StartTime:(*v1.Time)(0xc005ff2920), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002b820e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002b82150)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://8f09b70d0fbd23c03e84bb080df1b48419738b6e64cdbf499a8eeb5ec31794e0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005ff2960), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005ff2940), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004a03b4f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:28:00.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5899" for this suite.

• [SLOW TEST:47.482 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":309,"skipped":5288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:28:00.462: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Mar  2 23:28:00.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-927863346 --namespace=kubectl-5444 cluster-info'
Mar  2 23:28:01.059: INFO: stderr: ""
Mar  2 23:28:01.059: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:28:01.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5444" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":310,"skipped":5317,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  2 23:28:01.111: INFO: >>> kubeConfig: /tmp/kubeconfig-927863346
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-461bfa87-b661-46f6-8d6d-e3b14fca26cc
STEP: Creating a pod to test consume secrets
Mar  2 23:28:01.386: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5245078e-1ecf-468b-846b-990073857246" in namespace "projected-3364" to be "Succeeded or Failed"
Mar  2 23:28:01.395: INFO: Pod "pod-projected-secrets-5245078e-1ecf-468b-846b-990073857246": Phase="Pending", Reason="", readiness=false. Elapsed: 8.734621ms
Mar  2 23:28:03.408: INFO: Pod "pod-projected-secrets-5245078e-1ecf-468b-846b-990073857246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021447375s
Mar  2 23:28:05.422: INFO: Pod "pod-projected-secrets-5245078e-1ecf-468b-846b-990073857246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035600009s
STEP: Saw pod success
Mar  2 23:28:05.422: INFO: Pod "pod-projected-secrets-5245078e-1ecf-468b-846b-990073857246" satisfied condition "Succeeded or Failed"
Mar  2 23:28:05.433: INFO: Trying to get logs from node 10.138.244.159 pod pod-projected-secrets-5245078e-1ecf-468b-846b-990073857246 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:28:05.502: INFO: Waiting for pod pod-projected-secrets-5245078e-1ecf-468b-846b-990073857246 to disappear
Mar  2 23:28:05.513: INFO: Pod pod-projected-secrets-5245078e-1ecf-468b-846b-990073857246 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 23:28:05.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3364" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSMar  2 23:28:05.576: INFO: Running AfterSuite actions on all nodes
Mar  2 23:28:05.577: INFO: Running AfterSuite actions on node 1
Mar  2 23:28:05.577: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5357,"failed":0}

Ran 311 of 5668 Specs in 6128.823 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5357 Skipped
PASS

Ginkgo ran 1 suite in 1h42m10.496223212s
Test Suite Passed
