I0222 08:49:50.183245      19 e2e.go:129] Starting e2e run "713a9533-c473-4b1e-a1f1-a9c89069f15a" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1645519789 - Will randomize all specs
Will run 346 of 6433 specs

Feb 22 08:49:55.670: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 08:49:55.689: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 22 08:49:55.722: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 22 08:49:55.805: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 22 08:49:55.805: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Feb 22 08:49:55.805: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 22 08:49:55.832: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 22 08:49:55.832: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'dcgm-exporter' (0 seconds elapsed)
Feb 22 08:49:55.832: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 22 08:49:55.832: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 22 08:49:55.832: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Feb 22 08:49:55.832: INFO: e2e test version: v1.22.5
Feb 22 08:49:55.836: INFO: kube-apiserver version: v1.22.5-ske.p2
Feb 22 08:49:55.836: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 08:49:55.847: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:49:55.848: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replication-controller
W0222 08:49:55.986776      19 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Feb 22 08:49:55.986: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-6380c6e5-f5f6-45a7-abcf-5c8af33e36ad
Feb 22 08:49:56.015: INFO: Pod name my-hostname-basic-6380c6e5-f5f6-45a7-abcf-5c8af33e36ad: Found 0 pods out of 1
Feb 22 08:50:01.025: INFO: Pod name my-hostname-basic-6380c6e5-f5f6-45a7-abcf-5c8af33e36ad: Found 1 pods out of 1
Feb 22 08:50:01.025: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6380c6e5-f5f6-45a7-abcf-5c8af33e36ad" are running
Feb 22 08:50:01.031: INFO: Pod "my-hostname-basic-6380c6e5-f5f6-45a7-abcf-5c8af33e36ad-85xzl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-22 08:49:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-22 08:49:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-22 08:49:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-22 08:49:56 +0000 UTC Reason: Message:}])
Feb 22 08:50:01.031: INFO: Trying to dial the pod
Feb 22 08:50:06.135: INFO: Controller my-hostname-basic-6380c6e5-f5f6-45a7-abcf-5c8af33e36ad: Got expected result from replica 1 [my-hostname-basic-6380c6e5-f5f6-45a7-abcf-5c8af33e36ad-85xzl]: "my-hostname-basic-6380c6e5-f5f6-45a7-abcf-5c8af33e36ad-85xzl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:50:06.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8234" for this suite.

• [SLOW TEST:10.318 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":1,"skipped":18,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:50:06.166: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Feb 22 08:50:08.391: INFO: running pods: 0 < 1
Feb 22 08:50:10.414: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:50:12.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8082" for this suite.

• [SLOW TEST:6.448 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":2,"skipped":25,"failed":0}
S
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:50:12.614: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:50:13.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5068" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":3,"skipped":26,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:50:13.149: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-c293ee11-7fae-4b8d-8f26-d6eb275ed148 in namespace container-probe-503
Feb 22 08:50:17.557: INFO: Started pod liveness-c293ee11-7fae-4b8d-8f26-d6eb275ed148 in namespace container-probe-503
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 08:50:17.563: INFO: Initial restart count of pod liveness-c293ee11-7fae-4b8d-8f26-d6eb275ed148 is 0
Feb 22 08:50:35.669: INFO: Restart count of pod container-probe-503/liveness-c293ee11-7fae-4b8d-8f26-d6eb275ed148 is now 1 (18.105840041s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:50:35.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-503" for this suite.

• [SLOW TEST:22.611 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":38,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:50:35.760: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 22 08:50:35.856: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 08:50:38.446: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:51:00.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2576" for this suite.

• [SLOW TEST:24.546 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":5,"skipped":39,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:51:00.325: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:51:16.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3627" for this suite.

• [SLOW TEST:16.560 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":6,"skipped":141,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:51:16.886: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 08:51:17.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f59f525e-a123-4792-9dcd-85769851f465" in namespace "downward-api-8922" to be "Succeeded or Failed"
Feb 22 08:51:17.090: INFO: Pod "downwardapi-volume-f59f525e-a123-4792-9dcd-85769851f465": Phase="Pending", Reason="", readiness=false. Elapsed: 33.420745ms
Feb 22 08:51:19.110: INFO: Pod "downwardapi-volume-f59f525e-a123-4792-9dcd-85769851f465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053055213s
Feb 22 08:51:21.124: INFO: Pod "downwardapi-volume-f59f525e-a123-4792-9dcd-85769851f465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066899208s
STEP: Saw pod success
Feb 22 08:51:21.124: INFO: Pod "downwardapi-volume-f59f525e-a123-4792-9dcd-85769851f465" satisfied condition "Succeeded or Failed"
Feb 22 08:51:21.150: INFO: Trying to get logs from node node2 pod downwardapi-volume-f59f525e-a123-4792-9dcd-85769851f465 container client-container: <nil>
STEP: delete the pod
Feb 22 08:51:21.288: INFO: Waiting for pod downwardapi-volume-f59f525e-a123-4792-9dcd-85769851f465 to disappear
Feb 22 08:51:21.297: INFO: Pod downwardapi-volume-f59f525e-a123-4792-9dcd-85769851f465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:51:21.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8922" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":7,"skipped":173,"failed":0}
SSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:51:21.325: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Feb 22 08:51:21.527: INFO: Waiting up to 5m0s for pod "security-context-1cfa0461-c9f1-42be-96d8-6d9370f4c861" in namespace "security-context-755" to be "Succeeded or Failed"
Feb 22 08:51:21.553: INFO: Pod "security-context-1cfa0461-c9f1-42be-96d8-6d9370f4c861": Phase="Pending", Reason="", readiness=false. Elapsed: 25.846381ms
Feb 22 08:51:23.567: INFO: Pod "security-context-1cfa0461-c9f1-42be-96d8-6d9370f4c861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040007374s
Feb 22 08:51:25.607: INFO: Pod "security-context-1cfa0461-c9f1-42be-96d8-6d9370f4c861": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079361451s
STEP: Saw pod success
Feb 22 08:51:25.607: INFO: Pod "security-context-1cfa0461-c9f1-42be-96d8-6d9370f4c861" satisfied condition "Succeeded or Failed"
Feb 22 08:51:25.613: INFO: Trying to get logs from node node2 pod security-context-1cfa0461-c9f1-42be-96d8-6d9370f4c861 container test-container: <nil>
STEP: delete the pod
Feb 22 08:51:25.763: INFO: Waiting for pod security-context-1cfa0461-c9f1-42be-96d8-6d9370f4c861 to disappear
Feb 22 08:51:25.772: INFO: Pod security-context-1cfa0461-c9f1-42be-96d8-6d9370f4c861 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:51:25.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-755" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":8,"skipped":176,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:51:25.805: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 08:51:27.297: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 22 08:51:29.375: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116687, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116687, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116687, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116687, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 08:51:32.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:51:32.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8701" for this suite.
STEP: Destroying namespace "webhook-8701-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.392 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":9,"skipped":181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:51:33.198: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-657563fc-f4cf-4ca9-9c56-76357a388f3d
STEP: Creating a pod to test consume secrets
Feb 22 08:51:33.445: INFO: Waiting up to 5m0s for pod "pod-secrets-4501a376-98e7-4da7-b88f-b04393a34adb" in namespace "secrets-6592" to be "Succeeded or Failed"
Feb 22 08:51:33.467: INFO: Pod "pod-secrets-4501a376-98e7-4da7-b88f-b04393a34adb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.038298ms
Feb 22 08:51:35.509: INFO: Pod "pod-secrets-4501a376-98e7-4da7-b88f-b04393a34adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063908563s
Feb 22 08:51:37.520: INFO: Pod "pod-secrets-4501a376-98e7-4da7-b88f-b04393a34adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075067971s
STEP: Saw pod success
Feb 22 08:51:37.520: INFO: Pod "pod-secrets-4501a376-98e7-4da7-b88f-b04393a34adb" satisfied condition "Succeeded or Failed"
Feb 22 08:51:37.531: INFO: Trying to get logs from node node2 pod pod-secrets-4501a376-98e7-4da7-b88f-b04393a34adb container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 08:51:37.634: INFO: Waiting for pod pod-secrets-4501a376-98e7-4da7-b88f-b04393a34adb to disappear
Feb 22 08:51:37.640: INFO: Pod pod-secrets-4501a376-98e7-4da7-b88f-b04393a34adb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:51:37.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6592" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":240,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:51:37.676: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 08:51:37.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5476 create -f -'
Feb 22 08:51:39.564: INFO: stderr: ""
Feb 22 08:51:39.564: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 22 08:51:39.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5476 create -f -'
Feb 22 08:51:39.995: INFO: stderr: ""
Feb 22 08:51:39.995: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 22 08:51:41.005: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 08:51:41.006: INFO: Found 0 / 1
Feb 22 08:51:42.006: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 08:51:42.007: INFO: Found 0 / 1
Feb 22 08:51:43.015: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 08:51:43.015: INFO: Found 1 / 1
Feb 22 08:51:43.015: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 22 08:51:43.024: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 08:51:43.024: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 22 08:51:43.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5476 describe pod agnhost-primary-49fg8'
Feb 22 08:51:43.246: INFO: stderr: ""
Feb 22 08:51:43.246: INFO: stdout: "Name:         agnhost-primary-49fg8\nNamespace:    kubectl-5476\nPriority:     0\nNode:         node2/172.28.128.13\nStart Time:   Tue, 22 Feb 2022 08:51:39 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: e91fa3f60bc1c012cc925413e7ecd3fd9c1abd69f76aee59e3609cda70d0c1b8\n              cni.projectcalico.org/podIP: 172.21.104.32/32\n              cni.projectcalico.org/podIPs: 172.21.104.32/32\nStatus:       Running\nIP:           172.21.104.32\nIPs:\n  IP:           172.21.104.32\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://3f9e9c7cc14915b6d4ec89e384526bddbcf845e8a3499a0c1ff2459c1e563d67\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 22 Feb 2022 08:51:41 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-998km (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-998km:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 60s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 60s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-5476/agnhost-primary-49fg8 to node2\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Feb 22 08:51:43.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5476 describe rc agnhost-primary'
Feb 22 08:51:43.452: INFO: stderr: ""
Feb 22 08:51:43.452: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5476\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-49fg8\n"
Feb 22 08:51:43.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5476 describe service agnhost-primary'
Feb 22 08:51:43.615: INFO: stderr: ""
Feb 22 08:51:43.615: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5476\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.20.57.142\nIPs:               172.20.57.142\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.21.104.32:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 22 08:51:43.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5476 describe node node1'
Feb 22 08:51:43.992: INFO: stderr: ""
Feb 22 08:51:43.992: INFO: stdout: "Name:               node1\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node1\n                    kubernetes.io/os=linux\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.28.128.12/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 172.21.40.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 22 Feb 2022 05:57:49 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  node1\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 22 Feb 2022 08:51:36 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 22 Feb 2022 08:09:14 +0000   Tue, 22 Feb 2022 08:09:14 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 22 Feb 2022 08:49:07 +0000   Tue, 22 Feb 2022 08:07:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 22 Feb 2022 08:49:07 +0000   Tue, 22 Feb 2022 08:07:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 22 Feb 2022 08:49:07 +0000   Tue, 22 Feb 2022 08:07:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 22 Feb 2022 08:49:07 +0000   Tue, 22 Feb 2022 08:09:00 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.28.128.12\n  Hostname:    node1\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    39269648Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1816308Ki\n  pods:                 110\nAllocatable:\n  cpu:                  2\n  ephemeral-storage:    36190907537\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1713908Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 cea29fab07df4fbda4f9d54f9769564e\n  System UUID:                D3CF239F-58A5-B747-8C9B-761AC5A1A3B1\n  Boot ID:                    77723751-86ec-4e58-8dff-3617a3e4001f\n  Kernel Version:             3.10.0-1127.19.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.2\n  Kubelet Version:            v1.22.5-ske.p2\n  Kube-Proxy Version:         v1.22.5-ske.p2\nPodCIDR:                      172.21.0.0/24\nPodCIDRs:                     172.21.0.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-nnc68                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         168m\n  kube-system                 coredns-575c8f4bf-l92w8                                    100m (5%)     0 (0%)      70Mi (4%)        170Mi (10%)    38m\n  kube-system                 kube-proxy-gj7fq                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m37s\n  kube-system                 node-exporter-dn6cm                                        100m (5%)     200m (10%)  50Mi (2%)        200Mi (11%)    168m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m\n  sonobuoy                    sonobuoy-e2e-job-71a20d3e324c433a                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         118s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-whn5g    0 (0%)        0 (0%)      0 (0%)           0 (0%)         118s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests    Limits\n  --------             --------    ------\n  cpu                  450m (22%)  200m (10%)\n  memory               120Mi (7%)  370Mi (22%)\n  ephemeral-storage    0 (0%)      0 (0%)\n  hugepages-1Gi        0 (0%)      0 (0%)\n  hugepages-2Mi        0 (0%)      0 (0%)\n  example.com/fakecpu  0           0\nEvents:\n  Type    Reason                   Age                From        Message\n  ----    ------                   ----               ----        -------\n  Normal  Starting                 4m36s              kube-proxy  \n  Normal  Starting                 38m                kube-proxy  \n  Normal  NodeHasSufficientMemory  47m (x2 over 47m)  kubelet     Node node1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    47m (x2 over 47m)  kubelet     Node node1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     47m (x2 over 47m)  kubelet     Node node1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             47m                kubelet     Node node1 status is now: NodeNotReady\n  Normal  Starting                 47m                kubelet     Starting kubelet.\n  Normal  NodeReady                47m                kubelet     Node node1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  47m                kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 43m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  43m (x2 over 43m)  kubelet     Node node1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    43m (x2 over 43m)  kubelet     Node node1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     43m (x2 over 43m)  kubelet     Node node1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             43m                kubelet     Node node1 status is now: NodeNotReady\n  Normal  NodeReady                43m                kubelet     Node node1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  43m                kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 42m                kubelet     Starting kubelet.\n  Normal  NodeHasNoDiskPressure    42m                kubelet     Node node1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     42m                kubelet     Node node1 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  42m                kubelet     Node node1 status is now: NodeHasSufficientMemory\n  Normal  NodeNotReady             42m                kubelet     Node node1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  42m                kubelet     Updated Node Allocatable limit across pods\n  Normal  NodeReady                42m                kubelet     Node node1 status is now: NodeReady\n"
Feb 22 08:51:43.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5476 describe namespace kubectl-5476'
Feb 22 08:51:44.138: INFO: stderr: ""
Feb 22 08:51:44.138: INFO: stdout: "Name:         kubectl-5476\nLabels:       e2e-framework=kubectl\n              e2e-run=713a9533-c473-4b1e-a1f1-a9c89069f15a\n              kubernetes.io/metadata.name=kubectl-5476\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:51:44.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5476" for this suite.

• [SLOW TEST:6.489 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1094
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":11,"skipped":249,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:51:44.165: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 22 08:51:44.303: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 22 08:52:44.362: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Feb 22 08:52:44.446: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 22 08:52:44.470: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 22 08:52:44.560: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 22 08:52:44.584: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:53:08.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8362" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:84.831 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":12,"skipped":266,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:53:08.996: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 22 08:53:10.489: INFO: starting watch
STEP: patching
STEP: updating
Feb 22 08:53:10.608: INFO: waiting for watch events with expected annotations
Feb 22 08:53:10.609: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:53:10.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7356" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":13,"skipped":281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:53:10.980: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 08:53:11.309: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:53:12.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5512" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":14,"skipped":308,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:53:12.541: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:53:12.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-810" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":15,"skipped":314,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:53:12.923: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7512
STEP: creating service affinity-clusterip in namespace services-7512
STEP: creating replication controller affinity-clusterip in namespace services-7512
I0222 08:53:13.636975      19 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-7512, replica count: 3
I0222 08:53:16.691732      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 08:53:19.707778      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 08:53:22.708882      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 08:53:22.735: INFO: Creating new exec pod
Feb 22 08:53:27.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-7512 exec execpod-affinity9zv7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Feb 22 08:53:28.540: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 22 08:53:28.540: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 08:53:28.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-7512 exec execpod-affinity9zv7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.85.186 80'
Feb 22 08:53:29.188: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.85.186 80\nConnection to 172.20.85.186 80 port [tcp/http] succeeded!\n"
Feb 22 08:53:29.188: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 08:53:29.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-7512 exec execpod-affinity9zv7x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.85.186:80/ ; done'
Feb 22 08:53:29.996: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.85.186:80/\n"
Feb 22 08:53:29.996: INFO: stdout: "\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57\naffinity-clusterip-5cv57"
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Received response from host: affinity-clusterip-5cv57
Feb 22 08:53:29.996: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7512, will wait for the garbage collector to delete the pods
Feb 22 08:53:30.141: INFO: Deleting ReplicationController affinity-clusterip took: 18.746814ms
Feb 22 08:53:30.342: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.661372ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:53:34.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7512" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:21.302 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":16,"skipped":314,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:53:34.225: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 08:53:35.669: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 22 08:53:37.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116815, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116815, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116815, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116815, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 08:53:40.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:53:41.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8217" for this suite.
STEP: Destroying namespace "webhook-8217-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.738 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":17,"skipped":319,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:53:41.963: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 08:53:43.291: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 08:53:45.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116823, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116823, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116823, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116823, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 08:53:48.378: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:53:59.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1749" for this suite.
STEP: Destroying namespace "webhook-1749-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.475 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":18,"skipped":325,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:53:59.439: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 22 08:53:59.561: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 22 08:54:34.132: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 08:54:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:55:03.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2365" for this suite.

• [SLOW TEST:64.201 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":19,"skipped":334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:55:03.640: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9440
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 22 08:55:03.790: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 22 08:55:03.922: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:55:05.936: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:55:07.939: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:09.934: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:11.930: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:13.934: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:15.931: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:17.933: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:19.933: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:21.929: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:23.930: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 08:55:25.929: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 22 08:55:25.938: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Feb 22 08:55:30.035: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Feb 22 08:55:30.035: INFO: Going to poll 172.21.40.131 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Feb 22 08:55:30.041: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.40.131 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9440 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 08:55:30.042: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 08:55:31.587: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 22 08:55:31.587: INFO: Going to poll 172.21.104.39 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Feb 22 08:55:31.598: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.104.39 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9440 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 08:55:31.598: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 08:55:32.994: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:55:32.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9440" for this suite.

• [SLOW TEST:29.384 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":20,"skipped":368,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:55:33.024: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Feb 22 08:55:33.202: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:55:35.231: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:55:37.214: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 22 08:55:37.245: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:55:39.271: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:55:41.263: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 22 08:55:41.417: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 08:55:41.427: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 08:55:43.431: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 08:55:43.449: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 08:55:45.430: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 08:55:45.444: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:55:45.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7059" for this suite.

• [SLOW TEST:12.445 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":388,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:55:45.471: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 08:55:46.906: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 08:55:48.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116947, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116947, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116947, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781116946, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 08:55:52.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:55:52.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1932" for this suite.
STEP: Destroying namespace "webhook-1932-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.887 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":22,"skipped":397,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:55:52.359: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-31346d57-00aa-4a0f-a566-1daa8647d24e
STEP: Creating secret with name s-test-opt-upd-775a58ff-40f8-41d0-8ec1-978ea83f315c
STEP: Creating the pod
Feb 22 08:55:52.764: INFO: The status of Pod pod-secrets-84f68bdf-5229-4e37-bd9c-a365443ae67a is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:55:54.777: INFO: The status of Pod pod-secrets-84f68bdf-5229-4e37-bd9c-a365443ae67a is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:55:56.773: INFO: The status of Pod pod-secrets-84f68bdf-5229-4e37-bd9c-a365443ae67a is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-31346d57-00aa-4a0f-a566-1daa8647d24e
STEP: Updating secret s-test-opt-upd-775a58ff-40f8-41d0-8ec1-978ea83f315c
STEP: Creating secret with name s-test-opt-create-5caac105-8380-4299-954a-7f09d61f6c8a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:55:59.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-786" for this suite.

• [SLOW TEST:6.995 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":23,"skipped":406,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:55:59.355: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 22 08:55:59.515: INFO: Waiting up to 5m0s for pod "pod-f6010be6-b434-41e6-9216-79fe951b862f" in namespace "emptydir-4623" to be "Succeeded or Failed"
Feb 22 08:55:59.523: INFO: Pod "pod-f6010be6-b434-41e6-9216-79fe951b862f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.688065ms
Feb 22 08:56:01.549: INFO: Pod "pod-f6010be6-b434-41e6-9216-79fe951b862f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033362005s
Feb 22 08:56:03.562: INFO: Pod "pod-f6010be6-b434-41e6-9216-79fe951b862f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046495003s
STEP: Saw pod success
Feb 22 08:56:03.562: INFO: Pod "pod-f6010be6-b434-41e6-9216-79fe951b862f" satisfied condition "Succeeded or Failed"
Feb 22 08:56:03.572: INFO: Trying to get logs from node node1 pod pod-f6010be6-b434-41e6-9216-79fe951b862f container test-container: <nil>
STEP: delete the pod
Feb 22 08:56:03.695: INFO: Waiting for pod pod-f6010be6-b434-41e6-9216-79fe951b862f to disappear
Feb 22 08:56:03.703: INFO: Pod pod-f6010be6-b434-41e6-9216-79fe951b862f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:56:03.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4623" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":411,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:56:03.733: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Feb 22 08:56:03.943: INFO: created test-pod-1
Feb 22 08:56:04.018: INFO: created test-pod-2
Feb 22 08:56:04.147: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Feb 22 08:56:04.510: INFO: Pod quantity 3 is different from expected quantity 0
Feb 22 08:56:05.622: INFO: Pod quantity 3 is different from expected quantity 0
Feb 22 08:56:06.534: INFO: Pod quantity 3 is different from expected quantity 0
Feb 22 08:56:07.550: INFO: Pod quantity 3 is different from expected quantity 0
Feb 22 08:56:08.526: INFO: Pod quantity 3 is different from expected quantity 0
Feb 22 08:56:09.525: INFO: Pod quantity 3 is different from expected quantity 0
Feb 22 08:56:10.620: INFO: Pod quantity 3 is different from expected quantity 0
Feb 22 08:56:11.578: INFO: Pod quantity 3 is different from expected quantity 0
Feb 22 08:56:12.517: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:56:13.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8313" for this suite.

• [SLOW TEST:9.862 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":25,"skipped":425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:56:13.596: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Feb 22 08:56:39.900: INFO: EndpointSlice for Service endpointslice-6586/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:56:49.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6586" for this suite.

• [SLOW TEST:36.368 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":26,"skipped":452,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:56:49.964: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 22 08:56:50.146: INFO: The status of Pod annotationupdate68a9affc-9564-4402-bf74-f6b44e07a70b is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:56:52.170: INFO: The status of Pod annotationupdate68a9affc-9564-4402-bf74-f6b44e07a70b is Pending, waiting for it to be Running (with Ready = true)
Feb 22 08:56:54.152: INFO: The status of Pod annotationupdate68a9affc-9564-4402-bf74-f6b44e07a70b is Running (Ready = true)
Feb 22 08:56:54.754: INFO: Successfully updated pod "annotationupdate68a9affc-9564-4402-bf74-f6b44e07a70b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:56:56.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2367" for this suite.

• [SLOW TEST:6.891 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":469,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:56:56.855: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:57:25.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7291" for this suite.

• [SLOW TEST:28.493 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":28,"skipped":489,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:57:25.349: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Feb 22 08:57:25.481: INFO: Waiting up to 5m0s for pod "var-expansion-886da3b6-956a-4c8d-aaf8-1cfd10f64fee" in namespace "var-expansion-268" to be "Succeeded or Failed"
Feb 22 08:57:25.489: INFO: Pod "var-expansion-886da3b6-956a-4c8d-aaf8-1cfd10f64fee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.776564ms
Feb 22 08:57:27.503: INFO: Pod "var-expansion-886da3b6-956a-4c8d-aaf8-1cfd10f64fee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022272955s
Feb 22 08:57:29.513: INFO: Pod "var-expansion-886da3b6-956a-4c8d-aaf8-1cfd10f64fee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03177657s
STEP: Saw pod success
Feb 22 08:57:29.513: INFO: Pod "var-expansion-886da3b6-956a-4c8d-aaf8-1cfd10f64fee" satisfied condition "Succeeded or Failed"
Feb 22 08:57:29.518: INFO: Trying to get logs from node node2 pod var-expansion-886da3b6-956a-4c8d-aaf8-1cfd10f64fee container dapi-container: <nil>
STEP: delete the pod
Feb 22 08:57:29.597: INFO: Waiting for pod var-expansion-886da3b6-956a-4c8d-aaf8-1cfd10f64fee to disappear
Feb 22 08:57:29.606: INFO: Pod var-expansion-886da3b6-956a-4c8d-aaf8-1cfd10f64fee no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:57:29.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-268" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":29,"skipped":527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:57:29.635: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:57:41.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4762" for this suite.

• [SLOW TEST:12.196 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":30,"skipped":549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:57:41.833: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-355b3790-8ecd-43f7-9164-c8ba1d245670
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 08:57:46.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7461" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 08:57:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 08:58:54.646: INFO: File jessie_udp@dns-test-service-3.dns-5514.svc.cluster.local from pod  dns-5514/dns-test-184dea4e-e950-4906-a02e-7ab0404c3f6a contains '' instead of 'foo.example.com.'
Feb 22 08:58:54.647: INFO: Lookups using dns-5514/dns-test-184dea4e-e950-4906-a02e-7ab0404c3f6a failed for: [jessie_udp@dns-test-service-3.dns-5514.svc.cluster.local]

Feb 22 08:58:59.705: INFO: DNS probes using dns-test-184dea4e-e950-4906-a02e-7ab0404c3f6a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 09:00:10.292: INFO: DNS probes using dns-test-128265e3-6f5d-4310-bffd-1c33748a6e1b succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 09:00:16.610: INFO: DNS probes using dns-test-f5bf7189-742d-4533-bbcf-4671ad91fd2a succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:00:16.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5514" for this suite.

• [SLOW TEST:150.608 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":32,"skipped":665,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:00:16.812: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3177
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-3177
Feb 22 09:00:17.291: INFO: Found 0 stateful pods, waiting for 1
Feb 22 09:00:27.301: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 22 09:00:27.483: INFO: Deleting all statefulset in ns statefulset-3177
Feb 22 09:00:27.581: INFO: Scaling statefulset ss to 0
Feb 22 09:00:37.749: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 09:00:37.756: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:00:37.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3177" for this suite.

• [SLOW TEST:21.148 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":33,"skipped":668,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:00:37.960: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 22 09:00:38.272: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8260  858bebbd-6c3f-4cbb-95ee-36cc4582f388 40649 0 2022-02-22 09:00:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-02-22 09:00:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 09:00:38.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8260  858bebbd-6c3f-4cbb-95ee-36cc4582f388 40650 0 2022-02-22 09:00:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-02-22 09:00:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:00:38.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8260" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":34,"skipped":673,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:00:38.311: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5783 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5783;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5783 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5783;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5783.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5783.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5783.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5783.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5783.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5783.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5783.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5783.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5783.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5783.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5783.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 24.32.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.32.24_udp@PTR;check="$$(dig +tcp +noall +answer +search 24.32.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.32.24_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5783 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5783;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5783 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5783;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5783.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5783.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5783.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5783.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5783.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5783.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5783.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5783.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5783.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5783.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5783.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5783.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 24.32.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.32.24_udp@PTR;check="$$(dig +tcp +noall +answer +search 24.32.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.32.24_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 09:00:45.045: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.121: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.182: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.231: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.275: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.324: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.337: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.357: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.884: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.928: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:45.988: INFO: Unable to read jessie_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:46.031: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:46.082: INFO: Unable to read jessie_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:46.132: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:46.174: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:46.262: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:46.350: INFO: Lookups using dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5783 wheezy_tcp@dns-test-service.dns-5783 wheezy_udp@dns-test-service.dns-5783.svc wheezy_tcp@dns-test-service.dns-5783.svc wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5783 jessie_tcp@dns-test-service.dns-5783 jessie_udp@dns-test-service.dns-5783.svc jessie_tcp@dns-test-service.dns-5783.svc jessie_udp@_http._tcp.dns-test-service.dns-5783.svc jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc]

Feb 22 09:00:51.390: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.435: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.452: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.473: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.483: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.585: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.631: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.743: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.769: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.787: INFO: Unable to read jessie_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.799: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.810: INFO: Unable to read jessie_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.822: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.907: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:51.956: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:52.141: INFO: Lookups using dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5783 wheezy_tcp@dns-test-service.dns-5783 wheezy_udp@dns-test-service.dns-5783.svc wheezy_tcp@dns-test-service.dns-5783.svc wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5783 jessie_tcp@dns-test-service.dns-5783 jessie_udp@dns-test-service.dns-5783.svc jessie_tcp@dns-test-service.dns-5783.svc jessie_udp@_http._tcp.dns-test-service.dns-5783.svc jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc]

Feb 22 09:00:56.374: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.419: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.428: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.437: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.445: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.456: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.473: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.481: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.536: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.545: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.585: INFO: Unable to read jessie_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.600: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.608: INFO: Unable to read jessie_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.616: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.624: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.634: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:00:56.711: INFO: Lookups using dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5783 wheezy_tcp@dns-test-service.dns-5783 wheezy_udp@dns-test-service.dns-5783.svc wheezy_tcp@dns-test-service.dns-5783.svc wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5783 jessie_tcp@dns-test-service.dns-5783 jessie_udp@dns-test-service.dns-5783.svc jessie_tcp@dns-test-service.dns-5783.svc jessie_udp@_http._tcp.dns-test-service.dns-5783.svc jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc]

Feb 22 09:01:07.017: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.077: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.241: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.276: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.360: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.406: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.415: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.424: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.495: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.507: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.529: INFO: Unable to read jessie_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.538: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.547: INFO: Unable to read jessie_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.562: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.571: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.581: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:07.635: INFO: Lookups using dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5783 wheezy_tcp@dns-test-service.dns-5783 wheezy_udp@dns-test-service.dns-5783.svc wheezy_tcp@dns-test-service.dns-5783.svc wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5783 jessie_tcp@dns-test-service.dns-5783 jessie_udp@dns-test-service.dns-5783.svc jessie_tcp@dns-test-service.dns-5783.svc jessie_udp@_http._tcp.dns-test-service.dns-5783.svc jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc]

Feb 22 09:01:11.416: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.471: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.488: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.502: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.561: INFO: Unable to read wheezy_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.572: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.620: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.634: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.747: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.763: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.775: INFO: Unable to read jessie_udp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.787: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783 from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.806: INFO: Unable to read jessie_udp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.824: INFO: Unable to read jessie_tcp@dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.845: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.862: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc from pod dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31: the server could not find the requested resource (get pods dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31)
Feb 22 09:01:11.971: INFO: Lookups using dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5783 wheezy_tcp@dns-test-service.dns-5783 wheezy_udp@dns-test-service.dns-5783.svc wheezy_tcp@dns-test-service.dns-5783.svc wheezy_udp@_http._tcp.dns-test-service.dns-5783.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5783.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5783 jessie_tcp@dns-test-service.dns-5783 jessie_udp@dns-test-service.dns-5783.svc jessie_tcp@dns-test-service.dns-5783.svc jessie_udp@_http._tcp.dns-test-service.dns-5783.svc jessie_tcp@_http._tcp.dns-test-service.dns-5783.svc]

Feb 22 09:01:16.634: INFO: DNS probes using dns-5783/dns-test-8b84d4db-e8f9-4f16-8434-139df13adf31 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:01:17.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5783" for this suite.

• [SLOW TEST:39.323 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":35,"skipped":676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:01:17.635: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Feb 22 09:01:17.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1255 create -f -'
Feb 22 09:01:19.805: INFO: stderr: ""
Feb 22 09:01:19.805: INFO: stdout: "pod/pause created\n"
Feb 22 09:01:19.805: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 22 09:01:19.805: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1255" to be "running and ready"
Feb 22 09:01:19.824: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.338015ms
Feb 22 09:01:21.870: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064904959s
Feb 22 09:01:23.885: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.07947915s
Feb 22 09:01:23.885: INFO: Pod "pause" satisfied condition "running and ready"
Feb 22 09:01:23.885: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 22 09:01:23.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1255 label pods pause testing-label=testing-label-value'
Feb 22 09:01:24.104: INFO: stderr: ""
Feb 22 09:01:24.104: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 22 09:01:24.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1255 get pod pause -L testing-label'
Feb 22 09:01:24.254: INFO: stderr: ""
Feb 22 09:01:24.254: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 22 09:01:24.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1255 label pods pause testing-label-'
Feb 22 09:01:24.841: INFO: stderr: ""
Feb 22 09:01:24.841: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 22 09:01:24.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1255 get pod pause -L testing-label'
Feb 22 09:01:25.434: INFO: stderr: ""
Feb 22 09:01:25.434: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Feb 22 09:01:25.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1255 delete --grace-period=0 --force -f -'
Feb 22 09:01:26.050: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 09:01:26.050: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 22 09:01:26.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1255 get rc,svc -l name=pause --no-headers'
Feb 22 09:01:26.303: INFO: stderr: "No resources found in kubectl-1255 namespace.\n"
Feb 22 09:01:26.303: INFO: stdout: ""
Feb 22 09:01:26.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1255 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 09:01:26.535: INFO: stderr: ""
Feb 22 09:01:26.535: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:01:26.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1255" for this suite.

• [SLOW TEST:8.974 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1316
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":36,"skipped":719,"failed":0}
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:01:26.609: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:02:26.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6788" for this suite.

• [SLOW TEST:60.350 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":719,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:02:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:02:27.122: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 22 09:02:27.179: INFO: The status of Pod pod-exec-websocket-c2d48ba6-71ba-46a1-a195-7b01835d62b2 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:02:29.190: INFO: The status of Pod pod-exec-websocket-c2d48ba6-71ba-46a1-a195-7b01835d62b2 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:02:31.189: INFO: The status of Pod pod-exec-websocket-c2d48ba6-71ba-46a1-a195-7b01835d62b2 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:02:31.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5628" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:02:31.624: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:02:32.910: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 09:02:34.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117353, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117353, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117353, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117352, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:02:38.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:02:38.012: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2674-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:02:41.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7730" for this suite.
STEP: Destroying namespace "webhook-7730-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.199 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":39,"skipped":771,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:02:41.823: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:02:42.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18a6fdee-a260-4451-930f-c212091bbcf0" in namespace "downward-api-6560" to be "Succeeded or Failed"
Feb 22 09:02:42.163: INFO: Pod "downwardapi-volume-18a6fdee-a260-4451-930f-c212091bbcf0": Phase="Pending", Reason="", readiness=false. Elapsed: 57.338835ms
Feb 22 09:02:44.185: INFO: Pod "downwardapi-volume-18a6fdee-a260-4451-930f-c212091bbcf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079333792s
Feb 22 09:02:46.220: INFO: Pod "downwardapi-volume-18a6fdee-a260-4451-930f-c212091bbcf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114529388s
STEP: Saw pod success
Feb 22 09:02:46.220: INFO: Pod "downwardapi-volume-18a6fdee-a260-4451-930f-c212091bbcf0" satisfied condition "Succeeded or Failed"
Feb 22 09:02:46.228: INFO: Trying to get logs from node node2 pod downwardapi-volume-18a6fdee-a260-4451-930f-c212091bbcf0 container client-container: <nil>
STEP: delete the pod
Feb 22 09:02:46.381: INFO: Waiting for pod downwardapi-volume-18a6fdee-a260-4451-930f-c212091bbcf0 to disappear
Feb 22 09:02:46.407: INFO: Pod downwardapi-volume-18a6fdee-a260-4451-930f-c212091bbcf0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:02:46.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6560" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":40,"skipped":790,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:02:46.448: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 22 09:02:46.808: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 22 09:02:46.841: INFO: starting watch
STEP: patching
STEP: updating
Feb 22 09:02:46.950: INFO: waiting for watch events with expected annotations
Feb 22 09:02:46.950: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:02:47.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8602" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":41,"skipped":801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:02:47.200: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Feb 22 09:02:47.443: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:02:49.486: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:02:51.459: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Feb 22 09:02:51.522: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:02:53.578: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:02:55.530: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 22 09:02:55.538: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:55.538: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:02:56.141: INFO: Exec stderr: ""
Feb 22 09:02:56.141: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:56.141: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:02:56.691: INFO: Exec stderr: ""
Feb 22 09:02:56.691: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:56.691: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:02:57.280: INFO: Exec stderr: ""
Feb 22 09:02:57.280: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:57.280: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:02:57.808: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 22 09:02:57.809: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:57.809: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:02:58.317: INFO: Exec stderr: ""
Feb 22 09:02:58.317: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:58.317: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:02:58.838: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 22 09:02:58.838: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:58.838: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:02:59.310: INFO: Exec stderr: ""
Feb 22 09:02:59.310: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:59.310: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:02:59.695: INFO: Exec stderr: ""
Feb 22 09:02:59.695: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:02:59.696: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:03:00.107: INFO: Exec stderr: ""
Feb 22 09:03:00.107: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:03:00.107: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:03:00.622: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:03:00.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8115" for this suite.

• [SLOW TEST:13.477 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:03:00.679: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-d8355099-8e61-4388-9921-0c1eba3b8821
STEP: Creating configMap with name cm-test-opt-upd-70dc0962-cd35-483a-b3a5-9146457289c9
STEP: Creating the pod
Feb 22 09:03:00.878: INFO: The status of Pod pod-projected-configmaps-1c39b4b0-0d68-4f42-a14e-e29cc302ea56 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:03:02.901: INFO: The status of Pod pod-projected-configmaps-1c39b4b0-0d68-4f42-a14e-e29cc302ea56 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:03:04.889: INFO: The status of Pod pod-projected-configmaps-1c39b4b0-0d68-4f42-a14e-e29cc302ea56 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-d8355099-8e61-4388-9921-0c1eba3b8821
STEP: Updating configmap cm-test-opt-upd-70dc0962-cd35-483a-b3a5-9146457289c9
STEP: Creating configMap with name cm-test-opt-create-d52428cf-2d3e-46b9-95d2-9b666ae02eb8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:03:07.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3473" for this suite.

• [SLOW TEST:6.781 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":879,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:03:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-9b8a4e1f-3ba4-423a-bf54-c56697041bf3
STEP: Creating a pod to test consume configMaps
Feb 22 09:03:07.630: INFO: Waiting up to 5m0s for pod "pod-configmaps-401ef91a-cb15-4c35-8043-702b79c86100" in namespace "configmap-8871" to be "Succeeded or Failed"
Feb 22 09:03:07.698: INFO: Pod "pod-configmaps-401ef91a-cb15-4c35-8043-702b79c86100": Phase="Pending", Reason="", readiness=false. Elapsed: 67.520788ms
Feb 22 09:03:09.717: INFO: Pod "pod-configmaps-401ef91a-cb15-4c35-8043-702b79c86100": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087328255s
Feb 22 09:03:11.738: INFO: Pod "pod-configmaps-401ef91a-cb15-4c35-8043-702b79c86100": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.10760452s
STEP: Saw pod success
Feb 22 09:03:11.738: INFO: Pod "pod-configmaps-401ef91a-cb15-4c35-8043-702b79c86100" satisfied condition "Succeeded or Failed"
Feb 22 09:03:11.759: INFO: Trying to get logs from node node1 pod pod-configmaps-401ef91a-cb15-4c35-8043-702b79c86100 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:03:11.878: INFO: Waiting for pod pod-configmaps-401ef91a-cb15-4c35-8043-702b79c86100 to disappear
Feb 22 09:03:11.885: INFO: Pod pod-configmaps-401ef91a-cb15-4c35-8043-702b79c86100 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:03:11.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8871" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:03:11.924: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:03:13.444: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 22 09:03:15.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117393, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117393, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117393, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117393, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:03:17.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117393, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117393, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117393, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117393, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:03:20.691: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:03:20.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1012" for this suite.
STEP: Destroying namespace "webhook-1012-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.490 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":45,"skipped":917,"failed":0}
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:03:21.414: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:03:21.962: INFO: The status of Pod busybox-readonly-fs191ad611-7fb5-412f-b89c-adc87c12e616 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:03:24.002: INFO: The status of Pod busybox-readonly-fs191ad611-7fb5-412f-b89c-adc87c12e616 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:03:25.978: INFO: The status of Pod busybox-readonly-fs191ad611-7fb5-412f-b89c-adc87c12e616 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:03:27.984: INFO: The status of Pod busybox-readonly-fs191ad611-7fb5-412f-b89c-adc87c12e616 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:03:28.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-26" for this suite.

• [SLOW TEST:6.952 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":918,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:03:28.367: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Feb 22 09:03:29.084: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:03:29.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8712" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":47,"skipped":920,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:03:29.959: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3711
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3711
I0222 09:03:30.986217      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3711, replica count: 2
I0222 09:03:34.040606      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:03:37.042082      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:03:40.043326      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 09:03:40.043: INFO: Creating new exec pod
Feb 22 09:03:45.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 22 09:03:46.435: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:46.435: INFO: stdout: "externalname-service-lmkvc"
Feb 22 09:03:46.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:47.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.5.255 80\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:47.163: INFO: stdout: ""
Feb 22 09:03:48.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:48.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.5.255 80\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:48.824: INFO: stdout: ""
Feb 22 09:03:49.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:49.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.5.255 80\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:49.865: INFO: stdout: ""
Feb 22 09:03:50.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:50.859: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.5.255 80\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:50.859: INFO: stdout: ""
Feb 22 09:03:51.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:51.809: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.5.255 80\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:51.809: INFO: stdout: ""
Feb 22 09:03:52.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:52.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.5.255 80\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:52.994: INFO: stdout: ""
Feb 22 09:03:53.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:53.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.5.255 80\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:53.886: INFO: stdout: ""
Feb 22 09:03:54.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:54.847: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.5.255 80\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:54.847: INFO: stdout: ""
Feb 22 09:03:55.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.5.255 80'
Feb 22 09:03:55.850: INFO: stderr: "+ nc -v -t -w 2 172.20.5.255 80\n+ echo hostName\nConnection to 172.20.5.255 80 port [tcp/http] succeeded!\n"
Feb 22 09:03:55.850: INFO: stdout: "externalname-service-lmkvc"
Feb 22 09:03:55.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 31567'
Feb 22 09:03:56.546: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 31567\nConnection to 172.28.128.12 31567 port [tcp/*] succeeded!\n"
Feb 22 09:03:56.546: INFO: stdout: "externalname-service-lmkvc"
Feb 22 09:03:56.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 31567'
Feb 22 09:03:57.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.13 31567\nConnection to 172.28.128.13 31567 port [tcp/*] succeeded!\n"
Feb 22 09:03:57.239: INFO: stdout: ""
Feb 22 09:03:58.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3711 exec execpodg5265 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 31567'
Feb 22 09:03:58.857: INFO: stderr: "+ + nc -v -t -w 2 172.28.128.13 31567\necho hostName\nConnection to 172.28.128.13 31567 port [tcp/*] succeeded!\n"
Feb 22 09:03:58.857: INFO: stdout: "externalname-service-m6frg"
Feb 22 09:03:58.857: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:03:58.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3711" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:29.057 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":48,"skipped":933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:03:59.021: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:03:59.807: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 09:04:01.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117439, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117439, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117440, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781117439, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:04:04.890: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 22 09:04:05.048: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:04:05.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4752" for this suite.
STEP: Destroying namespace "webhook-4752-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.661 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":49,"skipped":966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:04:05.780: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-fa15543c-d839-4448-b54e-5da5ae2f0d00
STEP: Creating a pod to test consume configMaps
Feb 22 09:04:06.176: INFO: Waiting up to 5m0s for pod "pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299" in namespace "configmap-8967" to be "Succeeded or Failed"
Feb 22 09:04:06.266: INFO: Pod "pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299": Phase="Pending", Reason="", readiness=false. Elapsed: 90.015084ms
Feb 22 09:04:08.281: INFO: Pod "pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104447976s
Feb 22 09:04:10.292: INFO: Pod "pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115675283s
Feb 22 09:04:12.304: INFO: Pod "pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.127738386s
STEP: Saw pod success
Feb 22 09:04:12.304: INFO: Pod "pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299" satisfied condition "Succeeded or Failed"
Feb 22 09:04:12.313: INFO: Trying to get logs from node node1 pod pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 09:04:12.407: INFO: Waiting for pod pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299 to disappear
Feb 22 09:04:12.414: INFO: Pod pod-configmaps-49e38e24-f9a4-4269-aa91-1f0d74ad2299 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:04:12.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8967" for this suite.

• [SLOW TEST:6.691 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":1023,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:04:12.472: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Feb 22 09:04:12.618: INFO: Waiting up to 5m0s for pod "test-pod-02df43e7-5158-4871-ae26-cf5d0b8b1ef2" in namespace "svcaccounts-5927" to be "Succeeded or Failed"
Feb 22 09:04:12.641: INFO: Pod "test-pod-02df43e7-5158-4871-ae26-cf5d0b8b1ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 23.133393ms
Feb 22 09:04:14.659: INFO: Pod "test-pod-02df43e7-5158-4871-ae26-cf5d0b8b1ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040401272s
Feb 22 09:04:16.673: INFO: Pod "test-pod-02df43e7-5158-4871-ae26-cf5d0b8b1ef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054833365s
STEP: Saw pod success
Feb 22 09:04:16.673: INFO: Pod "test-pod-02df43e7-5158-4871-ae26-cf5d0b8b1ef2" satisfied condition "Succeeded or Failed"
Feb 22 09:04:16.679: INFO: Trying to get logs from node node2 pod test-pod-02df43e7-5158-4871-ae26-cf5d0b8b1ef2 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:04:16.798: INFO: Waiting for pod test-pod-02df43e7-5158-4871-ae26-cf5d0b8b1ef2 to disappear
Feb 22 09:04:16.805: INFO: Pod test-pod-02df43e7-5158-4871-ae26-cf5d0b8b1ef2 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:04:16.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5927" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":51,"skipped":1036,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:04:16.848: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 22 09:04:21.548: INFO: Successfully updated pod "adopt-release--1-kqrtm"
STEP: Checking that the Job readopts the Pod
Feb 22 09:04:21.548: INFO: Waiting up to 15m0s for pod "adopt-release--1-kqrtm" in namespace "job-1396" to be "adopted"
Feb 22 09:04:21.556: INFO: Pod "adopt-release--1-kqrtm": Phase="Running", Reason="", readiness=true. Elapsed: 7.864164ms
Feb 22 09:04:23.571: INFO: Pod "adopt-release--1-kqrtm": Phase="Running", Reason="", readiness=true. Elapsed: 2.022924353s
Feb 22 09:04:23.571: INFO: Pod "adopt-release--1-kqrtm" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 22 09:04:24.101: INFO: Successfully updated pod "adopt-release--1-kqrtm"
STEP: Checking that the Job releases the Pod
Feb 22 09:04:24.101: INFO: Waiting up to 15m0s for pod "adopt-release--1-kqrtm" in namespace "job-1396" to be "released"
Feb 22 09:04:24.109: INFO: Pod "adopt-release--1-kqrtm": Phase="Running", Reason="", readiness=true. Elapsed: 8.68006ms
Feb 22 09:04:26.118: INFO: Pod "adopt-release--1-kqrtm": Phase="Running", Reason="", readiness=true. Elapsed: 2.017430879s
Feb 22 09:04:26.118: INFO: Pod "adopt-release--1-kqrtm" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:04:26.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1396" for this suite.

• [SLOW TEST:9.295 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":52,"skipped":1054,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:04:26.143: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Feb 22 09:04:26.350: INFO: The status of Pod pod-hostip-b8d77521-2cef-4556-8bda-e6209840679d is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:04:28.363: INFO: The status of Pod pod-hostip-b8d77521-2cef-4556-8bda-e6209840679d is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:04:30.360: INFO: The status of Pod pod-hostip-b8d77521-2cef-4556-8bda-e6209840679d is Running (Ready = true)
Feb 22 09:04:30.372: INFO: Pod pod-hostip-b8d77521-2cef-4556-8bda-e6209840679d has hostIP: 172.28.128.12
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:04:30.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5047" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":1058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:04:30.400: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:04:38.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5011" for this suite.

• [SLOW TEST:8.310 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":54,"skipped":1083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:04:38.712: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2305
STEP: creating service affinity-nodeport in namespace services-2305
STEP: creating replication controller affinity-nodeport in namespace services-2305
I0222 09:04:39.120003      19 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2305, replica count: 3
I0222 09:04:42.171412      19 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:04:45.172603      19 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:04:48.175044      19 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 09:04:48.199: INFO: Creating new exec pod
Feb 22 09:04:53.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-2305 exec execpod-affinitywfvlg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Feb 22 09:04:53.934: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 22 09:04:53.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:04:53.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-2305 exec execpod-affinitywfvlg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.43.16 80'
Feb 22 09:04:54.546: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.43.16 80\nConnection to 172.20.43.16 80 port [tcp/http] succeeded!\n"
Feb 22 09:04:54.546: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:04:54.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-2305 exec execpod-affinitywfvlg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 30174'
Feb 22 09:04:55.290: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 30174\nConnection to 172.28.128.12 30174 port [tcp/*] succeeded!\n"
Feb 22 09:04:55.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:04:55.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-2305 exec execpod-affinitywfvlg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 30174'
Feb 22 09:04:55.991: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.13 30174\nConnection to 172.28.128.13 30174 port [tcp/*] succeeded!\n"
Feb 22 09:04:55.991: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:04:55.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-2305 exec execpod-affinitywfvlg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.28.128.12:30174/ ; done'
Feb 22 09:04:57.019: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30174/\n"
Feb 22 09:04:57.019: INFO: stdout: "\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf\naffinity-nodeport-8pmsf"
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Received response from host: affinity-nodeport-8pmsf
Feb 22 09:04:57.019: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2305, will wait for the garbage collector to delete the pods
Feb 22 09:04:57.187: INFO: Deleting ReplicationController affinity-nodeport took: 19.216412ms
Feb 22 09:04:57.293: INFO: Terminating ReplicationController affinity-nodeport pods took: 105.560412ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:05:01.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2305" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:22.523 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":55,"skipped":1110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:05:01.235: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:05:05.485: INFO: Deleting pod "var-expansion-898664bd-0257-4074-a653-3886a0d5ffa7" in namespace "var-expansion-660"
Feb 22 09:05:05.508: INFO: Wait up to 5m0s for pod "var-expansion-898664bd-0257-4074-a653-3886a0d5ffa7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:05:09.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-660" for this suite.

• [SLOW TEST:8.364 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":56,"skipped":1145,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:05:09.600: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1424
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1424
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1424
Feb 22 09:05:09.912: INFO: Found 0 stateful pods, waiting for 1
Feb 22 09:05:19.930: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 22 09:05:19.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-1424 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 09:05:20.756: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 09:05:20.756: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 09:05:20.756: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 09:05:20.764: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 22 09:05:30.772: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 09:05:30.772: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 09:05:30.812: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999995s
Feb 22 09:05:31.824: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993361651s
Feb 22 09:05:32.836: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981754025s
Feb 22 09:05:33.850: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.968208108s
Feb 22 09:05:34.867: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.955517788s
Feb 22 09:05:35.873: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.938468187s
Feb 22 09:05:36.884: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.932226236s
Feb 22 09:05:37.895: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.921649806s
Feb 22 09:05:38.903: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.909634682s
Feb 22 09:05:39.914: INFO: Verifying statefulset ss doesn't scale past 1 for another 902.540935ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1424
Feb 22 09:05:40.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-1424 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 09:05:41.642: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 22 09:05:41.642: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 09:05:41.642: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 22 09:05:41.653: INFO: Found 1 stateful pods, waiting for 3
Feb 22 09:05:51.667: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:05:51.667: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:05:51.667: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 22 09:05:51.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-1424 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 09:05:52.374: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 09:05:52.374: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 09:05:52.374: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 09:05:52.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-1424 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 09:05:53.069: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 09:05:53.069: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 09:05:53.069: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 09:05:53.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-1424 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 09:05:53.814: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 09:05:53.814: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 09:05:53.814: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 09:05:53.814: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 09:05:53.879: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 09:05:53.879: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 09:05:53.879: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 09:05:53.913: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999994s
Feb 22 09:05:54.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992374955s
Feb 22 09:05:55.932: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983825515s
Feb 22 09:05:56.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973668283s
Feb 22 09:05:57.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.948712219s
Feb 22 09:05:58.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.936110198s
Feb 22 09:06:00.009: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.906448355s
Feb 22 09:06:01.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.896621221s
Feb 22 09:06:02.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.888504379s
Feb 22 09:06:03.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 878.871044ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1424
Feb 22 09:06:04.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-1424 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 09:06:04.752: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 22 09:06:04.752: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 09:06:04.752: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 22 09:06:04.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-1424 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 09:06:05.380: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 22 09:06:05.380: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 09:06:05.380: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 22 09:06:05.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-1424 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 09:06:06.049: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 22 09:06:06.049: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 09:06:06.049: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 22 09:06:06.049: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 22 09:06:16.087: INFO: Deleting all statefulset in ns statefulset-1424
Feb 22 09:06:16.115: INFO: Scaling statefulset ss to 0
Feb 22 09:06:16.155: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 09:06:16.161: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:06:16.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1424" for this suite.

• [SLOW TEST:66.649 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":57,"skipped":1146,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:06:16.249: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4710
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 22 09:06:16.397: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 22 09:06:16.480: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:06:18.604: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:06:20.487: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:22.729: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:24.672: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:26.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:28.498: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:30.493: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:32.494: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:34.491: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:36.494: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:06:38.496: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 22 09:06:38.507: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Feb 22 09:06:42.607: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Feb 22 09:06:42.607: INFO: Breadth first check of 172.21.40.147 on host 172.28.128.12...
Feb 22 09:06:42.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.104.12:9080/dial?request=hostname&protocol=udp&host=172.21.40.147&port=8081&tries=1'] Namespace:pod-network-test-4710 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:06:42.613: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:06:43.167: INFO: Waiting for responses: map[]
Feb 22 09:06:43.167: INFO: reached 172.21.40.147 after 0/1 tries
Feb 22 09:06:43.167: INFO: Breadth first check of 172.21.104.11 on host 172.28.128.13...
Feb 22 09:06:43.179: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.104.12:9080/dial?request=hostname&protocol=udp&host=172.21.104.11&port=8081&tries=1'] Namespace:pod-network-test-4710 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:06:43.179: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:06:43.741: INFO: Waiting for responses: map[]
Feb 22 09:06:43.741: INFO: reached 172.21.104.11 after 0/1 tries
Feb 22 09:06:43.741: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:06:43.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4710" for this suite.

• [SLOW TEST:27.535 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":1153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:06:43.801: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Feb 22 09:06:43.958: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:06:45.976: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:06:47.973: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 22 09:06:48.008: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:06:50.038: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:06:52.052: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:06:54.020: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Feb 22 09:06:54.042: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 22 09:06:54.050: INFO: Pod pod-with-prestop-http-hook still exists
Feb 22 09:06:56.051: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 22 09:06:56.061: INFO: Pod pod-with-prestop-http-hook still exists
Feb 22 09:06:58.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 22 09:06:58.066: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:06:58.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4081" for this suite.

• [SLOW TEST:14.390 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1235,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:06:58.191: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:07:03.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-779" for this suite.

• [SLOW TEST:5.425 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":60,"skipped":1236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:07:03.621: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:07:03.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3abbb5ca-4df6-4403-bf17-d3413caa5873" in namespace "projected-9095" to be "Succeeded or Failed"
Feb 22 09:07:03.871: INFO: Pod "downwardapi-volume-3abbb5ca-4df6-4403-bf17-d3413caa5873": Phase="Pending", Reason="", readiness=false. Elapsed: 21.981599ms
Feb 22 09:07:05.879: INFO: Pod "downwardapi-volume-3abbb5ca-4df6-4403-bf17-d3413caa5873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030839317s
Feb 22 09:07:07.905: INFO: Pod "downwardapi-volume-3abbb5ca-4df6-4403-bf17-d3413caa5873": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056349958s
STEP: Saw pod success
Feb 22 09:07:07.905: INFO: Pod "downwardapi-volume-3abbb5ca-4df6-4403-bf17-d3413caa5873" satisfied condition "Succeeded or Failed"
Feb 22 09:07:07.921: INFO: Trying to get logs from node node2 pod downwardapi-volume-3abbb5ca-4df6-4403-bf17-d3413caa5873 container client-container: <nil>
STEP: delete the pod
Feb 22 09:07:08.044: INFO: Waiting for pod downwardapi-volume-3abbb5ca-4df6-4403-bf17-d3413caa5873 to disappear
Feb 22 09:07:08.074: INFO: Pod downwardapi-volume-3abbb5ca-4df6-4403-bf17-d3413caa5873 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:07:08.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9095" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:07:08.097: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-b1651675-8eb0-4a2c-96aa-f1e45eebc266
STEP: Creating a pod to test consume configMaps
Feb 22 09:07:08.267: INFO: Waiting up to 5m0s for pod "pod-configmaps-737a0fee-c98b-4a45-8c1e-c6733ab1d374" in namespace "configmap-5298" to be "Succeeded or Failed"
Feb 22 09:07:08.285: INFO: Pod "pod-configmaps-737a0fee-c98b-4a45-8c1e-c6733ab1d374": Phase="Pending", Reason="", readiness=false. Elapsed: 18.634014ms
Feb 22 09:07:10.294: INFO: Pod "pod-configmaps-737a0fee-c98b-4a45-8c1e-c6733ab1d374": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027230333s
Feb 22 09:07:12.309: INFO: Pod "pod-configmaps-737a0fee-c98b-4a45-8c1e-c6733ab1d374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042671721s
STEP: Saw pod success
Feb 22 09:07:12.310: INFO: Pod "pod-configmaps-737a0fee-c98b-4a45-8c1e-c6733ab1d374" satisfied condition "Succeeded or Failed"
Feb 22 09:07:12.327: INFO: Trying to get logs from node node2 pod pod-configmaps-737a0fee-c98b-4a45-8c1e-c6733ab1d374 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:07:12.397: INFO: Waiting for pod pod-configmaps-737a0fee-c98b-4a45-8c1e-c6733ab1d374 to disappear
Feb 22 09:07:12.409: INFO: Pod pod-configmaps-737a0fee-c98b-4a45-8c1e-c6733ab1d374 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:07:12.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5298" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":62,"skipped":1293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:07:12.450: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 22 09:07:23.759: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 22 09:07:23.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s96z" in namespace "gc-9435"
W0222 09:07:23.759156      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 22 09:07:23.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-74zz5" in namespace "gc-9435"
Feb 22 09:07:23.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bvtj" in namespace "gc-9435"
Feb 22 09:07:23.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfw8k" in namespace "gc-9435"
Feb 22 09:07:24.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz5d6" in namespace "gc-9435"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:07:24.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9435" for this suite.

• [SLOW TEST:11.926 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":63,"skipped":1320,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:07:24.395: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:07:24.569: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-3898
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:07:31.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-996" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:07:31.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3898" for this suite.

• [SLOW TEST:6.736 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":64,"skipped":1338,"failed":0}
SSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:07:31.113: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 22 09:07:31.383: INFO: starting watch
STEP: patching
STEP: updating
Feb 22 09:07:31.457: INFO: waiting for watch events with expected annotations
Feb 22 09:07:31.457: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:07:31.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9313" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":65,"skipped":1341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:07:31.653: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:07:31.792: INFO: Creating deployment "webserver-deployment"
Feb 22 09:07:31.811: INFO: Waiting for observed generation 1
Feb 22 09:07:33.863: INFO: Waiting for all required pods to come up
Feb 22 09:07:33.883: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 22 09:07:41.993: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 22 09:07:42.019: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 22 09:07:42.047: INFO: Updating deployment webserver-deployment
Feb 22 09:07:42.047: INFO: Waiting for observed generation 2
Feb 22 09:07:44.093: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 22 09:07:44.115: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 22 09:07:44.160: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 22 09:07:44.208: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 22 09:07:44.208: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 22 09:07:44.218: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 22 09:07:44.247: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 22 09:07:44.248: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 22 09:07:44.296: INFO: Updating deployment webserver-deployment
Feb 22 09:07:44.296: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 22 09:07:44.408: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 22 09:07:44.492: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 22 09:07:46.990: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1027  1da372e5-a7f0-4d4a-a9f9-851bdf823bef 43691 3 2022-02-22 09:07:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-22 09:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e13c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-02-22 09:07:44 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-02-22 09:07:45 +0000 UTC,LastTransitionTime:2022-02-22 09:07:31 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 22 09:07:47.049: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1027  89de68eb-1eb9-4820-89dd-6e66faddf514 43686 3 2022-02-22 09:07:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1da372e5-a7f0-4d4a-a9f9-851bdf823bef 0xc00641f077 0xc00641f078}] []  [{kube-controller-manager Update apps/v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1da372e5-a7f0-4d4a-a9f9-851bdf823bef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00641f118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 22 09:07:47.049: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 22 09:07:47.050: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-1027  b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 43677 3 2022-02-22 09:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1da372e5-a7f0-4d4a-a9f9-851bdf823bef 0xc00641f177 0xc00641f178}] []  [{kube-controller-manager Update apps/v1 2022-02-22 09:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1da372e5-a7f0-4d4a-a9f9-851bdf823bef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 09:07:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00641f208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 22 09:07:47.203: INFO: Pod "webserver-deployment-795d758f88-2w7ts" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2w7ts webserver-deployment-795d758f88- deployment-1027  7781bbb5-970e-46f0-a399-bea2be04a227 43669 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc00641f6e7 0xc00641f6e8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdj6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdj6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.208: INFO: Pod "webserver-deployment-795d758f88-4gtxd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4gtxd webserver-deployment-795d758f88- deployment-1027  7af6fe1f-1a51-4305-ae85-2bf1997ec2fc 43720 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc00641f860 0xc00641f861}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qqqn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qqqn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.209: INFO: Pod "webserver-deployment-795d758f88-8lsqx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8lsqx webserver-deployment-795d758f88- deployment-1027  3da12beb-9e2d-43a0-b7b8-b0762a42a900 43717 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc00641fa47 0xc00641fa48}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wtp9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wtp9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.213: INFO: Pod "webserver-deployment-795d758f88-bcc2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bcc2r webserver-deployment-795d758f88- deployment-1027  dff9abbe-edea-447a-b529-a6ab4e06f4c3 43657 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc00641fc37 0xc00641fc38}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvj7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvj7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.214: INFO: Pod "webserver-deployment-795d758f88-c4zsc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c4zsc webserver-deployment-795d758f88- deployment-1027  4cbce2fa-b23a-4bb9-bd48-21d5f254eed3 43722 0 2022-02-22 09:07:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:d04538bb94b9886b5f5b7f67d7297ea7d6cd2084e92d47c4947e3314530b8b57 cni.projectcalico.org/podIP:172.21.40.159/32 cni.projectcalico.org/podIPs:172.21.40.159/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc00641fe50 0xc00641fe51}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-02-22 09:07:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pp5nq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pp5nq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:07:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.214: INFO: Pod "webserver-deployment-795d758f88-dvsth" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dvsth webserver-deployment-795d758f88- deployment-1027  5fc9a541-7add-4db9-8758-7e97c45c8cfe 43565 0 2022-02-22 09:07:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc00067c0f7 0xc00067c0f8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lclwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lclwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 09:07:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.215: INFO: Pod "webserver-deployment-795d758f88-fkwrf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fkwrf webserver-deployment-795d758f88- deployment-1027  654c6b07-3420-4808-a9a1-812d515a7cee 43708 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc000654577 0xc000654578}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z58bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z58bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.218: INFO: Pod "webserver-deployment-795d758f88-h497b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-h497b webserver-deployment-795d758f88- deployment-1027  7628d118-7666-4f0a-833b-48dee41de4d5 43652 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc0006548d7 0xc0006548d8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4dlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4dlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.219: INFO: Pod "webserver-deployment-795d758f88-j687d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-j687d webserver-deployment-795d758f88- deployment-1027  ffb3c032-4cd9-4980-a399-89d9ee4e79c7 43577 0 2022-02-22 09:07:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc000654d37 0xc000654d38}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r5z9x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r5z9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 09:07:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.219: INFO: Pod "webserver-deployment-795d758f88-jchk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jchk2 webserver-deployment-795d758f88- deployment-1027  f8ecd13b-4520-4946-b599-c58e39daeef5 43599 0 2022-02-22 09:07:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc000655167 0xc000655168}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnbrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnbrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 09:07:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.253: INFO: Pod "webserver-deployment-795d758f88-kkkkh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kkkkh webserver-deployment-795d758f88- deployment-1027  6e341150-d8fd-4081-a802-c3b54c0134b5 43654 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc0006555b7 0xc0006555b8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-trtjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-trtjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.253: INFO: Pod "webserver-deployment-795d758f88-mtns8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mtns8 webserver-deployment-795d758f88- deployment-1027  428cae86-7cae-4758-aa48-12f05f032b95 43660 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc0006559f0 0xc0006559f1}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8sgm9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8sgm9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.254: INFO: Pod "webserver-deployment-795d758f88-nmlwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nmlwc webserver-deployment-795d758f88- deployment-1027  0b137428-4c30-452d-923f-09146897e075 43700 0 2022-02-22 09:07:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:596f7047201f56af4875e04a44e3db3750c6c7d51eb2931cd2c04b0d743ea0f3 cni.projectcalico.org/podIP:172.21.40.157/32 cni.projectcalico.org/podIPs:172.21.40.157/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 89de68eb-1eb9-4820-89dd-6e66faddf514 0xc000655ca0 0xc000655ca1}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89de68eb-1eb9-4820-89dd-6e66faddf514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-02-22 09:07:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkshs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkshs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:07:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.284: INFO: Pod "webserver-deployment-847dcfb7fb-5ccwz" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5ccwz webserver-deployment-847dcfb7fb- deployment-1027  ef6b6566-5bdf-4a92-a0c6-02686bdead41 43721 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc000655fc7 0xc000655fc8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7nz2z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7nz2z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.308: INFO: Pod "webserver-deployment-847dcfb7fb-6nlxr" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6nlxr webserver-deployment-847dcfb7fb- deployment-1027  44f3781c-7d2b-4283-9c36-9581fe7c3a4b 43474 0 2022-02-22 09:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:acb33de1e681d9f1be866cf0ba4094878c785b9b932ff0ca506f720e1a94f649 cni.projectcalico.org/podIP:172.21.104.20/32 cni.projectcalico.org/podIPs:172.21.104.20/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447e097 0xc00447e098}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:07:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8znc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8znc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.20,StartTime:2022-02-22 09:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:07:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://f8c87ad028043a211727fa5fafb6e722a5ea02973a5bf31623930bdef7118ab4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.317: INFO: Pod "webserver-deployment-847dcfb7fb-6p5zr" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6p5zr webserver-deployment-847dcfb7fb- deployment-1027  6089b0eb-d7f4-419b-8c24-d9158736f2ca 43689 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447e2a7 0xc00447e2a8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lk5vb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lk5vb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.324: INFO: Pod "webserver-deployment-847dcfb7fb-9f7tq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-9f7tq webserver-deployment-847dcfb7fb- deployment-1027  67325b6c-953a-4205-bf4f-481d5cab5337 43519 0 2022-02-22 09:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:6f9cd6479d2ded1f9c83f537f05df724d80876716aa36817d84e2d3558f81da8 cni.projectcalico.org/podIP:172.21.104.16/32 cni.projectcalico.org/podIPs:172.21.104.16/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447e4a7 0xc00447e4a8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:07:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7v6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7v6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.16,StartTime:2022-02-22 09:07:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:07:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://115741c568c96c8a09056e09fd3641f9acd91f9bff9aa6ac12f12f2db32aacbc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.325: INFO: Pod "webserver-deployment-847dcfb7fb-fz2zb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-fz2zb webserver-deployment-847dcfb7fb- deployment-1027  3df100e5-8d37-480a-b198-cdfde0ecaab2 43644 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447e6b7 0xc00447e6b8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5jn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5jn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.328: INFO: Pod "webserver-deployment-847dcfb7fb-hdsb8" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hdsb8 webserver-deployment-847dcfb7fb- deployment-1027  075fcdf7-3e54-46be-9be7-79b421f0916e 43714 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447e887 0xc00447e888}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v42f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v42f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.336: INFO: Pod "webserver-deployment-847dcfb7fb-hghwf" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hghwf webserver-deployment-847dcfb7fb- deployment-1027  5d5d1d68-8557-4080-a5fe-ffbde0a6075a 43543 0 2022-02-22 09:07:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:4e641e2729aecc0a10d6397db376eb120f5078f0f678ba8823fccf37a1a0be76 cni.projectcalico.org/podIP:172.21.40.153/32 cni.projectcalico.org/podIPs:172.21.40.153/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447ea77 0xc00447ea78}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:07:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.40.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f6zbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f6zbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.21.40.153,StartTime:2022-02-22 09:07:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:07:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://fd03a6e2c8c740e5a40a7c75315563f8754a275228454fdf2de3eded7fa4ec63,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.40.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.353: INFO: Pod "webserver-deployment-847dcfb7fb-l8rdt" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-l8rdt webserver-deployment-847dcfb7fb- deployment-1027  d6b347a9-fa5c-454b-bc67-02ad4d003b1d 43523 0 2022-02-22 09:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:6698da5747ddc5f44f1170ea57ce1d11d3949a4cd5d9da858d0d1c7d35e2467c cni.projectcalico.org/podIP:172.21.104.22/32 cni.projectcalico.org/podIPs:172.21.104.22/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447ecb7 0xc00447ecb8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:07:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lhtq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lhtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.22,StartTime:2022-02-22 09:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:07:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://23ee8daaa3fb5a72af7c92f230fe2e0c90ebbe87db1d551b9247f498fb852209,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.354: INFO: Pod "webserver-deployment-847dcfb7fb-m96cq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-m96cq webserver-deployment-847dcfb7fb- deployment-1027  4a9193cf-172b-4ff4-aac6-98ee802c017c 43532 0 2022-02-22 09:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:323f2bbf5230b2c354edc606b0e395349a3de5204059b6815b72b56e402d0db8 cni.projectcalico.org/podIP:172.21.40.152/32 cni.projectcalico.org/podIPs:172.21.40.152/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447eee7 0xc00447eee8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:07:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.40.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79dv8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79dv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.21.40.152,StartTime:2022-02-22 09:07:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:07:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://e126d26b9865d0234d85d922755de58055150b9cb5b8f95267d78216249e2124,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.40.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.369: INFO: Pod "webserver-deployment-847dcfb7fb-mkxzv" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mkxzv webserver-deployment-847dcfb7fb- deployment-1027  6b20e592-7d58-4525-a956-118ab426184c 43699 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447f0f7 0xc00447f0f8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dhx87,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dhx87,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.369: INFO: Pod "webserver-deployment-847dcfb7fb-mxd4f" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mxd4f webserver-deployment-847dcfb7fb- deployment-1027  351533b6-1753-4143-bf97-6178980cc837 43676 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447f2c7 0xc00447f2c8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4c8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4c8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.370: INFO: Pod "webserver-deployment-847dcfb7fb-nh5l5" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nh5l5 webserver-deployment-847dcfb7fb- deployment-1027  64989b48-15d8-49aa-8196-72c21fb97b35 43636 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447f497 0xc00447f498}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mfbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mfbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.375: INFO: Pod "webserver-deployment-847dcfb7fb-nm2sj" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nm2sj webserver-deployment-847dcfb7fb- deployment-1027  b506376b-5485-4190-bdd7-b0a532b26ff4 43662 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447f600 0xc00447f601}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px7p6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px7p6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.388: INFO: Pod "webserver-deployment-847dcfb7fb-pn7kh" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-pn7kh webserver-deployment-847dcfb7fb- deployment-1027  162ab0d0-f843-4f32-9fbc-7a97751fc28c 43658 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447f770 0xc00447f771}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qv79h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qv79h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.389: INFO: Pod "webserver-deployment-847dcfb7fb-s5pwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-s5pwx webserver-deployment-847dcfb7fb- deployment-1027  bd89b9ed-d0bc-4461-a0bf-0af827f896ea 43678 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447f8e0 0xc00447f8e1}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2txrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2txrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.438: INFO: Pod "webserver-deployment-847dcfb7fb-v4875" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-v4875 webserver-deployment-847dcfb7fb- deployment-1027  7ac713a4-2dd5-47d6-8286-63a157af29c1 43661 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447fab7 0xc00447fab8}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmwwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmwwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.447: INFO: Pod "webserver-deployment-847dcfb7fb-v82gw" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-v82gw webserver-deployment-847dcfb7fb- deployment-1027  6313ad5d-0766-4614-8355-e9f41b6cf6b9 43534 0 2022-02-22 09:07:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:5441c2bbbba7613f8c628537782c58e6002567d3154680f53c059860737ddad6 cni.projectcalico.org/podIP:172.21.40.156/32 cni.projectcalico.org/podIPs:172.21.40.156/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447fc40 0xc00447fc41}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:07:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:07:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.40.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k898s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k898s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.21.40.156,StartTime:2022-02-22 09:07:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:07:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://cd7b24efea38d2a3270d2ad32b46b40fff6bef73ad3019473797fed5bcac4d2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.40.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.462: INFO: Pod "webserver-deployment-847dcfb7fb-vg5gq" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vg5gq webserver-deployment-847dcfb7fb- deployment-1027  f414ad17-5f17-461c-88bd-be603cd7980d 43659 0 2022-02-22 09:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc00447fe87 0xc00447fe88}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7v9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7v9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.513: INFO: Pod "webserver-deployment-847dcfb7fb-w2mk6" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-w2mk6 webserver-deployment-847dcfb7fb- deployment-1027  f40700e6-1fa3-4302-99b6-ff7d592d2ab3 43539 0 2022-02-22 09:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:66ba37a4454b861cbd9eedd832057fdb77fb9cf882d34d5bee6ec397ac039d74 cni.projectcalico.org/podIP:172.21.40.155/32 cni.projectcalico.org/podIPs:172.21.40.155/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc0054a2030 0xc0054a2031}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:07:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.40.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmsld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmsld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.21.40.155,StartTime:2022-02-22 09:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:07:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://be880d9c75b1d29a18e42d464e49e86b799d814b13d11f33b6c98302d895bd82,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.40.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:07:47.535: INFO: Pod "webserver-deployment-847dcfb7fb-xdx29" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-xdx29 webserver-deployment-847dcfb7fb- deployment-1027  6002d9d3-5c57-4e41-9883-514edd29e29e 43537 0 2022-02-22 09:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:6032724586fe8aec8240bafd75a2c489b3f989c0eac2f0bf6cd5c68c20fcac6a cni.projectcalico.org/podIP:172.21.40.154/32 cni.projectcalico.org/podIPs:172.21.40.154/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8e4c838-0f38-4ab0-be5a-b6ed154d5c30 0xc0054a2257 0xc0054a2258}] []  [{kube-controller-manager Update v1 2022-02-22 09:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8e4c838-0f38-4ab0-be5a-b6ed154d5c30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:07:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:07:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.40.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wv92d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wv92d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:07:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.21.40.154,StartTime:2022-02-22 09:07:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:07:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://76027609a8e4fdc3d832e08569b9ad48b4be0892811391ba95c51795d8af3e86,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.40.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:07:47.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1027" for this suite.

• [SLOW TEST:15.987 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":66,"skipped":1369,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:07:47.643: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:08:20.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7639" for this suite.

• [SLOW TEST:33.346 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":67,"skipped":1374,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:08:20.990: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 22 09:08:21.254: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 22 09:08:21.286: INFO: Waiting for terminating namespaces to be deleted...
Feb 22 09:08:21.297: INFO: 
Logging pods the apiserver thinks is on node node1 before test
Feb 22 09:08:21.356: INFO: calico-node-nnc68 from kube-system started at 2022-02-22 06:03:22 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.356: INFO: 	Container calico-node ready: true, restart count 1
Feb 22 09:08:21.356: INFO: coredns-575c8f4bf-l92w8 from kube-system started at 2022-02-22 08:12:56 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.356: INFO: 	Container coredns ready: true, restart count 0
Feb 22 09:08:21.357: INFO: kube-proxy-gj7fq from kube-system started at 2022-02-22 08:47:06 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.357: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 09:08:21.357: INFO: node-exporter-dn6cm from kube-system started at 2022-02-22 06:03:24 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.357: INFO: 	Container prometheus-node-exporter ready: true, restart count 2
Feb 22 09:08:21.357: INFO: sonobuoy from sonobuoy started at 2022-02-22 08:49:43 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 22 09:08:21.357: INFO: sonobuoy-e2e-job-71a20d3e324c433a from sonobuoy started at 2022-02-22 08:49:45 +0000 UTC (2 container statuses recorded)
Feb 22 09:08:21.357: INFO: 	Container e2e ready: true, restart count 0
Feb 22 09:08:21.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 09:08:21.357: INFO: sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-whn5g from sonobuoy started at 2022-02-22 08:49:45 +0000 UTC (2 container statuses recorded)
Feb 22 09:08:21.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 09:08:21.357: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 22 09:08:21.357: INFO: 
Logging pods the apiserver thinks is on node node2 before test
Feb 22 09:08:21.375: INFO: calico-kube-controllers-5f67864b44-5sp85 from kube-system started at 2022-02-22 06:04:18 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container calico-kube-controllers ready: false, restart count 14
Feb 22 09:08:21.375: INFO: calico-node-nrshb from kube-system started at 2022-02-22 06:03:23 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container calico-node ready: true, restart count 1
Feb 22 09:08:21.375: INFO: coredns-575c8f4bf-r7lx6 from kube-system started at 2022-02-22 08:45:42 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container coredns ready: true, restart count 0
Feb 22 09:08:21.375: INFO: kube-proxy-65gc6 from kube-system started at 2022-02-22 08:13:14 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 09:08:21.375: INFO: metrics-server-6cb8f6bbb8-9nwcm from kube-system started at 2022-02-22 06:01:50 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container metrics-server ready: true, restart count 1
Feb 22 09:08:21.375: INFO: node-exporter-4t6rd from kube-system started at 2022-02-22 06:03:25 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container prometheus-node-exporter ready: true, restart count 1
Feb 22 09:08:21.375: INFO: vpn-target-5c5b44787-xsvc7 from kube-system started at 2022-02-22 06:01:50 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container vpn-target ready: true, restart count 1
Feb 22 09:08:21.375: INFO: bin-false03682413-8ea3-455b-9feb-c986eab9c104 from kubelet-test-7639 started at 2022-02-22 09:07:48 +0000 UTC (1 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container bin-false03682413-8ea3-455b-9feb-c986eab9c104 ready: false, restart count 0
Feb 22 09:08:21.375: INFO: sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-c4wkd from sonobuoy started at 2022-02-22 08:49:46 +0000 UTC (2 container statuses recorded)
Feb 22 09:08:21.375: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 09:08:21.375: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16d6108aa5e05cca], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:08:22.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4945" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":68,"skipped":1376,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:08:22.540: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 22 09:08:22.744: INFO: Waiting up to 5m0s for pod "pod-c24f1484-faa0-4be8-afaa-c043cd36e1a5" in namespace "emptydir-5395" to be "Succeeded or Failed"
Feb 22 09:08:22.751: INFO: Pod "pod-c24f1484-faa0-4be8-afaa-c043cd36e1a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.366671ms
Feb 22 09:08:24.786: INFO: Pod "pod-c24f1484-faa0-4be8-afaa-c043cd36e1a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041423068s
Feb 22 09:08:26.805: INFO: Pod "pod-c24f1484-faa0-4be8-afaa-c043cd36e1a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06008634s
STEP: Saw pod success
Feb 22 09:08:26.805: INFO: Pod "pod-c24f1484-faa0-4be8-afaa-c043cd36e1a5" satisfied condition "Succeeded or Failed"
Feb 22 09:08:26.850: INFO: Trying to get logs from node node2 pod pod-c24f1484-faa0-4be8-afaa-c043cd36e1a5 container test-container: <nil>
STEP: delete the pod
Feb 22 09:08:27.012: INFO: Waiting for pod pod-c24f1484-faa0-4be8-afaa-c043cd36e1a5 to disappear
Feb 22 09:08:27.020: INFO: Pod pod-c24f1484-faa0-4be8-afaa-c043cd36e1a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:08:27.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5395" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1384,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:08:27.097: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 22 09:08:27.242: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:08:31.782: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:09:01.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9332" for this suite.

• [SLOW TEST:34.283 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":70,"skipped":1394,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:09:01.381: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Feb 22 09:09:01.640: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 22 09:09:06.648: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Feb 22 09:09:06.656: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Feb 22 09:09:06.712: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Feb 22 09:09:06.719: INFO: Observed &ReplicaSet event: ADDED
Feb 22 09:09:06.719: INFO: Observed &ReplicaSet event: MODIFIED
Feb 22 09:09:06.719: INFO: Observed &ReplicaSet event: MODIFIED
Feb 22 09:09:06.719: INFO: Observed &ReplicaSet event: MODIFIED
Feb 22 09:09:06.719: INFO: Found replicaset test-rs in namespace replicaset-8110 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 22 09:09:06.719: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Feb 22 09:09:06.719: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 22 09:09:06.758: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Feb 22 09:09:06.762: INFO: Observed &ReplicaSet event: ADDED
Feb 22 09:09:06.762: INFO: Observed &ReplicaSet event: MODIFIED
Feb 22 09:09:06.762: INFO: Observed &ReplicaSet event: MODIFIED
Feb 22 09:09:06.763: INFO: Observed &ReplicaSet event: MODIFIED
Feb 22 09:09:06.763: INFO: Observed replicaset test-rs in namespace replicaset-8110 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 22 09:09:06.763: INFO: Observed &ReplicaSet event: MODIFIED
Feb 22 09:09:06.763: INFO: Found replicaset test-rs in namespace replicaset-8110 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 22 09:09:06.763: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:09:06.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8110" for this suite.

• [SLOW TEST:5.433 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":71,"skipped":1396,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:09:06.814: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-7f0d67af-81b0-4d55-a934-274fbba57459 in namespace container-probe-4769
Feb 22 09:09:11.034: INFO: Started pod busybox-7f0d67af-81b0-4d55-a934-274fbba57459 in namespace container-probe-4769
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 09:09:11.041: INFO: Initial restart count of pod busybox-7f0d67af-81b0-4d55-a934-274fbba57459 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:13:12.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4769" for this suite.

• [SLOW TEST:246.233 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1408,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:13:13.047: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:13:30.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8496" for this suite.

• [SLOW TEST:17.512 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":73,"skipped":1424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:13:30.560: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 22 09:13:30.757: INFO: Waiting up to 5m0s for pod "pod-1ef326f0-6d9b-4684-8c32-64f3cdfde309" in namespace "emptydir-7727" to be "Succeeded or Failed"
Feb 22 09:13:30.771: INFO: Pod "pod-1ef326f0-6d9b-4684-8c32-64f3cdfde309": Phase="Pending", Reason="", readiness=false. Elapsed: 14.183434ms
Feb 22 09:13:32.785: INFO: Pod "pod-1ef326f0-6d9b-4684-8c32-64f3cdfde309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028319528s
Feb 22 09:13:34.795: INFO: Pod "pod-1ef326f0-6d9b-4684-8c32-64f3cdfde309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038711939s
STEP: Saw pod success
Feb 22 09:13:34.796: INFO: Pod "pod-1ef326f0-6d9b-4684-8c32-64f3cdfde309" satisfied condition "Succeeded or Failed"
Feb 22 09:13:34.802: INFO: Trying to get logs from node node2 pod pod-1ef326f0-6d9b-4684-8c32-64f3cdfde309 container test-container: <nil>
STEP: delete the pod
Feb 22 09:13:34.915: INFO: Waiting for pod pod-1ef326f0-6d9b-4684-8c32-64f3cdfde309 to disappear
Feb 22 09:13:34.920: INFO: Pod pod-1ef326f0-6d9b-4684-8c32-64f3cdfde309 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:13:34.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7727" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1456,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:13:34.943: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 22 09:13:35.067: INFO: The status of Pod labelsupdate0ddd86a5-cb95-41d8-9d13-454d317bd897 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:13:37.080: INFO: The status of Pod labelsupdate0ddd86a5-cb95-41d8-9d13-454d317bd897 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:13:39.076: INFO: The status of Pod labelsupdate0ddd86a5-cb95-41d8-9d13-454d317bd897 is Running (Ready = true)
Feb 22 09:13:39.633: INFO: Successfully updated pod "labelsupdate0ddd86a5-cb95-41d8-9d13-454d317bd897"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:13:41.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9613" for this suite.

• [SLOW TEST:6.820 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1456,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:13:41.763: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 22 09:13:42.004: INFO: Number of nodes with available pods: 0
Feb 22 09:13:42.004: INFO: Node node1 is running more than one daemon pod
Feb 22 09:13:43.046: INFO: Number of nodes with available pods: 0
Feb 22 09:13:43.046: INFO: Node node1 is running more than one daemon pod
Feb 22 09:13:44.139: INFO: Number of nodes with available pods: 0
Feb 22 09:13:44.146: INFO: Node node1 is running more than one daemon pod
Feb 22 09:13:45.045: INFO: Number of nodes with available pods: 0
Feb 22 09:13:45.045: INFO: Node node1 is running more than one daemon pod
Feb 22 09:13:46.021: INFO: Number of nodes with available pods: 2
Feb 22 09:13:46.021: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Getting /status
Feb 22 09:13:46.038: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Feb 22 09:13:46.078: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Feb 22 09:13:46.082: INFO: Observed &DaemonSet event: ADDED
Feb 22 09:13:46.082: INFO: Observed &DaemonSet event: MODIFIED
Feb 22 09:13:46.083: INFO: Observed &DaemonSet event: MODIFIED
Feb 22 09:13:46.084: INFO: Observed &DaemonSet event: MODIFIED
Feb 22 09:13:46.084: INFO: Found daemon set daemon-set in namespace daemonsets-5910 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 22 09:13:46.084: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Feb 22 09:13:46.117: INFO: Observed &DaemonSet event: ADDED
Feb 22 09:13:46.117: INFO: Observed &DaemonSet event: MODIFIED
Feb 22 09:13:46.117: INFO: Observed &DaemonSet event: MODIFIED
Feb 22 09:13:46.117: INFO: Observed &DaemonSet event: MODIFIED
Feb 22 09:13:46.117: INFO: Observed daemon set daemon-set in namespace daemonsets-5910 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 22 09:13:46.118: INFO: Observed &DaemonSet event: MODIFIED
Feb 22 09:13:46.118: INFO: Found daemon set daemon-set in namespace daemonsets-5910 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 22 09:13:46.118: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5910, will wait for the garbage collector to delete the pods
Feb 22 09:13:46.217: INFO: Deleting DaemonSet.extensions daemon-set took: 16.131325ms
Feb 22 09:13:46.319: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.295032ms
Feb 22 09:13:50.640: INFO: Number of nodes with available pods: 0
Feb 22 09:13:50.640: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 09:13:50.671: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44798"},"items":null}

Feb 22 09:13:50.686: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44798"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:13:50.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5910" for this suite.

• [SLOW TEST:8.980 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":76,"skipped":1457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:13:50.745: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Feb 22 09:13:50.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-3500 create -f -'
Feb 22 09:13:52.725: INFO: stderr: ""
Feb 22 09:13:52.725: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 22 09:13:53.739: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 09:13:53.739: INFO: Found 0 / 1
Feb 22 09:13:54.734: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 09:13:54.734: INFO: Found 0 / 1
Feb 22 09:13:55.735: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 09:13:55.735: INFO: Found 1 / 1
Feb 22 09:13:55.735: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 22 09:13:55.742: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 09:13:55.742: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 22 09:13:55.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-3500 patch pod agnhost-primary-mv24t -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 22 09:13:55.951: INFO: stderr: ""
Feb 22 09:13:55.952: INFO: stdout: "pod/agnhost-primary-mv24t patched\n"
STEP: checking annotations
Feb 22 09:13:55.982: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 09:13:55.982: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:13:55.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3500" for this suite.

• [SLOW TEST:5.269 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":77,"skipped":1479,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:13:56.014: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3b4da90f-e415-41a0-8f3c-bad0d58c709a
STEP: Creating a pod to test consume secrets
Feb 22 09:13:56.283: INFO: Waiting up to 5m0s for pod "pod-secrets-5e80c4ad-f1bf-405a-8ac2-e2aed09c510f" in namespace "secrets-7313" to be "Succeeded or Failed"
Feb 22 09:13:56.301: INFO: Pod "pod-secrets-5e80c4ad-f1bf-405a-8ac2-e2aed09c510f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.27032ms
Feb 22 09:13:58.312: INFO: Pod "pod-secrets-5e80c4ad-f1bf-405a-8ac2-e2aed09c510f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029205323s
Feb 22 09:14:00.321: INFO: Pod "pod-secrets-5e80c4ad-f1bf-405a-8ac2-e2aed09c510f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038048441s
STEP: Saw pod success
Feb 22 09:14:00.321: INFO: Pod "pod-secrets-5e80c4ad-f1bf-405a-8ac2-e2aed09c510f" satisfied condition "Succeeded or Failed"
Feb 22 09:14:00.329: INFO: Trying to get logs from node node1 pod pod-secrets-5e80c4ad-f1bf-405a-8ac2-e2aed09c510f container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 09:14:00.460: INFO: Waiting for pod pod-secrets-5e80c4ad-f1bf-405a-8ac2-e2aed09c510f to disappear
Feb 22 09:14:00.469: INFO: Pod pod-secrets-5e80c4ad-f1bf-405a-8ac2-e2aed09c510f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:00.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7313" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":78,"skipped":1484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:00.523: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:14:00.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df06e3ee-d0af-40cb-9182-0b3b8927a475" in namespace "downward-api-4416" to be "Succeeded or Failed"
Feb 22 09:14:00.741: INFO: Pod "downwardapi-volume-df06e3ee-d0af-40cb-9182-0b3b8927a475": Phase="Pending", Reason="", readiness=false. Elapsed: 8.364362ms
Feb 22 09:14:02.809: INFO: Pod "downwardapi-volume-df06e3ee-d0af-40cb-9182-0b3b8927a475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076913204s
Feb 22 09:14:04.825: INFO: Pod "downwardapi-volume-df06e3ee-d0af-40cb-9182-0b3b8927a475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.09263629s
STEP: Saw pod success
Feb 22 09:14:04.825: INFO: Pod "downwardapi-volume-df06e3ee-d0af-40cb-9182-0b3b8927a475" satisfied condition "Succeeded or Failed"
Feb 22 09:14:04.832: INFO: Trying to get logs from node node1 pod downwardapi-volume-df06e3ee-d0af-40cb-9182-0b3b8927a475 container client-container: <nil>
STEP: delete the pod
Feb 22 09:14:04.965: INFO: Waiting for pod downwardapi-volume-df06e3ee-d0af-40cb-9182-0b3b8927a475 to disappear
Feb 22 09:14:04.971: INFO: Pod downwardapi-volume-df06e3ee-d0af-40cb-9182-0b3b8927a475 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:04.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4416" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1535,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:05.005: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:14:06.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 09:14:08.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118046, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118046, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118046, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118046, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:14:11.142: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:14:11.153: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3726-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2704" for this suite.
STEP: Destroying namespace "webhook-2704-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.316 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":80,"skipped":1547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:15.322: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-7f9s7 in namespace proxy-5499
I0222 09:14:15.668992      19 runners.go:190] Created replication controller with name: proxy-service-7f9s7, namespace: proxy-5499, replica count: 1
I0222 09:14:16.720343      19 runners.go:190] proxy-service-7f9s7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:14:17.720706      19 runners.go:190] proxy-service-7f9s7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:14:18.721083      19 runners.go:190] proxy-service-7f9s7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:14:19.722124      19 runners.go:190] proxy-service-7f9s7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:14:20.722807      19 runners.go:190] proxy-service-7f9s7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 09:14:20.733: INFO: setup took 5.206258844s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 22 09:14:20.770: INFO: (0) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 37.710326ms)
Feb 22 09:14:20.779: INFO: (0) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 44.966692ms)
Feb 22 09:14:20.781: INFO: (0) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 45.371591ms)
Feb 22 09:14:20.781: INFO: (0) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 46.456486ms)
Feb 22 09:14:20.782: INFO: (0) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 48.489676ms)
Feb 22 09:14:20.782: INFO: (0) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 48.917374ms)
Feb 22 09:14:20.782: INFO: (0) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 46.774484ms)
Feb 22 09:14:20.789: INFO: (0) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 52.12326ms)
Feb 22 09:14:20.789: INFO: (0) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 52.894955ms)
Feb 22 09:14:20.789: INFO: (0) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 55.018646ms)
Feb 22 09:14:20.789: INFO: (0) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 55.491644ms)
Feb 22 09:14:20.793: INFO: (0) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 57.093736ms)
Feb 22 09:14:20.794: INFO: (0) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 59.609424ms)
Feb 22 09:14:20.794: INFO: (0) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 58.849728ms)
Feb 22 09:14:20.800: INFO: (0) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 66.030295ms)
Feb 22 09:14:20.800: INFO: (0) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 65.509497ms)
Feb 22 09:14:20.853: INFO: (1) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 52.668056ms)
Feb 22 09:14:20.853: INFO: (1) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 52.917556ms)
Feb 22 09:14:20.853: INFO: (1) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 52.678656ms)
Feb 22 09:14:20.855: INFO: (1) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 54.740547ms)
Feb 22 09:14:20.855: INFO: (1) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 54.990646ms)
Feb 22 09:14:20.856: INFO: (1) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 53.770752ms)
Feb 22 09:14:20.856: INFO: (1) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 55.051745ms)
Feb 22 09:14:20.856: INFO: (1) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 54.312149ms)
Feb 22 09:14:20.856: INFO: (1) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 54.034051ms)
Feb 22 09:14:20.856: INFO: (1) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 55.380744ms)
Feb 22 09:14:20.856: INFO: (1) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 54.23675ms)
Feb 22 09:14:20.856: INFO: (1) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 55.953441ms)
Feb 22 09:14:20.858: INFO: (1) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 56.735838ms)
Feb 22 09:14:20.859: INFO: (1) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 57.067936ms)
Feb 22 09:14:20.859: INFO: (1) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 58.395731ms)
Feb 22 09:14:20.860: INFO: (1) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 59.462425ms)
Feb 22 09:14:20.886: INFO: (2) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 25.765581ms)
Feb 22 09:14:20.886: INFO: (2) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 24.053889ms)
Feb 22 09:14:20.886: INFO: (2) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 26.284279ms)
Feb 22 09:14:20.886: INFO: (2) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 25.397182ms)
Feb 22 09:14:20.886: INFO: (2) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 23.442791ms)
Feb 22 09:14:20.886: INFO: (2) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 26.103579ms)
Feb 22 09:14:20.886: INFO: (2) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 26.222379ms)
Feb 22 09:14:20.890: INFO: (2) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 27.445073ms)
Feb 22 09:14:20.890: INFO: (2) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 29.185365ms)
Feb 22 09:14:20.890: INFO: (2) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 28.11837ms)
Feb 22 09:14:20.891: INFO: (2) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 31.068556ms)
Feb 22 09:14:20.892: INFO: (2) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 31.211856ms)
Feb 22 09:14:20.892: INFO: (2) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 32.112252ms)
Feb 22 09:14:20.893: INFO: (2) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 29.745162ms)
Feb 22 09:14:20.894: INFO: (2) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 31.112456ms)
Feb 22 09:14:20.899: INFO: (2) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 36.541531ms)
Feb 22 09:14:20.918: INFO: (3) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 18.592414ms)
Feb 22 09:14:20.922: INFO: (3) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 21.5872ms)
Feb 22 09:14:20.931: INFO: (3) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 31.824353ms)
Feb 22 09:14:20.931: INFO: (3) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 30.634858ms)
Feb 22 09:14:20.931: INFO: (3) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 31.754153ms)
Feb 22 09:14:20.931: INFO: (3) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 30.983457ms)
Feb 22 09:14:20.931: INFO: (3) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 31.381555ms)
Feb 22 09:14:20.931: INFO: (3) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 32.188552ms)
Feb 22 09:14:20.932: INFO: (3) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 31.180155ms)
Feb 22 09:14:20.949: INFO: (3) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 47.286381ms)
Feb 22 09:14:20.952: INFO: (3) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 49.82757ms)
Feb 22 09:14:20.970: INFO: (3) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 67.979586ms)
Feb 22 09:14:20.971: INFO: (3) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 70.155476ms)
Feb 22 09:14:20.972: INFO: (3) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 71.917668ms)
Feb 22 09:14:20.972: INFO: (3) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 71.090772ms)
Feb 22 09:14:20.972: INFO: (3) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 72.190566ms)
Feb 22 09:14:20.993: INFO: (4) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 20.435705ms)
Feb 22 09:14:20.995: INFO: (4) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 21.7438ms)
Feb 22 09:14:20.995: INFO: (4) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 22.012398ms)
Feb 22 09:14:20.995: INFO: (4) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 22.113498ms)
Feb 22 09:14:20.995: INFO: (4) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 21.503501ms)
Feb 22 09:14:20.995: INFO: (4) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 22.609396ms)
Feb 22 09:14:20.995: INFO: (4) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 21.693199ms)
Feb 22 09:14:20.995: INFO: (4) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 21.984698ms)
Feb 22 09:14:20.995: INFO: (4) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 22.219397ms)
Feb 22 09:14:20.998: INFO: (4) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 25.171083ms)
Feb 22 09:14:21.009: INFO: (4) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 36.024734ms)
Feb 22 09:14:21.012: INFO: (4) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 39.848016ms)
Feb 22 09:14:21.039: INFO: (4) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 66.774691ms)
Feb 22 09:14:21.039: INFO: (4) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 66.391593ms)
Feb 22 09:14:21.041: INFO: (4) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 68.158486ms)
Feb 22 09:14:21.041: INFO: (4) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 67.496788ms)
Feb 22 09:14:21.065: INFO: (5) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 22.095898ms)
Feb 22 09:14:21.075: INFO: (5) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 34.018743ms)
Feb 22 09:14:21.076: INFO: (5) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 32.44965ms)
Feb 22 09:14:21.076: INFO: (5) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 33.935143ms)
Feb 22 09:14:21.088: INFO: (5) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 44.984492ms)
Feb 22 09:14:21.088: INFO: (5) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 45.176091ms)
Feb 22 09:14:21.089: INFO: (5) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 46.611185ms)
Feb 22 09:14:21.089: INFO: (5) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 46.424086ms)
Feb 22 09:14:21.089: INFO: (5) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 45.776689ms)
Feb 22 09:14:21.090: INFO: (5) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 46.175086ms)
Feb 22 09:14:21.118: INFO: (5) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 76.161649ms)
Feb 22 09:14:21.161: INFO: (5) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 119.793747ms)
Feb 22 09:14:21.164: INFO: (5) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 120.508144ms)
Feb 22 09:14:21.164: INFO: (5) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 122.844232ms)
Feb 22 09:14:21.164: INFO: (5) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 121.724437ms)
Feb 22 09:14:21.200: INFO: (5) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 158.731067ms)
Feb 22 09:14:21.278: INFO: (6) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 76.718845ms)
Feb 22 09:14:21.278: INFO: (6) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 76.579546ms)
Feb 22 09:14:21.278: INFO: (6) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 75.133053ms)
Feb 22 09:14:21.279: INFO: (6) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 75.980149ms)
Feb 22 09:14:21.283: INFO: (6) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 79.154534ms)
Feb 22 09:14:21.284: INFO: (6) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 80.506728ms)
Feb 22 09:14:21.284: INFO: (6) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 82.960017ms)
Feb 22 09:14:21.284: INFO: (6) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 81.435724ms)
Feb 22 09:14:21.285: INFO: (6) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 83.586013ms)
Feb 22 09:14:21.285: INFO: (6) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 83.892012ms)
Feb 22 09:14:21.285: INFO: (6) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 83.364515ms)
Feb 22 09:14:21.285: INFO: (6) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 81.111525ms)
Feb 22 09:14:21.293: INFO: (6) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 91.914375ms)
Feb 22 09:14:21.293: INFO: (6) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 89.455586ms)
Feb 22 09:14:21.293: INFO: (6) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 91.859875ms)
Feb 22 09:14:21.293: INFO: (6) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 90.195783ms)
Feb 22 09:14:21.333: INFO: (7) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 38.504422ms)
Feb 22 09:14:21.333: INFO: (7) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 38.325323ms)
Feb 22 09:14:21.334: INFO: (7) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 39.181919ms)
Feb 22 09:14:21.335: INFO: (7) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 41.789407ms)
Feb 22 09:14:21.335: INFO: (7) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 41.702707ms)
Feb 22 09:14:21.335: INFO: (7) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 40.964211ms)
Feb 22 09:14:21.335: INFO: (7) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 40.208414ms)
Feb 22 09:14:21.335: INFO: (7) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 40.640612ms)
Feb 22 09:14:21.336: INFO: (7) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 41.233609ms)
Feb 22 09:14:21.336: INFO: (7) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 40.99781ms)
Feb 22 09:14:21.356: INFO: (7) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 60.793019ms)
Feb 22 09:14:21.356: INFO: (7) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 60.61292ms)
Feb 22 09:14:21.372: INFO: (7) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 77.459342ms)
Feb 22 09:14:21.373: INFO: (7) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 77.871441ms)
Feb 22 09:14:21.373: INFO: (7) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 77.736041ms)
Feb 22 09:14:21.373: INFO: (7) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 78.828436ms)
Feb 22 09:14:21.404: INFO: (8) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 31.486854ms)
Feb 22 09:14:21.414: INFO: (8) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 31.341956ms)
Feb 22 09:14:21.420: INFO: (8) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 37.162728ms)
Feb 22 09:14:21.420: INFO: (8) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 37.018229ms)
Feb 22 09:14:21.420: INFO: (8) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 37.163328ms)
Feb 22 09:14:21.454: INFO: (8) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 70.543574ms)
Feb 22 09:14:21.454: INFO: (8) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 71.311271ms)
Feb 22 09:14:21.454: INFO: (8) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 70.849873ms)
Feb 22 09:14:21.454: INFO: (8) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 71.767468ms)
Feb 22 09:14:21.454: INFO: (8) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 71.556869ms)
Feb 22 09:14:21.462: INFO: (8) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 79.326033ms)
Feb 22 09:14:21.465: INFO: (8) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 81.482424ms)
Feb 22 09:14:21.465: INFO: (8) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 81.447423ms)
Feb 22 09:14:21.465: INFO: (8) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 81.671322ms)
Feb 22 09:14:21.465: INFO: (8) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 81.544223ms)
Feb 22 09:14:21.510: INFO: (8) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 126.352117ms)
Feb 22 09:14:21.534: INFO: (9) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 21.218602ms)
Feb 22 09:14:21.538: INFO: (9) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 27.953571ms)
Feb 22 09:14:21.538: INFO: (9) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 27.717372ms)
Feb 22 09:14:21.538: INFO: (9) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 25.386282ms)
Feb 22 09:14:21.538: INFO: (9) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 25.758281ms)
Feb 22 09:14:21.538: INFO: (9) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 26.999675ms)
Feb 22 09:14:21.540: INFO: (9) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 27.407774ms)
Feb 22 09:14:21.541: INFO: (9) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 28.455668ms)
Feb 22 09:14:21.541: INFO: (9) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 28.616668ms)
Feb 22 09:14:21.541: INFO: (9) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 28.860767ms)
Feb 22 09:14:21.541: INFO: (9) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 28.664868ms)
Feb 22 09:14:21.552: INFO: (9) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 39.031619ms)
Feb 22 09:14:21.552: INFO: (9) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 39.455318ms)
Feb 22 09:14:21.552: INFO: (9) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 40.010516ms)
Feb 22 09:14:21.554: INFO: (9) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 40.592612ms)
Feb 22 09:14:21.555: INFO: (9) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 42.865902ms)
Feb 22 09:14:21.580: INFO: (10) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 24.924885ms)
Feb 22 09:14:21.580: INFO: (10) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 23.88139ms)
Feb 22 09:14:21.580: INFO: (10) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 24.101489ms)
Feb 22 09:14:21.581: INFO: (10) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 22.292397ms)
Feb 22 09:14:21.581: INFO: (10) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 22.861195ms)
Feb 22 09:14:21.581: INFO: (10) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 23.406392ms)
Feb 22 09:14:21.582: INFO: (10) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 25.89688ms)
Feb 22 09:14:21.582: INFO: (10) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 24.710686ms)
Feb 22 09:14:21.583: INFO: (10) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 25.372283ms)
Feb 22 09:14:21.583: INFO: (10) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 24.858285ms)
Feb 22 09:14:21.585: INFO: (10) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 27.693572ms)
Feb 22 09:14:21.585: INFO: (10) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 28.512068ms)
Feb 22 09:14:21.587: INFO: (10) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 30.44776ms)
Feb 22 09:14:21.587: INFO: (10) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 29.117065ms)
Feb 22 09:14:21.587: INFO: (10) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 29.169065ms)
Feb 22 09:14:21.587: INFO: (10) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 31.528855ms)
Feb 22 09:14:21.606: INFO: (11) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 17.501519ms)
Feb 22 09:14:21.607: INFO: (11) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 18.062517ms)
Feb 22 09:14:21.607: INFO: (11) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 18.051316ms)
Feb 22 09:14:21.609: INFO: (11) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 20.566905ms)
Feb 22 09:14:21.609: INFO: (11) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 21.201002ms)
Feb 22 09:14:21.609: INFO: (11) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 19.953008ms)
Feb 22 09:14:21.609: INFO: (11) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 21.413101ms)
Feb 22 09:14:21.609: INFO: (11) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 20.268006ms)
Feb 22 09:14:21.609: INFO: (11) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 21.733499ms)
Feb 22 09:14:21.609: INFO: (11) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 21.149903ms)
Feb 22 09:14:21.615: INFO: (11) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 26.078079ms)
Feb 22 09:14:21.615: INFO: (11) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 26.812276ms)
Feb 22 09:14:21.615: INFO: (11) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 26.204379ms)
Feb 22 09:14:21.615: INFO: (11) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 26.280079ms)
Feb 22 09:14:21.617: INFO: (11) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 27.686172ms)
Feb 22 09:14:21.617: INFO: (11) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 28.17897ms)
Feb 22 09:14:21.643: INFO: (12) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 25.552282ms)
Feb 22 09:14:21.643: INFO: (12) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 25.494182ms)
Feb 22 09:14:21.643: INFO: (12) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 25.308583ms)
Feb 22 09:14:21.643: INFO: (12) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 25.560882ms)
Feb 22 09:14:21.644: INFO: (12) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 26.589577ms)
Feb 22 09:14:21.644: INFO: (12) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 27.079575ms)
Feb 22 09:14:21.644: INFO: (12) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 27.490773ms)
Feb 22 09:14:21.644: INFO: (12) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 27.182674ms)
Feb 22 09:14:21.644: INFO: (12) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 26.691377ms)
Feb 22 09:14:21.645: INFO: (12) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 27.452473ms)
Feb 22 09:14:21.647: INFO: (12) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 29.880962ms)
Feb 22 09:14:21.647: INFO: (12) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 29.867762ms)
Feb 22 09:14:21.657: INFO: (12) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 39.173319ms)
Feb 22 09:14:21.682: INFO: (12) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 64.796601ms)
Feb 22 09:14:21.683: INFO: (12) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 65.103899ms)
Feb 22 09:14:21.683: INFO: (12) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 64.792401ms)
Feb 22 09:14:21.697: INFO: (13) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 12.87594ms)
Feb 22 09:14:21.698: INFO: (13) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 15.657127ms)
Feb 22 09:14:21.698: INFO: (13) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 13.677537ms)
Feb 22 09:14:21.699: INFO: (13) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 14.338533ms)
Feb 22 09:14:21.702: INFO: (13) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 18.803713ms)
Feb 22 09:14:21.702: INFO: (13) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 17.073321ms)
Feb 22 09:14:21.702: INFO: (13) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 18.467514ms)
Feb 22 09:14:21.702: INFO: (13) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 17.828617ms)
Feb 22 09:14:21.703: INFO: (13) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 18.058417ms)
Feb 22 09:14:21.704: INFO: (13) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 17.864317ms)
Feb 22 09:14:21.704: INFO: (13) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 20.507905ms)
Feb 22 09:14:21.706: INFO: (13) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 22.927494ms)
Feb 22 09:14:21.706: INFO: (13) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 20.834604ms)
Feb 22 09:14:21.711: INFO: (13) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 25.90228ms)
Feb 22 09:14:21.712: INFO: (13) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 26.025279ms)
Feb 22 09:14:21.712: INFO: (13) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 27.375473ms)
Feb 22 09:14:21.730: INFO: (14) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 17.053921ms)
Feb 22 09:14:21.730: INFO: (14) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 16.269825ms)
Feb 22 09:14:21.730: INFO: (14) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 15.987926ms)
Feb 22 09:14:21.730: INFO: (14) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 16.947121ms)
Feb 22 09:14:21.730: INFO: (14) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 17.976517ms)
Feb 22 09:14:21.736: INFO: (14) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 23.013193ms)
Feb 22 09:14:21.736: INFO: (14) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 23.276992ms)
Feb 22 09:14:21.736: INFO: (14) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 21.866399ms)
Feb 22 09:14:21.736: INFO: (14) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 23.024294ms)
Feb 22 09:14:21.736: INFO: (14) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 24.095389ms)
Feb 22 09:14:21.736: INFO: (14) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 23.487692ms)
Feb 22 09:14:21.736: INFO: (14) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 22.995394ms)
Feb 22 09:14:21.740: INFO: (14) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 27.792572ms)
Feb 22 09:14:21.741: INFO: (14) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 28.10367ms)
Feb 22 09:14:21.741: INFO: (14) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 27.316974ms)
Feb 22 09:14:21.741: INFO: (14) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 26.489978ms)
Feb 22 09:14:21.756: INFO: (15) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 14.187135ms)
Feb 22 09:14:21.756: INFO: (15) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 15.791027ms)
Feb 22 09:14:21.756: INFO: (15) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 14.395834ms)
Feb 22 09:14:21.757: INFO: (15) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 11.473247ms)
Feb 22 09:14:21.758: INFO: (15) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 14.354134ms)
Feb 22 09:14:21.761: INFO: (15) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 19.252211ms)
Feb 22 09:14:21.761: INFO: (15) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 18.792113ms)
Feb 22 09:14:21.762: INFO: (15) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 18.688613ms)
Feb 22 09:14:21.762: INFO: (15) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 18.990412ms)
Feb 22 09:14:21.763: INFO: (15) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 20.482506ms)
Feb 22 09:14:21.763: INFO: (15) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 22.033198ms)
Feb 22 09:14:21.763: INFO: (15) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 18.032617ms)
Feb 22 09:14:21.764: INFO: (15) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 19.40141ms)
Feb 22 09:14:21.764: INFO: (15) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 19.330711ms)
Feb 22 09:14:21.764: INFO: (15) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 21.356902ms)
Feb 22 09:14:21.764: INFO: (15) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 22.062398ms)
Feb 22 09:14:21.780: INFO: (16) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 15.07563ms)
Feb 22 09:14:21.782: INFO: (16) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 15.499528ms)
Feb 22 09:14:21.784: INFO: (16) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 18.480515ms)
Feb 22 09:14:21.784: INFO: (16) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 18.894312ms)
Feb 22 09:14:21.784: INFO: (16) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 20.064907ms)
Feb 22 09:14:21.784: INFO: (16) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 19.677509ms)
Feb 22 09:14:21.785: INFO: (16) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 19.172411ms)
Feb 22 09:14:21.785: INFO: (16) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 18.425115ms)
Feb 22 09:14:21.785: INFO: (16) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 18.902513ms)
Feb 22 09:14:21.785: INFO: (16) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 18.437515ms)
Feb 22 09:14:21.788: INFO: (16) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 22.234897ms)
Feb 22 09:14:21.792: INFO: (16) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 26.997975ms)
Feb 22 09:14:21.792: INFO: (16) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 25.85838ms)
Feb 22 09:14:21.792: INFO: (16) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 25.97518ms)
Feb 22 09:14:21.792: INFO: (16) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 26.540878ms)
Feb 22 09:14:21.792: INFO: (16) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 25.674582ms)
Feb 22 09:14:21.813: INFO: (17) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 20.917803ms)
Feb 22 09:14:21.816: INFO: (17) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 23.189893ms)
Feb 22 09:14:21.816: INFO: (17) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 23.571791ms)
Feb 22 09:14:21.816: INFO: (17) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 22.291097ms)
Feb 22 09:14:21.816: INFO: (17) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 24.261588ms)
Feb 22 09:14:21.816: INFO: (17) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 21.313701ms)
Feb 22 09:14:21.816: INFO: (17) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 23.612991ms)
Feb 22 09:14:21.821: INFO: (17) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 26.555577ms)
Feb 22 09:14:21.821: INFO: (17) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 25.83578ms)
Feb 22 09:14:21.821: INFO: (17) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 26.04268ms)
Feb 22 09:14:21.823: INFO: (17) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 29.405864ms)
Feb 22 09:14:21.823: INFO: (17) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 28.774367ms)
Feb 22 09:14:21.826: INFO: (17) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 33.277446ms)
Feb 22 09:14:21.826: INFO: (17) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 32.196651ms)
Feb 22 09:14:21.827: INFO: (17) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 31.667054ms)
Feb 22 09:14:21.827: INFO: (17) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 33.249247ms)
Feb 22 09:14:21.852: INFO: (18) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 24.558286ms)
Feb 22 09:14:21.856: INFO: (18) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 28.850266ms)
Feb 22 09:14:21.856: INFO: (18) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 27.961271ms)
Feb 22 09:14:21.852: INFO: (18) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 24.006589ms)
Feb 22 09:14:21.859: INFO: (18) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 30.002962ms)
Feb 22 09:14:21.859: INFO: (18) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 30.868757ms)
Feb 22 09:14:21.859: INFO: (18) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 31.795653ms)
Feb 22 09:14:21.859: INFO: (18) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 29.685963ms)
Feb 22 09:14:21.859: INFO: (18) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 31.729653ms)
Feb 22 09:14:21.861: INFO: (18) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 32.61805ms)
Feb 22 09:14:21.861: INFO: (18) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 32.600349ms)
Feb 22 09:14:21.862: INFO: (18) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 32.191451ms)
Feb 22 09:14:21.863: INFO: (18) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 34.946438ms)
Feb 22 09:14:21.863: INFO: (18) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 34.439541ms)
Feb 22 09:14:21.863: INFO: (18) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 34.71754ms)
Feb 22 09:14:21.864: INFO: (18) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 36.87073ms)
Feb 22 09:14:21.883: INFO: (19) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 15.872726ms)
Feb 22 09:14:21.883: INFO: (19) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">... (200; 17.544719ms)
Feb 22 09:14:21.883: INFO: (19) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:1080/proxy/rewriteme">test<... (200; 17.895018ms)
Feb 22 09:14:21.883: INFO: (19) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:160/proxy/: foo (200; 17.125721ms)
Feb 22 09:14:21.884: INFO: (19) /api/v1/namespaces/proxy-5499/pods/http:proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 18.630914ms)
Feb 22 09:14:21.884: INFO: (19) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:443/proxy/tlsrewritem... (200; 18.534615ms)
Feb 22 09:14:21.894: INFO: (19) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/: <a href="/api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc/proxy/rewriteme">test</a> (200; 27.280674ms)
Feb 22 09:14:21.894: INFO: (19) /api/v1/namespaces/proxy-5499/pods/proxy-service-7f9s7-wtcpc:162/proxy/: bar (200; 26.597677ms)
Feb 22 09:14:21.895: INFO: (19) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname1/proxy/: tls baz (200; 29.205165ms)
Feb 22 09:14:21.895: INFO: (19) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:460/proxy/: tls baz (200; 26.501378ms)
Feb 22 09:14:21.895: INFO: (19) /api/v1/namespaces/proxy-5499/pods/https:proxy-service-7f9s7-wtcpc:462/proxy/: tls qux (200; 28.409868ms)
Feb 22 09:14:21.895: INFO: (19) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname2/proxy/: bar (200; 29.690663ms)
Feb 22 09:14:21.898: INFO: (19) /api/v1/namespaces/proxy-5499/services/https:proxy-service-7f9s7:tlsportname2/proxy/: tls qux (200; 31.160956ms)
Feb 22 09:14:21.903: INFO: (19) /api/v1/namespaces/proxy-5499/services/proxy-service-7f9s7:portname1/proxy/: foo (200; 38.90872ms)
Feb 22 09:14:21.903: INFO: (19) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname2/proxy/: bar (200; 35.826134ms)
Feb 22 09:14:21.904: INFO: (19) /api/v1/namespaces/proxy-5499/services/http:proxy-service-7f9s7:portname1/proxy/: foo (200; 37.347027ms)
STEP: deleting ReplicationController proxy-service-7f9s7 in namespace proxy-5499, will wait for the garbage collector to delete the pods
Feb 22 09:14:21.988: INFO: Deleting ReplicationController proxy-service-7f9s7 took: 18.048617ms
Feb 22 09:14:22.089: INFO: Terminating ReplicationController proxy-service-7f9s7 pods took: 100.859934ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:25.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5499" for this suite.

• [SLOW TEST:10.028 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":81,"skipped":1586,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:25.351: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:25.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-989" for this suite.
STEP: Destroying namespace "nspatchtest-93eacd28-712d-4aba-9559-b28e3f23afe3-3309" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":82,"skipped":1604,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:25.766: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-4572
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4572 to expose endpoints map[]
Feb 22 09:14:25.921: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 22 09:14:26.941: INFO: successfully validated that service endpoint-test2 in namespace services-4572 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4572
Feb 22 09:14:26.996: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:14:29.014: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:14:31.006: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4572 to expose endpoints map[pod1:[80]]
Feb 22 09:14:31.073: INFO: successfully validated that service endpoint-test2 in namespace services-4572 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Feb 22 09:14:31.073: INFO: Creating new exec pod
Feb 22 09:14:36.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4572 exec execpodg52p9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 22 09:14:36.673: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 22 09:14:36.673: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:14:36.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4572 exec execpodg52p9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.91.94 80'
Feb 22 09:14:37.241: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.91.94 80\nConnection to 172.20.91.94 80 port [tcp/http] succeeded!\n"
Feb 22 09:14:37.241: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-4572
Feb 22 09:14:37.269: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:14:39.291: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:14:41.276: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4572 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 22 09:14:41.312: INFO: successfully validated that service endpoint-test2 in namespace services-4572 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Feb 22 09:14:42.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4572 exec execpodg52p9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 22 09:14:42.968: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 22 09:14:42.968: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:14:42.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4572 exec execpodg52p9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.91.94 80'
Feb 22 09:14:43.597: INFO: stderr: "+ + echonc hostName\n -v -t -w 2 172.20.91.94 80\nConnection to 172.20.91.94 80 port [tcp/http] succeeded!\n"
Feb 22 09:14:43.598: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4572
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4572 to expose endpoints map[pod2:[80]]
Feb 22 09:14:43.789: INFO: successfully validated that service endpoint-test2 in namespace services-4572 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Feb 22 09:14:44.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4572 exec execpodg52p9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 22 09:14:45.472: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 22 09:14:45.472: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:14:45.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4572 exec execpodg52p9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.91.94 80'
Feb 22 09:14:46.204: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.91.94 80\nConnection to 172.20.91.94 80 port [tcp/http] succeeded!\n"
Feb 22 09:14:46.205: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-4572
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4572 to expose endpoints map[]
Feb 22 09:14:47.372: INFO: successfully validated that service endpoint-test2 in namespace services-4572 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:47.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4572" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:21.748 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":83,"skipped":1633,"failed":0}
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:47.514: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 22 09:14:47.832: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 22 09:14:47.850: INFO: starting watch
STEP: patching
STEP: updating
Feb 22 09:14:47.930: INFO: waiting for watch events with expected annotations
Feb 22 09:14:47.930: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:48.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-298" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":84,"skipped":1633,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:48.088: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Feb 22 09:14:48.270: INFO: observed Pod pod-test in namespace pods-5253 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 22 09:14:48.279: INFO: observed Pod pod-test in namespace pods-5253 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC  }]
Feb 22 09:14:48.315: INFO: observed Pod pod-test in namespace pods-5253 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC  }]
Feb 22 09:14:49.869: INFO: observed Pod pod-test in namespace pods-5253 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC  }]
Feb 22 09:14:50.774: INFO: Found Pod pod-test in namespace pods-5253 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 09:14:48 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Feb 22 09:14:50.801: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Feb 22 09:14:50.878: INFO: observed event type ADDED
Feb 22 09:14:50.878: INFO: observed event type MODIFIED
Feb 22 09:14:50.878: INFO: observed event type MODIFIED
Feb 22 09:14:50.878: INFO: observed event type MODIFIED
Feb 22 09:14:50.878: INFO: observed event type MODIFIED
Feb 22 09:14:50.879: INFO: observed event type MODIFIED
Feb 22 09:14:50.879: INFO: observed event type MODIFIED
Feb 22 09:14:50.879: INFO: observed event type MODIFIED
Feb 22 09:14:52.871: INFO: observed event type MODIFIED
Feb 22 09:14:53.446: INFO: observed event type MODIFIED
Feb 22 09:14:54.935: INFO: observed event type MODIFIED
Feb 22 09:14:54.955: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:54.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5253" for this suite.

• [SLOW TEST:6.931 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":85,"skipped":1648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:55.019: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 22 09:14:55.160: INFO: Waiting up to 5m0s for pod "pod-10ff33a3-873c-4c93-89a9-283e2e6a9f16" in namespace "emptydir-8147" to be "Succeeded or Failed"
Feb 22 09:14:55.176: INFO: Pod "pod-10ff33a3-873c-4c93-89a9-283e2e6a9f16": Phase="Pending", Reason="", readiness=false. Elapsed: 15.896927ms
Feb 22 09:14:57.188: INFO: Pod "pod-10ff33a3-873c-4c93-89a9-283e2e6a9f16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02791983s
Feb 22 09:14:59.200: INFO: Pod "pod-10ff33a3-873c-4c93-89a9-283e2e6a9f16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040159333s
STEP: Saw pod success
Feb 22 09:14:59.200: INFO: Pod "pod-10ff33a3-873c-4c93-89a9-283e2e6a9f16" satisfied condition "Succeeded or Failed"
Feb 22 09:14:59.217: INFO: Trying to get logs from node node2 pod pod-10ff33a3-873c-4c93-89a9-283e2e6a9f16 container test-container: <nil>
STEP: delete the pod
Feb 22 09:14:59.353: INFO: Waiting for pod pod-10ff33a3-873c-4c93-89a9-283e2e6a9f16 to disappear
Feb 22 09:14:59.360: INFO: Pod pod-10ff33a3-873c-4c93-89a9-283e2e6a9f16 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:14:59.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8147" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1691,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:14:59.405: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 22 09:14:59.628: INFO: Waiting up to 5m0s for pod "pod-f1da4ac0-7fc7-464e-81d5-711565f5c4ef" in namespace "emptydir-3071" to be "Succeeded or Failed"
Feb 22 09:14:59.646: INFO: Pod "pod-f1da4ac0-7fc7-464e-81d5-711565f5c4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 17.848518ms
Feb 22 09:15:01.663: INFO: Pod "pod-f1da4ac0-7fc7-464e-81d5-711565f5c4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035204496s
Feb 22 09:15:03.697: INFO: Pod "pod-f1da4ac0-7fc7-464e-81d5-711565f5c4ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069216498s
STEP: Saw pod success
Feb 22 09:15:03.697: INFO: Pod "pod-f1da4ac0-7fc7-464e-81d5-711565f5c4ef" satisfied condition "Succeeded or Failed"
Feb 22 09:15:03.708: INFO: Trying to get logs from node node2 pod pod-f1da4ac0-7fc7-464e-81d5-711565f5c4ef container test-container: <nil>
STEP: delete the pod
Feb 22 09:15:03.846: INFO: Waiting for pod pod-f1da4ac0-7fc7-464e-81d5-711565f5c4ef to disappear
Feb 22 09:15:03.854: INFO: Pod pod-f1da4ac0-7fc7-464e-81d5-711565f5c4ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:15:03.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3071" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1704,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:15:03.897: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:15:04.073: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 22 09:15:09.094: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Feb 22 09:15:09.119: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Feb 22 09:15:09.201: INFO: observed ReplicaSet test-rs in namespace replicaset-3109 with ReadyReplicas 1, AvailableReplicas 1
Feb 22 09:15:09.278: INFO: observed ReplicaSet test-rs in namespace replicaset-3109 with ReadyReplicas 1, AvailableReplicas 1
Feb 22 09:15:09.328: INFO: observed ReplicaSet test-rs in namespace replicaset-3109 with ReadyReplicas 1, AvailableReplicas 1
Feb 22 09:15:09.456: INFO: observed ReplicaSet test-rs in namespace replicaset-3109 with ReadyReplicas 1, AvailableReplicas 1
Feb 22 09:15:12.802: INFO: observed ReplicaSet test-rs in namespace replicaset-3109 with ReadyReplicas 2, AvailableReplicas 2
Feb 22 09:15:12.897: INFO: observed Replicaset test-rs in namespace replicaset-3109 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:15:12.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3109" for this suite.

• [SLOW TEST:9.053 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":88,"skipped":1711,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:15:12.950: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6659
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:15:13.205: INFO: Found 0 stateful pods, waiting for 1
Feb 22 09:15:23.224: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Feb 22 09:15:23.320: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:15:23.320: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Feb 22 09:15:33.340: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:15:33.340: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 22 09:15:33.404: INFO: Deleting all statefulset in ns statefulset-6659
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:15:33.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6659" for this suite.

• [SLOW TEST:20.553 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":89,"skipped":1716,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:15:33.503: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7636.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7636.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7636.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7636.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7636.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7636.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 09:15:37.912: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:37.958: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:37.966: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:37.976: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:38.000: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:38.009: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:38.016: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:38.026: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:38.042: INFO: Lookups using dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local]

Feb 22 09:15:43.052: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:43.098: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:43.110: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:43.119: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:43.146: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:43.156: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:43.165: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:43.176: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:43.193: INFO: Lookups using dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local]

Feb 22 09:15:48.052: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:48.102: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:48.110: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:48.123: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:48.172: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:48.185: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:48.200: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:48.218: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:48.237: INFO: Lookups using dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local]

Feb 22 09:15:53.054: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:53.100: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:53.108: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:53.116: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:53.138: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:53.154: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:53.165: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:53.176: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:53.200: INFO: Lookups using dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local]

Feb 22 09:15:58.052: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:58.062: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:58.106: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:58.114: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:58.138: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:58.146: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:58.153: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:58.161: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:15:58.181: INFO: Lookups using dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local]

Feb 22 09:16:03.053: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:16:03.097: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:16:03.106: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:16:03.113: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:16:03.140: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:16:03.148: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:16:03.155: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:16:03.164: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local from pod dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3: the server could not find the requested resource (get pods dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3)
Feb 22 09:16:03.179: INFO: Lookups using dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7636.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7636.svc.cluster.local jessie_udp@dns-test-service-2.dns-7636.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7636.svc.cluster.local]

Feb 22 09:16:08.164: INFO: DNS probes using dns-7636/dns-test-9d42f7eb-6e9a-456a-b5fd-39f58b8804f3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:16:08.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7636" for this suite.

• [SLOW TEST:35.012 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":90,"skipped":1720,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:16:08.515: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:16:08.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8254" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":91,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:16:08.763: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5409
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Feb 22 09:16:08.979: INFO: Found 0 stateful pods, waiting for 3
Feb 22 09:16:19.002: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:16:19.002: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:16:19.002: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Feb 22 09:16:19.069: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 22 09:16:29.168: INFO: Updating stateful set ss2
Feb 22 09:16:29.185: INFO: Waiting for Pod statefulset-5409/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Feb 22 09:16:39.368: INFO: Found 1 stateful pods, waiting for 3
Feb 22 09:16:49.387: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:16:49.387: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:16:49.387: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 22 09:16:59.388: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:16:59.388: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:16:59.388: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 22 09:17:09.385: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:17:09.385: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:17:09.385: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 22 09:17:19.385: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:17:19.385: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:17:19.385: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 22 09:17:29.391: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:17:29.391: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:17:29.391: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 22 09:17:29.525: INFO: Updating stateful set ss2
Feb 22 09:17:29.556: INFO: Waiting for Pod statefulset-5409/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Feb 22 09:17:39.603: INFO: Updating stateful set ss2
Feb 22 09:17:39.626: INFO: Waiting for StatefulSet statefulset-5409/ss2 to complete update
Feb 22 09:17:39.626: INFO: Waiting for Pod statefulset-5409/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Feb 22 09:17:49.645: INFO: Waiting for StatefulSet statefulset-5409/ss2 to complete update
Feb 22 09:17:49.645: INFO: Waiting for Pod statefulset-5409/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Feb 22 09:17:59.667: INFO: Waiting for StatefulSet statefulset-5409/ss2 to complete update
Feb 22 09:17:59.667: INFO: Waiting for Pod statefulset-5409/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Feb 22 09:18:09.652: INFO: Waiting for StatefulSet statefulset-5409/ss2 to complete update
Feb 22 09:18:09.652: INFO: Waiting for Pod statefulset-5409/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Feb 22 09:18:19.656: INFO: Waiting for StatefulSet statefulset-5409/ss2 to complete update
Feb 22 09:18:19.657: INFO: Waiting for Pod statefulset-5409/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 22 09:18:29.648: INFO: Deleting all statefulset in ns statefulset-5409
Feb 22 09:18:29.654: INFO: Scaling statefulset ss2 to 0
Feb 22 09:18:39.705: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 09:18:39.712: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:18:39.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5409" for this suite.

• [SLOW TEST:151.016 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":92,"skipped":1746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:18:39.780: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:18:40.952: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 22 09:18:42.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118321, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118321, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118321, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118320, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:18:46.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:18:47.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5156" for this suite.
STEP: Destroying namespace "webhook-5156-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.983 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":93,"skipped":1789,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:18:47.763: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:23:48.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6651" for this suite.

• [SLOW TEST:300.396 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":94,"skipped":1812,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:23:48.159: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 22 09:23:48.505: INFO: The status of Pod labelsupdate3c3e2936-2d3a-4cb1-8f72-6399d58a8209 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:23:50.513: INFO: The status of Pod labelsupdate3c3e2936-2d3a-4cb1-8f72-6399d58a8209 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:23:52.532: INFO: The status of Pod labelsupdate3c3e2936-2d3a-4cb1-8f72-6399d58a8209 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:23:54.520: INFO: The status of Pod labelsupdate3c3e2936-2d3a-4cb1-8f72-6399d58a8209 is Running (Ready = true)
Feb 22 09:23:55.130: INFO: Successfully updated pod "labelsupdate3c3e2936-2d3a-4cb1-8f72-6399d58a8209"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:23:57.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2185" for this suite.

• [SLOW TEST:9.083 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:23:57.245: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 22 09:23:57.446: INFO: Waiting up to 5m0s for pod "downward-api-f50dc6bd-f156-468d-917f-3f031230174b" in namespace "downward-api-5116" to be "Succeeded or Failed"
Feb 22 09:23:57.452: INFO: Pod "downward-api-f50dc6bd-f156-468d-917f-3f031230174b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50897ms
Feb 22 09:23:59.499: INFO: Pod "downward-api-f50dc6bd-f156-468d-917f-3f031230174b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053021614s
Feb 22 09:24:01.509: INFO: Pod "downward-api-f50dc6bd-f156-468d-917f-3f031230174b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062897427s
STEP: Saw pod success
Feb 22 09:24:01.509: INFO: Pod "downward-api-f50dc6bd-f156-468d-917f-3f031230174b" satisfied condition "Succeeded or Failed"
Feb 22 09:24:01.517: INFO: Trying to get logs from node node1 pod downward-api-f50dc6bd-f156-468d-917f-3f031230174b container dapi-container: <nil>
STEP: delete the pod
Feb 22 09:24:01.627: INFO: Waiting for pod downward-api-f50dc6bd-f156-468d-917f-3f031230174b to disappear
Feb 22 09:24:01.641: INFO: Pod downward-api-f50dc6bd-f156-468d-917f-3f031230174b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:24:01.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5116" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1908,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:24:01.717: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8785
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Feb 22 09:24:02.135: INFO: Found 0 stateful pods, waiting for 3
Feb 22 09:24:12.155: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:24:12.155: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:24:12.155: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 22 09:24:22.145: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:24:22.145: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:24:22.145: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 09:24:22.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-8785 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 09:24:23.455: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 09:24:23.455: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 09:24:23.455: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Feb 22 09:24:33.620: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 22 09:24:43.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-8785 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 09:24:44.402: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 22 09:24:44.402: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 09:24:44.402: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Feb 22 09:25:04.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-8785 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 09:25:05.149: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 09:25:05.149: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 09:25:05.149: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 09:25:15.245: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 22 09:25:25.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-8785 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 09:25:25.936: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 22 09:25:25.936: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 09:25:25.936: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 22 09:25:46.066: INFO: Deleting all statefulset in ns statefulset-8785
Feb 22 09:25:46.081: INFO: Scaling statefulset ss2 to 0
Feb 22 09:25:56.141: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 09:25:56.149: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:25:56.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8785" for this suite.

• [SLOW TEST:114.516 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":97,"skipped":1910,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:25:56.233: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Feb 22 09:25:56.445: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 22 09:26:01.469: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Feb 22 09:26:01.509: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:26:01.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1005" for this suite.

• [SLOW TEST:5.752 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":98,"skipped":1924,"failed":0}
SSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:26:01.986: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Feb 22 09:26:04.794: INFO: pods: 0 < 3
Feb 22 09:26:06.920: INFO: running pods: 0 < 3
Feb 22 09:26:08.846: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Feb 22 09:26:15.680: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:26:17.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6489" for this suite.

• [SLOW TEST:15.963 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":99,"skipped":1927,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:26:17.950: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:26:24.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1683" for this suite.
STEP: Destroying namespace "nsdeletetest-6647" for this suite.
Feb 22 09:26:25.061: INFO: Namespace nsdeletetest-6647 was already deleted
STEP: Destroying namespace "nsdeletetest-7899" for this suite.

• [SLOW TEST:7.154 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":100,"skipped":1943,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:26:25.103: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Feb 22 09:26:25.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-1065 cluster-info'
Feb 22 09:26:25.425: INFO: stderr: ""
Feb 22 09:26:25.425: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.20.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:26:25.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1065" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":101,"skipped":1953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:26:25.453: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1141.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1141.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1141.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1141.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1141.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1141.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 09:26:30.026: INFO: DNS probes using dns-1141/dns-test-5725a8b8-daca-45a9-aa60-515a26adaa30 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:26:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1141" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":102,"skipped":1985,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:26:30.198: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Feb 22 09:26:30.350: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:26:32.370: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:26:34.361: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.28.128.13 on the node which pod1 resides and expect scheduled
Feb 22 09:26:34.384: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:26:36.396: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:26:38.397: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.28.128.13 but use UDP protocol on the node which pod2 resides
Feb 22 09:26:38.424: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:26:40.440: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:26:42.440: INFO: The status of Pod pod3 is Running (Ready = true)
Feb 22 09:26:42.464: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:26:44.473: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Feb 22 09:26:44.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.28.128.13 http://127.0.0.1:54323/hostname] Namespace:hostport-2236 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:26:44.479: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.28.128.13, port: 54323
Feb 22 09:26:44.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.28.128.13:54323/hostname] Namespace:hostport-2236 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:26:44.977: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.28.128.13, port: 54323 UDP
Feb 22 09:26:45.521: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.28.128.13 54323] Namespace:hostport-2236 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:26:45.521: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:26:51.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-2236" for this suite.

• [SLOW TEST:20.847 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":103,"skipped":1996,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:26:51.046: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:26:52.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 09:26:54.111: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118812, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118812, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118812, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118812, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:26:57.190: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:26:57.200: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8746-crds.webhook.example.com via the AdmissionRegistration API
Feb 22 09:26:57.842: INFO: Waiting for webhook configuration to be ready...
Feb 22 09:26:57.981: INFO: Waiting for webhook configuration to be ready...
Feb 22 09:26:58.091: INFO: Waiting for webhook configuration to be ready...
Feb 22 09:26:58.191: INFO: Waiting for webhook configuration to be ready...
Feb 22 09:26:58.302: INFO: Waiting for webhook configuration to be ready...
Feb 22 09:26:58.388: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:27:01.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4345" for this suite.
STEP: Destroying namespace "webhook-4345-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.909 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":104,"skipped":1996,"failed":0}
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:27:01.955: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 22 09:27:02.281: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:27:09.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7282" for this suite.

• [SLOW TEST:7.188 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":1996,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:27:09.143: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 22 09:27:09.338: INFO: Waiting up to 5m0s for pod "pod-7195e58d-f403-4f1d-a850-664d5e24c3ec" in namespace "emptydir-1033" to be "Succeeded or Failed"
Feb 22 09:27:09.354: INFO: Pod "pod-7195e58d-f403-4f1d-a850-664d5e24c3ec": Phase="Pending", Reason="", readiness=false. Elapsed: 15.775827ms
Feb 22 09:27:11.399: INFO: Pod "pod-7195e58d-f403-4f1d-a850-664d5e24c3ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060948877s
Feb 22 09:27:13.406: INFO: Pod "pod-7195e58d-f403-4f1d-a850-664d5e24c3ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068071103s
STEP: Saw pod success
Feb 22 09:27:13.406: INFO: Pod "pod-7195e58d-f403-4f1d-a850-664d5e24c3ec" satisfied condition "Succeeded or Failed"
Feb 22 09:27:13.412: INFO: Trying to get logs from node node1 pod pod-7195e58d-f403-4f1d-a850-664d5e24c3ec container test-container: <nil>
STEP: delete the pod
Feb 22 09:27:13.537: INFO: Waiting for pod pod-7195e58d-f403-4f1d-a850-664d5e24c3ec to disappear
Feb 22 09:27:13.544: INFO: Pod pod-7195e58d-f403-4f1d-a850-664d5e24c3ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:27:13.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1033" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":106,"skipped":2014,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:27:13.591: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:27:13.860: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 22 09:27:13.958: INFO: The status of Pod pod-logs-websocket-1d532631-1363-40e1-bf44-a1a16102a3fe is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:27:15.966: INFO: The status of Pod pod-logs-websocket-1d532631-1363-40e1-bf44-a1a16102a3fe is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:27:17.977: INFO: The status of Pod pod-logs-websocket-1d532631-1363-40e1-bf44-a1a16102a3fe is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:27:18.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5730" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":2021,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:27:18.071: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Feb 22 09:27:18.228: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:27:20.271: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:27:22.284: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 22 09:27:23.375: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:27:24.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6896" for this suite.

• [SLOW TEST:6.415 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":108,"skipped":2041,"failed":0}
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:27:24.486: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:27:24.835: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d87c381a-6d83-4271-86d6-697dfa9933f9", Controller:(*bool)(0xc006361af6), BlockOwnerDeletion:(*bool)(0xc006361af7)}}
Feb 22 09:27:24.933: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"965a99ec-8426-49f9-bfb2-2bbec65fc926", Controller:(*bool)(0xc006361dee), BlockOwnerDeletion:(*bool)(0xc006361def)}}
Feb 22 09:27:24.965: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"403763ee-007e-4699-a699-e4280577a83c", Controller:(*bool)(0xc0065206ae), BlockOwnerDeletion:(*bool)(0xc0065206af)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:27:30.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5026" for this suite.

• [SLOW TEST:5.601 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":109,"skipped":2041,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:27:30.087: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:27:30.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1323" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":2046,"failed":0}

------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:27:30.511: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:27:30.788: INFO: Creating pod...
Feb 22 09:27:30.883: INFO: Pod Quantity: 1 Status: Pending
Feb 22 09:27:31.894: INFO: Pod Quantity: 1 Status: Pending
Feb 22 09:27:32.906: INFO: Pod Quantity: 1 Status: Pending
Feb 22 09:27:33.917: INFO: Pod Quantity: 1 Status: Pending
Feb 22 09:27:34.893: INFO: Pod Quantity: 1 Status: Pending
Feb 22 09:27:35.891: INFO: Pod Status: Running
Feb 22 09:27:35.891: INFO: Creating service...
Feb 22 09:27:35.930: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/pods/agnhost/proxy/some/path/with/DELETE
Feb 22 09:27:36.074: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 22 09:27:36.074: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/pods/agnhost/proxy/some/path/with/GET
Feb 22 09:27:36.141: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 22 09:27:36.141: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/pods/agnhost/proxy/some/path/with/HEAD
Feb 22 09:27:36.178: INFO: http.Client request:HEAD | StatusCode:200
Feb 22 09:27:36.178: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 22 09:27:36.201: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 22 09:27:36.202: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/pods/agnhost/proxy/some/path/with/PATCH
Feb 22 09:27:36.262: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 22 09:27:36.262: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/pods/agnhost/proxy/some/path/with/POST
Feb 22 09:27:36.275: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 22 09:27:36.275: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/pods/agnhost/proxy/some/path/with/PUT
Feb 22 09:27:36.324: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 22 09:27:36.324: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/services/test-service/proxy/some/path/with/DELETE
Feb 22 09:27:36.341: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 22 09:27:36.341: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/services/test-service/proxy/some/path/with/GET
Feb 22 09:27:36.389: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 22 09:27:36.389: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/services/test-service/proxy/some/path/with/HEAD
Feb 22 09:27:36.412: INFO: http.Client request:HEAD | StatusCode:200
Feb 22 09:27:36.412: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/services/test-service/proxy/some/path/with/OPTIONS
Feb 22 09:27:36.458: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 22 09:27:36.459: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/services/test-service/proxy/some/path/with/PATCH
Feb 22 09:27:36.470: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 22 09:27:36.470: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/services/test-service/proxy/some/path/with/POST
Feb 22 09:27:36.483: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 22 09:27:36.484: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-7792/services/test-service/proxy/some/path/with/PUT
Feb 22 09:27:36.502: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:27:36.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7792" for this suite.

• [SLOW TEST:6.052 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":111,"skipped":2046,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:27:36.564: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 22 09:27:36.785: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 22 09:28:36.872: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:28:36.878: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:28:37.083: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Feb 22 09:28:37.091: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:28:37.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8694" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:28:37.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1778" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.788 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":112,"skipped":2052,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:28:37.352: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:28:37.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-999" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":113,"skipped":2057,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:28:37.671: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:28:37.794: INFO: The status of Pod server-envvars-a7e65153-dcaf-420c-80b9-964784af28f5 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:28:39.807: INFO: The status of Pod server-envvars-a7e65153-dcaf-420c-80b9-964784af28f5 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:28:41.802: INFO: The status of Pod server-envvars-a7e65153-dcaf-420c-80b9-964784af28f5 is Running (Ready = true)
Feb 22 09:28:41.874: INFO: Waiting up to 5m0s for pod "client-envvars-cb14f1a2-39ed-4a48-b3c1-021a709bfc43" in namespace "pods-6407" to be "Succeeded or Failed"
Feb 22 09:28:41.962: INFO: Pod "client-envvars-cb14f1a2-39ed-4a48-b3c1-021a709bfc43": Phase="Pending", Reason="", readiness=false. Elapsed: 88.602891ms
Feb 22 09:28:43.978: INFO: Pod "client-envvars-cb14f1a2-39ed-4a48-b3c1-021a709bfc43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104297477s
Feb 22 09:28:45.989: INFO: Pod "client-envvars-cb14f1a2-39ed-4a48-b3c1-021a709bfc43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114831787s
STEP: Saw pod success
Feb 22 09:28:45.989: INFO: Pod "client-envvars-cb14f1a2-39ed-4a48-b3c1-021a709bfc43" satisfied condition "Succeeded or Failed"
Feb 22 09:28:45.999: INFO: Trying to get logs from node node1 pod client-envvars-cb14f1a2-39ed-4a48-b3c1-021a709bfc43 container env3cont: <nil>
STEP: delete the pod
Feb 22 09:28:46.129: INFO: Waiting for pod client-envvars-cb14f1a2-39ed-4a48-b3c1-021a709bfc43 to disappear
Feb 22 09:28:46.138: INFO: Pod client-envvars-cb14f1a2-39ed-4a48-b3c1-021a709bfc43 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:28:46.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6407" for this suite.

• [SLOW TEST:8.499 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2073,"failed":0}
SSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:28:46.170: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Feb 22 09:28:48.768: INFO: running pods: 0 < 3
Feb 22 09:28:50.848: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:28:52.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6427" for this suite.

• [SLOW TEST:6.730 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":115,"skipped":2077,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:28:52.901: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Feb 22 09:28:53.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5593 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 22 09:28:53.318: INFO: stderr: ""
Feb 22 09:28:53.319: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Feb 22 09:28:53.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5593 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Feb 22 09:28:55.411: INFO: stderr: ""
Feb 22 09:28:55.411: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Feb 22 09:28:55.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5593 delete pods e2e-test-httpd-pod'
Feb 22 09:28:58.878: INFO: stderr: ""
Feb 22 09:28:58.878: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:28:58.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5593" for this suite.

• [SLOW TEST:6.097 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:913
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":116,"skipped":2082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:28:59.014: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:28:59.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5475" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":117,"skipped":2114,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:28:59.437: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 22 09:28:59.906: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 22 09:28:59.927: INFO: starting watch
STEP: patching
STEP: updating
Feb 22 09:29:00.018: INFO: waiting for watch events with expected annotations
Feb 22 09:29:00.018: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:00.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5774" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":118,"skipped":2116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:29:00.209: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-336d6ff4-de74-4853-8193-eda263d8d652
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:00.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6353" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":119,"skipped":2162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:29:00.500: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Feb 22 09:29:00.885: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Feb 22 09:29:02.942: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Feb 22 09:29:05.063: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:07.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-8577" for this suite.

• [SLOW TEST:6.613 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":120,"skipped":2187,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:29:07.114: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Feb 22 09:29:07.245: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:41.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5608" for this suite.

• [SLOW TEST:34.589 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":121,"skipped":2188,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:29:41.703: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 22 09:29:41.837: INFO: Waiting up to 5m0s for pod "pod-3581a7e5-cb1a-4d48-b61d-c4cf75b30ac4" in namespace "emptydir-2443" to be "Succeeded or Failed"
Feb 22 09:29:41.850: INFO: Pod "pod-3581a7e5-cb1a-4d48-b61d-c4cf75b30ac4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.546742ms
Feb 22 09:29:43.874: INFO: Pod "pod-3581a7e5-cb1a-4d48-b61d-c4cf75b30ac4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037343287s
Feb 22 09:29:45.884: INFO: Pod "pod-3581a7e5-cb1a-4d48-b61d-c4cf75b30ac4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0471415s
STEP: Saw pod success
Feb 22 09:29:45.884: INFO: Pod "pod-3581a7e5-cb1a-4d48-b61d-c4cf75b30ac4" satisfied condition "Succeeded or Failed"
Feb 22 09:29:45.892: INFO: Trying to get logs from node node2 pod pod-3581a7e5-cb1a-4d48-b61d-c4cf75b30ac4 container test-container: <nil>
STEP: delete the pod
Feb 22 09:29:46.026: INFO: Waiting for pod pod-3581a7e5-cb1a-4d48-b61d-c4cf75b30ac4 to disappear
Feb 22 09:29:46.034: INFO: Pod pod-3581a7e5-cb1a-4d48-b61d-c4cf75b30ac4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:46.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2443" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2201,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:29:46.064: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Feb 22 09:29:46.198: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4826 proxy --unix-socket=/tmp/kubectl-proxy-unix055148933/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:46.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4826" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":123,"skipped":2212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:29:46.374: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:29:46.517: INFO: Creating simple deployment test-new-deployment
Feb 22 09:29:46.657: INFO: deployment "test-new-deployment" doesn't have the required revision set
Feb 22 09:29:48.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118986, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118986, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118986, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781118986, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 22 09:29:50.983: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8439  52ebe471-b15f-45f3-b704-1a101f417488 49154 3 2022-02-22 09:29:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-02-22 09:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 09:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00492ecf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-22 09:29:49 +0000 UTC,LastTransitionTime:2022-02-22 09:29:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-02-22 09:29:49 +0000 UTC,LastTransitionTime:2022-02-22 09:29:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 22 09:29:51.003: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-8439  cae659ae-b57f-4180-9a8e-d4bbf3e29e14 49162 2 2022-02-22 09:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 52ebe471-b15f-45f3-b704-1a101f417488 0xc00492f237 0xc00492f238}] []  [{kube-controller-manager Update apps/v1 2022-02-22 09:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52ebe471-b15f-45f3-b704-1a101f417488\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 09:29:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00492f2c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 22 09:29:51.032: INFO: Pod "test-new-deployment-847dcfb7fb-f6nrv" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-f6nrv test-new-deployment-847dcfb7fb- deployment-8439  a43c80b8-232d-4f50-aaf2-5e1a5babe5b7 49163 0 2022-02-22 09:29:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb cae659ae-b57f-4180-9a8e-d4bbf3e29e14 0xc00492f727 0xc00492f728}] []  [{kube-controller-manager Update v1 2022-02-22 09:29:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cae659ae-b57f-4180-9a8e-d4bbf3e29e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 09:29:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wppjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wppjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:29:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:29:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:29:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:29:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2022-02-22 09:29:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:29:51.033: INFO: Pod "test-new-deployment-847dcfb7fb-ltt9k" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-ltt9k test-new-deployment-847dcfb7fb- deployment-8439  fdf8b566-bdb1-495f-a6e9-843c23fc5edd 49149 0 2022-02-22 09:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:caccdc8786b531bd1ab9194ca565fcf91e563e2d3b502da0e5b51c08c5538b53 cni.projectcalico.org/podIP:172.21.104.24/32 cni.projectcalico.org/podIPs:172.21.104.24/32] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb cae659ae-b57f-4180-9a8e-d4bbf3e29e14 0xc00492fa27 0xc00492fa28}] []  [{kube-controller-manager Update v1 2022-02-22 09:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cae659ae-b57f-4180-9a8e-d4bbf3e29e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:29:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6vxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6vxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:29:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.24,StartTime:2022-02-22 09:29:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:29:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://f4edbe39b3943c52755b9564dad3af071dc7a204f3fee3034c8199fea289014b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:51.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8439" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":124,"skipped":2255,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:29:51.134: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:55.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6263" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2258,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:29:55.580: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6161/configmap-test-fc514b20-463a-493b-bc68-4104df2d109b
STEP: Creating a pod to test consume configMaps
Feb 22 09:29:55.753: INFO: Waiting up to 5m0s for pod "pod-configmaps-60fd8ace-c85a-4559-bc6c-d16d77518ff1" in namespace "configmap-6161" to be "Succeeded or Failed"
Feb 22 09:29:55.763: INFO: Pod "pod-configmaps-60fd8ace-c85a-4559-bc6c-d16d77518ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.841055ms
Feb 22 09:29:57.808: INFO: Pod "pod-configmaps-60fd8ace-c85a-4559-bc6c-d16d77518ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054880606s
Feb 22 09:29:59.823: INFO: Pod "pod-configmaps-60fd8ace-c85a-4559-bc6c-d16d77518ff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069500497s
STEP: Saw pod success
Feb 22 09:29:59.823: INFO: Pod "pod-configmaps-60fd8ace-c85a-4559-bc6c-d16d77518ff1" satisfied condition "Succeeded or Failed"
Feb 22 09:29:59.838: INFO: Trying to get logs from node node1 pod pod-configmaps-60fd8ace-c85a-4559-bc6c-d16d77518ff1 container env-test: <nil>
STEP: delete the pod
Feb 22 09:29:59.950: INFO: Waiting for pod pod-configmaps-60fd8ace-c85a-4559-bc6c-d16d77518ff1 to disappear
Feb 22 09:29:59.958: INFO: Pod pod-configmaps-60fd8ace-c85a-4559-bc6c-d16d77518ff1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:29:59.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6161" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":126,"skipped":2258,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:30:00.015: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Feb 22 09:30:00.780: INFO: created pod pod-service-account-defaultsa
Feb 22 09:30:00.780: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 22 09:30:00.810: INFO: created pod pod-service-account-mountsa
Feb 22 09:30:00.810: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 22 09:30:00.861: INFO: created pod pod-service-account-nomountsa
Feb 22 09:30:00.861: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 22 09:30:00.951: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 22 09:30:00.952: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 22 09:30:00.999: INFO: created pod pod-service-account-mountsa-mountspec
Feb 22 09:30:00.999: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 22 09:30:01.111: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 22 09:30:01.111: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 22 09:30:01.253: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 22 09:30:01.253: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 22 09:30:01.287: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 22 09:30:01.287: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 22 09:30:01.384: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 22 09:30:01.384: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:30:01.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9458" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":127,"skipped":2263,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:30:01.725: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Feb 22 09:30:02.332: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Feb 22 09:30:02.449: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 22 09:30:02.449: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Feb 22 09:30:02.560: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 22 09:30:02.560: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Feb 22 09:30:02.608: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 22 09:30:02.608: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Feb 22 09:30:10.337: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:30:10.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1122" for this suite.

• [SLOW TEST:8.799 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":128,"skipped":2270,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:30:10.525: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:30:11.300: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 22 09:30:11.597: INFO: Number of nodes with available pods: 0
Feb 22 09:30:11.597: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:12.863: INFO: Number of nodes with available pods: 0
Feb 22 09:30:12.863: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:13.749: INFO: Number of nodes with available pods: 0
Feb 22 09:30:13.749: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:14.668: INFO: Number of nodes with available pods: 0
Feb 22 09:30:14.668: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:15.686: INFO: Number of nodes with available pods: 0
Feb 22 09:30:15.686: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:17.525: INFO: Number of nodes with available pods: 0
Feb 22 09:30:17.525: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:18.073: INFO: Number of nodes with available pods: 0
Feb 22 09:30:18.073: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:19.678: INFO: Number of nodes with available pods: 0
Feb 22 09:30:19.694: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:21.042: INFO: Number of nodes with available pods: 0
Feb 22 09:30:21.042: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:21.854: INFO: Number of nodes with available pods: 0
Feb 22 09:30:21.854: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:22.898: INFO: Number of nodes with available pods: 0
Feb 22 09:30:22.898: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:23.698: INFO: Number of nodes with available pods: 0
Feb 22 09:30:23.698: INFO: Node node1 is running more than one daemon pod
Feb 22 09:30:24.763: INFO: Number of nodes with available pods: 2
Feb 22 09:30:24.763: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 22 09:30:24.988: INFO: Wrong image for pod: daemon-set-fr42h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:24.988: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:26.049: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:27.089: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:28.058: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:29.059: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:30.051: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:30.051: INFO: Pod daemon-set-zmw55 is not available
Feb 22 09:30:31.048: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:31.048: INFO: Pod daemon-set-zmw55 is not available
Feb 22 09:30:32.047: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:32.047: INFO: Pod daemon-set-zmw55 is not available
Feb 22 09:30:33.049: INFO: Wrong image for pod: daemon-set-vrvcn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 22 09:30:33.049: INFO: Pod daemon-set-zmw55 is not available
Feb 22 09:30:37.048: INFO: Pod daemon-set-mhvff is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 22 09:30:37.084: INFO: Number of nodes with available pods: 1
Feb 22 09:30:37.084: INFO: Node node2 is running more than one daemon pod
Feb 22 09:30:38.135: INFO: Number of nodes with available pods: 1
Feb 22 09:30:38.135: INFO: Node node2 is running more than one daemon pod
Feb 22 09:30:39.103: INFO: Number of nodes with available pods: 1
Feb 22 09:30:39.103: INFO: Node node2 is running more than one daemon pod
Feb 22 09:30:40.099: INFO: Number of nodes with available pods: 2
Feb 22 09:30:40.099: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3889, will wait for the garbage collector to delete the pods
Feb 22 09:30:40.218: INFO: Deleting DaemonSet.extensions daemon-set took: 22.935794ms
Feb 22 09:30:40.419: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.833372ms
Feb 22 09:30:43.932: INFO: Number of nodes with available pods: 0
Feb 22 09:30:43.932: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 09:30:43.939: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49693"},"items":null}

Feb 22 09:30:43.946: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49693"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:30:43.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3889" for this suite.

• [SLOW TEST:33.535 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":129,"skipped":2287,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:30:44.060: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6326
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6326
STEP: creating replication controller externalsvc in namespace services-6326
I0222 09:30:44.404916      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6326, replica count: 2
I0222 09:30:47.456353      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:30:50.457599      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 22 09:30:50.693: INFO: Creating new exec pod
Feb 22 09:30:54.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-6326 exec execpod4bq4f -- /bin/sh -x -c nslookup nodeport-service.services-6326.svc.cluster.local'
Feb 22 09:30:55.515: INFO: stderr: "+ nslookup nodeport-service.services-6326.svc.cluster.local\n"
Feb 22 09:30:55.515: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nnodeport-service.services-6326.svc.cluster.local\tcanonical name = externalsvc.services-6326.svc.cluster.local.\nName:\texternalsvc.services-6326.svc.cluster.local\nAddress: 172.20.73.73\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6326, will wait for the garbage collector to delete the pods
Feb 22 09:30:55.597: INFO: Deleting ReplicationController externalsvc took: 18.286115ms
Feb 22 09:30:55.697: INFO: Terminating ReplicationController externalsvc pods took: 100.285137ms
Feb 22 09:30:59.420: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:30:59.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6326" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:15.484 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":130,"skipped":2296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:30:59.545: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:30:59.739: INFO: The status of Pod busybox-host-aliases2f9840d3-fc1b-40c1-9135-b191fef9ae77 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:31:01.774: INFO: The status of Pod busybox-host-aliases2f9840d3-fc1b-40c1-9135-b191fef9ae77 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:31:03.757: INFO: The status of Pod busybox-host-aliases2f9840d3-fc1b-40c1-9135-b191fef9ae77 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:31:03.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-664" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":131,"skipped":2325,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:31:03.816: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 22 09:31:04.026: INFO: The status of Pod annotationupdate904ac6a3-7624-40ec-9cdc-8d5a6abf2f6e is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:31:06.057: INFO: The status of Pod annotationupdate904ac6a3-7624-40ec-9cdc-8d5a6abf2f6e is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:31:08.041: INFO: The status of Pod annotationupdate904ac6a3-7624-40ec-9cdc-8d5a6abf2f6e is Running (Ready = true)
Feb 22 09:31:08.627: INFO: Successfully updated pod "annotationupdate904ac6a3-7624-40ec-9cdc-8d5a6abf2f6e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:31:10.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8976" for this suite.

• [SLOW TEST:6.890 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":132,"skipped":2326,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:31:10.706: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Feb 22 09:31:11.010: INFO: created test-podtemplate-1
Feb 22 09:31:11.043: INFO: created test-podtemplate-2
Feb 22 09:31:11.075: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Feb 22 09:31:11.097: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Feb 22 09:31:11.205: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:31:11.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8455" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":133,"skipped":2342,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:31:11.257: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-a697bbc2-86f0-440b-9c8c-d874f68e74f7
STEP: Creating a pod to test consume secrets
Feb 22 09:31:11.622: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-63a9429a-ac40-42bb-a170-db5f3419d949" in namespace "projected-9913" to be "Succeeded or Failed"
Feb 22 09:31:11.648: INFO: Pod "pod-projected-secrets-63a9429a-ac40-42bb-a170-db5f3419d949": Phase="Pending", Reason="", readiness=false. Elapsed: 25.764081ms
Feb 22 09:31:13.670: INFO: Pod "pod-projected-secrets-63a9429a-ac40-42bb-a170-db5f3419d949": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047169441s
Feb 22 09:31:15.680: INFO: Pod "pod-projected-secrets-63a9429a-ac40-42bb-a170-db5f3419d949": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057045255s
STEP: Saw pod success
Feb 22 09:31:15.680: INFO: Pod "pod-projected-secrets-63a9429a-ac40-42bb-a170-db5f3419d949" satisfied condition "Succeeded or Failed"
Feb 22 09:31:15.691: INFO: Trying to get logs from node node2 pod pod-projected-secrets-63a9429a-ac40-42bb-a170-db5f3419d949 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 09:31:15.819: INFO: Waiting for pod pod-projected-secrets-63a9429a-ac40-42bb-a170-db5f3419d949 to disappear
Feb 22 09:31:15.859: INFO: Pod pod-projected-secrets-63a9429a-ac40-42bb-a170-db5f3419d949 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:31:15.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9913" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:31:15.907: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:31:16.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-3461 version'
Feb 22 09:31:16.286: INFO: stderr: ""
Feb 22 09:31:16.286: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22+\", GitVersion:\"v1.22.5-ske.p2\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2022-01-05T07:28:26Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:31:16.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3461" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":135,"skipped":2378,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:31:16.406: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Feb 22 09:31:16.748: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 22 09:32:16.812: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:32:16.821: INFO: Starting informer...
STEP: Starting pod...
Feb 22 09:32:17.089: INFO: Pod is running on node2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 22 09:32:17.202: INFO: Pod wasn't evicted. Proceeding
Feb 22 09:32:17.202: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 22 09:33:32.510: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:33:32.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9868" for this suite.

• [SLOW TEST:136.130 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":136,"skipped":2399,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:33:32.536: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:33:32.725: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6bbaaa24-2ebd-4918-bbf5-798887cc32b8" in namespace "security-context-test-6142" to be "Succeeded or Failed"
Feb 22 09:33:32.733: INFO: Pod "busybox-privileged-false-6bbaaa24-2ebd-4918-bbf5-798887cc32b8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.114967ms
Feb 22 09:33:34.748: INFO: Pod "busybox-privileged-false-6bbaaa24-2ebd-4918-bbf5-798887cc32b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022578554s
Feb 22 09:33:36.754: INFO: Pod "busybox-privileged-false-6bbaaa24-2ebd-4918-bbf5-798887cc32b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028922684s
Feb 22 09:33:36.755: INFO: Pod "busybox-privileged-false-6bbaaa24-2ebd-4918-bbf5-798887cc32b8" satisfied condition "Succeeded or Failed"
Feb 22 09:33:36.794: INFO: Got logs for pod "busybox-privileged-false-6bbaaa24-2ebd-4918-bbf5-798887cc32b8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:33:36.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6142" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2407,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:33:36.824: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-7571
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 22 09:33:36.941: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 22 09:33:37.081: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:33:39.094: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:33:41.090: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:33:43.090: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:33:45.094: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:33:47.089: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:33:49.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:33:51.089: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:33:53.089: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:33:55.096: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 09:33:57.101: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 22 09:33:57.117: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Feb 22 09:34:01.190: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Feb 22 09:34:01.190: INFO: Breadth first check of 172.21.40.138 on host 172.28.128.12...
Feb 22 09:34:01.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.104.32:9080/dial?request=hostname&protocol=http&host=172.21.40.138&port=8083&tries=1'] Namespace:pod-network-test-7571 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:34:01.196: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:34:01.765: INFO: Waiting for responses: map[]
Feb 22 09:34:01.765: INFO: reached 172.21.40.138 after 0/1 tries
Feb 22 09:34:01.765: INFO: Breadth first check of 172.21.104.15 on host 172.28.128.13...
Feb 22 09:34:01.775: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.104.32:9080/dial?request=hostname&protocol=http&host=172.21.104.15&port=8083&tries=1'] Namespace:pod-network-test-7571 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:34:01.775: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:34:02.333: INFO: Waiting for responses: map[]
Feb 22 09:34:02.333: INFO: reached 172.21.104.15 after 0/1 tries
Feb 22 09:34:02.333: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:34:02.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7571" for this suite.

• [SLOW TEST:25.533 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2407,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:34:02.357: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Feb 22 09:34:02.502: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:34:04.512: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:34:06.514: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:34:07.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3458" for this suite.

• [SLOW TEST:5.372 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":139,"skipped":2409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:34:07.738: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:34:19.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5420" for this suite.

• [SLOW TEST:11.437 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":140,"skipped":2475,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:34:19.177: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:34:23.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7620" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":141,"skipped":2494,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:34:23.679: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:34:40.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5672" for this suite.

• [SLOW TEST:16.463 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":142,"skipped":2496,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:34:40.142: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-1714
STEP: creating replication controller nodeport-test in namespace services-1714
I0222 09:34:40.367681      19 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-1714, replica count: 2
I0222 09:34:43.419042      19 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 09:34:46.419: INFO: Creating new exec pod
I0222 09:34:46.419569      19 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 09:34:51.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:34:52.497: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:34:52.497: INFO: stdout: ""
Feb 22 09:34:53.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:34:54.155: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:34:54.155: INFO: stdout: ""
Feb 22 09:34:54.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:34:55.154: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:34:55.154: INFO: stdout: ""
Feb 22 09:34:55.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:34:56.201: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:34:56.201: INFO: stdout: ""
Feb 22 09:34:56.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:34:57.107: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:34:57.107: INFO: stdout: ""
Feb 22 09:34:57.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:34:58.140: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:34:58.140: INFO: stdout: ""
Feb 22 09:34:58.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:34:59.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:34:59.177: INFO: stdout: ""
Feb 22 09:34:59.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:35:00.223: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:35:00.223: INFO: stdout: ""
Feb 22 09:35:00.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:35:01.111: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:35:01.111: INFO: stdout: ""
Feb 22 09:35:01.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 22 09:35:02.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 22 09:35:02.159: INFO: stdout: "nodeport-test-jpwvg"
Feb 22 09:35:02.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.43.195 80'
Feb 22 09:35:02.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.43.195 80\nConnection to 172.20.43.195 80 port [tcp/http] succeeded!\n"
Feb 22 09:35:02.884: INFO: stdout: ""
Feb 22 09:35:03.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.43.195 80'
Feb 22 09:35:04.559: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.43.195 80\nConnection to 172.20.43.195 80 port [tcp/http] succeeded!\n"
Feb 22 09:35:04.559: INFO: stdout: ""
Feb 22 09:35:04.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.43.195 80'
Feb 22 09:35:05.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.43.195 80\nConnection to 172.20.43.195 80 port [tcp/http] succeeded!\n"
Feb 22 09:35:05.520: INFO: stdout: ""
Feb 22 09:35:05.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.43.195 80'
Feb 22 09:35:06.538: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.43.195 80\nConnection to 172.20.43.195 80 port [tcp/http] succeeded!\n"
Feb 22 09:35:06.538: INFO: stdout: ""
Feb 22 09:35:06.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.43.195 80'
Feb 22 09:35:07.568: INFO: stderr: "+ nc -v -t -w 2 172.20.43.195 80\nConnection to 172.20.43.195 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Feb 22 09:35:07.568: INFO: stdout: "nodeport-test-xn2tj"
Feb 22 09:35:07.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 31015'
Feb 22 09:35:08.299: INFO: stderr: "+ nc -v -t -w 2 172.28.128.12 31015\n+ echo hostName\nConnection to 172.28.128.12 31015 port [tcp/*] succeeded!\n"
Feb 22 09:35:08.299: INFO: stdout: ""
Feb 22 09:35:09.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 31015'
Feb 22 09:35:09.959: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 31015\nConnection to 172.28.128.12 31015 port [tcp/*] succeeded!\n"
Feb 22 09:35:09.959: INFO: stdout: "nodeport-test-xn2tj"
Feb 22 09:35:09.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-1714 exec execpodhnxnj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 31015'
Feb 22 09:35:10.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.13 31015\nConnection to 172.28.128.13 31015 port [tcp/*] succeeded!\n"
Feb 22 09:35:10.650: INFO: stdout: "nodeport-test-jpwvg"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:35:10.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1714" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:30.539 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":143,"skipped":2504,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:35:10.681: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 22 09:35:10.815: INFO: Waiting up to 5m0s for pod "downward-api-93502f76-467b-41c7-8625-1608ab0749cf" in namespace "downward-api-3279" to be "Succeeded or Failed"
Feb 22 09:35:10.821: INFO: Pod "downward-api-93502f76-467b-41c7-8625-1608ab0749cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.44727ms
Feb 22 09:35:12.837: INFO: Pod "downward-api-93502f76-467b-41c7-8625-1608ab0749cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021977057s
Feb 22 09:35:14.852: INFO: Pod "downward-api-93502f76-467b-41c7-8625-1608ab0749cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036843947s
STEP: Saw pod success
Feb 22 09:35:14.852: INFO: Pod "downward-api-93502f76-467b-41c7-8625-1608ab0749cf" satisfied condition "Succeeded or Failed"
Feb 22 09:35:14.859: INFO: Trying to get logs from node node2 pod downward-api-93502f76-467b-41c7-8625-1608ab0749cf container dapi-container: <nil>
STEP: delete the pod
Feb 22 09:35:14.970: INFO: Waiting for pod downward-api-93502f76-467b-41c7-8625-1608ab0749cf to disappear
Feb 22 09:35:14.980: INFO: Pod downward-api-93502f76-467b-41c7-8625-1608ab0749cf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:35:14.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3279" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":144,"skipped":2515,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:35:15.023: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:35:15.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d" in namespace "projected-7485" to be "Succeeded or Failed"
Feb 22 09:35:15.197: INFO: Pod "downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.610937ms
Feb 22 09:35:17.238: INFO: Pod "downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054901605s
Feb 22 09:35:19.257: INFO: Pod "downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073671977s
Feb 22 09:35:21.270: INFO: Pod "downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086389977s
STEP: Saw pod success
Feb 22 09:35:21.270: INFO: Pod "downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d" satisfied condition "Succeeded or Failed"
Feb 22 09:35:21.282: INFO: Trying to get logs from node node2 pod downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d container client-container: <nil>
STEP: delete the pod
Feb 22 09:35:21.395: INFO: Waiting for pod downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d to disappear
Feb 22 09:35:21.411: INFO: Pod downwardapi-volume-7f340f83-1a8c-490e-be47-216e07071e8d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:35:21.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7485" for this suite.

• [SLOW TEST:6.519 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2519,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:35:21.543: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 22 09:35:21.732: INFO: Waiting up to 5m0s for pod "pod-9f302fe2-a508-437f-bf56-4c612566a06f" in namespace "emptydir-8953" to be "Succeeded or Failed"
Feb 22 09:35:21.770: INFO: Pod "pod-9f302fe2-a508-437f-bf56-4c612566a06f": Phase="Pending", Reason="", readiness=false. Elapsed: 37.701326ms
Feb 22 09:35:23.782: INFO: Pod "pod-9f302fe2-a508-437f-bf56-4c612566a06f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049707729s
Feb 22 09:35:25.790: INFO: Pod "pod-9f302fe2-a508-437f-bf56-4c612566a06f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057154454s
STEP: Saw pod success
Feb 22 09:35:25.790: INFO: Pod "pod-9f302fe2-a508-437f-bf56-4c612566a06f" satisfied condition "Succeeded or Failed"
Feb 22 09:35:25.795: INFO: Trying to get logs from node node2 pod pod-9f302fe2-a508-437f-bf56-4c612566a06f container test-container: <nil>
STEP: delete the pod
Feb 22 09:35:25.877: INFO: Waiting for pod pod-9f302fe2-a508-437f-bf56-4c612566a06f to disappear
Feb 22 09:35:25.883: INFO: Pod pod-9f302fe2-a508-437f-bf56-4c612566a06f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:35:25.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8953" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:35:25.910: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 22 09:35:26.044: INFO: Waiting up to 5m0s for pod "pod-4bc82a13-9e15-4ab5-92fb-8c3f442c027e" in namespace "emptydir-2991" to be "Succeeded or Failed"
Feb 22 09:35:26.050: INFO: Pod "pod-4bc82a13-9e15-4ab5-92fb-8c3f442c027e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.685174ms
Feb 22 09:35:28.058: INFO: Pod "pod-4bc82a13-9e15-4ab5-92fb-8c3f442c027e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013940595s
Feb 22 09:35:30.074: INFO: Pod "pod-4bc82a13-9e15-4ab5-92fb-8c3f442c027e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029700981s
STEP: Saw pod success
Feb 22 09:35:30.074: INFO: Pod "pod-4bc82a13-9e15-4ab5-92fb-8c3f442c027e" satisfied condition "Succeeded or Failed"
Feb 22 09:35:30.080: INFO: Trying to get logs from node node2 pod pod-4bc82a13-9e15-4ab5-92fb-8c3f442c027e container test-container: <nil>
STEP: delete the pod
Feb 22 09:35:30.134: INFO: Waiting for pod pod-4bc82a13-9e15-4ab5-92fb-8c3f442c027e to disappear
Feb 22 09:35:30.140: INFO: Pod pod-4bc82a13-9e15-4ab5-92fb-8c3f442c027e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:35:30.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2991" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":147,"skipped":2571,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:35:30.163: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 22 09:35:30.278: INFO: Waiting up to 5m0s for pod "pod-ea0b7211-3184-415e-bd5c-eb3fa6b1e964" in namespace "emptydir-494" to be "Succeeded or Failed"
Feb 22 09:35:30.292: INFO: Pod "pod-ea0b7211-3184-415e-bd5c-eb3fa6b1e964": Phase="Pending", Reason="", readiness=false. Elapsed: 13.551937ms
Feb 22 09:35:32.314: INFO: Pod "pod-ea0b7211-3184-415e-bd5c-eb3fa6b1e964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035781993s
Feb 22 09:35:34.324: INFO: Pod "pod-ea0b7211-3184-415e-bd5c-eb3fa6b1e964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045740306s
STEP: Saw pod success
Feb 22 09:35:34.324: INFO: Pod "pod-ea0b7211-3184-415e-bd5c-eb3fa6b1e964" satisfied condition "Succeeded or Failed"
Feb 22 09:35:34.328: INFO: Trying to get logs from node node2 pod pod-ea0b7211-3184-415e-bd5c-eb3fa6b1e964 container test-container: <nil>
STEP: delete the pod
Feb 22 09:35:34.387: INFO: Waiting for pod pod-ea0b7211-3184-415e-bd5c-eb3fa6b1e964 to disappear
Feb 22 09:35:34.392: INFO: Pod pod-ea0b7211-3184-415e-bd5c-eb3fa6b1e964 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:35:34.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-494" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2575,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:35:34.420: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-6gdn
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 09:35:34.601: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6gdn" in namespace "subpath-4658" to be "Succeeded or Failed"
Feb 22 09:35:34.631: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Pending", Reason="", readiness=false. Elapsed: 29.404064ms
Feb 22 09:35:36.642: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04070727s
Feb 22 09:35:38.651: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 4.049880487s
Feb 22 09:35:40.660: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 6.058995404s
Feb 22 09:35:42.670: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 8.068472319s
Feb 22 09:35:44.686: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 10.084524003s
Feb 22 09:35:46.694: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 12.092948923s
Feb 22 09:35:48.702: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 14.101043745s
Feb 22 09:35:50.708: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 16.106980176s
Feb 22 09:35:52.716: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 18.114989898s
Feb 22 09:35:54.724: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 20.122732121s
Feb 22 09:35:56.739: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Running", Reason="", readiness=true. Elapsed: 22.138055409s
Feb 22 09:35:58.758: INFO: Pod "pod-subpath-test-projected-6gdn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.156601982s
STEP: Saw pod success
Feb 22 09:35:58.758: INFO: Pod "pod-subpath-test-projected-6gdn" satisfied condition "Succeeded or Failed"
Feb 22 09:35:58.763: INFO: Trying to get logs from node node2 pod pod-subpath-test-projected-6gdn container test-container-subpath-projected-6gdn: <nil>
STEP: delete the pod
Feb 22 09:35:58.822: INFO: Waiting for pod pod-subpath-test-projected-6gdn to disappear
Feb 22 09:35:58.829: INFO: Pod pod-subpath-test-projected-6gdn no longer exists
STEP: Deleting pod pod-subpath-test-projected-6gdn
Feb 22 09:35:58.829: INFO: Deleting pod "pod-subpath-test-projected-6gdn" in namespace "subpath-4658"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:35:58.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4658" for this suite.

• [SLOW TEST:24.435 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":149,"skipped":2577,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:35:58.857: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Feb 22 09:35:58.967: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Feb 22 09:35:59.762: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Feb 22 09:36:01.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:03.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:05.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:07.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:09.948: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:11.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:13.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:15.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:17.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:19.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:21.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:23.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:25.952: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:27.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:29.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:31.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:33.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:35.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:37.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:39.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:41.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:43.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:45.952: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:47.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:49.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:51.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:53.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:55.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:57.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:36:59.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:01.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:03.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:05.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:07.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:09.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:11.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:13.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:15.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:17.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:19.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:21.955: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:23.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:25.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:27.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:29.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:31.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:33.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:35.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:37.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:39.948: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:41.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:43.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:45.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:47.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119359, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:37:50.217: INFO: Waited 249.744746ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Feb 22 09:37:50.805: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:37:51.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8816" for this suite.

• [SLOW TEST:112.541 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":150,"skipped":2582,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:37:51.398: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-nvtm
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 09:37:51.633: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nvtm" in namespace "subpath-9720" to be "Succeeded or Failed"
Feb 22 09:37:51.653: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Pending", Reason="", readiness=false. Elapsed: 20.726905ms
Feb 22 09:37:53.685: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052825015s
Feb 22 09:37:55.695: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 4.061920232s
Feb 22 09:37:57.709: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 6.076181625s
Feb 22 09:37:59.717: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 8.084756544s
Feb 22 09:38:01.726: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 10.093516163s
Feb 22 09:38:03.735: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 12.102100082s
Feb 22 09:38:05.740: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 14.107365216s
Feb 22 09:38:07.749: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 16.115958635s
Feb 22 09:38:09.760: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 18.127051543s
Feb 22 09:38:11.769: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 20.13614226s
Feb 22 09:38:13.782: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Running", Reason="", readiness=true. Elapsed: 22.149580957s
Feb 22 09:38:15.791: INFO: Pod "pod-subpath-test-downwardapi-nvtm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.158086376s
STEP: Saw pod success
Feb 22 09:38:15.791: INFO: Pod "pod-subpath-test-downwardapi-nvtm" satisfied condition "Succeeded or Failed"
Feb 22 09:38:15.796: INFO: Trying to get logs from node node2 pod pod-subpath-test-downwardapi-nvtm container test-container-subpath-downwardapi-nvtm: <nil>
STEP: delete the pod
Feb 22 09:38:15.844: INFO: Waiting for pod pod-subpath-test-downwardapi-nvtm to disappear
Feb 22 09:38:15.850: INFO: Pod pod-subpath-test-downwardapi-nvtm no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nvtm
Feb 22 09:38:15.850: INFO: Deleting pod "pod-subpath-test-downwardapi-nvtm" in namespace "subpath-9720"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:38:15.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9720" for this suite.

• [SLOW TEST:24.485 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":151,"skipped":2595,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:38:15.883: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-357cbd99-57dc-4ba1-9f3d-f0859fd2047b
STEP: Creating a pod to test consume configMaps
Feb 22 09:38:16.039: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e50b18a6-f12a-4866-bc2d-007ada813826" in namespace "projected-4611" to be "Succeeded or Failed"
Feb 22 09:38:16.053: INFO: Pod "pod-projected-configmaps-e50b18a6-f12a-4866-bc2d-007ada813826": Phase="Pending", Reason="", readiness=false. Elapsed: 14.136135ms
Feb 22 09:38:18.068: INFO: Pod "pod-projected-configmaps-e50b18a6-f12a-4866-bc2d-007ada813826": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028387528s
Feb 22 09:38:20.080: INFO: Pod "pod-projected-configmaps-e50b18a6-f12a-4866-bc2d-007ada813826": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04060573s
STEP: Saw pod success
Feb 22 09:38:20.080: INFO: Pod "pod-projected-configmaps-e50b18a6-f12a-4866-bc2d-007ada813826" satisfied condition "Succeeded or Failed"
Feb 22 09:38:20.086: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-e50b18a6-f12a-4866-bc2d-007ada813826 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:38:20.192: INFO: Waiting for pod pod-projected-configmaps-e50b18a6-f12a-4866-bc2d-007ada813826 to disappear
Feb 22 09:38:20.198: INFO: Pod pod-projected-configmaps-e50b18a6-f12a-4866-bc2d-007ada813826 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:38:20.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4611" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":152,"skipped":2599,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:38:20.241: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 22 09:38:20.362: INFO: Waiting up to 5m0s for pod "pod-d28e09a9-c8ea-4c6d-8b82-1b4e272b0399" in namespace "emptydir-459" to be "Succeeded or Failed"
Feb 22 09:38:20.381: INFO: Pod "pod-d28e09a9-c8ea-4c6d-8b82-1b4e272b0399": Phase="Pending", Reason="", readiness=false. Elapsed: 18.427415ms
Feb 22 09:38:22.400: INFO: Pod "pod-d28e09a9-c8ea-4c6d-8b82-1b4e272b0399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037163987s
Feb 22 09:38:24.410: INFO: Pod "pod-d28e09a9-c8ea-4c6d-8b82-1b4e272b0399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047555198s
STEP: Saw pod success
Feb 22 09:38:24.410: INFO: Pod "pod-d28e09a9-c8ea-4c6d-8b82-1b4e272b0399" satisfied condition "Succeeded or Failed"
Feb 22 09:38:24.417: INFO: Trying to get logs from node node2 pod pod-d28e09a9-c8ea-4c6d-8b82-1b4e272b0399 container test-container: <nil>
STEP: delete the pod
Feb 22 09:38:24.484: INFO: Waiting for pod pod-d28e09a9-c8ea-4c6d-8b82-1b4e272b0399 to disappear
Feb 22 09:38:24.493: INFO: Pod pod-d28e09a9-c8ea-4c6d-8b82-1b4e272b0399 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:38:24.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-459" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":2600,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:38:24.534: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:38:24.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4233f77-b25e-4258-8aac-a5429e084072" in namespace "projected-6145" to be "Succeeded or Failed"
Feb 22 09:38:24.703: INFO: Pod "downwardapi-volume-d4233f77-b25e-4258-8aac-a5429e084072": Phase="Pending", Reason="", readiness=false. Elapsed: 19.896508ms
Feb 22 09:38:26.715: INFO: Pod "downwardapi-volume-d4233f77-b25e-4258-8aac-a5429e084072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031346314s
Feb 22 09:38:28.727: INFO: Pod "downwardapi-volume-d4233f77-b25e-4258-8aac-a5429e084072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043887115s
STEP: Saw pod success
Feb 22 09:38:28.728: INFO: Pod "downwardapi-volume-d4233f77-b25e-4258-8aac-a5429e084072" satisfied condition "Succeeded or Failed"
Feb 22 09:38:28.733: INFO: Trying to get logs from node node2 pod downwardapi-volume-d4233f77-b25e-4258-8aac-a5429e084072 container client-container: <nil>
STEP: delete the pod
Feb 22 09:38:28.795: INFO: Waiting for pod downwardapi-volume-d4233f77-b25e-4258-8aac-a5429e084072 to disappear
Feb 22 09:38:28.803: INFO: Pod downwardapi-volume-d4233f77-b25e-4258-8aac-a5429e084072 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:38:28.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6145" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":154,"skipped":2600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:38:28.830: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Feb 22 09:38:28.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4504 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 22 09:38:29.109: INFO: stderr: ""
Feb 22 09:38:29.109: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 22 09:38:34.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4504 get pod e2e-test-httpd-pod -o json'
Feb 22 09:38:34.312: INFO: stderr: ""
Feb 22 09:38:34.312: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"110e67f1cb8b155b99d7cd96b14afe625b3d379cc6889b0bbb66df689720589a\",\n            \"cni.projectcalico.org/podIP\": \"172.21.104.6/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.21.104.6/32\"\n        },\n        \"creationTimestamp\": \"2022-02-22T09:38:29Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4504\",\n        \"resourceVersion\": \"51639\",\n        \"uid\": \"cd722cf6-74b5-471f-9457-0d4f675ff9fa\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f469h\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 60\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 60\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f469h\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-22T09:38:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-22T09:38:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-22T09:38:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-22T09:38:29Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://89ddd19db865e5d428b69dc8e1d50192f7a60739f5d572f8f8eee287ec3e226e\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-02-22T09:38:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.28.128.13\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.21.104.6\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.21.104.6\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-02-22T09:38:29Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 22 09:38:34.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4504 replace -f -'
Feb 22 09:38:35.955: INFO: stderr: ""
Feb 22 09:38:35.955: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Feb 22 09:38:35.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4504 delete pods e2e-test-httpd-pod'
Feb 22 09:38:38.646: INFO: stderr: ""
Feb 22 09:38:38.646: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:38:38.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4504" for this suite.

• [SLOW TEST:9.849 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":155,"skipped":2623,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:38:38.679: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:38:39.884: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 22 09:38:41.908: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119519, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119519, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119520, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119519, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:38:44.979: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:38:44.991: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:38:48.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6909" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.231 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":156,"skipped":2628,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:38:48.910: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:38:49.167: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80a071d7-1c44-4a59-9bdf-64ce1e7aacb7" in namespace "projected-7236" to be "Succeeded or Failed"
Feb 22 09:38:49.185: INFO: Pod "downwardapi-volume-80a071d7-1c44-4a59-9bdf-64ce1e7aacb7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.751218ms
Feb 22 09:38:51.210: INFO: Pod "downwardapi-volume-80a071d7-1c44-4a59-9bdf-64ce1e7aacb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042831861s
Feb 22 09:38:53.219: INFO: Pod "downwardapi-volume-80a071d7-1c44-4a59-9bdf-64ce1e7aacb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051844779s
STEP: Saw pod success
Feb 22 09:38:53.219: INFO: Pod "downwardapi-volume-80a071d7-1c44-4a59-9bdf-64ce1e7aacb7" satisfied condition "Succeeded or Failed"
Feb 22 09:38:53.226: INFO: Trying to get logs from node node2 pod downwardapi-volume-80a071d7-1c44-4a59-9bdf-64ce1e7aacb7 container client-container: <nil>
STEP: delete the pod
Feb 22 09:38:53.292: INFO: Waiting for pod downwardapi-volume-80a071d7-1c44-4a59-9bdf-64ce1e7aacb7 to disappear
Feb 22 09:38:53.299: INFO: Pod downwardapi-volume-80a071d7-1c44-4a59-9bdf-64ce1e7aacb7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:38:53.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7236" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:38:53.325: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6868
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6868
STEP: creating replication controller externalsvc in namespace services-6868
I0222 09:38:53.875384      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6868, replica count: 2
I0222 09:38:56.928744      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:38:59.929780      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 22 09:39:00.003: INFO: Creating new exec pod
Feb 22 09:39:04.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-6868 exec execpodlrc2k -- /bin/sh -x -c nslookup clusterip-service.services-6868.svc.cluster.local'
Feb 22 09:39:04.763: INFO: stderr: "+ nslookup clusterip-service.services-6868.svc.cluster.local\n"
Feb 22 09:39:04.763: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nclusterip-service.services-6868.svc.cluster.local\tcanonical name = externalsvc.services-6868.svc.cluster.local.\nName:\texternalsvc.services-6868.svc.cluster.local\nAddress: 172.20.121.58\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6868, will wait for the garbage collector to delete the pods
Feb 22 09:39:04.837: INFO: Deleting ReplicationController externalsvc took: 16.345225ms
Feb 22 09:39:04.942: INFO: Terminating ReplicationController externalsvc pods took: 104.632716ms
Feb 22 09:39:08.205: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:39:08.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6868" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.983 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":158,"skipped":2655,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:39:08.308: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-4z26
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 09:39:08.520: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4z26" in namespace "subpath-1093" to be "Succeeded or Failed"
Feb 22 09:39:08.536: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Pending", Reason="", readiness=false. Elapsed: 16.541124ms
Feb 22 09:39:10.577: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056987296s
Feb 22 09:39:12.593: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 4.073472179s
Feb 22 09:39:14.608: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 6.08800107s
Feb 22 09:39:16.617: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 8.097412886s
Feb 22 09:39:18.628: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 10.107820896s
Feb 22 09:39:20.646: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 12.125938072s
Feb 22 09:39:22.657: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 14.13692628s
Feb 22 09:39:24.669: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 16.148987483s
Feb 22 09:39:26.686: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 18.165715764s
Feb 22 09:39:28.701: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 20.181423451s
Feb 22 09:39:30.709: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Running", Reason="", readiness=true. Elapsed: 22.189632972s
Feb 22 09:39:32.727: INFO: Pod "pod-subpath-test-configmap-4z26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.206799251s
STEP: Saw pod success
Feb 22 09:39:32.727: INFO: Pod "pod-subpath-test-configmap-4z26" satisfied condition "Succeeded or Failed"
Feb 22 09:39:32.736: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-4z26 container test-container-subpath-configmap-4z26: <nil>
STEP: delete the pod
Feb 22 09:39:32.865: INFO: Waiting for pod pod-subpath-test-configmap-4z26 to disappear
Feb 22 09:39:32.886: INFO: Pod pod-subpath-test-configmap-4z26 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4z26
Feb 22 09:39:32.886: INFO: Deleting pod "pod-subpath-test-configmap-4z26" in namespace "subpath-1093"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:39:32.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1093" for this suite.

• [SLOW TEST:24.636 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":159,"skipped":2664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:39:32.944: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-609abfa2-fa01-4490-95ee-880b5ecc992d in namespace container-probe-3521
Feb 22 09:39:37.213: INFO: Started pod liveness-609abfa2-fa01-4490-95ee-880b5ecc992d in namespace container-probe-3521
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 09:39:37.218: INFO: Initial restart count of pod liveness-609abfa2-fa01-4490-95ee-880b5ecc992d is 0
Feb 22 09:39:55.350: INFO: Restart count of pod container-probe-3521/liveness-609abfa2-fa01-4490-95ee-880b5ecc992d is now 1 (18.131457623s elapsed)
Feb 22 09:40:15.460: INFO: Restart count of pod container-probe-3521/liveness-609abfa2-fa01-4490-95ee-880b5ecc992d is now 2 (38.241400203s elapsed)
Feb 22 09:40:35.601: INFO: Restart count of pod container-probe-3521/liveness-609abfa2-fa01-4490-95ee-880b5ecc992d is now 3 (58.38252354s elapsed)
Feb 22 09:40:55.715: INFO: Restart count of pod container-probe-3521/liveness-609abfa2-fa01-4490-95ee-880b5ecc992d is now 4 (1m18.4968705s elapsed)
Feb 22 09:41:58.047: INFO: Restart count of pod container-probe-3521/liveness-609abfa2-fa01-4490-95ee-880b5ecc992d is now 5 (2m20.82902619s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:41:58.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3521" for this suite.

• [SLOW TEST:145.164 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":2709,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:41:58.109: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 22 09:41:58.256: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 22 09:42:58.323: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Feb 22 09:42:58.385: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 22 09:42:58.424: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 22 09:42:58.515: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 22 09:42:58.566: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:43:18.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7652" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:80.732 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":161,"skipped":2709,"failed":0}
SS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:43:18.841: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Feb 22 09:43:19.180: INFO: Waiting up to 5m0s for pod "client-containers-93c9fa3c-8a38-4657-808a-d2d8cfe464e3" in namespace "containers-2413" to be "Succeeded or Failed"
Feb 22 09:43:19.207: INFO: Pod "client-containers-93c9fa3c-8a38-4657-808a-d2d8cfe464e3": Phase="Pending", Reason="", readiness=false. Elapsed: 27.484673ms
Feb 22 09:43:21.231: INFO: Pod "client-containers-93c9fa3c-8a38-4657-808a-d2d8cfe464e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051273321s
Feb 22 09:43:23.247: INFO: Pod "client-containers-93c9fa3c-8a38-4657-808a-d2d8cfe464e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067887004s
STEP: Saw pod success
Feb 22 09:43:23.248: INFO: Pod "client-containers-93c9fa3c-8a38-4657-808a-d2d8cfe464e3" satisfied condition "Succeeded or Failed"
Feb 22 09:43:23.257: INFO: Trying to get logs from node node2 pod client-containers-93c9fa3c-8a38-4657-808a-d2d8cfe464e3 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:43:23.472: INFO: Waiting for pod client-containers-93c9fa3c-8a38-4657-808a-d2d8cfe464e3 to disappear
Feb 22 09:43:23.508: INFO: Pod client-containers-93c9fa3c-8a38-4657-808a-d2d8cfe464e3 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:43:23.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2413" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":2711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:43:23.615: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:43:23.846: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:43:29.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5363" for this suite.

• [SLOW TEST:6.161 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":163,"skipped":2733,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:43:29.791: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-8b82ceae-b2cf-443e-b172-4dc34cbd3d63
STEP: Creating a pod to test consume configMaps
Feb 22 09:43:31.261: INFO: Waiting up to 5m0s for pod "pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91" in namespace "configmap-7663" to be "Succeeded or Failed"
Feb 22 09:43:31.447: INFO: Pod "pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91": Phase="Pending", Reason="", readiness=false. Elapsed: 186.041641ms
Feb 22 09:43:33.460: INFO: Pod "pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199104739s
Feb 22 09:43:35.497: INFO: Pod "pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.235963228s
Feb 22 09:43:37.519: INFO: Pod "pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.257984885s
STEP: Saw pod success
Feb 22 09:43:37.519: INFO: Pod "pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91" satisfied condition "Succeeded or Failed"
Feb 22 09:43:37.527: INFO: Trying to get logs from node node2 pod pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:43:37.574: INFO: Waiting for pod pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91 to disappear
Feb 22 09:43:37.581: INFO: Pod pod-configmaps-59aca94f-2085-49c5-81ef-1edd4c941d91 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:43:37.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7663" for this suite.

• [SLOW TEST:7.816 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":2749,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:43:37.607: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:43:38.687: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 09:43:40.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119818, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119818, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119818, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119818, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:43:43.966: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:43:44.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8253" for this suite.
STEP: Destroying namespace "webhook-8253-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.184 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":165,"skipped":2766,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:43:44.791: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Feb 22 09:43:45.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 create -f -'
Feb 22 09:43:47.490: INFO: stderr: ""
Feb 22 09:43:47.490: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 09:43:47.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:43:47.781: INFO: stderr: ""
Feb 22 09:43:47.781: INFO: stdout: "update-demo-nautilus-dw6gk update-demo-nautilus-zgbvt "
Feb 22 09:43:47.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-dw6gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:43:47.955: INFO: stderr: ""
Feb 22 09:43:47.955: INFO: stdout: ""
Feb 22 09:43:47.955: INFO: update-demo-nautilus-dw6gk is created but not running
Feb 22 09:43:52.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:43:53.500: INFO: stderr: ""
Feb 22 09:43:53.500: INFO: stdout: "update-demo-nautilus-dw6gk update-demo-nautilus-zgbvt "
Feb 22 09:43:53.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-dw6gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:43:53.865: INFO: stderr: ""
Feb 22 09:43:53.865: INFO: stdout: ""
Feb 22 09:43:53.865: INFO: update-demo-nautilus-dw6gk is created but not running
Feb 22 09:43:58.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:43:59.045: INFO: stderr: ""
Feb 22 09:43:59.045: INFO: stdout: "update-demo-nautilus-dw6gk update-demo-nautilus-zgbvt "
Feb 22 09:43:59.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-dw6gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:43:59.181: INFO: stderr: ""
Feb 22 09:43:59.182: INFO: stdout: ""
Feb 22 09:43:59.182: INFO: update-demo-nautilus-dw6gk is created but not running
Feb 22 09:44:04.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:44:04.443: INFO: stderr: ""
Feb 22 09:44:04.443: INFO: stdout: "update-demo-nautilus-dw6gk update-demo-nautilus-zgbvt "
Feb 22 09:44:04.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-dw6gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:04.627: INFO: stderr: ""
Feb 22 09:44:04.627: INFO: stdout: ""
Feb 22 09:44:04.627: INFO: update-demo-nautilus-dw6gk is created but not running
Feb 22 09:44:09.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:44:09.919: INFO: stderr: ""
Feb 22 09:44:09.919: INFO: stdout: "update-demo-nautilus-dw6gk update-demo-nautilus-zgbvt "
Feb 22 09:44:09.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-dw6gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:10.214: INFO: stderr: ""
Feb 22 09:44:10.215: INFO: stdout: ""
Feb 22 09:44:10.215: INFO: update-demo-nautilus-dw6gk is created but not running
Feb 22 09:44:15.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:44:15.515: INFO: stderr: ""
Feb 22 09:44:15.515: INFO: stdout: "update-demo-nautilus-dw6gk update-demo-nautilus-zgbvt "
Feb 22 09:44:15.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-dw6gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:15.810: INFO: stderr: ""
Feb 22 09:44:15.810: INFO: stdout: ""
Feb 22 09:44:15.810: INFO: update-demo-nautilus-dw6gk is created but not running
Feb 22 09:44:20.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:44:20.954: INFO: stderr: ""
Feb 22 09:44:20.954: INFO: stdout: "update-demo-nautilus-dw6gk update-demo-nautilus-zgbvt "
Feb 22 09:44:20.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-dw6gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:21.092: INFO: stderr: ""
Feb 22 09:44:21.092: INFO: stdout: "true"
Feb 22 09:44:21.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-dw6gk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 22 09:44:21.218: INFO: stderr: ""
Feb 22 09:44:21.218: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 22 09:44:21.218: INFO: validating pod update-demo-nautilus-dw6gk
Feb 22 09:44:21.305: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 09:44:21.307: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 09:44:21.308: INFO: update-demo-nautilus-dw6gk is verified up and running
Feb 22 09:44:21.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-zgbvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:21.451: INFO: stderr: ""
Feb 22 09:44:21.451: INFO: stdout: "true"
Feb 22 09:44:21.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-zgbvt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 22 09:44:21.630: INFO: stderr: ""
Feb 22 09:44:21.630: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 22 09:44:21.630: INFO: validating pod update-demo-nautilus-zgbvt
Feb 22 09:44:21.719: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 09:44:21.719: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 09:44:21.719: INFO: update-demo-nautilus-zgbvt is verified up and running
STEP: scaling down the replication controller
Feb 22 09:44:21.722: INFO: scanned /root for discovery docs: <nil>
Feb 22 09:44:21.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 22 09:44:22.954: INFO: stderr: ""
Feb 22 09:44:22.954: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 09:44:22.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:44:23.152: INFO: stderr: ""
Feb 22 09:44:23.152: INFO: stdout: "update-demo-nautilus-dw6gk update-demo-nautilus-zgbvt "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 22 09:44:28.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:44:28.325: INFO: stderr: ""
Feb 22 09:44:28.325: INFO: stdout: "update-demo-nautilus-zgbvt "
Feb 22 09:44:28.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-zgbvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:28.492: INFO: stderr: ""
Feb 22 09:44:28.492: INFO: stdout: "true"
Feb 22 09:44:28.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-zgbvt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 22 09:44:28.611: INFO: stderr: ""
Feb 22 09:44:28.611: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 22 09:44:28.611: INFO: validating pod update-demo-nautilus-zgbvt
Feb 22 09:44:28.619: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 09:44:28.619: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 09:44:28.619: INFO: update-demo-nautilus-zgbvt is verified up and running
STEP: scaling up the replication controller
Feb 22 09:44:28.622: INFO: scanned /root for discovery docs: <nil>
Feb 22 09:44:28.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 22 09:44:29.861: INFO: stderr: ""
Feb 22 09:44:29.861: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 09:44:29.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:44:30.097: INFO: stderr: ""
Feb 22 09:44:30.097: INFO: stdout: "update-demo-nautilus-ns6mb update-demo-nautilus-zgbvt "
Feb 22 09:44:30.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-ns6mb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:30.291: INFO: stderr: ""
Feb 22 09:44:30.291: INFO: stdout: ""
Feb 22 09:44:30.291: INFO: update-demo-nautilus-ns6mb is created but not running
Feb 22 09:44:35.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 09:44:35.435: INFO: stderr: ""
Feb 22 09:44:35.435: INFO: stdout: "update-demo-nautilus-ns6mb update-demo-nautilus-zgbvt "
Feb 22 09:44:35.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-ns6mb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:35.580: INFO: stderr: ""
Feb 22 09:44:35.580: INFO: stdout: "true"
Feb 22 09:44:35.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-ns6mb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 22 09:44:35.725: INFO: stderr: ""
Feb 22 09:44:35.725: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 22 09:44:35.725: INFO: validating pod update-demo-nautilus-ns6mb
Feb 22 09:44:35.812: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 09:44:35.812: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 09:44:35.812: INFO: update-demo-nautilus-ns6mb is verified up and running
Feb 22 09:44:35.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-zgbvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 09:44:35.949: INFO: stderr: ""
Feb 22 09:44:35.949: INFO: stdout: "true"
Feb 22 09:44:35.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods update-demo-nautilus-zgbvt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 22 09:44:36.089: INFO: stderr: ""
Feb 22 09:44:36.089: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 22 09:44:36.089: INFO: validating pod update-demo-nautilus-zgbvt
Feb 22 09:44:36.100: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 09:44:36.100: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 09:44:36.100: INFO: update-demo-nautilus-zgbvt is verified up and running
STEP: using delete to clean up resources
Feb 22 09:44:36.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 delete --grace-period=0 --force -f -'
Feb 22 09:44:36.332: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 09:44:36.332: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 22 09:44:36.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get rc,svc -l name=update-demo --no-headers'
Feb 22 09:44:36.563: INFO: stderr: "No resources found in kubectl-806 namespace.\n"
Feb 22 09:44:36.563: INFO: stdout: ""
Feb 22 09:44:36.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-806 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 09:44:36.776: INFO: stderr: ""
Feb 22 09:44:36.776: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:44:36.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-806" for this suite.

• [SLOW TEST:52.017 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":166,"skipped":2766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:44:36.809: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-e9cd6db0-c7c6-4e60-a5fd-271dd53c0501
STEP: Creating a pod to test consume secrets
Feb 22 09:44:37.120: INFO: Waiting up to 5m0s for pod "pod-secrets-06abdec0-360c-4ec9-ba98-c743b2bee255" in namespace "secrets-156" to be "Succeeded or Failed"
Feb 22 09:44:37.153: INFO: Pod "pod-secrets-06abdec0-360c-4ec9-ba98-c743b2bee255": Phase="Pending", Reason="", readiness=false. Elapsed: 33.803743ms
Feb 22 09:44:39.166: INFO: Pod "pod-secrets-06abdec0-360c-4ec9-ba98-c743b2bee255": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045953646s
Feb 22 09:44:41.176: INFO: Pod "pod-secrets-06abdec0-360c-4ec9-ba98-c743b2bee255": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056314957s
STEP: Saw pod success
Feb 22 09:44:41.176: INFO: Pod "pod-secrets-06abdec0-360c-4ec9-ba98-c743b2bee255" satisfied condition "Succeeded or Failed"
Feb 22 09:44:41.184: INFO: Trying to get logs from node node2 pod pod-secrets-06abdec0-360c-4ec9-ba98-c743b2bee255 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 09:44:41.267: INFO: Waiting for pod pod-secrets-06abdec0-360c-4ec9-ba98-c743b2bee255 to disappear
Feb 22 09:44:41.273: INFO: Pod pod-secrets-06abdec0-360c-4ec9-ba98-c743b2bee255 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:44:41.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-156" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":2797,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:44:41.296: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-23efa04b-cc44-4bf0-9a7c-3cba619fbcda
STEP: Creating a pod to test consume configMaps
Feb 22 09:44:41.504: INFO: Waiting up to 5m0s for pod "pod-configmaps-58a33fe5-b741-4634-9a21-4515daedac7f" in namespace "configmap-6434" to be "Succeeded or Failed"
Feb 22 09:44:41.530: INFO: Pod "pod-configmaps-58a33fe5-b741-4634-9a21-4515daedac7f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.04748ms
Feb 22 09:44:43.538: INFO: Pod "pod-configmaps-58a33fe5-b741-4634-9a21-4515daedac7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033525104s
Feb 22 09:44:45.545: INFO: Pod "pod-configmaps-58a33fe5-b741-4634-9a21-4515daedac7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040428731s
STEP: Saw pod success
Feb 22 09:44:45.545: INFO: Pod "pod-configmaps-58a33fe5-b741-4634-9a21-4515daedac7f" satisfied condition "Succeeded or Failed"
Feb 22 09:44:45.553: INFO: Trying to get logs from node node2 pod pod-configmaps-58a33fe5-b741-4634-9a21-4515daedac7f container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:44:45.610: INFO: Waiting for pod pod-configmaps-58a33fe5-b741-4634-9a21-4515daedac7f to disappear
Feb 22 09:44:45.617: INFO: Pod pod-configmaps-58a33fe5-b741-4634-9a21-4515daedac7f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:44:45.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6434" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":2797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:44:45.648: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-419ef998-ff4d-4a31-8109-05aff20adb76
STEP: Creating secret with name secret-projected-all-test-volume-caea3b4b-f9a2-4723-83bf-68384d8c0999
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 22 09:44:45.867: INFO: Waiting up to 5m0s for pod "projected-volume-9540fbae-8a31-436d-a208-e8344279cc24" in namespace "projected-3437" to be "Succeeded or Failed"
Feb 22 09:44:45.885: INFO: Pod "projected-volume-9540fbae-8a31-436d-a208-e8344279cc24": Phase="Pending", Reason="", readiness=false. Elapsed: 17.759118ms
Feb 22 09:44:47.905: INFO: Pod "projected-volume-9540fbae-8a31-436d-a208-e8344279cc24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037144287s
Feb 22 09:44:49.917: INFO: Pod "projected-volume-9540fbae-8a31-436d-a208-e8344279cc24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049998487s
STEP: Saw pod success
Feb 22 09:44:49.917: INFO: Pod "projected-volume-9540fbae-8a31-436d-a208-e8344279cc24" satisfied condition "Succeeded or Failed"
Feb 22 09:44:49.922: INFO: Trying to get logs from node node2 pod projected-volume-9540fbae-8a31-436d-a208-e8344279cc24 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 22 09:44:49.969: INFO: Waiting for pod projected-volume-9540fbae-8a31-436d-a208-e8344279cc24 to disappear
Feb 22 09:44:49.980: INFO: Pod projected-volume-9540fbae-8a31-436d-a208-e8344279cc24 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:44:49.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3437" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":169,"skipped":2824,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:44:50.023: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:44:50.162: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 22 09:44:54.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-2269 --namespace=crd-publish-openapi-2269 create -f -'
Feb 22 09:44:55.930: INFO: stderr: ""
Feb 22 09:44:55.930: INFO: stdout: "e2e-test-crd-publish-openapi-4610-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 22 09:44:55.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-2269 --namespace=crd-publish-openapi-2269 delete e2e-test-crd-publish-openapi-4610-crds test-cr'
Feb 22 09:44:56.177: INFO: stderr: ""
Feb 22 09:44:56.177: INFO: stdout: "e2e-test-crd-publish-openapi-4610-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 22 09:44:56.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-2269 --namespace=crd-publish-openapi-2269 apply -f -'
Feb 22 09:44:56.501: INFO: stderr: ""
Feb 22 09:44:56.501: INFO: stdout: "e2e-test-crd-publish-openapi-4610-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 22 09:44:56.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-2269 --namespace=crd-publish-openapi-2269 delete e2e-test-crd-publish-openapi-4610-crds test-cr'
Feb 22 09:44:56.674: INFO: stderr: ""
Feb 22 09:44:56.674: INFO: stdout: "e2e-test-crd-publish-openapi-4610-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 22 09:44:56.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-2269 explain e2e-test-crd-publish-openapi-4610-crds'
Feb 22 09:44:57.068: INFO: stderr: ""
Feb 22 09:44:57.068: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4610-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:45:00.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2269" for this suite.

• [SLOW TEST:10.632 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":170,"skipped":2832,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:45:00.656: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-t7hz
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 09:45:00.880: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-t7hz" in namespace "subpath-2987" to be "Succeeded or Failed"
Feb 22 09:45:00.898: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Pending", Reason="", readiness=false. Elapsed: 17.857617ms
Feb 22 09:45:02.906: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02562074s
Feb 22 09:45:04.917: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 4.035951551s
Feb 22 09:45:06.924: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 6.043859174s
Feb 22 09:45:08.938: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 8.057120671s
Feb 22 09:45:10.947: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 10.066210988s
Feb 22 09:45:12.968: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 12.08721755s
Feb 22 09:45:14.981: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 14.100255949s
Feb 22 09:45:16.997: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 16.11690663s
Feb 22 09:45:19.023: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 18.142194872s
Feb 22 09:45:21.032: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 20.151816887s
Feb 22 09:45:23.118: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Running", Reason="", readiness=true. Elapsed: 22.237008752s
Feb 22 09:45:25.145: INFO: Pod "pod-subpath-test-configmap-t7hz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.264541184s
STEP: Saw pod success
Feb 22 09:45:25.145: INFO: Pod "pod-subpath-test-configmap-t7hz" satisfied condition "Succeeded or Failed"
Feb 22 09:45:25.152: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-t7hz container test-container-subpath-configmap-t7hz: <nil>
STEP: delete the pod
Feb 22 09:45:25.246: INFO: Waiting for pod pod-subpath-test-configmap-t7hz to disappear
Feb 22 09:45:25.253: INFO: Pod pod-subpath-test-configmap-t7hz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-t7hz
Feb 22 09:45:25.253: INFO: Deleting pod "pod-subpath-test-configmap-t7hz" in namespace "subpath-2987"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:45:25.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2987" for this suite.

• [SLOW TEST:24.716 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":171,"skipped":2845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:45:25.373: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:45:27.065: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 22 09:45:29.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119927, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119927, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119927, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119927, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:45:32.232: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:45:32.245: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:45:35.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7745" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.621 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":172,"skipped":2890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:45:35.995: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:45:36.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8661" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":173,"skipped":2913,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:45:36.303: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Feb 22 09:45:36.610: INFO: Waiting up to 5m0s for pod "client-containers-b97d2d37-d2f2-45f0-881f-fde71da7540a" in namespace "containers-7240" to be "Succeeded or Failed"
Feb 22 09:45:36.642: INFO: Pod "client-containers-b97d2d37-d2f2-45f0-881f-fde71da7540a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.252351ms
Feb 22 09:45:38.660: INFO: Pod "client-containers-b97d2d37-d2f2-45f0-881f-fde71da7540a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04967363s
Feb 22 09:45:40.667: INFO: Pod "client-containers-b97d2d37-d2f2-45f0-881f-fde71da7540a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056440757s
STEP: Saw pod success
Feb 22 09:45:40.667: INFO: Pod "client-containers-b97d2d37-d2f2-45f0-881f-fde71da7540a" satisfied condition "Succeeded or Failed"
Feb 22 09:45:40.672: INFO: Trying to get logs from node node2 pod client-containers-b97d2d37-d2f2-45f0-881f-fde71da7540a container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:45:40.737: INFO: Waiting for pod client-containers-b97d2d37-d2f2-45f0-881f-fde71da7540a to disappear
Feb 22 09:45:40.742: INFO: Pod client-containers-b97d2d37-d2f2-45f0-881f-fde71da7540a no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:45:40.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7240" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":174,"skipped":2924,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:45:40.771: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:45:42.750: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 09:45:44.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119942, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119942, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119943, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119942, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 09:45:46.807: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119942, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119942, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119943, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781119942, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:45:49.833: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
Feb 22 09:45:49.887: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:45:50.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2921" for this suite.
STEP: Destroying namespace "webhook-2921-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":175,"skipped":2936,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:45:50.618: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 22 09:45:50.834: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 22 09:45:50.865: INFO: Waiting for terminating namespaces to be deleted...
Feb 22 09:45:50.900: INFO: 
Logging pods the apiserver thinks is on node node1 before test
Feb 22 09:45:50.975: INFO: calico-kube-controllers-5f67864b44-c472v from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 22 09:45:50.975: INFO: calico-node-nnc68 from kube-system started at 2022-02-22 06:03:22 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container calico-node ready: true, restart count 1
Feb 22 09:45:50.975: INFO: coredns-575c8f4bf-l92w8 from kube-system started at 2022-02-22 08:12:56 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container coredns ready: true, restart count 0
Feb 22 09:45:50.975: INFO: kube-proxy-gj7fq from kube-system started at 2022-02-22 08:47:06 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 09:45:50.975: INFO: node-exporter-dn6cm from kube-system started at 2022-02-22 06:03:24 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container prometheus-node-exporter ready: true, restart count 2
Feb 22 09:45:50.975: INFO: vpn-target-5c5b44787-wpr2h from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container vpn-target ready: true, restart count 0
Feb 22 09:45:50.975: INFO: sonobuoy from sonobuoy started at 2022-02-22 08:49:43 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 22 09:45:50.975: INFO: sonobuoy-e2e-job-71a20d3e324c433a from sonobuoy started at 2022-02-22 08:49:45 +0000 UTC (2 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container e2e ready: true, restart count 0
Feb 22 09:45:50.975: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 09:45:50.975: INFO: sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-whn5g from sonobuoy started at 2022-02-22 08:49:45 +0000 UTC (2 container statuses recorded)
Feb 22 09:45:50.975: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 09:45:50.975: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 22 09:45:50.975: INFO: 
Logging pods the apiserver thinks is on node node2 before test
Feb 22 09:45:50.998: INFO: calico-node-nrshb from kube-system started at 2022-02-22 06:03:23 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.999: INFO: 	Container calico-node ready: true, restart count 1
Feb 22 09:45:50.999: INFO: coredns-575c8f4bf-cxcvn from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.999: INFO: 	Container coredns ready: true, restart count 0
Feb 22 09:45:50.999: INFO: kube-proxy-65gc6 from kube-system started at 2022-02-22 08:13:14 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.999: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 09:45:50.999: INFO: metrics-server-6cb8f6bbb8-rqv7t from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.999: INFO: 	Container metrics-server ready: true, restart count 0
Feb 22 09:45:50.999: INFO: node-exporter-4t6rd from kube-system started at 2022-02-22 06:03:25 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.999: INFO: 	Container prometheus-node-exporter ready: true, restart count 1
Feb 22 09:45:50.999: INFO: pod-qos-class-4b5584ae-833b-4c06-b5fe-f694e6a57cf8 from pods-8661 started at 2022-02-22 09:45:36 +0000 UTC (1 container statuses recorded)
Feb 22 09:45:50.999: INFO: 	Container agnhost ready: false, restart count 0
Feb 22 09:45:50.999: INFO: sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-c4wkd from sonobuoy started at 2022-02-22 08:49:46 +0000 UTC (2 container statuses recorded)
Feb 22 09:45:50.999: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 09:45:50.999: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node node1
STEP: verifying the node has the label node node2
Feb 22 09:45:51.196: INFO: Pod calico-kube-controllers-5f67864b44-c472v requesting resource cpu=0m on Node node1
Feb 22 09:45:51.197: INFO: Pod calico-node-nnc68 requesting resource cpu=250m on Node node1
Feb 22 09:45:51.197: INFO: Pod calico-node-nrshb requesting resource cpu=250m on Node node2
Feb 22 09:45:51.197: INFO: Pod coredns-575c8f4bf-cxcvn requesting resource cpu=100m on Node node2
Feb 22 09:45:51.197: INFO: Pod coredns-575c8f4bf-l92w8 requesting resource cpu=100m on Node node1
Feb 22 09:45:51.197: INFO: Pod kube-proxy-65gc6 requesting resource cpu=0m on Node node2
Feb 22 09:45:51.197: INFO: Pod kube-proxy-gj7fq requesting resource cpu=0m on Node node1
Feb 22 09:45:51.197: INFO: Pod metrics-server-6cb8f6bbb8-rqv7t requesting resource cpu=100m on Node node2
Feb 22 09:45:51.197: INFO: Pod node-exporter-4t6rd requesting resource cpu=100m on Node node2
Feb 22 09:45:51.197: INFO: Pod node-exporter-dn6cm requesting resource cpu=100m on Node node1
Feb 22 09:45:51.197: INFO: Pod vpn-target-5c5b44787-wpr2h requesting resource cpu=100m on Node node1
Feb 22 09:45:51.197: INFO: Pod pod-qos-class-4b5584ae-833b-4c06-b5fe-f694e6a57cf8 requesting resource cpu=100m on Node node2
Feb 22 09:45:51.197: INFO: Pod sonobuoy requesting resource cpu=0m on Node node1
Feb 22 09:45:51.197: INFO: Pod sonobuoy-e2e-job-71a20d3e324c433a requesting resource cpu=0m on Node node1
Feb 22 09:45:51.197: INFO: Pod sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-c4wkd requesting resource cpu=0m on Node node2
Feb 22 09:45:51.197: INFO: Pod sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-whn5g requesting resource cpu=0m on Node node1
STEP: Starting Pods to consume most of the cluster CPU.
Feb 22 09:45:51.197: INFO: Creating a pod which consumes cpu=1015m on Node node1
Feb 22 09:45:51.236: INFO: Creating a pod which consumes cpu=945m on Node node2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c00854f5-48d4-439c-bdbe-c238ff7cbd03.16d61296769ccb26], Reason = [Scheduled], Message = [Successfully assigned sched-pred-815/filler-pod-c00854f5-48d4-439c-bdbe-c238ff7cbd03 to node1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c00854f5-48d4-439c-bdbe-c238ff7cbd03.16d61296f52c904d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c00854f5-48d4-439c-bdbe-c238ff7cbd03.16d61296fd2801b6], Reason = [Created], Message = [Created container filler-pod-c00854f5-48d4-439c-bdbe-c238ff7cbd03]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c00854f5-48d4-439c-bdbe-c238ff7cbd03.16d6129714dbd0f1], Reason = [Started], Message = [Started container filler-pod-c00854f5-48d4-439c-bdbe-c238ff7cbd03]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d2d3ade8-cc9a-43c3-ad99-abeaa4aa75ba.16d612967976bbcf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-815/filler-pod-d2d3ade8-cc9a-43c3-ad99-abeaa4aa75ba to node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d2d3ade8-cc9a-43c3-ad99-abeaa4aa75ba.16d61296fe8ddb37], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d2d3ade8-cc9a-43c3-ad99-abeaa4aa75ba.16d6129705878064], Reason = [Created], Message = [Created container filler-pod-d2d3ade8-cc9a-43c3-ad99-abeaa4aa75ba]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d2d3ade8-cc9a-43c3-ad99-abeaa4aa75ba.16d6129714b7c2e4], Reason = [Started], Message = [Started container filler-pod-d2d3ade8-cc9a-43c3-ad99-abeaa4aa75ba]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16d612976a25e53f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node node1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:45:56.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-815" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.880 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":176,"skipped":2957,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:45:56.499: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4684
Feb 22 09:45:56.642: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:45:58.656: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 22 09:45:58.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4684 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 22 09:45:59.345: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 22 09:45:59.345: INFO: stdout: "iptables"
Feb 22 09:45:59.345: INFO: proxyMode: iptables
Feb 22 09:45:59.378: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 22 09:45:59.397: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4684
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4684
I0222 09:45:59.501994      19 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4684, replica count: 3
I0222 09:46:02.653642      19 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:46:05.653975      19 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 09:46:05.693: INFO: Creating new exec pod
Feb 22 09:46:10.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4684 exec execpod-affinity54rdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Feb 22 09:46:11.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 22 09:46:11.395: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:46:11.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4684 exec execpod-affinity54rdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.50.132 80'
Feb 22 09:46:12.159: INFO: stderr: "+ + nc -v -t -w 2 172.20.50.132 80\necho hostName\nConnection to 172.20.50.132 80 port [tcp/http] succeeded!\n"
Feb 22 09:46:12.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:46:12.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4684 exec execpod-affinity54rdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 32743'
Feb 22 09:46:12.861: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 32743\nConnection to 172.28.128.12 32743 port [tcp/*] succeeded!\n"
Feb 22 09:46:12.861: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:46:12.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4684 exec execpod-affinity54rdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 32743'
Feb 22 09:46:13.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.13 32743\nConnection to 172.28.128.13 32743 port [tcp/*] succeeded!\n"
Feb 22 09:46:13.553: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:46:13.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4684 exec execpod-affinity54rdq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.28.128.12:32743/ ; done'
Feb 22 09:46:14.447: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n"
Feb 22 09:46:14.460: INFO: stdout: "\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq\naffinity-nodeport-timeout-whhxq"
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Received response from host: affinity-nodeport-timeout-whhxq
Feb 22 09:46:14.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4684 exec execpod-affinity54rdq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.28.128.12:32743/'
Feb 22 09:46:15.060: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n"
Feb 22 09:46:15.060: INFO: stdout: "affinity-nodeport-timeout-whhxq"
Feb 22 09:46:35.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-4684 exec execpod-affinity54rdq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.28.128.12:32743/'
Feb 22 09:46:35.655: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.28.128.12:32743/\n"
Feb 22 09:46:35.655: INFO: stdout: "affinity-nodeport-timeout-8d9kw"
Feb 22 09:46:35.655: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4684, will wait for the garbage collector to delete the pods
Feb 22 09:46:35.835: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 64.197403ms
Feb 22 09:46:36.038: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 203.125361ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:46:40.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4684" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:43.663 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":177,"skipped":2986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:46:40.163: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 22 09:46:40.337: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7092  98d86363-ce85-4741-b8af-7d8d5e66807a 53715 0 2022-02-22 09:46:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-22 09:46:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 09:46:40.338: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7092  98d86363-ce85-4741-b8af-7d8d5e66807a 53716 0 2022-02-22 09:46:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-22 09:46:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 09:46:40.338: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7092  98d86363-ce85-4741-b8af-7d8d5e66807a 53717 0 2022-02-22 09:46:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-22 09:46:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 22 09:46:50.478: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7092  98d86363-ce85-4741-b8af-7d8d5e66807a 53768 0 2022-02-22 09:46:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-22 09:46:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 09:46:50.478: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7092  98d86363-ce85-4741-b8af-7d8d5e66807a 53769 0 2022-02-22 09:46:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-22 09:46:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 09:46:50.479: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7092  98d86363-ce85-4741-b8af-7d8d5e66807a 53770 0 2022-02-22 09:46:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-22 09:46:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:46:50.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7092" for this suite.

• [SLOW TEST:10.351 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":178,"skipped":3012,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:46:50.515: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-399e95da-8224-488f-ae0a-75c3f51cb93e
STEP: Creating a pod to test consume configMaps
Feb 22 09:46:50.751: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a877290-afe0-431e-8bb8-585405d9e70b" in namespace "projected-8592" to be "Succeeded or Failed"
Feb 22 09:46:50.757: INFO: Pod "pod-projected-configmaps-3a877290-afe0-431e-8bb8-585405d9e70b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.279076ms
Feb 22 09:46:52.773: INFO: Pod "pod-projected-configmaps-3a877290-afe0-431e-8bb8-585405d9e70b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02153276s
Feb 22 09:46:54.786: INFO: Pod "pod-projected-configmaps-3a877290-afe0-431e-8bb8-585405d9e70b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034971957s
STEP: Saw pod success
Feb 22 09:46:54.786: INFO: Pod "pod-projected-configmaps-3a877290-afe0-431e-8bb8-585405d9e70b" satisfied condition "Succeeded or Failed"
Feb 22 09:46:54.794: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-3a877290-afe0-431e-8bb8-585405d9e70b container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:46:54.857: INFO: Waiting for pod pod-projected-configmaps-3a877290-afe0-431e-8bb8-585405d9e70b to disappear
Feb 22 09:46:54.863: INFO: Pod pod-projected-configmaps-3a877290-afe0-431e-8bb8-585405d9e70b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:46:54.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8592" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:46:54.895: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:46:55.019: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: creating replication controller svc-latency-rc in namespace svc-latency-743
I0222 09:46:55.062516      19 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-743, replica count: 1
I0222 09:46:56.114259      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:46:57.114915      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:46:58.115298      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:46:59.115631      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 09:46:59.253: INFO: Created: latency-svc-j9cxh
Feb 22 09:46:59.293: INFO: Got endpoints: latency-svc-j9cxh [76.670846ms]
Feb 22 09:46:59.340: INFO: Created: latency-svc-k7brq
Feb 22 09:46:59.440: INFO: Got endpoints: latency-svc-k7brq [146.797522ms]
Feb 22 09:46:59.460: INFO: Created: latency-svc-27fws
Feb 22 09:46:59.488: INFO: Created: latency-svc-jth8w
Feb 22 09:46:59.494: INFO: Got endpoints: latency-svc-27fws [199.361179ms]
Feb 22 09:46:59.526: INFO: Got endpoints: latency-svc-jth8w [231.335131ms]
Feb 22 09:46:59.637: INFO: Created: latency-svc-j5lgd
Feb 22 09:46:59.638: INFO: Created: latency-svc-v6jkm
Feb 22 09:46:59.735: INFO: Created: latency-svc-twpsl
Feb 22 09:46:59.736: INFO: Got endpoints: latency-svc-j5lgd [440.268566ms]
Feb 22 09:46:59.737: INFO: Got endpoints: latency-svc-v6jkm [441.304261ms]
Feb 22 09:46:59.790: INFO: Got endpoints: latency-svc-twpsl [492.294526ms]
Feb 22 09:46:59.838: INFO: Created: latency-svc-xdx56
Feb 22 09:46:59.867: INFO: Got endpoints: latency-svc-xdx56 [570.177365ms]
Feb 22 09:46:59.912: INFO: Created: latency-svc-68f58
Feb 22 09:46:59.952: INFO: Created: latency-svc-mq222
Feb 22 09:47:00.028: INFO: Got endpoints: latency-svc-68f58 [731.412021ms]
Feb 22 09:47:00.049: INFO: Got endpoints: latency-svc-mq222 [752.707323ms]
Feb 22 09:47:00.079: INFO: Created: latency-svc-69j6l
Feb 22 09:47:00.183: INFO: Got endpoints: latency-svc-69j6l [886.575404ms]
Feb 22 09:47:00.200: INFO: Created: latency-svc-pjqlx
Feb 22 09:47:00.229: INFO: Got endpoints: latency-svc-pjqlx [933.518087ms]
Feb 22 09:47:00.351: INFO: Created: latency-svc-84zgf
Feb 22 09:47:00.370: INFO: Created: latency-svc-x4ljm
Feb 22 09:47:00.475: INFO: Got endpoints: latency-svc-x4ljm [1.178925053s]
Feb 22 09:47:00.476: INFO: Got endpoints: latency-svc-84zgf [1.178587854s]
Feb 22 09:47:00.531: INFO: Created: latency-svc-sjvk9
Feb 22 09:47:00.575: INFO: Got endpoints: latency-svc-sjvk9 [1.277354098s]
Feb 22 09:47:00.638: INFO: Created: latency-svc-77vgp
Feb 22 09:47:00.676: INFO: Got endpoints: latency-svc-77vgp [1.378242031s]
Feb 22 09:47:00.686: INFO: Created: latency-svc-xjf6t
Feb 22 09:47:00.706: INFO: Got endpoints: latency-svc-xjf6t [1.265173254s]
Feb 22 09:47:00.784: INFO: Created: latency-svc-fvqmp
Feb 22 09:47:00.838: INFO: Created: latency-svc-g6f7x
Feb 22 09:47:00.849: INFO: Got endpoints: latency-svc-fvqmp [1.35484054s]
Feb 22 09:47:00.908: INFO: Got endpoints: latency-svc-g6f7x [1.381807316s]
Feb 22 09:47:00.927: INFO: Created: latency-svc-98kxb
Feb 22 09:47:00.951: INFO: Got endpoints: latency-svc-98kxb [1.214873287s]
Feb 22 09:47:00.993: INFO: Created: latency-svc-rcxkr
Feb 22 09:47:01.076: INFO: Got endpoints: latency-svc-rcxkr [1.339305511s]
Feb 22 09:47:01.134: INFO: Created: latency-svc-wgxxg
Feb 22 09:47:01.184: INFO: Got endpoints: latency-svc-wgxxg [1.393495661s]
Feb 22 09:47:01.209: INFO: Created: latency-svc-bfc65
Feb 22 09:47:01.269: INFO: Got endpoints: latency-svc-bfc65 [1.402003722s]
Feb 22 09:47:01.276: INFO: Created: latency-svc-msh4p
Feb 22 09:47:01.335: INFO: Got endpoints: latency-svc-msh4p [1.307279359s]
Feb 22 09:47:01.392: INFO: Created: latency-svc-gn7dz
Feb 22 09:47:01.509: INFO: Got endpoints: latency-svc-gn7dz [1.460226353s]
Feb 22 09:47:01.553: INFO: Created: latency-svc-bzmks
Feb 22 09:47:01.577: INFO: Got endpoints: latency-svc-bzmks [1.394114658s]
Feb 22 09:47:01.601: INFO: Created: latency-svc-4l5rj
Feb 22 09:47:01.673: INFO: Got endpoints: latency-svc-4l5rj [1.44354913s]
Feb 22 09:47:01.693: INFO: Created: latency-svc-rqf45
Feb 22 09:47:01.731: INFO: Got endpoints: latency-svc-rqf45 [1.255518898s]
Feb 22 09:47:01.740: INFO: Created: latency-svc-f9628
Feb 22 09:47:01.781: INFO: Got endpoints: latency-svc-f9628 [1.305641968s]
Feb 22 09:47:01.804: INFO: Created: latency-svc-d9rqx
Feb 22 09:47:01.828: INFO: Got endpoints: latency-svc-d9rqx [1.252876911s]
Feb 22 09:47:01.843: INFO: Created: latency-svc-89v77
Feb 22 09:47:01.898: INFO: Created: latency-svc-xn8v2
Feb 22 09:47:01.902: INFO: Got endpoints: latency-svc-89v77 [1.226203734s]
Feb 22 09:47:01.945: INFO: Got endpoints: latency-svc-xn8v2 [1.239610273s]
Feb 22 09:47:01.965: INFO: Created: latency-svc-fnwrq
Feb 22 09:47:01.983: INFO: Got endpoints: latency-svc-fnwrq [1.13422556s]
Feb 22 09:47:02.022: INFO: Created: latency-svc-j4rdv
Feb 22 09:47:02.065: INFO: Got endpoints: latency-svc-j4rdv [1.156387857s]
Feb 22 09:47:02.080: INFO: Created: latency-svc-h999c
Feb 22 09:47:02.111: INFO: Created: latency-svc-zd2pn
Feb 22 09:47:02.125: INFO: Got endpoints: latency-svc-h999c [1.174579373s]
Feb 22 09:47:02.190: INFO: Got endpoints: latency-svc-zd2pn [1.113526355s]
Feb 22 09:47:02.217: INFO: Created: latency-svc-6knwz
Feb 22 09:47:02.257: INFO: Got endpoints: latency-svc-6knwz [1.07331934s]
Feb 22 09:47:02.274: INFO: Created: latency-svc-xpjf6
Feb 22 09:47:02.311: INFO: Got endpoints: latency-svc-xpjf6 [1.041846686s]
Feb 22 09:47:02.367: INFO: Created: latency-svc-vffmb
Feb 22 09:47:02.401: INFO: Got endpoints: latency-svc-vffmb [1.065727076s]
Feb 22 09:47:02.424: INFO: Created: latency-svc-rh299
Feb 22 09:47:02.483: INFO: Got endpoints: latency-svc-rh299 [973.9124ms]
Feb 22 09:47:02.487: INFO: Created: latency-svc-ps5x5
Feb 22 09:47:02.547: INFO: Got endpoints: latency-svc-ps5x5 [969.748819ms]
Feb 22 09:47:02.563: INFO: Created: latency-svc-gqvlx
Feb 22 09:47:02.642: INFO: Got endpoints: latency-svc-gqvlx [969.52622ms]
Feb 22 09:47:02.671: INFO: Created: latency-svc-r58lf
Feb 22 09:47:02.740: INFO: Got endpoints: latency-svc-r58lf [1.009522135s]
Feb 22 09:47:02.803: INFO: Created: latency-svc-hpwzr
Feb 22 09:47:02.862: INFO: Created: latency-svc-qw6gv
Feb 22 09:47:02.865: INFO: Got endpoints: latency-svc-hpwzr [1.083084896s]
Feb 22 09:47:02.960: INFO: Got endpoints: latency-svc-qw6gv [1.132324968s]
Feb 22 09:47:03.008: INFO: Created: latency-svc-vsxdw
Feb 22 09:47:03.043: INFO: Got endpoints: latency-svc-vsxdw [1.140817429s]
Feb 22 09:47:03.077: INFO: Created: latency-svc-62zjz
Feb 22 09:47:03.152: INFO: Got endpoints: latency-svc-62zjz [1.206933123s]
Feb 22 09:47:03.225: INFO: Created: latency-svc-r28l6
Feb 22 09:47:03.243: INFO: Got endpoints: latency-svc-r28l6 [1.260419976s]
Feb 22 09:47:03.270: INFO: Created: latency-svc-rjrvw
Feb 22 09:47:03.291: INFO: Got endpoints: latency-svc-rjrvw [1.226174134s]
Feb 22 09:47:03.319: INFO: Created: latency-svc-tk89x
Feb 22 09:47:03.362: INFO: Got endpoints: latency-svc-tk89x [1.236429387s]
Feb 22 09:47:03.401: INFO: Created: latency-svc-vszrv
Feb 22 09:47:03.433: INFO: Got endpoints: latency-svc-vszrv [1.243432255s]
Feb 22 09:47:03.448: INFO: Created: latency-svc-g5vwz
Feb 22 09:47:03.497: INFO: Got endpoints: latency-svc-g5vwz [1.240386669s]
Feb 22 09:47:03.530: INFO: Created: latency-svc-kkcgr
Feb 22 09:47:03.557: INFO: Created: latency-svc-xbx6s
Feb 22 09:47:03.559: INFO: Got endpoints: latency-svc-kkcgr [1.24878313s]
Feb 22 09:47:03.606: INFO: Created: latency-svc-wmnt6
Feb 22 09:47:03.611: INFO: Got endpoints: latency-svc-xbx6s [1.209979709s]
Feb 22 09:47:03.684: INFO: Created: latency-svc-v6hdg
Feb 22 09:47:03.700: INFO: Got endpoints: latency-svc-wmnt6 [1.217565174s]
Feb 22 09:47:03.719: INFO: Got endpoints: latency-svc-v6hdg [1.172055084s]
Feb 22 09:47:03.755: INFO: Created: latency-svc-4wcjt
Feb 22 09:47:03.845: INFO: Got endpoints: latency-svc-4wcjt [1.202956841s]
Feb 22 09:47:03.853: INFO: Created: latency-svc-95l6f
Feb 22 09:47:03.993: INFO: Got endpoints: latency-svc-95l6f [1.252427313s]
Feb 22 09:47:04.013: INFO: Created: latency-svc-p44c5
Feb 22 09:47:04.052: INFO: Created: latency-svc-zjqq9
Feb 22 09:47:04.085: INFO: Got endpoints: latency-svc-p44c5 [1.220167362s]
Feb 22 09:47:04.118: INFO: Got endpoints: latency-svc-zjqq9 [1.157297553s]
Feb 22 09:47:04.133: INFO: Created: latency-svc-ktv2p
Feb 22 09:47:04.156: INFO: Got endpoints: latency-svc-ktv2p [1.112944557s]
Feb 22 09:47:04.183: INFO: Created: latency-svc-4f6vc
Feb 22 09:47:04.201: INFO: Got endpoints: latency-svc-4f6vc [1.048803454s]
Feb 22 09:47:04.269: INFO: Created: latency-svc-8s9jg
Feb 22 09:47:04.316: INFO: Got endpoints: latency-svc-8s9jg [1.072608444s]
Feb 22 09:47:04.337: INFO: Created: latency-svc-rr27b
Feb 22 09:47:04.394: INFO: Got endpoints: latency-svc-rr27b [1.103512901s]
Feb 22 09:47:04.427: INFO: Created: latency-svc-gdfwn
Feb 22 09:47:04.444: INFO: Got endpoints: latency-svc-gdfwn [1.082228799s]
Feb 22 09:47:04.486: INFO: Created: latency-svc-44qvs
Feb 22 09:47:04.520: INFO: Got endpoints: latency-svc-44qvs [1.087015478s]
Feb 22 09:47:04.543: INFO: Created: latency-svc-cxj4b
Feb 22 09:47:04.573: INFO: Got endpoints: latency-svc-cxj4b [1.075050633s]
Feb 22 09:47:04.608: INFO: Created: latency-svc-bbfjj
Feb 22 09:47:04.660: INFO: Got endpoints: latency-svc-bbfjj [1.100829213s]
Feb 22 09:47:04.682: INFO: Created: latency-svc-gv8tb
Feb 22 09:47:04.707: INFO: Got endpoints: latency-svc-gv8tb [1.096150336s]
Feb 22 09:47:04.725: INFO: Created: latency-svc-zrcdl
Feb 22 09:47:04.747: INFO: Got endpoints: latency-svc-zrcdl [1.046350266s]
Feb 22 09:47:04.767: INFO: Created: latency-svc-rkzcq
Feb 22 09:47:04.800: INFO: Got endpoints: latency-svc-rkzcq [1.080633306s]
Feb 22 09:47:04.832: INFO: Created: latency-svc-5nl7k
Feb 22 09:47:04.844: INFO: Got endpoints: latency-svc-5nl7k [998.327087ms]
Feb 22 09:47:04.876: INFO: Created: latency-svc-rpcfp
Feb 22 09:47:04.924: INFO: Got endpoints: latency-svc-rpcfp [930.796799ms]
Feb 22 09:47:04.948: INFO: Created: latency-svc-sj77d
Feb 22 09:47:04.961: INFO: Created: latency-svc-k28wc
Feb 22 09:47:04.964: INFO: Got endpoints: latency-svc-sj77d [879.610636ms]
Feb 22 09:47:04.995: INFO: Got endpoints: latency-svc-k28wc [877.082447ms]
Feb 22 09:47:05.036: INFO: Created: latency-svc-6qkn6
Feb 22 09:47:05.087: INFO: Got endpoints: latency-svc-6qkn6 [931.013698ms]
Feb 22 09:47:05.103: INFO: Created: latency-svc-dtzdq
Feb 22 09:47:05.153: INFO: Got endpoints: latency-svc-dtzdq [951.158505ms]
Feb 22 09:47:05.215: INFO: Created: latency-svc-d9qvm
Feb 22 09:47:05.257: INFO: Got endpoints: latency-svc-d9qvm [940.845153ms]
Feb 22 09:47:05.277: INFO: Created: latency-svc-slfnl
Feb 22 09:47:05.388: INFO: Got endpoints: latency-svc-slfnl [993.412909ms]
Feb 22 09:47:05.454: INFO: Created: latency-svc-8tk8q
Feb 22 09:47:05.508: INFO: Created: latency-svc-hzn8v
Feb 22 09:47:05.517: INFO: Got endpoints: latency-svc-8tk8q [1.072405245s]
Feb 22 09:47:05.535: INFO: Got endpoints: latency-svc-hzn8v [1.01512451s]
Feb 22 09:47:05.583: INFO: Created: latency-svc-5vtgl
Feb 22 09:47:05.606: INFO: Got endpoints: latency-svc-5vtgl [1.033341525s]
Feb 22 09:47:05.647: INFO: Created: latency-svc-9dsw4
Feb 22 09:47:05.687: INFO: Got endpoints: latency-svc-9dsw4 [1.026254558s]
Feb 22 09:47:05.693: INFO: Created: latency-svc-v69mn
Feb 22 09:47:05.718: INFO: Got endpoints: latency-svc-v69mn [1.009948933s]
Feb 22 09:47:05.732: INFO: Created: latency-svc-wwxdf
Feb 22 09:47:05.780: INFO: Got endpoints: latency-svc-wwxdf [1.032827728s]
Feb 22 09:47:05.788: INFO: Created: latency-svc-bg6cp
Feb 22 09:47:05.815: INFO: Got endpoints: latency-svc-bg6cp [1.015492508s]
Feb 22 09:47:05.835: INFO: Created: latency-svc-6zcjh
Feb 22 09:47:05.871: INFO: Created: latency-svc-hkg92
Feb 22 09:47:05.874: INFO: Got endpoints: latency-svc-6zcjh [1.030117541s]
Feb 22 09:47:05.921: INFO: Got endpoints: latency-svc-hkg92 [997.356191ms]
Feb 22 09:47:05.946: INFO: Created: latency-svc-l854z
Feb 22 09:47:05.965: INFO: Got endpoints: latency-svc-l854z [1.000370778s]
Feb 22 09:47:05.981: INFO: Created: latency-svc-t2nzq
Feb 22 09:47:06.042: INFO: Got endpoints: latency-svc-t2nzq [1.047179761s]
Feb 22 09:47:06.060: INFO: Created: latency-svc-6xxkv
Feb 22 09:47:06.083: INFO: Got endpoints: latency-svc-6xxkv [996.053198ms]
Feb 22 09:47:06.109: INFO: Created: latency-svc-97fzm
Feb 22 09:47:06.137: INFO: Got endpoints: latency-svc-97fzm [984.203453ms]
Feb 22 09:47:06.156: INFO: Created: latency-svc-cdhbc
Feb 22 09:47:06.186: INFO: Created: latency-svc-vzgfm
Feb 22 09:47:06.205: INFO: Got endpoints: latency-svc-cdhbc [947.435623ms]
Feb 22 09:47:06.228: INFO: Got endpoints: latency-svc-vzgfm [839.56562ms]
Feb 22 09:47:06.238: INFO: Created: latency-svc-sfpxs
Feb 22 09:47:06.285: INFO: Created: latency-svc-x6qqp
Feb 22 09:47:06.312: INFO: Got endpoints: latency-svc-sfpxs [795.157226ms]
Feb 22 09:47:06.349: INFO: Created: latency-svc-62bdg
Feb 22 09:47:06.353: INFO: Got endpoints: latency-svc-x6qqp [817.041724ms]
Feb 22 09:47:06.619: INFO: Got endpoints: latency-svc-62bdg [1.013113119s]
Feb 22 09:47:06.700: INFO: Created: latency-svc-dhngt
Feb 22 09:47:06.769: INFO: Got endpoints: latency-svc-dhngt [1.0820182s]
Feb 22 09:47:06.810: INFO: Created: latency-svc-q9f5t
Feb 22 09:47:06.852: INFO: Got endpoints: latency-svc-q9f5t [1.134567558s]
Feb 22 09:47:06.860: INFO: Created: latency-svc-q66x7
Feb 22 09:47:06.893: INFO: Got endpoints: latency-svc-q66x7 [1.112609359s]
Feb 22 09:47:06.934: INFO: Created: latency-svc-p96nw
Feb 22 09:47:06.959: INFO: Created: latency-svc-qfhb8
Feb 22 09:47:07.013: INFO: Got endpoints: latency-svc-p96nw [1.197340767s]
Feb 22 09:47:07.029: INFO: Got endpoints: latency-svc-qfhb8 [1.154399366s]
Feb 22 09:47:07.038: INFO: Created: latency-svc-2p24b
Feb 22 09:47:07.071: INFO: Got endpoints: latency-svc-2p24b [1.149629388s]
Feb 22 09:47:07.106: INFO: Created: latency-svc-6ssc2
Feb 22 09:47:07.151: INFO: Got endpoints: latency-svc-6ssc2 [1.186389619s]
Feb 22 09:47:07.172: INFO: Created: latency-svc-mfwk5
Feb 22 09:47:07.200: INFO: Got endpoints: latency-svc-mfwk5 [1.158120149s]
Feb 22 09:47:07.233: INFO: Created: latency-svc-b9wg5
Feb 22 09:47:07.268: INFO: Got endpoints: latency-svc-b9wg5 [1.185130124s]
Feb 22 09:47:07.335: INFO: Created: latency-svc-fccqs
Feb 22 09:47:07.392: INFO: Got endpoints: latency-svc-fccqs [1.255496398s]
Feb 22 09:47:07.410: INFO: Created: latency-svc-hqg9q
Feb 22 09:47:07.446: INFO: Got endpoints: latency-svc-hqg9q [1.241487564s]
Feb 22 09:47:07.489: INFO: Created: latency-svc-wkhqt
Feb 22 09:47:07.544: INFO: Got endpoints: latency-svc-wkhqt [1.31593162s]
Feb 22 09:47:07.636: INFO: Created: latency-svc-gnvf2
Feb 22 09:47:07.731: INFO: Created: latency-svc-rtwdd
Feb 22 09:47:07.747: INFO: Got endpoints: latency-svc-gnvf2 [1.435626467s]
Feb 22 09:47:07.907: INFO: Got endpoints: latency-svc-rtwdd [1.55402022s]
Feb 22 09:47:07.908: INFO: Created: latency-svc-vvffz
Feb 22 09:47:07.976: INFO: Got endpoints: latency-svc-vvffz [1.356695631s]
Feb 22 09:47:08.013: INFO: Created: latency-svc-wb2p8
Feb 22 09:47:08.058: INFO: Got endpoints: latency-svc-wb2p8 [1.288988344s]
Feb 22 09:47:08.059: INFO: Created: latency-svc-7h978
Feb 22 09:47:08.085: INFO: Got endpoints: latency-svc-7h978 [1.232341506s]
Feb 22 09:47:08.150: INFO: Created: latency-svc-6xlvm
Feb 22 09:47:08.189: INFO: Got endpoints: latency-svc-6xlvm [1.296022011s]
Feb 22 09:47:08.199: INFO: Created: latency-svc-lgrbj
Feb 22 09:47:08.225: INFO: Got endpoints: latency-svc-lgrbj [1.212446698s]
Feb 22 09:47:08.246: INFO: Created: latency-svc-qtphp
Feb 22 09:47:08.281: INFO: Got endpoints: latency-svc-qtphp [1.252137515s]
Feb 22 09:47:08.313: INFO: Created: latency-svc-tjg8x
Feb 22 09:47:08.360: INFO: Got endpoints: latency-svc-tjg8x [1.288565546s]
Feb 22 09:47:08.377: INFO: Created: latency-svc-tpws9
Feb 22 09:47:08.419: INFO: Got endpoints: latency-svc-tpws9 [1.267214245s]
Feb 22 09:47:08.440: INFO: Created: latency-svc-c62t5
Feb 22 09:47:08.462: INFO: Got endpoints: latency-svc-c62t5 [1.261480172s]
Feb 22 09:47:08.497: INFO: Created: latency-svc-bgrkv
Feb 22 09:47:08.581: INFO: Got endpoints: latency-svc-bgrkv [1.312978734s]
Feb 22 09:47:08.624: INFO: Created: latency-svc-c8gw2
Feb 22 09:47:08.660: INFO: Got endpoints: latency-svc-c8gw2 [1.267880541s]
Feb 22 09:47:08.687: INFO: Created: latency-svc-6vqbf
Feb 22 09:47:08.743: INFO: Got endpoints: latency-svc-6vqbf [1.296755808s]
Feb 22 09:47:08.762: INFO: Created: latency-svc-c6dm8
Feb 22 09:47:08.849: INFO: Got endpoints: latency-svc-c6dm8 [1.304875071s]
Feb 22 09:47:08.889: INFO: Created: latency-svc-krb27
Feb 22 09:47:08.943: INFO: Got endpoints: latency-svc-krb27 [1.195034479s]
Feb 22 09:47:08.962: INFO: Created: latency-svc-ww6hh
Feb 22 09:47:09.011: INFO: Got endpoints: latency-svc-ww6hh [1.104531997s]
Feb 22 09:47:09.087: INFO: Created: latency-svc-2c25h
Feb 22 09:47:09.186: INFO: Got endpoints: latency-svc-2c25h [1.209894509s]
Feb 22 09:47:09.227: INFO: Created: latency-svc-gjkcs
Feb 22 09:47:09.285: INFO: Got endpoints: latency-svc-gjkcs [1.227230629s]
Feb 22 09:47:09.344: INFO: Created: latency-svc-sk8pk
Feb 22 09:47:09.376: INFO: Got endpoints: latency-svc-sk8pk [1.291018134s]
Feb 22 09:47:09.402: INFO: Created: latency-svc-svbpb
Feb 22 09:47:09.464: INFO: Got endpoints: latency-svc-svbpb [1.275039808s]
Feb 22 09:47:09.473: INFO: Created: latency-svc-fl9n9
Feb 22 09:47:09.493: INFO: Got endpoints: latency-svc-fl9n9 [1.267433243s]
Feb 22 09:47:09.536: INFO: Created: latency-svc-spfvl
Feb 22 09:47:09.576: INFO: Got endpoints: latency-svc-spfvl [1.295459114s]
Feb 22 09:47:09.607: INFO: Created: latency-svc-nqz9k
Feb 22 09:47:09.636: INFO: Got endpoints: latency-svc-nqz9k [1.276550702s]
Feb 22 09:47:09.706: INFO: Created: latency-svc-rr9t4
Feb 22 09:47:09.725: INFO: Created: latency-svc-f2xl8
Feb 22 09:47:09.748: INFO: Got endpoints: latency-svc-rr9t4 [1.329383457s]
Feb 22 09:47:09.762: INFO: Got endpoints: latency-svc-f2xl8 [1.300124793s]
Feb 22 09:47:09.835: INFO: Created: latency-svc-lp9tq
Feb 22 09:47:09.853: INFO: Created: latency-svc-q8ctf
Feb 22 09:47:09.874: INFO: Got endpoints: latency-svc-lp9tq [1.29223223s]
Feb 22 09:47:09.886: INFO: Got endpoints: latency-svc-q8ctf [1.225931436s]
Feb 22 09:47:09.909: INFO: Created: latency-svc-x46pm
Feb 22 09:47:09.959: INFO: Created: latency-svc-cpnsx
Feb 22 09:47:09.967: INFO: Got endpoints: latency-svc-x46pm [1.224089144s]
Feb 22 09:47:09.999: INFO: Got endpoints: latency-svc-cpnsx [1.149571488s]
Feb 22 09:47:10.028: INFO: Created: latency-svc-qrs6j
Feb 22 09:47:10.062: INFO: Got endpoints: latency-svc-qrs6j [1.118868331s]
Feb 22 09:47:10.086: INFO: Created: latency-svc-bqw45
Feb 22 09:47:10.121: INFO: Created: latency-svc-m8hfm
Feb 22 09:47:10.121: INFO: Got endpoints: latency-svc-bqw45 [1.110091571s]
Feb 22 09:47:10.152: INFO: Got endpoints: latency-svc-m8hfm [966.355435ms]
Feb 22 09:47:10.199: INFO: Created: latency-svc-gl2m2
Feb 22 09:47:10.212: INFO: Created: latency-svc-4dnk2
Feb 22 09:47:10.231: INFO: Got endpoints: latency-svc-gl2m2 [946.043529ms]
Feb 22 09:47:10.265: INFO: Got endpoints: latency-svc-4dnk2 [888.870393ms]
Feb 22 09:47:10.268: INFO: Created: latency-svc-cwd67
Feb 22 09:47:10.474: INFO: Got endpoints: latency-svc-cwd67 [1.010526731s]
Feb 22 09:47:10.488: INFO: Created: latency-svc-n8kh8
Feb 22 09:47:10.567: INFO: Got endpoints: latency-svc-n8kh8 [1.073993537s]
Feb 22 09:47:10.628: INFO: Created: latency-svc-p7pp5
Feb 22 09:47:10.673: INFO: Got endpoints: latency-svc-p7pp5 [1.096142635s]
Feb 22 09:47:10.738: INFO: Created: latency-svc-75549
Feb 22 09:47:10.794: INFO: Created: latency-svc-b6zkf
Feb 22 09:47:10.802: INFO: Got endpoints: latency-svc-75549 [1.165959613s]
Feb 22 09:47:10.855: INFO: Got endpoints: latency-svc-b6zkf [1.107097784s]
Feb 22 09:47:10.874: INFO: Created: latency-svc-kgrt8
Feb 22 09:47:10.914: INFO: Created: latency-svc-97r76
Feb 22 09:47:10.984: INFO: Got endpoints: latency-svc-kgrt8 [1.221349356s]
Feb 22 09:47:11.010: INFO: Got endpoints: latency-svc-97r76 [1.136025551s]
Feb 22 09:47:11.032: INFO: Created: latency-svc-vzzrl
Feb 22 09:47:11.069: INFO: Got endpoints: latency-svc-vzzrl [1.182830435s]
Feb 22 09:47:11.141: INFO: Created: latency-svc-v7r2g
Feb 22 09:47:11.174: INFO: Created: latency-svc-cjgb8
Feb 22 09:47:11.199: INFO: Got endpoints: latency-svc-v7r2g [1.231597109s]
Feb 22 09:47:11.203: INFO: Got endpoints: latency-svc-cjgb8 [1.204490535s]
Feb 22 09:47:11.260: INFO: Created: latency-svc-cxzkb
Feb 22 09:47:11.315: INFO: Created: latency-svc-ln2np
Feb 22 09:47:11.339: INFO: Got endpoints: latency-svc-cxzkb [1.277569997s]
Feb 22 09:47:11.402: INFO: Got endpoints: latency-svc-ln2np [1.280142585s]
Feb 22 09:47:11.454: INFO: Created: latency-svc-5s7rb
Feb 22 09:47:11.754: INFO: Got endpoints: latency-svc-5s7rb [1.601418501s]
Feb 22 09:47:11.847: INFO: Created: latency-svc-qhclh
Feb 22 09:47:11.967: INFO: Got endpoints: latency-svc-qhclh [1.735479781s]
Feb 22 09:47:11.986: INFO: Created: latency-svc-scx2z
Feb 22 09:47:12.039: INFO: Got endpoints: latency-svc-scx2z [1.773882003s]
Feb 22 09:47:12.056: INFO: Created: latency-svc-4v549
Feb 22 09:47:12.085: INFO: Got endpoints: latency-svc-4v549 [1.610042561s]
Feb 22 09:47:12.140: INFO: Created: latency-svc-56gfm
Feb 22 09:47:12.176: INFO: Got endpoints: latency-svc-56gfm [1.608776167s]
Feb 22 09:47:12.177: INFO: Created: latency-svc-c98x8
Feb 22 09:47:12.193: INFO: Got endpoints: latency-svc-c98x8 [1.520437574s]
Feb 22 09:47:12.232: INFO: Created: latency-svc-m29hr
Feb 22 09:47:12.262: INFO: Created: latency-svc-8whmg
Feb 22 09:47:12.284: INFO: Got endpoints: latency-svc-m29hr [1.481029057s]
Feb 22 09:47:12.345: INFO: Got endpoints: latency-svc-8whmg [1.489840616s]
Feb 22 09:47:12.383: INFO: Created: latency-svc-m9hwn
Feb 22 09:47:12.416: INFO: Got endpoints: latency-svc-m9hwn [1.431889884s]
Feb 22 09:47:12.421: INFO: Created: latency-svc-5mt5b
Feb 22 09:47:12.454: INFO: Got endpoints: latency-svc-5mt5b [1.444376426s]
Feb 22 09:47:12.488: INFO: Created: latency-svc-l7gbm
Feb 22 09:47:12.534: INFO: Got endpoints: latency-svc-l7gbm [1.464142234s]
Feb 22 09:47:12.565: INFO: Created: latency-svc-6b87w
Feb 22 09:47:12.615: INFO: Got endpoints: latency-svc-6b87w [1.416018357s]
Feb 22 09:47:12.646: INFO: Created: latency-svc-fsg7h
Feb 22 09:47:12.680: INFO: Got endpoints: latency-svc-fsg7h [1.476566778s]
Feb 22 09:47:12.696: INFO: Created: latency-svc-vbq57
Feb 22 09:47:12.847: INFO: Got endpoints: latency-svc-vbq57 [1.508186431s]
Feb 22 09:47:12.852: INFO: Created: latency-svc-v9gkk
Feb 22 09:47:12.918: INFO: Got endpoints: latency-svc-v9gkk [1.516638692s]
Feb 22 09:47:13.212: INFO: Created: latency-svc-6vgrq
Feb 22 09:47:13.247: INFO: Got endpoints: latency-svc-6vgrq [1.492690903s]
Feb 22 09:47:13.312: INFO: Created: latency-svc-qkgfx
Feb 22 09:47:13.376: INFO: Got endpoints: latency-svc-qkgfx [1.40886709s]
Feb 22 09:47:13.445: INFO: Created: latency-svc-wprsr
Feb 22 09:47:13.501: INFO: Created: latency-svc-cxc5c
Feb 22 09:47:13.524: INFO: Got endpoints: latency-svc-wprsr [1.485163137s]
Feb 22 09:47:13.543: INFO: Got endpoints: latency-svc-cxc5c [1.458776659s]
Feb 22 09:47:13.607: INFO: Created: latency-svc-l7zsv
Feb 22 09:47:13.673: INFO: Got endpoints: latency-svc-l7zsv [1.496896184s]
Feb 22 09:47:13.677: INFO: Created: latency-svc-k2trd
Feb 22 09:47:13.723: INFO: Got endpoints: latency-svc-k2trd [1.53002163s]
Feb 22 09:47:13.741: INFO: Created: latency-svc-rn6s6
Feb 22 09:47:13.779: INFO: Got endpoints: latency-svc-rn6s6 [1.494995692s]
Feb 22 09:47:13.800: INFO: Created: latency-svc-jmtft
Feb 22 09:47:13.807: INFO: Got endpoints: latency-svc-jmtft [1.462052844s]
Feb 22 09:47:13.895: INFO: Created: latency-svc-jqqwc
Feb 22 09:47:13.937: INFO: Got endpoints: latency-svc-jqqwc [1.52142437s]
Feb 22 09:47:13.987: INFO: Created: latency-svc-bmpnl
Feb 22 09:47:13.997: INFO: Created: latency-svc-tlsz4
Feb 22 09:47:14.038: INFO: Got endpoints: latency-svc-bmpnl [1.583288685s]
Feb 22 09:47:14.099: INFO: Got endpoints: latency-svc-tlsz4 [1.564878069s]
Feb 22 09:47:14.171: INFO: Created: latency-svc-qtsth
Feb 22 09:47:14.259: INFO: Got endpoints: latency-svc-qtsth [1.643539305s]
Feb 22 09:47:14.334: INFO: Created: latency-svc-7djbm
Feb 22 09:47:14.551: INFO: Created: latency-svc-fr5p2
Feb 22 09:47:14.591: INFO: Got endpoints: latency-svc-7djbm [1.911348569s]
Feb 22 09:47:14.739: INFO: Got endpoints: latency-svc-fr5p2 [1.890960463s]
Feb 22 09:47:14.739: INFO: Created: latency-svc-vtsrw
Feb 22 09:47:14.778: INFO: Created: latency-svc-9jmf6
Feb 22 09:47:14.783: INFO: Got endpoints: latency-svc-vtsrw [1.864497285s]
Feb 22 09:47:14.823: INFO: Got endpoints: latency-svc-9jmf6 [1.575917818s]
Feb 22 09:47:14.896: INFO: Created: latency-svc-6bl9h
Feb 22 09:47:14.908: INFO: Created: latency-svc-jmrq7
Feb 22 09:47:14.947: INFO: Got endpoints: latency-svc-6bl9h [1.570800842s]
Feb 22 09:47:14.959: INFO: Created: latency-svc-p4fqx
Feb 22 09:47:14.969: INFO: Got endpoints: latency-svc-jmrq7 [1.444401526s]
Feb 22 09:47:15.024: INFO: Got endpoints: latency-svc-p4fqx [1.480148361s]
Feb 22 09:47:15.037: INFO: Created: latency-svc-hcdjs
Feb 22 09:47:15.067: INFO: Created: latency-svc-t4h88
Feb 22 09:47:15.078: INFO: Got endpoints: latency-svc-hcdjs [1.405551906s]
Feb 22 09:47:15.109: INFO: Got endpoints: latency-svc-t4h88 [1.385623898s]
Feb 22 09:47:15.122: INFO: Created: latency-svc-r42kz
Feb 22 09:47:15.185: INFO: Got endpoints: latency-svc-r42kz [1.405804204s]
Feb 22 09:47:15.232: INFO: Created: latency-svc-7pptq
Feb 22 09:47:15.257: INFO: Got endpoints: latency-svc-7pptq [1.449416103s]
Feb 22 09:47:15.282: INFO: Created: latency-svc-2vbjb
Feb 22 09:47:15.314: INFO: Got endpoints: latency-svc-2vbjb [1.376202742s]
Feb 22 09:47:15.390: INFO: Created: latency-svc-5t7ks
Feb 22 09:47:15.481: INFO: Got endpoints: latency-svc-5t7ks [1.443472831s]
Feb 22 09:47:15.706: INFO: Created: latency-svc-ggpwd
Feb 22 09:47:15.737: INFO: Got endpoints: latency-svc-ggpwd [1.638005532s]
Feb 22 09:47:15.740: INFO: Created: latency-svc-h4545
Feb 22 09:47:15.810: INFO: Got endpoints: latency-svc-h4545 [1.550929134s]
Feb 22 09:47:15.827: INFO: Created: latency-svc-94vmp
Feb 22 09:47:15.882: INFO: Got endpoints: latency-svc-94vmp [1.290258239s]
Feb 22 09:47:15.883: INFO: Created: latency-svc-clbqh
Feb 22 09:47:15.916: INFO: Got endpoints: latency-svc-clbqh [1.177743458s]
Feb 22 09:47:15.942: INFO: Created: latency-svc-4hqrx
Feb 22 09:47:15.982: INFO: Got endpoints: latency-svc-4hqrx [1.198804661s]
Feb 22 09:47:15.982: INFO: Latencies: [146.797522ms 199.361179ms 231.335131ms 440.268566ms 441.304261ms 492.294526ms 570.177365ms 731.412021ms 752.707323ms 795.157226ms 817.041724ms 839.56562ms 877.082447ms 879.610636ms 886.575404ms 888.870393ms 930.796799ms 931.013698ms 933.518087ms 940.845153ms 946.043529ms 947.435623ms 951.158505ms 966.355435ms 969.52622ms 969.748819ms 973.9124ms 984.203453ms 993.412909ms 996.053198ms 997.356191ms 998.327087ms 1.000370778s 1.009522135s 1.009948933s 1.010526731s 1.013113119s 1.01512451s 1.015492508s 1.026254558s 1.030117541s 1.032827728s 1.033341525s 1.041846686s 1.046350266s 1.047179761s 1.048803454s 1.065727076s 1.072405245s 1.072608444s 1.07331934s 1.073993537s 1.075050633s 1.080633306s 1.0820182s 1.082228799s 1.083084896s 1.087015478s 1.096142635s 1.096150336s 1.100829213s 1.103512901s 1.104531997s 1.107097784s 1.110091571s 1.112609359s 1.112944557s 1.113526355s 1.118868331s 1.132324968s 1.13422556s 1.134567558s 1.136025551s 1.140817429s 1.149571488s 1.149629388s 1.154399366s 1.156387857s 1.157297553s 1.158120149s 1.165959613s 1.172055084s 1.174579373s 1.177743458s 1.178587854s 1.178925053s 1.182830435s 1.185130124s 1.186389619s 1.195034479s 1.197340767s 1.198804661s 1.202956841s 1.204490535s 1.206933123s 1.209894509s 1.209979709s 1.212446698s 1.214873287s 1.217565174s 1.220167362s 1.221349356s 1.224089144s 1.225931436s 1.226174134s 1.226203734s 1.227230629s 1.231597109s 1.232341506s 1.236429387s 1.239610273s 1.240386669s 1.241487564s 1.243432255s 1.24878313s 1.252137515s 1.252427313s 1.252876911s 1.255496398s 1.255518898s 1.260419976s 1.261480172s 1.265173254s 1.267214245s 1.267433243s 1.267880541s 1.275039808s 1.276550702s 1.277354098s 1.277569997s 1.280142585s 1.288565546s 1.288988344s 1.290258239s 1.291018134s 1.29223223s 1.295459114s 1.296022011s 1.296755808s 1.300124793s 1.304875071s 1.305641968s 1.307279359s 1.312978734s 1.31593162s 1.329383457s 1.339305511s 1.35484054s 1.356695631s 1.376202742s 1.378242031s 1.381807316s 1.385623898s 1.393495661s 1.394114658s 1.402003722s 1.405551906s 1.405804204s 1.40886709s 1.416018357s 1.431889884s 1.435626467s 1.443472831s 1.44354913s 1.444376426s 1.444401526s 1.449416103s 1.458776659s 1.460226353s 1.462052844s 1.464142234s 1.476566778s 1.480148361s 1.481029057s 1.485163137s 1.489840616s 1.492690903s 1.494995692s 1.496896184s 1.508186431s 1.516638692s 1.520437574s 1.52142437s 1.53002163s 1.550929134s 1.55402022s 1.564878069s 1.570800842s 1.575917818s 1.583288685s 1.601418501s 1.608776167s 1.610042561s 1.638005532s 1.643539305s 1.735479781s 1.773882003s 1.864497285s 1.890960463s 1.911348569s]
Feb 22 09:47:15.984: INFO: 50 %ile: 1.220167362s
Feb 22 09:47:15.984: INFO: 90 %ile: 1.516638692s
Feb 22 09:47:15.984: INFO: 99 %ile: 1.890960463s
Feb 22 09:47:15.984: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:47:15.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-743" for this suite.

• [SLOW TEST:21.119 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":180,"skipped":3059,"failed":0}
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:47:16.014: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Feb 22 09:47:16.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-9691 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 22 09:47:16.373: INFO: stderr: ""
Feb 22 09:47:16.373: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Feb 22 09:47:16.373: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 22 09:47:16.373: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9691" to be "running and ready, or succeeded"
Feb 22 09:47:16.381: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506165ms
Feb 22 09:47:18.423: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04952193s
Feb 22 09:47:20.472: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.099164359s
Feb 22 09:47:20.472: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 22 09:47:20.472: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 22 09:47:20.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-9691 logs logs-generator logs-generator'
Feb 22 09:47:20.835: INFO: stderr: ""
Feb 22 09:47:20.835: INFO: stdout: "I0222 09:47:19.353839       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/4fq 212\nI0222 09:47:19.550624       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/rbf8 365\nI0222 09:47:19.750797       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/xtt 461\nI0222 09:47:19.950435       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jqh 568\nI0222 09:47:20.149851       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/9mjs 278\nI0222 09:47:20.350509       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/psj 579\nI0222 09:47:20.549910       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/pm2 460\nI0222 09:47:20.750639       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/f57 293\n"
STEP: limiting log lines
Feb 22 09:47:20.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-9691 logs logs-generator logs-generator --tail=1'
Feb 22 09:47:21.058: INFO: stderr: ""
Feb 22 09:47:21.058: INFO: stdout: "I0222 09:47:20.950063       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/7tx 229\n"
Feb 22 09:47:21.058: INFO: got output "I0222 09:47:20.950063       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/7tx 229\n"
STEP: limiting log bytes
Feb 22 09:47:21.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-9691 logs logs-generator logs-generator --limit-bytes=1'
Feb 22 09:47:21.641: INFO: stderr: ""
Feb 22 09:47:21.641: INFO: stdout: "I"
Feb 22 09:47:21.641: INFO: got output "I"
STEP: exposing timestamps
Feb 22 09:47:21.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-9691 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 22 09:47:22.434: INFO: stderr: ""
Feb 22 09:47:22.434: INFO: stdout: "2022-02-22T09:47:22.354476506Z I0222 09:47:22.353145       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/qxf 362\n"
Feb 22 09:47:22.434: INFO: got output "2022-02-22T09:47:22.354476506Z I0222 09:47:22.353145       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/qxf 362\n"
STEP: restricting to a time range
Feb 22 09:47:24.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-9691 logs logs-generator logs-generator --since=1s'
Feb 22 09:47:25.198: INFO: stderr: ""
Feb 22 09:47:25.198: INFO: stdout: "I0222 09:47:24.351931       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/xgp 287\nI0222 09:47:24.550526       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/rbb 311\nI0222 09:47:24.749911       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/k5p 452\nI0222 09:47:24.950476       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/n7dw 343\nI0222 09:47:25.149967       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/g48 516\n"
Feb 22 09:47:25.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-9691 logs logs-generator logs-generator --since=24h'
Feb 22 09:47:25.757: INFO: stderr: ""
Feb 22 09:47:25.757: INFO: stdout: "I0222 09:47:19.353839       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/4fq 212\nI0222 09:47:19.550624       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/rbf8 365\nI0222 09:47:19.750797       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/xtt 461\nI0222 09:47:19.950435       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jqh 568\nI0222 09:47:20.149851       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/9mjs 278\nI0222 09:47:20.350509       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/psj 579\nI0222 09:47:20.549910       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/pm2 460\nI0222 09:47:20.750639       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/f57 293\nI0222 09:47:20.950063       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/7tx 229\nI0222 09:47:21.167663       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/xbx 321\nI0222 09:47:21.349941       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/v97 300\nI0222 09:47:21.550352       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/jx8z 272\nI0222 09:47:21.750926       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ngt7 541\nI0222 09:47:21.957813       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/rtw 527\nI0222 09:47:22.150714       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/t42 360\nI0222 09:47:22.353145       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/qxf 362\nI0222 09:47:22.572866       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/7t47 505\nI0222 09:47:22.750558       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/rdsp 498\nI0222 09:47:22.950554       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/qcsb 400\nI0222 09:47:23.149972       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/jk22 204\nI0222 09:47:23.350600       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/ml9 325\nI0222 09:47:23.550528       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/wrt5 210\nI0222 09:47:23.749981       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/svj 562\nI0222 09:47:23.950608       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/2t5 408\nI0222 09:47:24.150055       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/htl 499\nI0222 09:47:24.351931       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/xgp 287\nI0222 09:47:24.550526       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/rbb 311\nI0222 09:47:24.749911       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/k5p 452\nI0222 09:47:24.950476       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/n7dw 343\nI0222 09:47:25.149967       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/g48 516\nI0222 09:47:25.350445       1 logs_generator.go:76] 30 GET /api/v1/namespaces/kube-system/pods/cvk 328\nI0222 09:47:25.551637       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/6kv 545\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Feb 22 09:47:25.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-9691 delete pod logs-generator'
Feb 22 09:47:29.125: INFO: stderr: ""
Feb 22 09:47:29.125: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:47:29.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9691" for this suite.

• [SLOW TEST:13.370 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":181,"skipped":3059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:47:29.397: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 09:47:30.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 09:47:32.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120050, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120050, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120051, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120050, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 09:47:36.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
Feb 22 09:47:36.175: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:47:36.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-48" for this suite.
STEP: Destroying namespace "webhook-48-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.031 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":182,"skipped":3118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:47:37.429: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-0937f2c5-b0b9-4cbf-8447-398ca27784fd
STEP: Creating configMap with name cm-test-opt-upd-d23fbbf0-133d-4ab6-946f-d8c950a7f096
STEP: Creating the pod
Feb 22 09:47:37.834: INFO: The status of Pod pod-configmaps-01ae5894-2205-4b16-8ded-2ba2e45b221a is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:47:39.863: INFO: The status of Pod pod-configmaps-01ae5894-2205-4b16-8ded-2ba2e45b221a is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:47:41.870: INFO: The status of Pod pod-configmaps-01ae5894-2205-4b16-8ded-2ba2e45b221a is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:47:43.876: INFO: The status of Pod pod-configmaps-01ae5894-2205-4b16-8ded-2ba2e45b221a is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-0937f2c5-b0b9-4cbf-8447-398ca27784fd
STEP: Updating configmap cm-test-opt-upd-d23fbbf0-133d-4ab6-946f-d8c950a7f096
STEP: Creating configMap with name cm-test-opt-create-4c7e8b67-f020-475f-a785-62edac475129
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:49:02.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8806" for this suite.

• [SLOW TEST:84.630 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:49:02.060: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:49:02.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d783560-f400-4580-9740-c99e309378b8" in namespace "downward-api-1632" to be "Succeeded or Failed"
Feb 22 09:49:02.315: INFO: Pod "downwardapi-volume-5d783560-f400-4580-9740-c99e309378b8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.877418ms
Feb 22 09:49:04.322: INFO: Pod "downwardapi-volume-5d783560-f400-4580-9740-c99e309378b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025108043s
Feb 22 09:49:06.347: INFO: Pod "downwardapi-volume-5d783560-f400-4580-9740-c99e309378b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050411085s
STEP: Saw pod success
Feb 22 09:49:06.347: INFO: Pod "downwardapi-volume-5d783560-f400-4580-9740-c99e309378b8" satisfied condition "Succeeded or Failed"
Feb 22 09:49:06.362: INFO: Trying to get logs from node node2 pod downwardapi-volume-5d783560-f400-4580-9740-c99e309378b8 container client-container: <nil>
STEP: delete the pod
Feb 22 09:49:06.501: INFO: Waiting for pod downwardapi-volume-5d783560-f400-4580-9740-c99e309378b8 to disappear
Feb 22 09:49:06.511: INFO: Pod downwardapi-volume-5d783560-f400-4580-9740-c99e309378b8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:49:06.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1632" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3175,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:49:06.548: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 22 09:49:07.067: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 22 09:50:07.157: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:50:07.166: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Feb 22 09:50:11.620: INFO: found a healthy node: node2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:50:26.363: INFO: pods created so far: [1 1 1]
Feb 22 09:50:26.363: INFO: length of pods created so far: 3
Feb 22 09:50:30.393: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:50:37.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-373" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:50:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8486" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:91.237 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":185,"skipped":3189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:50:37.786: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:50:38.059: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:50:40.077: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:50:42.069: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:50:44.086: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:50:46.073: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:50:48.075: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:50:50.068: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:50:52.069: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:50:54.072: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:50:56.069: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:50:58.069: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = false)
Feb 22 09:51:00.069: INFO: The status of Pod test-webserver-7127f1c4-4d98-4ee9-b3fc-91f9753611d9 is Running (Ready = true)
Feb 22 09:51:00.074: INFO: Container started at 2022-02-22 09:50:40 +0000 UTC, pod became ready at 2022-02-22 09:50:58 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:51:00.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-492" for this suite.

• [SLOW TEST:22.311 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3214,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:51:00.099: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 22 09:51:00.217: INFO: Waiting up to 5m0s for pod "downward-api-252ad864-580f-4b59-8bca-0f67d90e83bd" in namespace "downward-api-4000" to be "Succeeded or Failed"
Feb 22 09:51:00.231: INFO: Pod "downward-api-252ad864-580f-4b59-8bca-0f67d90e83bd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.133835ms
Feb 22 09:51:02.247: INFO: Pod "downward-api-252ad864-580f-4b59-8bca-0f67d90e83bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029712921s
Feb 22 09:51:04.263: INFO: Pod "downward-api-252ad864-580f-4b59-8bca-0f67d90e83bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045544907s
STEP: Saw pod success
Feb 22 09:51:04.263: INFO: Pod "downward-api-252ad864-580f-4b59-8bca-0f67d90e83bd" satisfied condition "Succeeded or Failed"
Feb 22 09:51:04.269: INFO: Trying to get logs from node node2 pod downward-api-252ad864-580f-4b59-8bca-0f67d90e83bd container dapi-container: <nil>
STEP: delete the pod
Feb 22 09:51:04.346: INFO: Waiting for pod downward-api-252ad864-580f-4b59-8bca-0f67d90e83bd to disappear
Feb 22 09:51:04.352: INFO: Pod downward-api-252ad864-580f-4b59-8bca-0f67d90e83bd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:51:04.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4000" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3220,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:51:04.396: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:51:11.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6063" for this suite.

• [SLOW TEST:7.396 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":188,"skipped":3230,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:51:11.792: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:51:11.959: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 22 09:51:14.202: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:51:15.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-245" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":189,"skipped":3240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:51:15.266: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:51:15.701: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 22 09:51:24.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 --namespace=crd-publish-openapi-8208 create -f -'
Feb 22 09:51:26.438: INFO: stderr: ""
Feb 22 09:51:26.438: INFO: stdout: "e2e-test-crd-publish-openapi-5981-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 22 09:51:26.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 --namespace=crd-publish-openapi-8208 delete e2e-test-crd-publish-openapi-5981-crds test-foo'
Feb 22 09:51:26.607: INFO: stderr: ""
Feb 22 09:51:26.607: INFO: stdout: "e2e-test-crd-publish-openapi-5981-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 22 09:51:26.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 --namespace=crd-publish-openapi-8208 apply -f -'
Feb 22 09:51:27.078: INFO: stderr: ""
Feb 22 09:51:27.078: INFO: stdout: "e2e-test-crd-publish-openapi-5981-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 22 09:51:27.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 --namespace=crd-publish-openapi-8208 delete e2e-test-crd-publish-openapi-5981-crds test-foo'
Feb 22 09:51:27.214: INFO: stderr: ""
Feb 22 09:51:27.214: INFO: stdout: "e2e-test-crd-publish-openapi-5981-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 22 09:51:27.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 --namespace=crd-publish-openapi-8208 create -f -'
Feb 22 09:51:27.519: INFO: rc: 1
Feb 22 09:51:27.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 --namespace=crd-publish-openapi-8208 apply -f -'
Feb 22 09:51:27.859: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 22 09:51:27.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 --namespace=crd-publish-openapi-8208 create -f -'
Feb 22 09:51:28.161: INFO: rc: 1
Feb 22 09:51:28.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 --namespace=crd-publish-openapi-8208 apply -f -'
Feb 22 09:51:28.491: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 22 09:51:28.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 explain e2e-test-crd-publish-openapi-5981-crds'
Feb 22 09:51:28.854: INFO: stderr: ""
Feb 22 09:51:28.854: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5981-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 22 09:51:28.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 explain e2e-test-crd-publish-openapi-5981-crds.metadata'
Feb 22 09:51:29.168: INFO: stderr: ""
Feb 22 09:51:29.168: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5981-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 22 09:51:29.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 explain e2e-test-crd-publish-openapi-5981-crds.spec'
Feb 22 09:51:29.525: INFO: stderr: ""
Feb 22 09:51:29.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5981-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 22 09:51:29.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 explain e2e-test-crd-publish-openapi-5981-crds.spec.bars'
Feb 22 09:51:29.871: INFO: stderr: ""
Feb 22 09:51:29.871: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5981-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 22 09:51:29.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-8208 explain e2e-test-crd-publish-openapi-5981-crds.spec.bars2'
Feb 22 09:51:30.164: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:51:34.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8208" for this suite.

• [SLOW TEST:18.790 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":190,"skipped":3273,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:51:34.055: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4152.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4152.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4152.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4152.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4152.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4152.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 09:51:38.610: INFO: DNS probes using dns-4152/dns-test-808a772d-f5b8-4e32-9bdf-b2ff7d62363b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:51:38.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4152" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":191,"skipped":3283,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:51:38.804: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:51:39.062: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c5b58d49-3aa4-47d4-80c6-c56c013b58ca" in namespace "security-context-test-6079" to be "Succeeded or Failed"
Feb 22 09:51:39.070: INFO: Pod "busybox-readonly-false-c5b58d49-3aa4-47d4-80c6-c56c013b58ca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.297862ms
Feb 22 09:51:41.144: INFO: Pod "busybox-readonly-false-c5b58d49-3aa4-47d4-80c6-c56c013b58ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081774781s
Feb 22 09:51:43.184: INFO: Pod "busybox-readonly-false-c5b58d49-3aa4-47d4-80c6-c56c013b58ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.122600651s
Feb 22 09:51:43.184: INFO: Pod "busybox-readonly-false-c5b58d49-3aa4-47d4-80c6-c56c013b58ca" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:51:43.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6079" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:51:43.238: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 22 09:51:43.415: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:51:49.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3324" for this suite.

• [SLOW TEST:6.289 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":193,"skipped":3309,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:51:49.529: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-7878
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7878
STEP: Waiting until pod test-pod will start running in namespace statefulset-7878
STEP: Creating statefulset with conflicting port in namespace statefulset-7878
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7878
Feb 22 09:51:54.095: INFO: Observed stateful pod in namespace: statefulset-7878, name: ss-0, uid: c6c4a009-edfc-47e1-b52f-e3af0afcf4ac, status phase: Pending. Waiting for statefulset controller to delete.
Feb 22 09:51:54.164: INFO: Observed stateful pod in namespace: statefulset-7878, name: ss-0, uid: c6c4a009-edfc-47e1-b52f-e3af0afcf4ac, status phase: Failed. Waiting for statefulset controller to delete.
Feb 22 09:51:54.274: INFO: Observed stateful pod in namespace: statefulset-7878, name: ss-0, uid: c6c4a009-edfc-47e1-b52f-e3af0afcf4ac, status phase: Failed. Waiting for statefulset controller to delete.
Feb 22 09:51:54.319: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7878
STEP: Removing pod with conflicting port in namespace statefulset-7878
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7878 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 22 09:52:00.674: INFO: Deleting all statefulset in ns statefulset-7878
Feb 22 09:52:00.681: INFO: Scaling statefulset ss to 0
Feb 22 09:52:10.730: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 09:52:10.739: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:52:10.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7878" for this suite.

• [SLOW TEST:21.446 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":194,"skipped":3320,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:52:10.975: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:52:11.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4158" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":195,"skipped":3327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:52:11.704: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:52:11.910: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25bab23f-b51d-4b18-a32e-8b764a867569" in namespace "projected-5891" to be "Succeeded or Failed"
Feb 22 09:52:11.918: INFO: Pod "downwardapi-volume-25bab23f-b51d-4b18-a32e-8b764a867569": Phase="Pending", Reason="", readiness=false. Elapsed: 7.407266ms
Feb 22 09:52:13.943: INFO: Pod "downwardapi-volume-25bab23f-b51d-4b18-a32e-8b764a867569": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032965307s
Feb 22 09:52:15.954: INFO: Pod "downwardapi-volume-25bab23f-b51d-4b18-a32e-8b764a867569": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043629916s
STEP: Saw pod success
Feb 22 09:52:15.954: INFO: Pod "downwardapi-volume-25bab23f-b51d-4b18-a32e-8b764a867569" satisfied condition "Succeeded or Failed"
Feb 22 09:52:15.961: INFO: Trying to get logs from node node2 pod downwardapi-volume-25bab23f-b51d-4b18-a32e-8b764a867569 container client-container: <nil>
STEP: delete the pod
Feb 22 09:52:16.117: INFO: Waiting for pod downwardapi-volume-25bab23f-b51d-4b18-a32e-8b764a867569 to disappear
Feb 22 09:52:16.128: INFO: Pod downwardapi-volume-25bab23f-b51d-4b18-a32e-8b764a867569 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:52:16.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5891" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:52:16.201: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Feb 22 09:52:16.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5036 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Feb 22 09:52:16.602: INFO: stderr: ""
Feb 22 09:52:16.602: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Feb 22 09:52:16.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-5036 delete pods e2e-test-httpd-pod'
Feb 22 09:52:22.460: INFO: stderr: ""
Feb 22 09:52:22.460: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:52:22.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5036" for this suite.

• [SLOW TEST:6.428 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":197,"skipped":3381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:52:22.630: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 22 09:52:26.929: INFO: &Pod{ObjectMeta:{send-events-ef8ea8bc-bf90-4b7b-8481-28393557f747  events-3377  d43a6aae-35d0-4aed-b6ec-cc62fc7c6a8c 56864 0 2022-02-22 09:52:22 +0000 UTC <nil> <nil> map[name:foo time:855030868] map[cni.projectcalico.org/containerID:8f3ee56b6f2844b01b9ec2c007e9a61147acecbbd448f1dfb34e4c9f2cdfb8e5 cni.projectcalico.org/podIP:172.21.104.44/32 cni.projectcalico.org/podIPs:172.21.104.44/32] [] []  [{e2e.test Update v1 2022-02-22 09:52:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 09:52:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 09:52:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwh7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwh7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 09:52:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.44,StartTime:2022-02-22 09:52:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 09:52:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://70860d0f0db39d32d830af8fed24b8cc466bf44e6c12670e16f34b00cc3eb4e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 22 09:52:28.942: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 22 09:52:30.949: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:52:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3377" for this suite.

• [SLOW TEST:8.417 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":198,"skipped":3432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:52:31.051: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 09:52:31.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1fbda7c-d6d9-480d-a492-f6122772df29" in namespace "downward-api-1959" to be "Succeeded or Failed"
Feb 22 09:52:31.257: INFO: Pod "downwardapi-volume-c1fbda7c-d6d9-480d-a492-f6122772df29": Phase="Pending", Reason="", readiness=false. Elapsed: 5.914973ms
Feb 22 09:52:33.275: INFO: Pod "downwardapi-volume-c1fbda7c-d6d9-480d-a492-f6122772df29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023787849s
Feb 22 09:52:35.300: INFO: Pod "downwardapi-volume-c1fbda7c-d6d9-480d-a492-f6122772df29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048353195s
STEP: Saw pod success
Feb 22 09:52:35.300: INFO: Pod "downwardapi-volume-c1fbda7c-d6d9-480d-a492-f6122772df29" satisfied condition "Succeeded or Failed"
Feb 22 09:52:35.306: INFO: Trying to get logs from node node2 pod downwardapi-volume-c1fbda7c-d6d9-480d-a492-f6122772df29 container client-container: <nil>
STEP: delete the pod
Feb 22 09:52:35.467: INFO: Waiting for pod downwardapi-volume-c1fbda7c-d6d9-480d-a492-f6122772df29 to disappear
Feb 22 09:52:35.474: INFO: Pod downwardapi-volume-c1fbda7c-d6d9-480d-a492-f6122772df29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:52:35.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1959" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3498,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:52:35.516: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 22 09:52:37.010: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0222 09:52:37.010552      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 22 09:52:37.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6321" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":200,"skipped":3509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:52:37.074: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Feb 22 09:52:43.536: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2104 PodName:var-expansion-f2cb94e3-d1d4-4923-ae0c-2291f8a560d6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:52:43.536: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: test for file in mounted path
Feb 22 09:52:44.191: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2104 PodName:var-expansion-f2cb94e3-d1d4-4923-ae0c-2291f8a560d6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:52:44.191: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: updating the annotation value
Feb 22 09:52:45.242: INFO: Successfully updated pod "var-expansion-f2cb94e3-d1d4-4923-ae0c-2291f8a560d6"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Feb 22 09:52:45.248: INFO: Deleting pod "var-expansion-f2cb94e3-d1d4-4923-ae0c-2291f8a560d6" in namespace "var-expansion-2104"
Feb 22 09:52:45.302: INFO: Wait up to 5m0s for pod "var-expansion-f2cb94e3-d1d4-4923-ae0c-2291f8a560d6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:53:19.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2104" for this suite.

• [SLOW TEST:42.282 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":201,"skipped":3537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:53:19.358: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:53:19.594: INFO: Create a RollingUpdate DaemonSet
Feb 22 09:53:19.617: INFO: Check that daemon pods launch on every node of the cluster
Feb 22 09:53:19.633: INFO: Number of nodes with available pods: 0
Feb 22 09:53:19.633: INFO: Node node1 is running more than one daemon pod
Feb 22 09:53:20.691: INFO: Number of nodes with available pods: 0
Feb 22 09:53:20.691: INFO: Node node1 is running more than one daemon pod
Feb 22 09:53:21.683: INFO: Number of nodes with available pods: 0
Feb 22 09:53:21.683: INFO: Node node1 is running more than one daemon pod
Feb 22 09:53:22.729: INFO: Number of nodes with available pods: 0
Feb 22 09:53:22.729: INFO: Node node1 is running more than one daemon pod
Feb 22 09:53:23.676: INFO: Number of nodes with available pods: 2
Feb 22 09:53:23.676: INFO: Number of running nodes: 2, number of available pods: 2
Feb 22 09:53:23.676: INFO: Update the DaemonSet to trigger a rollout
Feb 22 09:53:23.706: INFO: Updating DaemonSet daemon-set
Feb 22 09:53:28.756: INFO: Roll back the DaemonSet before rollout is complete
Feb 22 09:53:28.799: INFO: Updating DaemonSet daemon-set
Feb 22 09:53:28.799: INFO: Make sure DaemonSet rollback is complete
Feb 22 09:53:28.828: INFO: Wrong image for pod: daemon-set-rrk5p. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Feb 22 09:53:28.828: INFO: Pod daemon-set-rrk5p is not available
Feb 22 09:53:35.856: INFO: Pod daemon-set-s5bnq is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6471, will wait for the garbage collector to delete the pods
Feb 22 09:53:35.967: INFO: Deleting DaemonSet.extensions daemon-set took: 25.136284ms
Feb 22 09:53:36.068: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.363532ms
Feb 22 09:53:41.706: INFO: Number of nodes with available pods: 0
Feb 22 09:53:41.706: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 09:53:41.727: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"57219"},"items":null}

Feb 22 09:53:41.735: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"57219"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:53:41.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6471" for this suite.

• [SLOW TEST:22.447 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":202,"skipped":3564,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:53:41.806: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:55:01.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4866" for this suite.

• [SLOW TEST:80.260 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":203,"skipped":3574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:55:02.067: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Feb 22 09:55:02.352: INFO: Waiting up to 5m0s for pod "security-context-f076776e-54fd-41f3-86ec-331234466fe2" in namespace "security-context-9643" to be "Succeeded or Failed"
Feb 22 09:55:02.404: INFO: Pod "security-context-f076776e-54fd-41f3-86ec-331234466fe2": Phase="Pending", Reason="", readiness=false. Elapsed: 51.763061ms
Feb 22 09:55:04.428: INFO: Pod "security-context-f076776e-54fd-41f3-86ec-331234466fe2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075088612s
Feb 22 09:55:06.436: INFO: Pod "security-context-f076776e-54fd-41f3-86ec-331234466fe2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083700831s
STEP: Saw pod success
Feb 22 09:55:06.436: INFO: Pod "security-context-f076776e-54fd-41f3-86ec-331234466fe2" satisfied condition "Succeeded or Failed"
Feb 22 09:55:06.442: INFO: Trying to get logs from node node2 pod security-context-f076776e-54fd-41f3-86ec-331234466fe2 container test-container: <nil>
STEP: delete the pod
Feb 22 09:55:06.579: INFO: Waiting for pod security-context-f076776e-54fd-41f3-86ec-331234466fe2 to disappear
Feb 22 09:55:06.589: INFO: Pod security-context-f076776e-54fd-41f3-86ec-331234466fe2 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:55:06.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9643" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":204,"skipped":3607,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:55:06.707: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 22 09:55:06.857: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9716  ae8c3483-9346-41f0-8143-033784da029e 57434 0 2022-02-22 09:55:06 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-02-22 09:55:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2x2z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2x2z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 22 09:55:06.869: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:55:08.970: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:55:10.875: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 22 09:55:10.875: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9716 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:55:10.875: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Verifying customized DNS server is configured on pod...
Feb 22 09:55:11.457: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9716 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 09:55:11.457: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 09:55:12.047: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:55:12.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9716" for this suite.

• [SLOW TEST:5.426 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":205,"skipped":3615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:55:12.134: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-4abfe9aa-ddf4-4bbe-985d-4d8c7428a7d9
STEP: Creating a pod to test consume configMaps
Feb 22 09:55:12.351: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff28ae00-121d-4806-b641-7f2ed385f919" in namespace "configmap-25" to be "Succeeded or Failed"
Feb 22 09:55:12.359: INFO: Pod "pod-configmaps-ff28ae00-121d-4806-b641-7f2ed385f919": Phase="Pending", Reason="", readiness=false. Elapsed: 7.096967ms
Feb 22 09:55:14.408: INFO: Pod "pod-configmaps-ff28ae00-121d-4806-b641-7f2ed385f919": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056173299s
Feb 22 09:55:16.421: INFO: Pod "pod-configmaps-ff28ae00-121d-4806-b641-7f2ed385f919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069172898s
STEP: Saw pod success
Feb 22 09:55:16.421: INFO: Pod "pod-configmaps-ff28ae00-121d-4806-b641-7f2ed385f919" satisfied condition "Succeeded or Failed"
Feb 22 09:55:16.427: INFO: Trying to get logs from node node2 pod pod-configmaps-ff28ae00-121d-4806-b641-7f2ed385f919 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:55:16.521: INFO: Waiting for pod pod-configmaps-ff28ae00-121d-4806-b641-7f2ed385f919 to disappear
Feb 22 09:55:16.533: INFO: Pod pod-configmaps-ff28ae00-121d-4806-b641-7f2ed385f919 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:55:16.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-25" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":206,"skipped":3638,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:55:16.620: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 22 09:55:21.060: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:55:21.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1963" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":207,"skipped":3643,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:55:21.183: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:55:21.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1583" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":208,"skipped":3650,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:55:21.453: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Feb 22 09:55:21.704: INFO: Waiting up to 5m0s for pod "var-expansion-75b1ce53-9449-4e26-adfb-e6ef01d0f00c" in namespace "var-expansion-263" to be "Succeeded or Failed"
Feb 22 09:55:21.716: INFO: Pod "var-expansion-75b1ce53-9449-4e26-adfb-e6ef01d0f00c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.092444ms
Feb 22 09:55:23.733: INFO: Pod "var-expansion-75b1ce53-9449-4e26-adfb-e6ef01d0f00c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029794522s
Feb 22 09:55:25.741: INFO: Pod "var-expansion-75b1ce53-9449-4e26-adfb-e6ef01d0f00c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037875443s
STEP: Saw pod success
Feb 22 09:55:25.742: INFO: Pod "var-expansion-75b1ce53-9449-4e26-adfb-e6ef01d0f00c" satisfied condition "Succeeded or Failed"
Feb 22 09:55:25.749: INFO: Trying to get logs from node node2 pod var-expansion-75b1ce53-9449-4e26-adfb-e6ef01d0f00c container dapi-container: <nil>
STEP: delete the pod
Feb 22 09:55:25.801: INFO: Waiting for pod var-expansion-75b1ce53-9449-4e26-adfb-e6ef01d0f00c to disappear
Feb 22 09:55:25.809: INFO: Pod var-expansion-75b1ce53-9449-4e26-adfb-e6ef01d0f00c no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:55:25.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-263" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":3658,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:55:25.849: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 22 09:55:26.200: INFO: Waiting up to 5m0s for pod "downward-api-2cc4d954-f5ba-4c92-b887-97f92201a692" in namespace "downward-api-355" to be "Succeeded or Failed"
Feb 22 09:55:26.209: INFO: Pod "downward-api-2cc4d954-f5ba-4c92-b887-97f92201a692": Phase="Pending", Reason="", readiness=false. Elapsed: 9.165658ms
Feb 22 09:55:28.233: INFO: Pod "downward-api-2cc4d954-f5ba-4c92-b887-97f92201a692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033025506s
Feb 22 09:55:30.245: INFO: Pod "downward-api-2cc4d954-f5ba-4c92-b887-97f92201a692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045207609s
STEP: Saw pod success
Feb 22 09:55:30.245: INFO: Pod "downward-api-2cc4d954-f5ba-4c92-b887-97f92201a692" satisfied condition "Succeeded or Failed"
Feb 22 09:55:30.250: INFO: Trying to get logs from node node2 pod downward-api-2cc4d954-f5ba-4c92-b887-97f92201a692 container dapi-container: <nil>
STEP: delete the pod
Feb 22 09:55:30.348: INFO: Waiting for pod downward-api-2cc4d954-f5ba-4c92-b887-97f92201a692 to disappear
Feb 22 09:55:30.359: INFO: Pod downward-api-2cc4d954-f5ba-4c92-b887-97f92201a692 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:55:30.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-355" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":210,"skipped":3668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:55:30.398: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:56:00.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9981" for this suite.

• [SLOW TEST:30.342 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":3690,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:56:00.740: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-131
Feb 22 09:56:01.133: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:56:03.144: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 22 09:56:03.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-131 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 22 09:56:03.881: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 22 09:56:03.881: INFO: stdout: "iptables"
Feb 22 09:56:03.881: INFO: proxyMode: iptables
Feb 22 09:56:03.930: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 22 09:56:03.936: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-131
STEP: creating replication controller affinity-clusterip-timeout in namespace services-131
I0222 09:56:04.219100      19 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-131, replica count: 3
I0222 09:56:07.275714      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:56:10.276064      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 09:56:13.276435      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 09:56:13.307: INFO: Creating new exec pod
Feb 22 09:56:18.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-131 exec execpod-affinitycf7qn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Feb 22 09:56:19.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 22 09:56:19.373: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:56:19.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-131 exec execpod-affinitycf7qn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.65.163 80'
Feb 22 09:56:20.356: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.65.163 80\nConnection to 172.20.65.163 80 port [tcp/http] succeeded!\n"
Feb 22 09:56:20.356: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 09:56:20.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-131 exec execpod-affinitycf7qn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.65.163:80/ ; done'
Feb 22 09:56:21.545: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n"
Feb 22 09:56:21.546: INFO: stdout: "\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp\naffinity-clusterip-timeout-xvsnp"
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Received response from host: affinity-clusterip-timeout-xvsnp
Feb 22 09:56:21.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-131 exec execpod-affinitycf7qn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.20.65.163:80/'
Feb 22 09:56:22.536: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n"
Feb 22 09:56:22.537: INFO: stdout: "affinity-clusterip-timeout-xvsnp"
Feb 22 09:56:42.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-131 exec execpod-affinitycf7qn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.20.65.163:80/'
Feb 22 09:56:43.280: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.20.65.163:80/\n"
Feb 22 09:56:43.280: INFO: stdout: "affinity-clusterip-timeout-44r5j"
Feb 22 09:56:43.280: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-131, will wait for the garbage collector to delete the pods
Feb 22 09:56:43.520: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 45.341791ms
Feb 22 09:56:43.935: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 415.651279ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:56:48.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-131" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:47.582 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":212,"skipped":3697,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:56:48.322: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-5650e35a-3d66-41db-b5e2-c37165a8625b
STEP: Creating a pod to test consume configMaps
Feb 22 09:56:48.607: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-878360d4-47a2-421b-b700-9eeee6454428" in namespace "projected-908" to be "Succeeded or Failed"
Feb 22 09:56:48.619: INFO: Pod "pod-projected-configmaps-878360d4-47a2-421b-b700-9eeee6454428": Phase="Pending", Reason="", readiness=false. Elapsed: 11.752946ms
Feb 22 09:56:50.628: INFO: Pod "pod-projected-configmaps-878360d4-47a2-421b-b700-9eeee6454428": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020043267s
Feb 22 09:56:52.644: INFO: Pod "pod-projected-configmaps-878360d4-47a2-421b-b700-9eeee6454428": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036811548s
STEP: Saw pod success
Feb 22 09:56:52.644: INFO: Pod "pod-projected-configmaps-878360d4-47a2-421b-b700-9eeee6454428" satisfied condition "Succeeded or Failed"
Feb 22 09:56:52.689: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-878360d4-47a2-421b-b700-9eeee6454428 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:56:52.795: INFO: Waiting for pod pod-projected-configmaps-878360d4-47a2-421b-b700-9eeee6454428 to disappear
Feb 22 09:56:52.806: INFO: Pod pod-projected-configmaps-878360d4-47a2-421b-b700-9eeee6454428 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:56:52.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-908" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":3700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:56:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 22 09:56:54.518: INFO: Pod name wrapped-volume-race-1307da7a-3b86-4c0a-9900-b3d4db6c5a9b: Found 0 pods out of 5
Feb 22 09:56:59.761: INFO: Pod name wrapped-volume-race-1307da7a-3b86-4c0a-9900-b3d4db6c5a9b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1307da7a-3b86-4c0a-9900-b3d4db6c5a9b in namespace emptydir-wrapper-3161, will wait for the garbage collector to delete the pods
Feb 22 09:57:17.955: INFO: Deleting ReplicationController wrapped-volume-race-1307da7a-3b86-4c0a-9900-b3d4db6c5a9b took: 31.430254ms
Feb 22 09:57:18.314: INFO: Terminating ReplicationController wrapped-volume-race-1307da7a-3b86-4c0a-9900-b3d4db6c5a9b pods took: 359.815038ms
STEP: Creating RC which spawns configmap-volume pods
Feb 22 09:57:25.707: INFO: Pod name wrapped-volume-race-18a4f2e2-2175-4696-85d4-aae1a5f4aeee: Found 0 pods out of 5
Feb 22 09:57:30.727: INFO: Pod name wrapped-volume-race-18a4f2e2-2175-4696-85d4-aae1a5f4aeee: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-18a4f2e2-2175-4696-85d4-aae1a5f4aeee in namespace emptydir-wrapper-3161, will wait for the garbage collector to delete the pods
Feb 22 09:57:46.871: INFO: Deleting ReplicationController wrapped-volume-race-18a4f2e2-2175-4696-85d4-aae1a5f4aeee took: 33.268346ms
Feb 22 09:57:47.275: INFO: Terminating ReplicationController wrapped-volume-race-18a4f2e2-2175-4696-85d4-aae1a5f4aeee pods took: 404.467832ms
STEP: Creating RC which spawns configmap-volume pods
Feb 22 09:57:53.193: INFO: Pod name wrapped-volume-race-6573aeca-efc0-49d7-8027-1fda44b212a5: Found 0 pods out of 5
Feb 22 09:57:58.216: INFO: Pod name wrapped-volume-race-6573aeca-efc0-49d7-8027-1fda44b212a5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6573aeca-efc0-49d7-8027-1fda44b212a5 in namespace emptydir-wrapper-3161, will wait for the garbage collector to delete the pods
Feb 22 09:58:12.557: INFO: Deleting ReplicationController wrapped-volume-race-6573aeca-efc0-49d7-8027-1fda44b212a5 took: 62.819309ms
Feb 22 09:58:13.158: INFO: Terminating ReplicationController wrapped-volume-race-6573aeca-efc0-49d7-8027-1fda44b212a5 pods took: 601.039823ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:58:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3161" for this suite.

• [SLOW TEST:88.827 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":214,"skipped":3778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:58:21.678: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:58:22.050: INFO: The status of Pod pod-secrets-5dadc14e-3822-4a67-a6d9-641754c02bfb is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:58:24.067: INFO: The status of Pod pod-secrets-5dadc14e-3822-4a67-a6d9-641754c02bfb is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:58:26.058: INFO: The status of Pod pod-secrets-5dadc14e-3822-4a67-a6d9-641754c02bfb is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:58:26.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6016" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":215,"skipped":3809,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:58:26.306: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Feb 22 09:58:26.490: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:58:28.518: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:58:30.528: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:58:32.536: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 22 09:58:32.690: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:58:34.724: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 09:58:36.703: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Feb 22 09:58:36.744: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 09:58:36.755: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 09:58:38.756: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 09:58:38.781: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 09:58:40.755: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 09:58:40.819: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:58:40.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1428" for this suite.

• [SLOW TEST:14.670 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":3814,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:58:40.977: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 09:58:41.159: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 22 09:58:49.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-7453 --namespace=crd-publish-openapi-7453 create -f -'
Feb 22 09:58:51.131: INFO: stderr: ""
Feb 22 09:58:51.131: INFO: stdout: "e2e-test-crd-publish-openapi-1715-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 22 09:58:51.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-7453 --namespace=crd-publish-openapi-7453 delete e2e-test-crd-publish-openapi-1715-crds test-cr'
Feb 22 09:58:51.308: INFO: stderr: ""
Feb 22 09:58:51.308: INFO: stdout: "e2e-test-crd-publish-openapi-1715-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 22 09:58:51.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-7453 --namespace=crd-publish-openapi-7453 apply -f -'
Feb 22 09:58:51.780: INFO: stderr: ""
Feb 22 09:58:51.780: INFO: stdout: "e2e-test-crd-publish-openapi-1715-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 22 09:58:51.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-7453 --namespace=crd-publish-openapi-7453 delete e2e-test-crd-publish-openapi-1715-crds test-cr'
Feb 22 09:58:51.941: INFO: stderr: ""
Feb 22 09:58:51.941: INFO: stdout: "e2e-test-crd-publish-openapi-1715-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 22 09:58:51.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-7453 explain e2e-test-crd-publish-openapi-1715-crds'
Feb 22 09:58:52.382: INFO: stderr: ""
Feb 22 09:58:52.382: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1715-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:58:59.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7453" for this suite.

• [SLOW TEST:18.909 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":217,"skipped":3822,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:58:59.886: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 22 09:59:00.141: INFO: Number of nodes with available pods: 0
Feb 22 09:59:00.141: INFO: Node node1 is running more than one daemon pod
Feb 22 09:59:01.161: INFO: Number of nodes with available pods: 0
Feb 22 09:59:01.161: INFO: Node node1 is running more than one daemon pod
Feb 22 09:59:02.162: INFO: Number of nodes with available pods: 0
Feb 22 09:59:02.163: INFO: Node node1 is running more than one daemon pod
Feb 22 09:59:03.337: INFO: Number of nodes with available pods: 0
Feb 22 09:59:03.337: INFO: Node node1 is running more than one daemon pod
Feb 22 09:59:04.158: INFO: Number of nodes with available pods: 0
Feb 22 09:59:04.158: INFO: Node node1 is running more than one daemon pod
Feb 22 09:59:05.182: INFO: Number of nodes with available pods: 2
Feb 22 09:59:05.182: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 22 09:59:05.746: INFO: Number of nodes with available pods: 1
Feb 22 09:59:05.747: INFO: Node node2 is running more than one daemon pod
Feb 22 09:59:06.826: INFO: Number of nodes with available pods: 1
Feb 22 09:59:06.826: INFO: Node node2 is running more than one daemon pod
Feb 22 09:59:07.785: INFO: Number of nodes with available pods: 1
Feb 22 09:59:07.785: INFO: Node node2 is running more than one daemon pod
Feb 22 09:59:08.766: INFO: Number of nodes with available pods: 1
Feb 22 09:59:08.766: INFO: Node node2 is running more than one daemon pod
Feb 22 09:59:09.765: INFO: Number of nodes with available pods: 1
Feb 22 09:59:09.765: INFO: Node node2 is running more than one daemon pod
Feb 22 09:59:10.772: INFO: Number of nodes with available pods: 2
Feb 22 09:59:10.772: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7584, will wait for the garbage collector to delete the pods
Feb 22 09:59:10.875: INFO: Deleting DaemonSet.extensions daemon-set took: 32.48655ms
Feb 22 09:59:11.276: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.867648ms
Feb 22 09:59:14.799: INFO: Number of nodes with available pods: 0
Feb 22 09:59:14.799: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 09:59:14.817: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59434"},"items":null}

Feb 22 09:59:14.822: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59434"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:59:14.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7584" for this suite.

• [SLOW TEST:14.992 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":218,"skipped":3835,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:59:14.878: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:59:31.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5392" for this suite.
STEP: Destroying namespace "nsdeletetest-9481" for this suite.
Feb 22 09:59:31.662: INFO: Namespace nsdeletetest-9481 was already deleted
STEP: Destroying namespace "nsdeletetest-1093" for this suite.

• [SLOW TEST:16.815 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":219,"skipped":3838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:59:31.698: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 22 09:59:35.991: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:59:36.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3866" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":220,"skipped":3892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:59:36.120: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Feb 22 09:59:36.311: INFO: Waiting up to 5m0s for pod "client-containers-e3e9b471-6953-4438-9c22-4b3c60f0cb0d" in namespace "containers-6991" to be "Succeeded or Failed"
Feb 22 09:59:36.320: INFO: Pod "client-containers-e3e9b471-6953-4438-9c22-4b3c60f0cb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.579356ms
Feb 22 09:59:38.336: INFO: Pod "client-containers-e3e9b471-6953-4438-9c22-4b3c60f0cb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025073643s
Feb 22 09:59:40.358: INFO: Pod "client-containers-e3e9b471-6953-4438-9c22-4b3c60f0cb0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047840597s
STEP: Saw pod success
Feb 22 09:59:40.358: INFO: Pod "client-containers-e3e9b471-6953-4438-9c22-4b3c60f0cb0d" satisfied condition "Succeeded or Failed"
Feb 22 09:59:40.367: INFO: Trying to get logs from node node2 pod client-containers-e3e9b471-6953-4438-9c22-4b3c60f0cb0d container agnhost-container: <nil>
STEP: delete the pod
Feb 22 09:59:40.506: INFO: Waiting for pod client-containers-e3e9b471-6953-4438-9c22-4b3c60f0cb0d to disappear
Feb 22 09:59:40.513: INFO: Pod client-containers-e3e9b471-6953-4438-9c22-4b3c60f0cb0d no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:59:40.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6991" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":3931,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:59:40.709: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 09:59:41.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9597" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":222,"skipped":3937,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 09:59:41.110: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-8ef9fa83-75e4-487e-9d80-e3ee510e7076 in namespace container-probe-520
Feb 22 09:59:45.344: INFO: Started pod busybox-8ef9fa83-75e4-487e-9d80-e3ee510e7076 in namespace container-probe-520
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 09:59:45.348: INFO: Initial restart count of pod busybox-8ef9fa83-75e4-487e-9d80-e3ee510e7076 is 0
Feb 22 10:00:34.341: INFO: Restart count of pod container-probe-520/busybox-8ef9fa83-75e4-487e-9d80-e3ee510e7076 is now 1 (48.992125628s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:00:34.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-520" for this suite.

• [SLOW TEST:53.654 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":3943,"failed":0}
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:00:34.764: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:00:35.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4042" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":224,"skipped":3943,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:00:35.257: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Feb 22 10:00:35.419: INFO: Creating simple deployment test-deployment-pghns
Feb 22 10:00:35.454: INFO: new replicaset for deployment "test-deployment-pghns" is yet to be created
Feb 22 10:00:37.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120835, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120835, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120836, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120835, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-pghns-794dd694d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:00:39.520: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120835, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120835, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120836, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120835, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-pghns-794dd694d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Feb 22 10:00:41.775: INFO: Deployment test-deployment-pghns has Conditions: [{Available True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pghns-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Feb 22 10:00:41.806: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120839, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120839, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120839, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781120835, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pghns-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Feb 22 10:00:41.815: INFO: Observed &Deployment event: ADDED
Feb 22 10:00:41.816: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:35 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pghns-794dd694d8"}
Feb 22 10:00:41.816: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.816: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:35 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pghns-794dd694d8"}
Feb 22 10:00:41.816: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-22 10:00:35 +0000 UTC 2022-02-22 10:00:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 22 10:00:41.816: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.816: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-22 10:00:35 +0000 UTC 2022-02-22 10:00:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 22 10:00:41.817: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:36 +0000 UTC 2022-02-22 10:00:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pghns-794dd694d8" is progressing.}
Feb 22 10:00:41.817: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.817: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 22 10:00:41.817: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pghns-794dd694d8" has successfully progressed.}
Feb 22 10:00:41.817: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.817: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 22 10:00:41.817: INFO: Observed Deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pghns-794dd694d8" has successfully progressed.}
Feb 22 10:00:41.817: INFO: Found Deployment test-deployment-pghns in namespace deployment-1747 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 22 10:00:41.817: INFO: Deployment test-deployment-pghns has an updated status
STEP: patching the Statefulset Status
Feb 22 10:00:41.817: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 22 10:00:41.849: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Feb 22 10:00:41.853: INFO: Observed &Deployment event: ADDED
Feb 22 10:00:41.853: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:35 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pghns-794dd694d8"}
Feb 22 10:00:41.854: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.854: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:35 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pghns-794dd694d8"}
Feb 22 10:00:41.854: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-22 10:00:35 +0000 UTC 2022-02-22 10:00:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 22 10:00:41.855: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.855: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-22 10:00:35 +0000 UTC 2022-02-22 10:00:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 22 10:00:41.855: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:36 +0000 UTC 2022-02-22 10:00:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pghns-794dd694d8" is progressing.}
Feb 22 10:00:41.855: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.855: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 22 10:00:41.855: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pghns-794dd694d8" has successfully progressed.}
Feb 22 10:00:41.855: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.855: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 22 10:00:41.855: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-22 10:00:39 +0000 UTC 2022-02-22 10:00:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pghns-794dd694d8" has successfully progressed.}
Feb 22 10:00:41.855: INFO: Observed deployment test-deployment-pghns in namespace deployment-1747 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 22 10:00:41.856: INFO: Observed &Deployment event: MODIFIED
Feb 22 10:00:41.856: INFO: Found deployment test-deployment-pghns in namespace deployment-1747 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 22 10:00:41.856: INFO: Deployment test-deployment-pghns has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 22 10:00:41.887: INFO: Deployment "test-deployment-pghns":
&Deployment{ObjectMeta:{test-deployment-pghns  deployment-1747  d737c645-f987-4a2d-af5c-0dd35bd6c427 59801 1 2022-02-22 10:00:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-02-22 10:00:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:00:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-02-22 10:00:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c7b1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 22 10:00:41.897: INFO: New ReplicaSet "test-deployment-pghns-794dd694d8" of Deployment "test-deployment-pghns":
&ReplicaSet{ObjectMeta:{test-deployment-pghns-794dd694d8  deployment-1747  7e7c8c6c-7a9e-49b7-8e75-3df0123e8cda 59776 1 2022-02-22 10:00:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pghns d737c645-f987-4a2d-af5c-0dd35bd6c427 0xc004c7b760 0xc004c7b761}] []  [{kube-controller-manager Update apps/v1 2022-02-22 10:00:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d737c645-f987-4a2d-af5c-0dd35bd6c427\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:00:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c7b808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 22 10:00:41.904: INFO: Pod "test-deployment-pghns-794dd694d8-98gc8" is available:
&Pod{ObjectMeta:{test-deployment-pghns-794dd694d8-98gc8 test-deployment-pghns-794dd694d8- deployment-1747  8a0cfd06-a705-4f50-bf2a-21dea099d9c7 59775 0 2022-02-22 10:00:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/containerID:2786f3129584ffa9dbe06bb04131f63c66aedcd0cfaaf1e6660f29e982997c82 cni.projectcalico.org/podIP:172.21.104.62/32 cni.projectcalico.org/podIPs:172.21.104.62/32] [{apps/v1 ReplicaSet test-deployment-pghns-794dd694d8 7e7c8c6c-7a9e-49b7-8e75-3df0123e8cda 0xc004c57ba0 0xc004c57ba1}] []  [{kube-controller-manager Update v1 2022-02-22 10:00:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e7c8c6c-7a9e-49b7-8e75-3df0123e8cda\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 10:00:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 10:00:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjbhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjbhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:00:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:00:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:00:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:00:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.62,StartTime:2022-02-22 10:00:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 10:00:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://cdf0b6cdaa80a8904ea1720333843f828936e4b982196bbbb487c724ea8012ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:00:41.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1747" for this suite.

• [SLOW TEST:6.728 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":225,"skipped":3952,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:00:41.986: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:00:42.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6318" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":226,"skipped":3967,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:00:42.263: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-5f31753c-67a5-456b-b869-1c010bacabe3
STEP: Creating a pod to test consume secrets
Feb 22 10:00:42.577: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f550e99c-ae4a-44fc-be4a-5cc5794598fe" in namespace "projected-3536" to be "Succeeded or Failed"
Feb 22 10:00:42.585: INFO: Pod "pod-projected-secrets-f550e99c-ae4a-44fc-be4a-5cc5794598fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.412061ms
Feb 22 10:00:44.597: INFO: Pod "pod-projected-secrets-f550e99c-ae4a-44fc-be4a-5cc5794598fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019789267s
Feb 22 10:00:46.621: INFO: Pod "pod-projected-secrets-f550e99c-ae4a-44fc-be4a-5cc5794598fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044040714s
STEP: Saw pod success
Feb 22 10:00:46.621: INFO: Pod "pod-projected-secrets-f550e99c-ae4a-44fc-be4a-5cc5794598fe" satisfied condition "Succeeded or Failed"
Feb 22 10:00:46.698: INFO: Trying to get logs from node node2 pod pod-projected-secrets-f550e99c-ae4a-44fc-be4a-5cc5794598fe container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:00:46.918: INFO: Waiting for pod pod-projected-secrets-f550e99c-ae4a-44fc-be4a-5cc5794598fe to disappear
Feb 22 10:00:46.935: INFO: Pod pod-projected-secrets-f550e99c-ae4a-44fc-be4a-5cc5794598fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:00:46.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3536" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":3995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:00:47.216: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:00:47.684: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 22 10:00:56.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-4918 --namespace=crd-publish-openapi-4918 create -f -'
Feb 22 10:00:58.461: INFO: stderr: ""
Feb 22 10:00:58.461: INFO: stdout: "e2e-test-crd-publish-openapi-7366-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 22 10:00:58.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-4918 --namespace=crd-publish-openapi-4918 delete e2e-test-crd-publish-openapi-7366-crds test-cr'
Feb 22 10:00:58.715: INFO: stderr: ""
Feb 22 10:00:58.715: INFO: stdout: "e2e-test-crd-publish-openapi-7366-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 22 10:00:58.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-4918 --namespace=crd-publish-openapi-4918 apply -f -'
Feb 22 10:00:59.160: INFO: stderr: ""
Feb 22 10:00:59.160: INFO: stdout: "e2e-test-crd-publish-openapi-7366-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 22 10:00:59.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-4918 --namespace=crd-publish-openapi-4918 delete e2e-test-crd-publish-openapi-7366-crds test-cr'
Feb 22 10:00:59.370: INFO: stderr: ""
Feb 22 10:00:59.370: INFO: stdout: "e2e-test-crd-publish-openapi-7366-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 22 10:00:59.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=crd-publish-openapi-4918 explain e2e-test-crd-publish-openapi-7366-crds'
Feb 22 10:00:59.817: INFO: stderr: ""
Feb 22 10:00:59.817: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7366-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:01:08.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4918" for this suite.

• [SLOW TEST:20.961 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":228,"skipped":4017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:01:08.179: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-b714ccb0-fb56-4ae4-b644-937b9c7adbfe
STEP: Creating a pod to test consume configMaps
Feb 22 10:01:08.718: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98ada98b-5ded-4a76-bcd9-f373f966a71d" in namespace "projected-2737" to be "Succeeded or Failed"
Feb 22 10:01:08.726: INFO: Pod "pod-projected-configmaps-98ada98b-5ded-4a76-bcd9-f373f966a71d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082263ms
Feb 22 10:01:10.744: INFO: Pod "pod-projected-configmaps-98ada98b-5ded-4a76-bcd9-f373f966a71d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026374137s
Feb 22 10:01:12.781: INFO: Pod "pod-projected-configmaps-98ada98b-5ded-4a76-bcd9-f373f966a71d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063005826s
STEP: Saw pod success
Feb 22 10:01:12.781: INFO: Pod "pod-projected-configmaps-98ada98b-5ded-4a76-bcd9-f373f966a71d" satisfied condition "Succeeded or Failed"
Feb 22 10:01:12.787: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-98ada98b-5ded-4a76-bcd9-f373f966a71d container agnhost-container: <nil>
STEP: delete the pod
Feb 22 10:01:13.258: INFO: Waiting for pod pod-projected-configmaps-98ada98b-5ded-4a76-bcd9-f373f966a71d to disappear
Feb 22 10:01:13.403: INFO: Pod pod-projected-configmaps-98ada98b-5ded-4a76-bcd9-f373f966a71d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:01:13.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2737" for this suite.

• [SLOW TEST:5.593 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:01:13.772: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-jb4x
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 10:01:14.238: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jb4x" in namespace "subpath-9170" to be "Succeeded or Failed"
Feb 22 10:01:14.300: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Pending", Reason="", readiness=false. Elapsed: 61.892214ms
Feb 22 10:01:16.328: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090226242s
Feb 22 10:01:18.344: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105706929s
Feb 22 10:01:20.355: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 6.117556633s
Feb 22 10:01:22.376: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 8.138334496s
Feb 22 10:01:24.395: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 10.15662147s
Feb 22 10:01:26.409: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 12.17154326s
Feb 22 10:01:28.429: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 14.19089893s
Feb 22 10:01:30.457: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 16.219400657s
Feb 22 10:01:32.474: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 18.235660041s
Feb 22 10:01:34.506: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 20.267714151s
Feb 22 10:01:36.602: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 22.363924166s
Feb 22 10:01:38.674: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Running", Reason="", readiness=true. Elapsed: 24.436508589s
Feb 22 10:01:40.731: INFO: Pod "pod-subpath-test-secret-jb4x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.493208786s
STEP: Saw pod success
Feb 22 10:01:40.731: INFO: Pod "pod-subpath-test-secret-jb4x" satisfied condition "Succeeded or Failed"
Feb 22 10:01:40.755: INFO: Trying to get logs from node node2 pod pod-subpath-test-secret-jb4x container test-container-subpath-secret-jb4x: <nil>
STEP: delete the pod
Feb 22 10:01:41.938: INFO: Waiting for pod pod-subpath-test-secret-jb4x to disappear
Feb 22 10:01:42.127: INFO: Pod pod-subpath-test-secret-jb4x no longer exists
STEP: Deleting pod pod-subpath-test-secret-jb4x
Feb 22 10:01:42.127: INFO: Deleting pod "pod-subpath-test-secret-jb4x" in namespace "subpath-9170"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:01:42.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9170" for this suite.

• [SLOW TEST:28.814 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":230,"skipped":4090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:01:42.617: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:03:01.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6014" for this suite.

• [SLOW TEST:79.088 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":231,"skipped":4141,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:03:01.707: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 22 10:03:02.245: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1019  1a7bc42a-1351-4e09-98c7-738dc9829b71 60215 0 2022-02-22 10:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-22 10:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 10:03:02.245: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1019  1a7bc42a-1351-4e09-98c7-738dc9829b71 60216 0 2022-02-22 10:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-22 10:03:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 22 10:03:02.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1019  1a7bc42a-1351-4e09-98c7-738dc9829b71 60218 0 2022-02-22 10:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-22 10:03:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 10:03:02.306: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1019  1a7bc42a-1351-4e09-98c7-738dc9829b71 60219 0 2022-02-22 10:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-22 10:03:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:03:02.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1019" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":232,"skipped":4190,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:03:02.335: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 22 10:03:07.205: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1687 pod-service-account-db9f6b49-1507-4478-a02b-217b84236ce7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 22 10:03:08.134: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1687 pod-service-account-db9f6b49-1507-4478-a02b-217b84236ce7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 22 10:03:08.897: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1687 pod-service-account-db9f6b49-1507-4478-a02b-217b84236ce7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:03:09.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1687" for this suite.

• [SLOW TEST:7.236 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":233,"skipped":4198,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:03:09.572: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Feb 22 10:03:09.742: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 22 10:03:09.742: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 22 10:03:09.785: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 22 10:03:09.785: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 22 10:03:09.898: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 22 10:03:09.899: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 22 10:03:10.066: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 22 10:03:10.066: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 22 10:03:13.367: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 22 10:03:13.367: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 22 10:03:13.490: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Feb 22 10:03:13.545: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Feb 22 10:03:13.553: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0
Feb 22 10:03:13.553: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 0
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:13.554: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:13.555: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:13.641: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:13.641: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:13.917: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:13.917: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:14.054: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:14.054: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:14.087: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:14.087: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:18.458: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:18.458: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:19.171: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
STEP: listing Deployments
Feb 22 10:03:19.440: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Feb 22 10:03:19.556: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Feb 22 10:03:19.583: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:19.740: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:20.153: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:20.345: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:20.773: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:28.312: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:29.240: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:30.300: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:30.759: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 22 10:03:37.322: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Feb 22 10:03:38.167: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:38.167: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:38.169: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:38.170: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:38.170: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 1
Feb 22 10:03:38.171: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:38.171: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 3
Feb 22 10:03:38.171: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:38.179: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 2
Feb 22 10:03:38.179: INFO: observed Deployment test-deployment in namespace deployment-2952 with ReadyReplicas 3
STEP: deleting the Deployment
Feb 22 10:03:38.381: INFO: observed event type MODIFIED
Feb 22 10:03:38.381: INFO: observed event type MODIFIED
Feb 22 10:03:38.384: INFO: observed event type MODIFIED
Feb 22 10:03:38.384: INFO: observed event type MODIFIED
Feb 22 10:03:38.384: INFO: observed event type MODIFIED
Feb 22 10:03:38.384: INFO: observed event type MODIFIED
Feb 22 10:03:38.384: INFO: observed event type MODIFIED
Feb 22 10:03:38.385: INFO: observed event type MODIFIED
Feb 22 10:03:38.385: INFO: observed event type MODIFIED
Feb 22 10:03:38.385: INFO: observed event type MODIFIED
Feb 22 10:03:38.385: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 22 10:03:38.597: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 22 10:03:38.694: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-2952  f30a0533-cb5f-4f85-b5c4-e787049061dd 60515 4 2022-02-22 10:03:13 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment dc3c7b58-fe46-492a-a83c-f365a7181fae 0xc006570697 0xc006570698}] []  [{kube-controller-manager Update apps/v1 2022-02-22 10:03:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc3c7b58-fe46-492a-a83c-f365a7181fae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:03:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006570720 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 22 10:03:38.754: INFO: pod: "test-deployment-56c98d85f9-4jmmd":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-4jmmd test-deployment-56c98d85f9- deployment-2952  6f676b46-064f-4a0f-bbfb-21106ab41eea 60520 0 2022-02-22 10:03:19 +0000 UTC 2022-02-22 10:03:38 +0000 UTC 0xc003631748 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:d6e9868f3006c1707dcfc29f119ef6e534fc08ef8c57cf2b05885f1f5a2d74f1 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 f30a0533-cb5f-4f85-b5c4-e787049061dd 0xc003631777 0xc003631778}] []  [{kube-controller-manager Update v1 2022-02-22 10:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f30a0533-cb5f-4f85-b5c4-e787049061dd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 10:03:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 10:03:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.40.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d85rr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d85rr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.21.40.161,StartTime:2022-02-22 10:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 10:03:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:docker-pullable://k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:docker://0129552fa54a0c0998f0fdb492d8ebceb1503adf36df971663e3cf0cca706c3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.40.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 22 10:03:38.758: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-2952  986862b0-612e-4386-8468-1a9696151326 60506 2 2022-02-22 10:03:19 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment dc3c7b58-fe46-492a-a83c-f365a7181fae 0xc006570787 0xc006570788}] []  [{kube-controller-manager Update apps/v1 2022-02-22 10:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc3c7b58-fe46-492a-a83c-f365a7181fae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:03:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006570810 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Feb 22 10:03:38.873: INFO: pod: "test-deployment-d4dfddfbf-klr84":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-klr84 test-deployment-d4dfddfbf- deployment-2952  73a0a31f-43bd-4396-9856-78f64c8d972a 60446 0 2022-02-22 10:03:19 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:bb14d80cfd1c85b6013e705b672aa697550cf61328a6b70678b698db2ecaec6f cni.projectcalico.org/podIP:172.21.104.11/32 cni.projectcalico.org/podIPs:172.21.104.11/32] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 986862b0-612e-4386-8468-1a9696151326 0xc006570ab7 0xc006570ab8}] []  [{kube-controller-manager Update v1 2022-02-22 10:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"986862b0-612e-4386-8468-1a9696151326\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 10:03:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 10:03:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmgc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmgc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.11,StartTime:2022-02-22 10:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 10:03:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://cfb7097313af7f8548569667f1a4978e977c73a48a1e06e3ce423d254d43ec16,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 22 10:03:38.874: INFO: pod: "test-deployment-d4dfddfbf-wqzkl":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-wqzkl test-deployment-d4dfddfbf- deployment-2952  a7546b0d-615e-448a-9895-e48b5c9138a2 60505 0 2022-02-22 10:03:29 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:bbd860456369fc428b7ff8d10575ca1f61c4a8743db5496c0604a2c723c3d143 cni.projectcalico.org/podIP:172.21.40.171/32 cni.projectcalico.org/podIPs:172.21.40.171/32] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 986862b0-612e-4386-8468-1a9696151326 0xc006570ce7 0xc006570ce8}] []  [{kube-controller-manager Update v1 2022-02-22 10:03:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"986862b0-612e-4386-8468-1a9696151326\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 10:03:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 10:03:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.40.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zsvch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zsvch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:03:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.21.40.171,StartTime:2022-02-22 10:03:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 10:03:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://b23cfbf8264443e1bcc12b8b4b872332ccac225fa99fa2e764b29d372e43de7b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.40.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:03:38.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2952" for this suite.

• [SLOW TEST:29.544 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":234,"skipped":4217,"failed":0}
S
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:03:39.116: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 22 10:03:40.036: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:03:48.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3022" for this suite.

• [SLOW TEST:9.620 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":235,"skipped":4218,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:03:48.736: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Feb 22 10:03:55.293: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9870 PodName:pod-sharedvolume-6e77f794-9159-4e31-875c-f5a61f11e22c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 10:03:55.293: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 10:03:56.022: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:03:56.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9870" for this suite.

• [SLOW TEST:7.359 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":236,"skipped":4230,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:03:56.095: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:03:56.355: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 22 10:03:56.448: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 22 10:04:01.466: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 22 10:04:01.466: INFO: Creating deployment "test-rolling-update-deployment"
Feb 22 10:04:01.502: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 22 10:04:01.642: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 22 10:04:03.663: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 22 10:04:03.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121042, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121042, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121042, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121041, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:04:05.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121042, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121042, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121042, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121041, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:04:07.690: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 22 10:04:07.710: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2816  747a1559-8863-4c4f-803c-3a5d73828377 60770 1 2022-02-22 10:04:01 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-02-22 10:04:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e478d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-22 10:04:02 +0000 UTC,LastTransitionTime:2022-02-22 10:04:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2022-02-22 10:04:06 +0000 UTC,LastTransitionTime:2022-02-22 10:04:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 22 10:04:07.717: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-2816  62baefe8-86d8-48e9-bd5f-97920de573f7 60760 1 2022-02-22 10:04:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 747a1559-8863-4c4f-803c-3a5d73828377 0xc002599c27 0xc002599c28}] []  [{kube-controller-manager Update apps/v1 2022-02-22 10:04:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"747a1559-8863-4c4f-803c-3a5d73828377\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:04:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002599cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 22 10:04:07.717: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 22 10:04:07.717: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2816  1437d6df-a152-434c-91af-2906566e2c59 60769 2 2022-02-22 10:03:56 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 747a1559-8863-4c4f-803c-3a5d73828377 0xc002599af7 0xc002599af8}] []  [{e2e.test Update apps/v1 2022-02-22 10:03:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"747a1559-8863-4c4f-803c-3a5d73828377\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002599bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 22 10:04:07.722: INFO: Pod "test-rolling-update-deployment-585b757574-rv2cr" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-rv2cr test-rolling-update-deployment-585b757574- deployment-2816  c08034ec-4c83-49ef-a43a-6f620dc2040e 60759 0 2022-02-22 10:04:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/containerID:f87a24098fea202d411d4d8807f4c3ca567c3c1fc07f6e0f9ccfca3c1bd9502f cni.projectcalico.org/podIP:172.21.104.21/32 cni.projectcalico.org/podIPs:172.21.104.21/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 62baefe8-86d8-48e9-bd5f-97920de573f7 0xc004e47ca7 0xc004e47ca8}] []  [{kube-controller-manager Update v1 2022-02-22 10:04:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62baefe8-86d8-48e9-bd5f-97920de573f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 10:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 10:04:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djr9p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djr9p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:04:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:04:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.21,StartTime:2022-02-22 10:04:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 10:04:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://e3f3d1fb016024cd94f8bb0f982478b79533ece67d7e4ffd01f1dd712c520932,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:04:07.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2816" for this suite.

• [SLOW TEST:11.666 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":237,"skipped":4249,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:04:07.761: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 22 10:04:08.069: INFO: Number of nodes with available pods: 0
Feb 22 10:04:08.069: INFO: Node node1 is running more than one daemon pod
Feb 22 10:04:09.101: INFO: Number of nodes with available pods: 0
Feb 22 10:04:09.101: INFO: Node node1 is running more than one daemon pod
Feb 22 10:04:10.149: INFO: Number of nodes with available pods: 0
Feb 22 10:04:10.149: INFO: Node node1 is running more than one daemon pod
Feb 22 10:04:11.188: INFO: Number of nodes with available pods: 0
Feb 22 10:04:11.188: INFO: Node node1 is running more than one daemon pod
Feb 22 10:04:12.180: INFO: Number of nodes with available pods: 2
Feb 22 10:04:12.180: INFO: Number of running nodes: 2, number of available pods: 2
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Feb 22 10:04:12.420: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60819"},"items":null}

Feb 22 10:04:12.467: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60820"},"items":[{"metadata":{"name":"daemon-set-lj72m","generateName":"daemon-set-","namespace":"daemonsets-2588","uid":"b06efb0e-216f-44dc-a19a-c1a729a0cf15","resourceVersion":"60820","creationTimestamp":"2022-02-22T10:04:08Z","deletionTimestamp":"2022-02-22T10:04:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a99b4f76a343c2396eaf801b5c7e60a0e21745645c98062666d1dc58ae6d8dd1","cni.projectcalico.org/podIP":"172.21.40.178/32","cni.projectcalico.org/podIPs":"172.21.40.178/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"af6e2a8f-639d-485e-9d65-519480547248","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-22T10:04:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af6e2a8f-639d-485e-9d65-519480547248\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-02-22T10:04:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-02-22T10:04:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.40.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vwhf5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vwhf5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-22T10:04:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-22T10:04:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-22T10:04:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-22T10:04:08Z"}],"hostIP":"172.28.128.12","podIP":"172.21.40.178","podIPs":[{"ip":"172.21.40.178"}],"startTime":"2022-02-22T10:04:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-22T10:04:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://8ba1662dc9f9a452bad2a7a30a5b2ce44aa039c848694841b9197160d14505b9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v25mn","generateName":"daemon-set-","namespace":"daemonsets-2588","uid":"3a8167ca-5535-405a-8d73-5f6d299be1fb","resourceVersion":"60817","creationTimestamp":"2022-02-22T10:04:08Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6302182dcce32d48cd9adc4811cc36d52b9102297ef4ae00aa4710d6400e89cb","cni.projectcalico.org/podIP":"172.21.104.24/32","cni.projectcalico.org/podIPs":"172.21.104.24/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"af6e2a8f-639d-485e-9d65-519480547248","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-22T10:04:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af6e2a8f-639d-485e-9d65-519480547248\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-02-22T10:04:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-02-22T10:04:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s8zb5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s8zb5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-22T10:04:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-22T10:04:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-22T10:04:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-22T10:04:08Z"}],"hostIP":"172.28.128.13","podIP":"172.21.104.24","podIPs":[{"ip":"172.21.104.24"}],"startTime":"2022-02-22T10:04:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-22T10:04:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://cd3228f035032ef5a2335e21f0792f3afabad419aa8f9866dc010d91fc0d67e2","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:04:12.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2588" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":238,"skipped":4262,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:04:12.613: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8832
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8832
I0222 10:04:13.441935      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8832, replica count: 2
I0222 10:04:16.498962      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 10:04:19.499653      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 10:04:22.503390      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 10:04:22.503: INFO: Creating new exec pod
Feb 22 10:04:27.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-8832 exec execpodmcnrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 22 10:04:28.287: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 22 10:04:28.287: INFO: stdout: ""
Feb 22 10:04:29.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-8832 exec execpodmcnrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 22 10:04:30.057: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 22 10:04:30.057: INFO: stdout: ""
Feb 22 10:04:30.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-8832 exec execpodmcnrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 22 10:04:31.059: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 22 10:04:31.059: INFO: stdout: "externalname-service-wpb7l"
Feb 22 10:04:31.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-8832 exec execpodmcnrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.104.190 80'
Feb 22 10:04:32.056: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.104.190 80\nConnection to 172.20.104.190 80 port [tcp/http] succeeded!\n"
Feb 22 10:04:32.056: INFO: stdout: ""
Feb 22 10:04:33.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-8832 exec execpodmcnrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.104.190 80'
Feb 22 10:04:33.857: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.104.190 80\nConnection to 172.20.104.190 80 port [tcp/http] succeeded!\n"
Feb 22 10:04:33.857: INFO: stdout: ""
Feb 22 10:04:34.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-8832 exec execpodmcnrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.104.190 80'
Feb 22 10:04:34.739: INFO: stderr: "+ nc -v -t -w 2 172.20.104.190 80\n+ echo hostName\nConnection to 172.20.104.190 80 port [tcp/http] succeeded!\n"
Feb 22 10:04:34.739: INFO: stdout: "externalname-service-wpb7l"
Feb 22 10:04:34.739: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:04:34.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8832" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:22.382 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":239,"skipped":4266,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:04:34.996: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3135e9b5-8f90-47c0-ab83-f117d553e738
STEP: Creating the pod
Feb 22 10:04:35.370: INFO: The status of Pod pod-projected-configmaps-9d681cac-37ed-473b-9725-c8471b554285 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:04:37.386: INFO: The status of Pod pod-projected-configmaps-9d681cac-37ed-473b-9725-c8471b554285 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:04:39.383: INFO: The status of Pod pod-projected-configmaps-9d681cac-37ed-473b-9725-c8471b554285 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-3135e9b5-8f90-47c0-ab83-f117d553e738
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:04:43.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3208" for this suite.

• [SLOW TEST:8.736 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":240,"skipped":4278,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:04:43.732: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:04:44.148: INFO: Waiting up to 5m0s for pod "busybox-user-65534-f9be5bb4-d52f-4290-9e90-2370e91985c1" in namespace "security-context-test-4815" to be "Succeeded or Failed"
Feb 22 10:04:44.154: INFO: Pod "busybox-user-65534-f9be5bb4-d52f-4290-9e90-2370e91985c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.950272ms
Feb 22 10:04:46.180: INFO: Pod "busybox-user-65534-f9be5bb4-d52f-4290-9e90-2370e91985c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031535813s
Feb 22 10:04:48.193: INFO: Pod "busybox-user-65534-f9be5bb4-d52f-4290-9e90-2370e91985c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044674111s
Feb 22 10:04:50.205: INFO: Pod "busybox-user-65534-f9be5bb4-d52f-4290-9e90-2370e91985c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056098517s
Feb 22 10:04:50.205: INFO: Pod "busybox-user-65534-f9be5bb4-d52f-4290-9e90-2370e91985c1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:04:50.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4815" for this suite.

• [SLOW TEST:6.596 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:50
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4285,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:04:50.328: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:04:50.535: INFO: Creating deployment "test-recreate-deployment"
Feb 22 10:04:50.689: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 22 10:04:50.710: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Feb 22 10:04:52.762: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 22 10:04:52.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121090, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121090, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121091, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121090, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:04:54.839: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 22 10:04:54.867: INFO: Updating deployment test-recreate-deployment
Feb 22 10:04:54.867: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 22 10:04:55.876: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8954  d9bdbf8a-dc33-43b0-9c43-7bc7684c632f 61151 2 2022-02-22 10:04:50 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-22 10:04:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ae7a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-02-22 10:04:55 +0000 UTC,LastTransitionTime:2022-02-22 10:04:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-02-22 10:04:55 +0000 UTC,LastTransitionTime:2022-02-22 10:04:50 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 22 10:04:55.952: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-8954  cca0623a-7617-4cb1-84dc-1207627cbff1 61146 1 2022-02-22 10:04:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d9bdbf8a-dc33-43b0-9c43-7bc7684c632f 0xc004ae7f20 0xc004ae7f21}] []  [{kube-controller-manager Update apps/v1 2022-02-22 10:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bdbf8a-dc33-43b0-9c43-7bc7684c632f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:04:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ae7fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 22 10:04:55.952: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 22 10:04:55.952: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-8954  8a625e41-b389-499e-b3d7-8ed007c46201 61138 2 2022-02-22 10:04:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d9bdbf8a-dc33-43b0-9c43-7bc7684c632f 0xc004ae7e07 0xc004ae7e08}] []  [{kube-controller-manager Update apps/v1 2022-02-22 10:04:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bdbf8a-dc33-43b0-9c43-7bc7684c632f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:04:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ae7eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 22 10:04:55.994: INFO: Pod "test-recreate-deployment-85d47dcb4-fnn72" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-fnn72 test-recreate-deployment-85d47dcb4- deployment-8954  aaa61496-b3b2-4c2f-80bf-5607905778b0 61149 0 2022-02-22 10:04:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 cca0623a-7617-4cb1-84dc-1207627cbff1 0xc006518490 0xc006518491}] []  [{kube-controller-manager Update v1 2022-02-22 10:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca0623a-7617-4cb1-84dc-1207627cbff1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-22 10:04:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-88d25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-88d25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:04:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:04:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:04:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:04:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2022-02-22 10:04:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:04:55.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8954" for this suite.

• [SLOW TEST:5.741 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":242,"skipped":4300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:04:56.072: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 22 10:04:56.729: INFO: Number of nodes with available pods: 0
Feb 22 10:04:56.729: INFO: Node node1 is running more than one daemon pod
Feb 22 10:04:57.865: INFO: Number of nodes with available pods: 0
Feb 22 10:04:57.865: INFO: Node node1 is running more than one daemon pod
Feb 22 10:04:58.834: INFO: Number of nodes with available pods: 0
Feb 22 10:04:58.834: INFO: Node node1 is running more than one daemon pod
Feb 22 10:04:59.772: INFO: Number of nodes with available pods: 0
Feb 22 10:04:59.772: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:00.972: INFO: Number of nodes with available pods: 0
Feb 22 10:05:00.972: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:01.776: INFO: Number of nodes with available pods: 2
Feb 22 10:05:01.776: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 22 10:05:01.924: INFO: Number of nodes with available pods: 1
Feb 22 10:05:01.925: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:03.126: INFO: Number of nodes with available pods: 1
Feb 22 10:05:03.126: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:03.966: INFO: Number of nodes with available pods: 1
Feb 22 10:05:03.966: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:04.945: INFO: Number of nodes with available pods: 1
Feb 22 10:05:04.945: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:05.953: INFO: Number of nodes with available pods: 1
Feb 22 10:05:05.953: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:06.960: INFO: Number of nodes with available pods: 1
Feb 22 10:05:06.960: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:07.974: INFO: Number of nodes with available pods: 1
Feb 22 10:05:07.974: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:08.949: INFO: Number of nodes with available pods: 1
Feb 22 10:05:08.949: INFO: Node node1 is running more than one daemon pod
Feb 22 10:05:09.952: INFO: Number of nodes with available pods: 2
Feb 22 10:05:09.952: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7645, will wait for the garbage collector to delete the pods
Feb 22 10:05:10.060: INFO: Deleting DaemonSet.extensions daemon-set took: 35.277737ms
Feb 22 10:05:10.261: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.579874ms
Feb 22 10:05:14.871: INFO: Number of nodes with available pods: 0
Feb 22 10:05:14.871: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 10:05:14.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61301"},"items":null}

Feb 22 10:05:14.882: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61301"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:05:14.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7645" for this suite.

• [SLOW TEST:18.935 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":243,"skipped":4359,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:05:15.007: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:05:15.531: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 22 10:05:20.561: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 22 10:05:20.562: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 22 10:05:22.575: INFO: Creating deployment "test-rollover-deployment"
Feb 22 10:05:22.681: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 22 10:05:24.719: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 22 10:05:24.734: INFO: Ensure that both replica sets have 1 created replica
Feb 22 10:05:24.747: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 22 10:05:24.798: INFO: Updating deployment test-rollover-deployment
Feb 22 10:05:24.798: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 22 10:05:26.826: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 22 10:05:26.856: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 22 10:05:26.876: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 10:05:26.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121125, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121122, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:05:28.994: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 10:05:28.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121125, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121122, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:05:30.900: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 10:05:30.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121129, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121122, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:05:32.931: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 10:05:32.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121129, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121122, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:05:34.901: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 10:05:34.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121129, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121122, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:05:36.981: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 10:05:36.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121129, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121122, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:05:38.894: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 10:05:38.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121123, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121129, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121122, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:05:40.923: INFO: 
Feb 22 10:05:40.923: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 22 10:05:40.943: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8960  19ed4288-ca9f-4523-82b9-8346aaac940d 61441 2 2022-02-22 10:05:22 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-22 10:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:05:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006571c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-22 10:05:23 +0000 UTC,LastTransitionTime:2022-02-22 10:05:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2022-02-22 10:05:40 +0000 UTC,LastTransitionTime:2022-02-22 10:05:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 22 10:05:40.952: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-8960  10568a7c-7482-45e9-8ab6-5c32fac81116 61431 2 2022-02-22 10:05:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 19ed4288-ca9f-4523-82b9-8346aaac940d 0xc003c00310 0xc003c00311}] []  [{kube-controller-manager Update apps/v1 2022-02-22 10:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19ed4288-ca9f-4523-82b9-8346aaac940d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:05:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c003a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 22 10:05:40.952: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 22 10:05:40.952: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8960  03632894-f66b-45fc-8e6e-581570e20d27 61440 2 2022-02-22 10:05:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 19ed4288-ca9f-4523-82b9-8346aaac940d 0xc003c000c7 0xc003c000c8}] []  [{e2e.test Update apps/v1 2022-02-22 10:05:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:05:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19ed4288-ca9f-4523-82b9-8346aaac940d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:05:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c00188 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 22 10:05:40.952: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8960  135a2485-bff4-44a7-90b1-d6c65c8e3217 61398 2 2022-02-22 10:05:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 19ed4288-ca9f-4523-82b9-8346aaac940d 0xc003c001f7 0xc003c001f8}] []  [{kube-controller-manager Update apps/v1 2022-02-22 10:05:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19ed4288-ca9f-4523-82b9-8346aaac940d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-22 10:05:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c002a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 22 10:05:40.971: INFO: Pod "test-rollover-deployment-98c5f4599-pcpmr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-pcpmr test-rollover-deployment-98c5f4599- deployment-8960  c2830282-056d-4f7b-9b00-d86e48cb6059 61415 0 2022-02-22 10:05:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/containerID:ad3ee965bfcb95da9f2ce595cae547ed034e3873e4c09caaaa3b5314d76bbf20 cni.projectcalico.org/podIP:172.21.104.20/32 cni.projectcalico.org/podIPs:172.21.104.20/32] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 10568a7c-7482-45e9-8ab6-5c32fac81116 0xc003c008f0 0xc003c008f1}] []  [{kube-controller-manager Update v1 2022-02-22 10:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10568a7c-7482-45e9-8ab6-5c32fac81116\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-22 10:05:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-02-22 10:05:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.104.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vvfg9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vvfg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:05:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:05:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-22 10:05:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.21.104.20,StartTime:2022-02-22 10:05:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-22 10:05:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://0b3ddf7879dea73559aeda7abf88580b98ab9f092cd48ffdab8fd40d3a7f5647,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.104.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:05:40.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8960" for this suite.

• [SLOW TEST:26.033 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":244,"skipped":4371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:05:41.040: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-e1c03c8c-47f6-4f20-b022-22832ba870aa
STEP: Creating a pod to test consume configMaps
Feb 22 10:05:41.722: INFO: Waiting up to 5m0s for pod "pod-configmaps-c7f4ced5-9812-4692-ac7f-2f68c0180797" in namespace "configmap-3187" to be "Succeeded or Failed"
Feb 22 10:05:41.731: INFO: Pod "pod-configmaps-c7f4ced5-9812-4692-ac7f-2f68c0180797": Phase="Pending", Reason="", readiness=false. Elapsed: 8.935058ms
Feb 22 10:05:43.749: INFO: Pod "pod-configmaps-c7f4ced5-9812-4692-ac7f-2f68c0180797": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026910234s
Feb 22 10:05:45.769: INFO: Pod "pod-configmaps-c7f4ced5-9812-4692-ac7f-2f68c0180797": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047533898s
STEP: Saw pod success
Feb 22 10:05:45.769: INFO: Pod "pod-configmaps-c7f4ced5-9812-4692-ac7f-2f68c0180797" satisfied condition "Succeeded or Failed"
Feb 22 10:05:45.934: INFO: Trying to get logs from node node2 pod pod-configmaps-c7f4ced5-9812-4692-ac7f-2f68c0180797 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 10:05:46.226: INFO: Waiting for pod pod-configmaps-c7f4ced5-9812-4692-ac7f-2f68c0180797 to disappear
Feb 22 10:05:46.281: INFO: Pod pod-configmaps-c7f4ced5-9812-4692-ac7f-2f68c0180797 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:05:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3187" for this suite.

• [SLOW TEST:5.454 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:05:46.506: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-3732
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3732 to expose endpoints map[]
Feb 22 10:05:47.354: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb 22 10:05:48.441: INFO: successfully validated that service multi-endpoint-test in namespace services-3732 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3732
Feb 22 10:05:48.708: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:05:50.717: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:05:52.734: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:05:54.721: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3732 to expose endpoints map[pod1:[100]]
Feb 22 10:05:54.761: INFO: successfully validated that service multi-endpoint-test in namespace services-3732 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3732
Feb 22 10:05:54.803: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:05:56.820: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:05:58.843: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3732 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 22 10:05:58.920: INFO: successfully validated that service multi-endpoint-test in namespace services-3732 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Feb 22 10:05:58.920: INFO: Creating new exec pod
Feb 22 10:06:03.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3732 exec execpod2gjpc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Feb 22 10:06:04.724: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 22 10:06:04.724: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:06:04.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3732 exec execpod2gjpc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.98.95 80'
Feb 22 10:06:05.415: INFO: stderr: "+ nc -v -t -w 2 172.20.98.95 80\nConnection to 172.20.98.95 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Feb 22 10:06:05.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:06:05.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3732 exec execpod2gjpc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Feb 22 10:06:06.118: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 22 10:06:06.118: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:06:06.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-3732 exec execpod2gjpc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.98.95 81'
Feb 22 10:06:06.845: INFO: stderr: "+ nc -v -t -w 2 172.20.98.95 81\n+ echo hostName\nConnection to 172.20.98.95 81 port [tcp/*] succeeded!\n"
Feb 22 10:06:06.845: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3732
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3732 to expose endpoints map[pod2:[101]]
Feb 22 10:06:07.037: INFO: successfully validated that service multi-endpoint-test in namespace services-3732 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3732
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3732 to expose endpoints map[]
Feb 22 10:06:07.236: INFO: successfully validated that service multi-endpoint-test in namespace services-3732 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:06:07.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3732" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:21.046 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":246,"skipped":4425,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:06:07.552: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 10:06:08.931: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 22 10:06:11.005: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121169, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121169, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121169, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121168, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:06:13.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121169, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121169, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121169, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121168, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 10:06:16.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:06:16.062: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:06:20.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7180" for this suite.
STEP: Destroying namespace "webhook-7180-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.031 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":247,"skipped":4433,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:06:21.583: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Feb 22 10:06:22.715: INFO: Major version: 1
STEP: Confirm minor version
Feb 22 10:06:22.715: INFO: cleanMinorVersion: 22
Feb 22 10:06:22.715: INFO: Minor version: 22+
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:06:22.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8186" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":248,"skipped":4440,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:06:22.876: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-bf58a2e6-790c-46c3-82b6-0033c6bb51c8
STEP: Creating a pod to test consume configMaps
Feb 22 10:06:24.091: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3" in namespace "projected-8845" to be "Succeeded or Failed"
Feb 22 10:06:24.155: INFO: Pod "pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 64.762901ms
Feb 22 10:06:27.424: INFO: Pod "pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3329807s
Feb 22 10:06:29.456: INFO: Pod "pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.365035311s
Feb 22 10:06:31.477: INFO: Pod "pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.38658407s
Feb 22 10:06:33.497: INFO: Pod "pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.406500337s
STEP: Saw pod success
Feb 22 10:06:33.497: INFO: Pod "pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3" satisfied condition "Succeeded or Failed"
Feb 22 10:06:33.505: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 10:06:33.665: INFO: Waiting for pod pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3 to disappear
Feb 22 10:06:33.684: INFO: Pod pod-projected-configmaps-3fdf2c5e-42de-4bb2-909c-58663e8f4ec3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:06:33.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8845" for this suite.

• [SLOW TEST:10.905 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":249,"skipped":4453,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:06:33.781: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Feb 22 10:06:40.950: INFO: 0 pods remaining
Feb 22 10:06:40.950: INFO: 0 pods has nil DeletionTimestamp
Feb 22 10:06:40.950: INFO: 
STEP: Gathering metrics
W0222 10:06:42.376329      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 22 10:06:42.376: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:06:42.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2907" for this suite.

• [SLOW TEST:9.047 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":250,"skipped":4459,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:06:42.829: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-a03e0b4d-bfbd-4e30-8e54-b6ff77f6260c
STEP: Creating a pod to test consume configMaps
Feb 22 10:06:43.481: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687" in namespace "projected-3556" to be "Succeeded or Failed"
Feb 22 10:06:43.549: INFO: Pod "pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687": Phase="Pending", Reason="", readiness=false. Elapsed: 67.881586ms
Feb 22 10:06:45.593: INFO: Pod "pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111017145s
Feb 22 10:06:47.610: INFO: Pod "pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687": Phase="Pending", Reason="", readiness=false. Elapsed: 4.128095425s
Feb 22 10:06:49.668: INFO: Pod "pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687": Phase="Pending", Reason="", readiness=false. Elapsed: 6.186917012s
Feb 22 10:06:51.684: INFO: Pod "pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.202159301s
STEP: Saw pod success
Feb 22 10:06:51.684: INFO: Pod "pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687" satisfied condition "Succeeded or Failed"
Feb 22 10:06:51.698: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687 container agnhost-container: <nil>
STEP: delete the pod
Feb 22 10:06:51.831: INFO: Waiting for pod pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687 to disappear
Feb 22 10:06:51.882: INFO: Pod pod-projected-configmaps-be2412a1-5c99-451b-a17c-98263b8d3687 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:06:51.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3556" for this suite.

• [SLOW TEST:9.095 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":251,"skipped":4463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:06:51.925: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 22 10:06:52.197: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 22 10:06:57.218: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:06:58.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2933" for this suite.

• [SLOW TEST:6.394 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":252,"skipped":4487,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:06:58.320: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Feb 22 10:06:58.524: INFO: created test-event-1
Feb 22 10:06:58.614: INFO: created test-event-2
Feb 22 10:06:58.682: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Feb 22 10:06:58.771: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Feb 22 10:06:58.863: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:06:58.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1093" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":253,"skipped":4499,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:06:58.927: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Feb 22 10:06:59.105: INFO: namespace kubectl-4069
Feb 22 10:06:59.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4069 create -f -'
Feb 22 10:07:01.515: INFO: stderr: ""
Feb 22 10:07:01.515: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 22 10:07:02.541: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 10:07:02.542: INFO: Found 0 / 1
Feb 22 10:07:03.552: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 10:07:03.552: INFO: Found 0 / 1
Feb 22 10:07:04.530: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 10:07:04.530: INFO: Found 0 / 1
Feb 22 10:07:05.528: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 10:07:05.528: INFO: Found 0 / 1
Feb 22 10:07:06.526: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 10:07:06.527: INFO: Found 1 / 1
Feb 22 10:07:06.527: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 22 10:07:06.532: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 22 10:07:06.532: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 22 10:07:06.532: INFO: wait on agnhost-primary startup in kubectl-4069 
Feb 22 10:07:06.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4069 logs agnhost-primary-2jnjg agnhost-primary'
Feb 22 10:07:06.746: INFO: stderr: ""
Feb 22 10:07:06.746: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 22 10:07:06.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4069 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 22 10:07:07.032: INFO: stderr: ""
Feb 22 10:07:07.032: INFO: stdout: "service/rm2 exposed\n"
Feb 22 10:07:07.046: INFO: Service rm2 in namespace kubectl-4069 found.
STEP: exposing service
Feb 22 10:07:09.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4069 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 22 10:07:09.328: INFO: stderr: ""
Feb 22 10:07:09.328: INFO: stdout: "service/rm3 exposed\n"
Feb 22 10:07:09.381: INFO: Service rm3 in namespace kubectl-4069 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:07:11.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4069" for this suite.

• [SLOW TEST:12.520 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":254,"skipped":4507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:07:11.448: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:07:11.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7686" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":255,"skipped":4539,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:07:11.937: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-207111fa-3b43-4640-87b7-4dc78a47cf06
STEP: Creating the pod
Feb 22 10:07:12.234: INFO: The status of Pod pod-configmaps-fd5f0a14-849f-409e-b8b9-4d3b6ac92ad4 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:07:14.242: INFO: The status of Pod pod-configmaps-fd5f0a14-849f-409e-b8b9-4d3b6ac92ad4 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:07:16.260: INFO: The status of Pod pod-configmaps-fd5f0a14-849f-409e-b8b9-4d3b6ac92ad4 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-207111fa-3b43-4640-87b7-4dc78a47cf06
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:07:20.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8330" for this suite.

• [SLOW TEST:8.805 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4554,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:07:20.743: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-f66b3c89-341d-4c3a-a8e5-fdd51882a0a0
STEP: Creating a pod to test consume secrets
Feb 22 10:07:21.408: INFO: Waiting up to 5m0s for pod "pod-secrets-2d90da57-e2ce-4102-888d-c3904b1a7cb2" in namespace "secrets-6918" to be "Succeeded or Failed"
Feb 22 10:07:21.470: INFO: Pod "pod-secrets-2d90da57-e2ce-4102-888d-c3904b1a7cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 62.581511ms
Feb 22 10:07:23.493: INFO: Pod "pod-secrets-2d90da57-e2ce-4102-888d-c3904b1a7cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085492364s
Feb 22 10:07:25.502: INFO: Pod "pod-secrets-2d90da57-e2ce-4102-888d-c3904b1a7cb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.094212883s
STEP: Saw pod success
Feb 22 10:07:25.502: INFO: Pod "pod-secrets-2d90da57-e2ce-4102-888d-c3904b1a7cb2" satisfied condition "Succeeded or Failed"
Feb 22 10:07:25.557: INFO: Trying to get logs from node node2 pod pod-secrets-2d90da57-e2ce-4102-888d-c3904b1a7cb2 container secret-env-test: <nil>
STEP: delete the pod
Feb 22 10:07:25.660: INFO: Waiting for pod pod-secrets-2d90da57-e2ce-4102-888d-c3904b1a7cb2 to disappear
Feb 22 10:07:25.699: INFO: Pod pod-secrets-2d90da57-e2ce-4102-888d-c3904b1a7cb2 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:07:25.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6918" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4565,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:07:25.734: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 10:07:27.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 22 10:07:29.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121247, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121247, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121247, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121247, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 10:07:31.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121247, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121247, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121247, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121247, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 10:07:34.523: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:07:47.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2856" for this suite.
STEP: Destroying namespace "webhook-2856-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:22.430 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":258,"skipped":4580,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:07:48.164: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 22 10:07:48.504: INFO: The status of Pod pod-update-activedeadlineseconds-708f2d06-1202-4470-8a1e-5b2f8ff93a27 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:07:50.521: INFO: The status of Pod pod-update-activedeadlineseconds-708f2d06-1202-4470-8a1e-5b2f8ff93a27 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:07:52.517: INFO: The status of Pod pod-update-activedeadlineseconds-708f2d06-1202-4470-8a1e-5b2f8ff93a27 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 22 10:07:53.113: INFO: Successfully updated pod "pod-update-activedeadlineseconds-708f2d06-1202-4470-8a1e-5b2f8ff93a27"
Feb 22 10:07:53.113: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-708f2d06-1202-4470-8a1e-5b2f8ff93a27" in namespace "pods-6534" to be "terminated due to deadline exceeded"
Feb 22 10:07:53.121: INFO: Pod "pod-update-activedeadlineseconds-708f2d06-1202-4470-8a1e-5b2f8ff93a27": Phase="Running", Reason="", readiness=true. Elapsed: 7.462866ms
Feb 22 10:07:55.142: INFO: Pod "pod-update-activedeadlineseconds-708f2d06-1202-4470-8a1e-5b2f8ff93a27": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 2.028504428s
Feb 22 10:07:55.142: INFO: Pod "pod-update-activedeadlineseconds-708f2d06-1202-4470-8a1e-5b2f8ff93a27" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:07:55.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6534" for this suite.

• [SLOW TEST:7.018 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":259,"skipped":4582,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:07:55.182: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9357
STEP: creating service affinity-nodeport-transition in namespace services-9357
STEP: creating replication controller affinity-nodeport-transition in namespace services-9357
I0222 10:07:55.570233      19 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9357, replica count: 3
I0222 10:07:58.629703      19 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 10:08:01.630801      19 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 10:08:01.953: INFO: Creating new exec pod
Feb 22 10:08:07.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-9357 exec execpod-affinityjgsbr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Feb 22 10:08:07.712: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 22 10:08:07.712: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:08:07.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-9357 exec execpod-affinityjgsbr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.34.115 80'
Feb 22 10:08:08.392: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.34.115 80\nConnection to 172.20.34.115 80 port [tcp/http] succeeded!\n"
Feb 22 10:08:08.392: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:08:08.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-9357 exec execpod-affinityjgsbr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 32766'
Feb 22 10:08:09.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 32766\nConnection to 172.28.128.12 32766 port [tcp/*] succeeded!\n"
Feb 22 10:08:09.232: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:08:09.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-9357 exec execpod-affinityjgsbr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 32766'
Feb 22 10:08:09.901: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.13 32766\nConnection to 172.28.128.13 32766 port [tcp/*] succeeded!\n"
Feb 22 10:08:09.901: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:08:09.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-9357 exec execpod-affinityjgsbr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.28.128.12:32766/ ; done'
Feb 22 10:08:10.923: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n"
Feb 22 10:08:10.923: INFO: stdout: "\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-j7tb8\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-blrdn\naffinity-nodeport-transition-blrdn\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-blrdn\naffinity-nodeport-transition-j7tb8\naffinity-nodeport-transition-j7tb8\naffinity-nodeport-transition-blrdn\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-blrdn\naffinity-nodeport-transition-blrdn\naffinity-nodeport-transition-j7tb8\naffinity-nodeport-transition-blrdn\naffinity-nodeport-transition-j7tb8"
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-j7tb8
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-blrdn
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-blrdn
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-blrdn
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-j7tb8
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-j7tb8
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-blrdn
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-blrdn
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-blrdn
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-j7tb8
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-blrdn
Feb 22 10:08:10.923: INFO: Received response from host: affinity-nodeport-transition-j7tb8
Feb 22 10:08:10.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-9357 exec execpod-affinityjgsbr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.28.128.12:32766/ ; done'
Feb 22 10:08:12.198: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32766/\n"
Feb 22 10:08:12.198: INFO: stdout: "\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd\naffinity-nodeport-transition-6rnqd"
Feb 22 10:08:12.198: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.198: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Received response from host: affinity-nodeport-transition-6rnqd
Feb 22 10:08:12.199: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9357, will wait for the garbage collector to delete the pods
Feb 22 10:08:12.447: INFO: Deleting ReplicationController affinity-nodeport-transition took: 24.836185ms
Feb 22 10:08:12.653: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 205.500951ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:17.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9357" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:22.525 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":260,"skipped":4589,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:17.708: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Feb 22 10:08:17.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-8099 create -f -'
Feb 22 10:08:18.496: INFO: stderr: ""
Feb 22 10:08:18.496: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Feb 22 10:08:18.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-8099 diff -f -'
Feb 22 10:08:19.493: INFO: rc: 1
Feb 22 10:08:19.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-8099 delete -f -'
Feb 22 10:08:19.859: INFO: stderr: ""
Feb 22 10:08:19.859: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:19.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8099" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":261,"skipped":4596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:20.067: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 22 10:08:25.619: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:25.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5122" for this suite.

• [SLOW TEST:5.870 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":4632,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:25.937: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 22 10:08:26.216: INFO: Waiting up to 5m0s for pod "pod-417809f2-86db-4f09-a3e7-d599741fa51c" in namespace "emptydir-8450" to be "Succeeded or Failed"
Feb 22 10:08:26.306: INFO: Pod "pod-417809f2-86db-4f09-a3e7-d599741fa51c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.927785ms
Feb 22 10:08:28.326: INFO: Pod "pod-417809f2-86db-4f09-a3e7-d599741fa51c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109393154s
Feb 22 10:08:30.354: INFO: Pod "pod-417809f2-86db-4f09-a3e7-d599741fa51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.138221679s
STEP: Saw pod success
Feb 22 10:08:30.355: INFO: Pod "pod-417809f2-86db-4f09-a3e7-d599741fa51c" satisfied condition "Succeeded or Failed"
Feb 22 10:08:30.383: INFO: Trying to get logs from node node2 pod pod-417809f2-86db-4f09-a3e7-d599741fa51c container test-container: <nil>
STEP: delete the pod
Feb 22 10:08:30.698: INFO: Waiting for pod pod-417809f2-86db-4f09-a3e7-d599741fa51c to disappear
Feb 22 10:08:30.740: INFO: Pod pod-417809f2-86db-4f09-a3e7-d599741fa51c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:30.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8450" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":4641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:30.793: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:31.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6980" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":264,"skipped":4664,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:31.236: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 22 10:08:31.456: INFO: Waiting up to 5m0s for pod "pod-ffacf13f-1d99-47ab-a21d-fc160de34d7e" in namespace "emptydir-3380" to be "Succeeded or Failed"
Feb 22 10:08:31.485: INFO: Pod "pod-ffacf13f-1d99-47ab-a21d-fc160de34d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.769667ms
Feb 22 10:08:33.507: INFO: Pod "pod-ffacf13f-1d99-47ab-a21d-fc160de34d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050881524s
Feb 22 10:08:35.519: INFO: Pod "pod-ffacf13f-1d99-47ab-a21d-fc160de34d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063667524s
STEP: Saw pod success
Feb 22 10:08:35.520: INFO: Pod "pod-ffacf13f-1d99-47ab-a21d-fc160de34d7e" satisfied condition "Succeeded or Failed"
Feb 22 10:08:35.569: INFO: Trying to get logs from node node2 pod pod-ffacf13f-1d99-47ab-a21d-fc160de34d7e container test-container: <nil>
STEP: delete the pod
Feb 22 10:08:35.743: INFO: Waiting for pod pod-ffacf13f-1d99-47ab-a21d-fc160de34d7e to disappear
Feb 22 10:08:35.757: INFO: Pod pod-ffacf13f-1d99-47ab-a21d-fc160de34d7e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:35.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3380" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":4664,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:35.788: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 10:08:36.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2a4faaa-779d-4e39-a6ba-4305f34a2f70" in namespace "projected-7457" to be "Succeeded or Failed"
Feb 22 10:08:36.133: INFO: Pod "downwardapi-volume-a2a4faaa-779d-4e39-a6ba-4305f34a2f70": Phase="Pending", Reason="", readiness=false. Elapsed: 11.914945ms
Feb 22 10:08:38.161: INFO: Pod "downwardapi-volume-a2a4faaa-779d-4e39-a6ba-4305f34a2f70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039005579s
Feb 22 10:08:40.177: INFO: Pod "downwardapi-volume-a2a4faaa-779d-4e39-a6ba-4305f34a2f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055120763s
STEP: Saw pod success
Feb 22 10:08:40.177: INFO: Pod "downwardapi-volume-a2a4faaa-779d-4e39-a6ba-4305f34a2f70" satisfied condition "Succeeded or Failed"
Feb 22 10:08:40.183: INFO: Trying to get logs from node node2 pod downwardapi-volume-a2a4faaa-779d-4e39-a6ba-4305f34a2f70 container client-container: <nil>
STEP: delete the pod
Feb 22 10:08:40.302: INFO: Waiting for pod downwardapi-volume-a2a4faaa-779d-4e39-a6ba-4305f34a2f70 to disappear
Feb 22 10:08:40.321: INFO: Pod downwardapi-volume-a2a4faaa-779d-4e39-a6ba-4305f34a2f70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:40.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7457" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":266,"skipped":4682,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:40.382: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:43.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4517" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":267,"skipped":4691,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:43.407: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 10:08:43.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4ec5cd1-5100-4bef-a347-37ab9e75bf75" in namespace "downward-api-5971" to be "Succeeded or Failed"
Feb 22 10:08:43.856: INFO: Pod "downwardapi-volume-f4ec5cd1-5100-4bef-a347-37ab9e75bf75": Phase="Pending", Reason="", readiness=false. Elapsed: 21.832899ms
Feb 22 10:08:45.875: INFO: Pod "downwardapi-volume-f4ec5cd1-5100-4bef-a347-37ab9e75bf75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041245968s
Feb 22 10:08:47.904: INFO: Pod "downwardapi-volume-f4ec5cd1-5100-4bef-a347-37ab9e75bf75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070290292s
STEP: Saw pod success
Feb 22 10:08:47.904: INFO: Pod "downwardapi-volume-f4ec5cd1-5100-4bef-a347-37ab9e75bf75" satisfied condition "Succeeded or Failed"
Feb 22 10:08:47.914: INFO: Trying to get logs from node node2 pod downwardapi-volume-f4ec5cd1-5100-4bef-a347-37ab9e75bf75 container client-container: <nil>
STEP: delete the pod
Feb 22 10:08:48.042: INFO: Waiting for pod downwardapi-volume-f4ec5cd1-5100-4bef-a347-37ab9e75bf75 to disappear
Feb 22 10:08:48.065: INFO: Pod downwardapi-volume-f4ec5cd1-5100-4bef-a347-37ab9e75bf75 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:08:48.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5971" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":4692,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:08:48.116: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 22 10:08:48.280: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 22 10:08:48.299: INFO: Waiting for terminating namespaces to be deleted...
Feb 22 10:08:48.341: INFO: 
Logging pods the apiserver thinks is on node node1 before test
Feb 22 10:08:48.464: INFO: calico-kube-controllers-5f67864b44-c472v from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.464: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 22 10:08:48.464: INFO: calico-node-nnc68 from kube-system started at 2022-02-22 06:03:22 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.464: INFO: 	Container calico-node ready: true, restart count 1
Feb 22 10:08:48.464: INFO: coredns-575c8f4bf-l92w8 from kube-system started at 2022-02-22 08:12:56 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.464: INFO: 	Container coredns ready: true, restart count 0
Feb 22 10:08:48.464: INFO: kube-proxy-gj7fq from kube-system started at 2022-02-22 08:47:06 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.464: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 10:08:48.464: INFO: node-exporter-dn6cm from kube-system started at 2022-02-22 06:03:24 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.464: INFO: 	Container prometheus-node-exporter ready: true, restart count 2
Feb 22 10:08:48.464: INFO: vpn-target-5c5b44787-wpr2h from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.465: INFO: 	Container vpn-target ready: true, restart count 0
Feb 22 10:08:48.465: INFO: sonobuoy from sonobuoy started at 2022-02-22 08:49:43 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.465: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 22 10:08:48.465: INFO: sonobuoy-e2e-job-71a20d3e324c433a from sonobuoy started at 2022-02-22 08:49:45 +0000 UTC (2 container statuses recorded)
Feb 22 10:08:48.465: INFO: 	Container e2e ready: true, restart count 0
Feb 22 10:08:48.465: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 10:08:48.465: INFO: sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-whn5g from sonobuoy started at 2022-02-22 08:49:45 +0000 UTC (2 container statuses recorded)
Feb 22 10:08:48.465: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 10:08:48.465: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 22 10:08:48.465: INFO: 
Logging pods the apiserver thinks is on node node2 before test
Feb 22 10:08:48.489: INFO: calico-node-nrshb from kube-system started at 2022-02-22 06:03:23 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.489: INFO: 	Container calico-node ready: true, restart count 1
Feb 22 10:08:48.489: INFO: coredns-575c8f4bf-cxcvn from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.489: INFO: 	Container coredns ready: true, restart count 0
Feb 22 10:08:48.489: INFO: kube-proxy-65gc6 from kube-system started at 2022-02-22 08:13:14 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.489: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 10:08:48.489: INFO: metrics-server-6cb8f6bbb8-rqv7t from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.489: INFO: 	Container metrics-server ready: true, restart count 0
Feb 22 10:08:48.489: INFO: node-exporter-4t6rd from kube-system started at 2022-02-22 06:03:25 +0000 UTC (1 container statuses recorded)
Feb 22 10:08:48.489: INFO: 	Container prometheus-node-exporter ready: true, restart count 1
Feb 22 10:08:48.489: INFO: sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-c4wkd from sonobuoy started at 2022-02-22 08:49:46 +0000 UTC (2 container statuses recorded)
Feb 22 10:08:48.489: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 10:08:48.489: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e4472ba8-4949-4d19-9ed2-ee41bab4083b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.28.128.13 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e4472ba8-4949-4d19-9ed2-ee41bab4083b off the node node2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e4472ba8-4949-4d19-9ed2-ee41bab4083b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:13:57.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4552" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.993 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":269,"skipped":4693,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:13:57.109: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 10:13:57.856: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 10:14:00.922: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:14:01.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5571" for this suite.
STEP: Destroying namespace "webhook-5571-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":270,"skipped":4701,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:14:01.519: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4652
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-4652
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4652
Feb 22 10:14:01.772: INFO: Found 0 stateful pods, waiting for 1
Feb 22 10:14:11.782: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 22 10:14:11.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-4652 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 10:14:12.807: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 10:14:12.807: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 10:14:12.807: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 10:14:12.819: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 22 10:14:22.830: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 10:14:22.830: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 10:14:22.872: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Feb 22 10:14:22.872: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:01 +0000 UTC  }]
Feb 22 10:14:22.872: INFO: 
Feb 22 10:14:22.872: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 22 10:14:23.888: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99137626s
Feb 22 10:14:24.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975349655s
Feb 22 10:14:25.913: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960765643s
Feb 22 10:14:26.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951019509s
Feb 22 10:14:27.940: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937014294s
Feb 22 10:14:28.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.925112269s
Feb 22 10:14:29.963: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.913298745s
Feb 22 10:14:30.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.901771918s
Feb 22 10:14:31.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 891.448287ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4652
Feb 22 10:14:32.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-4652 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 10:14:33.653: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 22 10:14:33.653: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 10:14:33.653: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 22 10:14:33.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-4652 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 10:14:34.318: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 22 10:14:34.318: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 10:14:34.318: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 22 10:14:34.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-4652 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 22 10:14:34.928: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 22 10:14:34.928: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 22 10:14:34.928: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 22 10:14:34.936: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 10:14:34.936: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 10:14:34.936: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 22 10:14:34.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-4652 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 10:14:35.544: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 10:14:35.544: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 10:14:35.544: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 10:14:35.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-4652 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 10:14:36.100: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 10:14:36.100: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 10:14:36.100: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 10:14:36.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=statefulset-4652 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 22 10:14:36.787: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 22 10:14:36.787: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 22 10:14:36.787: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 22 10:14:36.787: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 10:14:36.796: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 22 10:14:46.821: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 10:14:46.822: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 10:14:46.822: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 10:14:46.874: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Feb 22 10:14:46.875: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:01 +0000 UTC  }]
Feb 22 10:14:46.875: INFO: ss-1  node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  }]
Feb 22 10:14:46.875: INFO: ss-2  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  }]
Feb 22 10:14:46.875: INFO: 
Feb 22 10:14:46.875: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 22 10:14:47.917: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Feb 22 10:14:47.917: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:01 +0000 UTC  }]
Feb 22 10:14:47.917: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  }]
Feb 22 10:14:47.917: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  }]
Feb 22 10:14:47.917: INFO: 
Feb 22 10:14:47.917: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 22 10:14:48.928: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Feb 22 10:14:48.928: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:01 +0000 UTC  }]
Feb 22 10:14:48.928: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  }]
Feb 22 10:14:48.928: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-22 10:14:22 +0000 UTC  }]
Feb 22 10:14:48.928: INFO: 
Feb 22 10:14:48.928: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 22 10:14:49.967: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.93546036s
Feb 22 10:14:50.976: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.897244557s
Feb 22 10:14:51.986: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.887888721s
Feb 22 10:14:52.993: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.878646984s
Feb 22 10:14:54.002: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.870726641s
Feb 22 10:14:55.015: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.861122106s
Feb 22 10:14:56.027: INFO: Verifying statefulset ss doesn't scale past 0 for another 848.806984ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4652
Feb 22 10:14:57.037: INFO: Scaling statefulset ss to 0
Feb 22 10:14:57.064: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 22 10:14:57.070: INFO: Deleting all statefulset in ns statefulset-4652
Feb 22 10:14:57.075: INFO: Scaling statefulset ss to 0
Feb 22 10:14:57.101: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 10:14:57.108: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:14:57.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4652" for this suite.

• [SLOW TEST:55.671 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":271,"skipped":4702,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:14:57.191: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Feb 22 10:14:57.315: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:15:30.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5315" for this suite.

• [SLOW TEST:33.651 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":272,"skipped":4703,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:15:30.842: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:15:31.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8327" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":273,"skipped":4721,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:15:31.375: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 22 10:15:31.575: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:15:38.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6168" for this suite.

• [SLOW TEST:7.363 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":274,"skipped":4732,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:15:38.740: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 10:15:38.983: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5559f907-6a9a-4d7e-a400-28514c45cbed" in namespace "projected-5455" to be "Succeeded or Failed"
Feb 22 10:15:38.992: INFO: Pod "downwardapi-volume-5559f907-6a9a-4d7e-a400-28514c45cbed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.77516ms
Feb 22 10:15:41.032: INFO: Pod "downwardapi-volume-5559f907-6a9a-4d7e-a400-28514c45cbed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049321431s
Feb 22 10:15:43.046: INFO: Pod "downwardapi-volume-5559f907-6a9a-4d7e-a400-28514c45cbed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063011727s
STEP: Saw pod success
Feb 22 10:15:43.046: INFO: Pod "downwardapi-volume-5559f907-6a9a-4d7e-a400-28514c45cbed" satisfied condition "Succeeded or Failed"
Feb 22 10:15:43.055: INFO: Trying to get logs from node node2 pod downwardapi-volume-5559f907-6a9a-4d7e-a400-28514c45cbed container client-container: <nil>
STEP: delete the pod
Feb 22 10:15:43.204: INFO: Waiting for pod downwardapi-volume-5559f907-6a9a-4d7e-a400-28514c45cbed to disappear
Feb 22 10:15:43.211: INFO: Pod downwardapi-volume-5559f907-6a9a-4d7e-a400-28514c45cbed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:15:43.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5455" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":4752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:15:43.286: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-c767c6f0-b88f-4c25-825f-efab9e43b6da in namespace container-probe-5538
Feb 22 10:15:47.507: INFO: Started pod test-webserver-c767c6f0-b88f-4c25-825f-efab9e43b6da in namespace container-probe-5538
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 10:15:47.520: INFO: Initial restart count of pod test-webserver-c767c6f0-b88f-4c25-825f-efab9e43b6da is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:19:47.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5538" for this suite.

• [SLOW TEST:244.731 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":4808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:19:48.019: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-015cdc64-f453-4a4b-82b5-8e639501ce4f
STEP: Creating a pod to test consume secrets
Feb 22 10:19:48.401: INFO: Waiting up to 5m0s for pod "pod-secrets-3a87dbd0-7233-414c-baa1-b7127a8741a3" in namespace "secrets-4563" to be "Succeeded or Failed"
Feb 22 10:19:48.410: INFO: Pod "pod-secrets-3a87dbd0-7233-414c-baa1-b7127a8741a3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.574856ms
Feb 22 10:19:50.479: INFO: Pod "pod-secrets-3a87dbd0-7233-414c-baa1-b7127a8741a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078131898s
Feb 22 10:19:52.491: INFO: Pod "pod-secrets-3a87dbd0-7233-414c-baa1-b7127a8741a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090889498s
STEP: Saw pod success
Feb 22 10:19:52.492: INFO: Pod "pod-secrets-3a87dbd0-7233-414c-baa1-b7127a8741a3" satisfied condition "Succeeded or Failed"
Feb 22 10:19:52.497: INFO: Trying to get logs from node node2 pod pod-secrets-3a87dbd0-7233-414c-baa1-b7127a8741a3 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:19:52.562: INFO: Waiting for pod pod-secrets-3a87dbd0-7233-414c-baa1-b7127a8741a3 to disappear
Feb 22 10:19:52.597: INFO: Pod pod-secrets-3a87dbd0-7233-414c-baa1-b7127a8741a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:19:52.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4563" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":277,"skipped":4853,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:19:52.628: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 22 10:19:52.826: INFO: PodSpec: initContainers in spec.initContainers
Feb 22 10:20:41.192: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1cb01c00-3500-45b2-b9b7-8b59696737f2", GenerateName:"", Namespace:"init-container-7363", SelfLink:"", UID:"b59427e8-4e1a-4500-a86b-ad898ef94eba", ResourceVersion:"64475", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63781121992, loc:(*time.Location)(0xa0a4dc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"826897951"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"555267d09b7771bae7f4aa1a922847aad1f1809a0eb32bf76acdf85e0dfe8578", "cni.projectcalico.org/podIP":"172.21.104.57/32", "cni.projectcalico.org/podIPs":"172.21.104.57/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0049d2ac8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0049d2ae0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0049d2af8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0049d2b10), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0049d2b28), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0049d2b40), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-n854g", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003262f40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n854g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n854g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n854g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0049907d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003c5bb90), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004990860)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004990880)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004990888), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00499088c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002cb6320), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121992, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121992, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121992, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781121992, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.28.128.13", PodIP:"172.21.104.57", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.21.104.57"}}, StartTime:(*v1.Time)(0xc0049d2b70), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003c5bce0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003c5bd50)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"docker://c1108437d61e3c18a33e8064079d873ae94300d1d83f911ad00cc19e1b33c6b6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003262fc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003262fa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc004990904)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:20:41.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7363" for this suite.

• [SLOW TEST:48.610 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":278,"skipped":4857,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:20:41.238: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-13a58ecb-bb1b-4987-9e95-5d8a6964ce70
STEP: Creating a pod to test consume secrets
Feb 22 10:20:41.475: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b388e29-3032-4e7b-9f30-ca58bbc5c917" in namespace "projected-8946" to be "Succeeded or Failed"
Feb 22 10:20:41.485: INFO: Pod "pod-projected-secrets-6b388e29-3032-4e7b-9f30-ca58bbc5c917": Phase="Pending", Reason="", readiness=false. Elapsed: 10.279953ms
Feb 22 10:20:43.504: INFO: Pod "pod-projected-secrets-6b388e29-3032-4e7b-9f30-ca58bbc5c917": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028325328s
Feb 22 10:20:45.560: INFO: Pod "pod-projected-secrets-6b388e29-3032-4e7b-9f30-ca58bbc5c917": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084417628s
STEP: Saw pod success
Feb 22 10:20:45.560: INFO: Pod "pod-projected-secrets-6b388e29-3032-4e7b-9f30-ca58bbc5c917" satisfied condition "Succeeded or Failed"
Feb 22 10:20:45.591: INFO: Trying to get logs from node node2 pod pod-projected-secrets-6b388e29-3032-4e7b-9f30-ca58bbc5c917 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:20:45.794: INFO: Waiting for pod pod-projected-secrets-6b388e29-3032-4e7b-9f30-ca58bbc5c917 to disappear
Feb 22 10:20:45.819: INFO: Pod pod-projected-secrets-6b388e29-3032-4e7b-9f30-ca58bbc5c917 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:20:45.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8946" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":4875,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:20:45.911: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 22 10:20:48.153: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 22 10:20:50.231: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781122048, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781122048, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781122048, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781122048, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 22 10:20:53.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 22 10:20:57.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=webhook-4960 attach --namespace=webhook-4960 to-be-attached-pod -i -c=container1'
Feb 22 10:20:57.968: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:20:57.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4960" for this suite.
STEP: Destroying namespace "webhook-4960-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.531 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":280,"skipped":4880,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:20:58.443: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:20:59.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3038" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":281,"skipped":4897,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:20:59.429: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Feb 22 10:20:59.642: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:21:01.677: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:21:03.661: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 22 10:21:03.734: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:21:05.742: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:21:07.751: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 22 10:21:07.903: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 22 10:21:07.924: INFO: Pod pod-with-poststart-http-hook still exists
Feb 22 10:21:09.925: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 22 10:21:09.938: INFO: Pod pod-with-poststart-http-hook still exists
Feb 22 10:21:11.925: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 22 10:21:11.935: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:21:11.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-958" for this suite.

• [SLOW TEST:12.529 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":282,"skipped":4906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:21:11.958: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 10:21:12.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-474be50a-4804-41c8-8196-13554a710616" in namespace "projected-283" to be "Succeeded or Failed"
Feb 22 10:21:12.192: INFO: Pod "downwardapi-volume-474be50a-4804-41c8-8196-13554a710616": Phase="Pending", Reason="", readiness=false. Elapsed: 39.08932ms
Feb 22 10:21:14.208: INFO: Pod "downwardapi-volume-474be50a-4804-41c8-8196-13554a710616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055215104s
Feb 22 10:21:16.218: INFO: Pod "downwardapi-volume-474be50a-4804-41c8-8196-13554a710616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065351216s
STEP: Saw pod success
Feb 22 10:21:16.218: INFO: Pod "downwardapi-volume-474be50a-4804-41c8-8196-13554a710616" satisfied condition "Succeeded or Failed"
Feb 22 10:21:16.233: INFO: Trying to get logs from node node2 pod downwardapi-volume-474be50a-4804-41c8-8196-13554a710616 container client-container: <nil>
STEP: delete the pod
Feb 22 10:21:16.324: INFO: Waiting for pod downwardapi-volume-474be50a-4804-41c8-8196-13554a710616 to disappear
Feb 22 10:21:16.339: INFO: Pod downwardapi-volume-474be50a-4804-41c8-8196-13554a710616 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:21:16.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-283" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":4933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:21:16.442: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:21:16.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9456" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":284,"skipped":4986,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:21:16.869: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Feb 22 10:21:17.114: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 22 10:21:17.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 create -f -'
Feb 22 10:21:19.009: INFO: stderr: ""
Feb 22 10:21:19.009: INFO: stdout: "service/agnhost-replica created\n"
Feb 22 10:21:19.014: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 22 10:21:19.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 create -f -'
Feb 22 10:21:19.940: INFO: stderr: ""
Feb 22 10:21:19.940: INFO: stdout: "service/agnhost-primary created\n"
Feb 22 10:21:19.940: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 22 10:21:19.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 create -f -'
Feb 22 10:21:20.715: INFO: stderr: ""
Feb 22 10:21:20.715: INFO: stdout: "service/frontend created\n"
Feb 22 10:21:20.715: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 22 10:21:20.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 create -f -'
Feb 22 10:21:21.128: INFO: stderr: ""
Feb 22 10:21:21.128: INFO: stdout: "deployment.apps/frontend created\n"
Feb 22 10:21:21.128: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 22 10:21:21.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 create -f -'
Feb 22 10:21:21.777: INFO: stderr: ""
Feb 22 10:21:21.778: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 22 10:21:21.778: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 22 10:21:21.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 create -f -'
Feb 22 10:21:22.743: INFO: stderr: ""
Feb 22 10:21:22.743: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Feb 22 10:21:22.743: INFO: Waiting for all frontend pods to be Running.
Feb 22 10:21:32.794: INFO: Waiting for frontend to serve content.
Feb 22 10:21:32.945: INFO: Trying to add a new entry to the guestbook.
Feb 22 10:21:33.096: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 22 10:21:33.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 delete --grace-period=0 --force -f -'
Feb 22 10:21:33.545: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 10:21:33.545: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 10:21:33.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 delete --grace-period=0 --force -f -'
Feb 22 10:21:34.434: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 10:21:34.434: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 10:21:34.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 delete --grace-period=0 --force -f -'
Feb 22 10:21:34.719: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 10:21:34.720: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 10:21:34.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 delete --grace-period=0 --force -f -'
Feb 22 10:21:34.886: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 10:21:34.886: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 10:21:34.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 delete --grace-period=0 --force -f -'
Feb 22 10:21:35.464: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 10:21:35.464: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 10:21:35.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-7148 delete --grace-period=0 --force -f -'
Feb 22 10:21:36.386: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 10:21:36.386: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:21:36.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7148" for this suite.

• [SLOW TEST:19.725 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":285,"skipped":5000,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:21:36.596: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:21:37.386: INFO: The status of Pod busybox-scheduling-385d041d-fc36-472f-afd9-d7080b1fa1fa is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:21:39.593: INFO: The status of Pod busybox-scheduling-385d041d-fc36-472f-afd9-d7080b1fa1fa is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:21:41.525: INFO: The status of Pod busybox-scheduling-385d041d-fc36-472f-afd9-d7080b1fa1fa is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:21:43.402: INFO: The status of Pod busybox-scheduling-385d041d-fc36-472f-afd9-d7080b1fa1fa is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:21:43.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1856" for this suite.

• [SLOW TEST:6.968 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":286,"skipped":5002,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:21:43.564: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:21:43.858: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 22 10:21:48.906: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 22 10:21:48.906: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 22 10:21:49.095: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1257  99a9b73f-5024-4903-b67a-b62f165d7549 65148 1 2022-02-22 10:21:49 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-02-22 10:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049b9dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 22 10:21:49.110: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:21:49.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1257" for this suite.

• [SLOW TEST:5.638 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":287,"skipped":5005,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:21:49.203: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:21:49.545: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:21:50.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3207" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":288,"skipped":5006,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:21:50.367: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-b9a35630-5db7-4f29-9d46-ce407fbe8d5b
STEP: Creating a pod to test consume secrets
Feb 22 10:21:50.708: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c0bdd063-64b0-43e9-a419-c33e639fda8e" in namespace "projected-4713" to be "Succeeded or Failed"
Feb 22 10:21:50.715: INFO: Pod "pod-projected-secrets-c0bdd063-64b0-43e9-a419-c33e639fda8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.639069ms
Feb 22 10:21:52.734: INFO: Pod "pod-projected-secrets-c0bdd063-64b0-43e9-a419-c33e639fda8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026083338s
Feb 22 10:21:54.841: INFO: Pod "pod-projected-secrets-c0bdd063-64b0-43e9-a419-c33e639fda8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.133065503s
STEP: Saw pod success
Feb 22 10:21:54.841: INFO: Pod "pod-projected-secrets-c0bdd063-64b0-43e9-a419-c33e639fda8e" satisfied condition "Succeeded or Failed"
Feb 22 10:21:55.352: INFO: Trying to get logs from node node2 pod pod-projected-secrets-c0bdd063-64b0-43e9-a419-c33e639fda8e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:21:56.716: INFO: Waiting for pod pod-projected-secrets-c0bdd063-64b0-43e9-a419-c33e639fda8e to disappear
Feb 22 10:21:56.975: INFO: Pod pod-projected-secrets-c0bdd063-64b0-43e9-a419-c33e639fda8e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:21:56.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4713" for this suite.

• [SLOW TEST:8.169 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":5012,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:21:58.537: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6334
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-6334
Feb 22 10:22:02.498: INFO: Found 0 stateful pods, waiting for 1
Feb 22 10:22:12.511: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Feb 22 10:22:12.611: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Feb 22 10:22:12.653: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Feb 22 10:22:12.657: INFO: Observed &StatefulSet event: ADDED
Feb 22 10:22:12.658: INFO: Found Statefulset ss in namespace statefulset-6334 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 22 10:22:12.658: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Feb 22 10:22:12.658: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 22 10:22:12.707: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Feb 22 10:22:12.712: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 22 10:22:12.712: INFO: Deleting all statefulset in ns statefulset-6334
Feb 22 10:22:12.745: INFO: Scaling statefulset ss to 0
Feb 22 10:22:22.957: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 10:22:23.005: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:22:23.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6334" for this suite.

• [SLOW TEST:24.665 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":290,"skipped":5020,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:22:23.202: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Feb 22 10:22:23.588: INFO: Found Service test-service-fktk9 in namespace services-3205 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 22 10:22:23.588: INFO: Service test-service-fktk9 created
STEP: Getting /status
Feb 22 10:22:23.613: INFO: Service test-service-fktk9 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Feb 22 10:22:23.661: INFO: observed Service test-service-fktk9 in namespace services-3205 with annotations: map[] & LoadBalancer: {[]}
Feb 22 10:22:23.661: INFO: Found Service test-service-fktk9 in namespace services-3205 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 22 10:22:23.661: INFO: Service test-service-fktk9 has service status patched
STEP: updating the ServiceStatus
Feb 22 10:22:23.737: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Feb 22 10:22:23.744: INFO: Observed Service test-service-fktk9 in namespace services-3205 with annotations: map[] & Conditions: {[]}
Feb 22 10:22:23.744: INFO: Observed event: &Service{ObjectMeta:{test-service-fktk9  services-3205  c6014e8d-6e66-412a-a552-c28429559b45 65352 0 2022-02-22 10:22:23 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-02-22 10:22:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-02-22 10:22:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.20.97.141,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.20.97.141],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 22 10:22:23.756: INFO: Found Service test-service-fktk9 in namespace services-3205 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 22 10:22:23.756: INFO: Service test-service-fktk9 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Feb 22 10:22:23.806: INFO: observed Service test-service-fktk9 in namespace services-3205 with labels: map[test-service-static:true]
Feb 22 10:22:23.806: INFO: observed Service test-service-fktk9 in namespace services-3205 with labels: map[test-service-static:true]
Feb 22 10:22:23.806: INFO: observed Service test-service-fktk9 in namespace services-3205 with labels: map[test-service-static:true]
Feb 22 10:22:23.806: INFO: Found Service test-service-fktk9 in namespace services-3205 with labels: map[test-service:patched test-service-static:true]
Feb 22 10:22:23.806: INFO: Service test-service-fktk9 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Feb 22 10:22:24.119: INFO: Observed event: ADDED
Feb 22 10:22:24.119: INFO: Observed event: MODIFIED
Feb 22 10:22:24.119: INFO: Observed event: MODIFIED
Feb 22 10:22:24.119: INFO: Observed event: MODIFIED
Feb 22 10:22:24.119: INFO: Found Service test-service-fktk9 in namespace services-3205 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 22 10:22:24.119: INFO: Service test-service-fktk9 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:22:24.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3205" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":291,"skipped":5029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:22:24.298: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Feb 22 10:22:24.454: INFO: Waiting up to 5m0s for pod "var-expansion-8110cb10-dbd8-4c9a-ae8f-78b9e65bbf3b" in namespace "var-expansion-7170" to be "Succeeded or Failed"
Feb 22 10:22:24.467: INFO: Pod "var-expansion-8110cb10-dbd8-4c9a-ae8f-78b9e65bbf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.678842ms
Feb 22 10:22:26.484: INFO: Pod "var-expansion-8110cb10-dbd8-4c9a-ae8f-78b9e65bbf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03015182s
Feb 22 10:22:28.499: INFO: Pod "var-expansion-8110cb10-dbd8-4c9a-ae8f-78b9e65bbf3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044889711s
STEP: Saw pod success
Feb 22 10:22:28.499: INFO: Pod "var-expansion-8110cb10-dbd8-4c9a-ae8f-78b9e65bbf3b" satisfied condition "Succeeded or Failed"
Feb 22 10:22:28.506: INFO: Trying to get logs from node node2 pod var-expansion-8110cb10-dbd8-4c9a-ae8f-78b9e65bbf3b container dapi-container: <nil>
STEP: delete the pod
Feb 22 10:22:28.838: INFO: Waiting for pod var-expansion-8110cb10-dbd8-4c9a-ae8f-78b9e65bbf3b to disappear
Feb 22 10:22:28.895: INFO: Pod var-expansion-8110cb10-dbd8-4c9a-ae8f-78b9e65bbf3b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:22:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7170" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5076,"failed":0}

------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:22:28.996: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-20dbe194-d77a-441a-8e95-c1500eceecb3
STEP: Creating secret with name s-test-opt-upd-35cb7f7a-baf7-4fa5-9dc5-bc7c6a406c37
STEP: Creating the pod
Feb 22 10:22:29.499: INFO: The status of Pod pod-projected-secrets-4bc39166-94a9-4b70-8eed-1394e32bf364 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:22:31.706: INFO: The status of Pod pod-projected-secrets-4bc39166-94a9-4b70-8eed-1394e32bf364 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:22:33.523: INFO: The status of Pod pod-projected-secrets-4bc39166-94a9-4b70-8eed-1394e32bf364 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:22:35.508: INFO: The status of Pod pod-projected-secrets-4bc39166-94a9-4b70-8eed-1394e32bf364 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-20dbe194-d77a-441a-8e95-c1500eceecb3
STEP: Updating secret s-test-opt-upd-35cb7f7a-baf7-4fa5-9dc5-bc7c6a406c37
STEP: Creating secret with name s-test-opt-create-235030e6-8b85-4dab-8514-f32801c0fb5e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:23:59.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5072" for this suite.

• [SLOW TEST:90.665 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5076,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:23:59.661: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Feb 22 10:23:59.914: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 22 10:24:04.931: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:24:05.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2785" for this suite.

• [SLOW TEST:5.613 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":294,"skipped":5081,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:24:05.276: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8938.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8938.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 10:24:16.042: INFO: DNS probes using dns-8938/dns-test-ee683386-01d2-43f6-9fe3-6da242bfdd89 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:24:16.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8938" for this suite.

• [SLOW TEST:11.057 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":295,"skipped":5095,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:24:16.333: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 22 10:24:16.666: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 22 10:24:16.713: INFO: Waiting for terminating namespaces to be deleted...
Feb 22 10:24:16.750: INFO: 
Logging pods the apiserver thinks is on node node1 before test
Feb 22 10:24:16.845: INFO: calico-kube-controllers-5f67864b44-c472v from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 22 10:24:16.845: INFO: calico-node-nnc68 from kube-system started at 2022-02-22 06:03:22 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container calico-node ready: true, restart count 1
Feb 22 10:24:16.845: INFO: coredns-575c8f4bf-l92w8 from kube-system started at 2022-02-22 08:12:56 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container coredns ready: true, restart count 0
Feb 22 10:24:16.845: INFO: kube-proxy-gj7fq from kube-system started at 2022-02-22 08:47:06 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 10:24:16.845: INFO: node-exporter-dn6cm from kube-system started at 2022-02-22 06:03:24 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container prometheus-node-exporter ready: true, restart count 2
Feb 22 10:24:16.845: INFO: vpn-target-5c5b44787-wpr2h from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container vpn-target ready: true, restart count 0
Feb 22 10:24:16.845: INFO: sonobuoy from sonobuoy started at 2022-02-22 08:49:43 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 22 10:24:16.845: INFO: sonobuoy-e2e-job-71a20d3e324c433a from sonobuoy started at 2022-02-22 08:49:45 +0000 UTC (2 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container e2e ready: true, restart count 0
Feb 22 10:24:16.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 10:24:16.845: INFO: sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-whn5g from sonobuoy started at 2022-02-22 08:49:45 +0000 UTC (2 container statuses recorded)
Feb 22 10:24:16.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 10:24:16.845: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 22 10:24:16.845: INFO: 
Logging pods the apiserver thinks is on node node2 before test
Feb 22 10:24:16.866: INFO: calico-node-nrshb from kube-system started at 2022-02-22 06:03:23 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.866: INFO: 	Container calico-node ready: true, restart count 1
Feb 22 10:24:16.866: INFO: coredns-575c8f4bf-cxcvn from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.866: INFO: 	Container coredns ready: true, restart count 0
Feb 22 10:24:16.866: INFO: kube-proxy-65gc6 from kube-system started at 2022-02-22 08:13:14 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.866: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 10:24:16.866: INFO: metrics-server-6cb8f6bbb8-rqv7t from kube-system started at 2022-02-22 09:32:18 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.866: INFO: 	Container metrics-server ready: true, restart count 0
Feb 22 10:24:16.866: INFO: node-exporter-4t6rd from kube-system started at 2022-02-22 06:03:25 +0000 UTC (1 container statuses recorded)
Feb 22 10:24:16.866: INFO: 	Container prometheus-node-exporter ready: true, restart count 1
Feb 22 10:24:16.866: INFO: sonobuoy-systemd-logs-daemon-set-783b05b6fc094c6e-c4wkd from sonobuoy started at 2022-02-22 08:49:46 +0000 UTC (2 container statuses recorded)
Feb 22 10:24:16.866: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 10:24:16.866: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-acbe000c-1855-4344-9430-e4cbbcf678e1 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-acbe000c-1855-4344-9430-e4cbbcf678e1 off the node node2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-acbe000c-1855-4344-9430-e4cbbcf678e1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:24:27.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3680" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:10.853 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":296,"skipped":5098,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:24:27.187: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 22 10:24:27.376: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4085  92a85386-231a-4f52-889e-f7455e1edb27 65772 0 2022-02-22 10:24:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-22 10:24:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 10:24:27.376: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4085  92a85386-231a-4f52-889e-f7455e1edb27 65772 0 2022-02-22 10:24:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-22 10:24:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 22 10:24:37.406: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4085  92a85386-231a-4f52-889e-f7455e1edb27 65810 0 2022-02-22 10:24:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-22 10:24:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 10:24:37.406: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4085  92a85386-231a-4f52-889e-f7455e1edb27 65810 0 2022-02-22 10:24:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-22 10:24:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 22 10:24:47.426: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4085  92a85386-231a-4f52-889e-f7455e1edb27 65824 0 2022-02-22 10:24:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-22 10:24:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 10:24:47.426: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4085  92a85386-231a-4f52-889e-f7455e1edb27 65824 0 2022-02-22 10:24:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-22 10:24:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 22 10:24:57.453: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4085  92a85386-231a-4f52-889e-f7455e1edb27 65838 0 2022-02-22 10:24:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-22 10:24:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 10:24:57.454: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4085  92a85386-231a-4f52-889e-f7455e1edb27 65838 0 2022-02-22 10:24:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-22 10:24:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 22 10:25:07.501: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4085  641b4c1d-01e5-4c5c-a8dd-5f968dd0a2f5 65852 0 2022-02-22 10:25:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-22 10:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 10:25:07.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4085  641b4c1d-01e5-4c5c-a8dd-5f968dd0a2f5 65852 0 2022-02-22 10:25:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-22 10:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 22 10:25:17.534: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4085  641b4c1d-01e5-4c5c-a8dd-5f968dd0a2f5 65865 0 2022-02-22 10:25:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-22 10:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 22 10:25:17.534: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4085  641b4c1d-01e5-4c5c-a8dd-5f968dd0a2f5 65865 0 2022-02-22 10:25:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-22 10:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:25:27.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4085" for this suite.

• [SLOW TEST:60.422 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":297,"skipped":5107,"failed":0}
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:25:27.609: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-2709/configmap-test-5cf6b32d-09e8-486f-b707-74b9c781d5de
STEP: Creating a pod to test consume configMaps
Feb 22 10:25:27.865: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdaf3fda-7df3-42f0-a49d-d6d7c46542cf" in namespace "configmap-2709" to be "Succeeded or Failed"
Feb 22 10:25:27.870: INFO: Pod "pod-configmaps-bdaf3fda-7df3-42f0-a49d-d6d7c46542cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660179ms
Feb 22 10:25:29.912: INFO: Pod "pod-configmaps-bdaf3fda-7df3-42f0-a49d-d6d7c46542cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047273363s
Feb 22 10:25:31.926: INFO: Pod "pod-configmaps-bdaf3fda-7df3-42f0-a49d-d6d7c46542cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060917178s
STEP: Saw pod success
Feb 22 10:25:31.926: INFO: Pod "pod-configmaps-bdaf3fda-7df3-42f0-a49d-d6d7c46542cf" satisfied condition "Succeeded or Failed"
Feb 22 10:25:31.947: INFO: Trying to get logs from node node2 pod pod-configmaps-bdaf3fda-7df3-42f0-a49d-d6d7c46542cf container env-test: <nil>
STEP: delete the pod
Feb 22 10:25:32.058: INFO: Waiting for pod pod-configmaps-bdaf3fda-7df3-42f0-a49d-d6d7c46542cf to disappear
Feb 22 10:25:32.065: INFO: Pod pod-configmaps-bdaf3fda-7df3-42f0-a49d-d6d7c46542cf no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:25:32.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2709" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":298,"skipped":5107,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:25:32.089: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-a17280bd-0e98-49af-a38c-eefe460514d1
STEP: Creating a pod to test consume secrets
Feb 22 10:25:32.351: INFO: Waiting up to 5m0s for pod "pod-secrets-26539fa2-89c2-4dc6-ba26-d5a9da337d58" in namespace "secrets-726" to be "Succeeded or Failed"
Feb 22 10:25:32.361: INFO: Pod "pod-secrets-26539fa2-89c2-4dc6-ba26-d5a9da337d58": Phase="Pending", Reason="", readiness=false. Elapsed: 10.199553ms
Feb 22 10:25:34.375: INFO: Pod "pod-secrets-26539fa2-89c2-4dc6-ba26-d5a9da337d58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023857569s
Feb 22 10:25:36.387: INFO: Pod "pod-secrets-26539fa2-89c2-4dc6-ba26-d5a9da337d58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035247895s
STEP: Saw pod success
Feb 22 10:25:36.387: INFO: Pod "pod-secrets-26539fa2-89c2-4dc6-ba26-d5a9da337d58" satisfied condition "Succeeded or Failed"
Feb 22 10:25:36.465: INFO: Trying to get logs from node node2 pod pod-secrets-26539fa2-89c2-4dc6-ba26-d5a9da337d58 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:25:36.522: INFO: Waiting for pod pod-secrets-26539fa2-89c2-4dc6-ba26-d5a9da337d58 to disappear
Feb 22 10:25:36.531: INFO: Pod pod-secrets-26539fa2-89c2-4dc6-ba26-d5a9da337d58 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:25:36.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-726" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:25:36.611: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:25:40.817: INFO: Deleting pod "var-expansion-0847b9f3-b1d7-4041-807d-71b5e0494eaf" in namespace "var-expansion-2747"
Feb 22 10:25:40.868: INFO: Wait up to 5m0s for pod "var-expansion-0847b9f3-b1d7-4041-807d-71b5e0494eaf" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:25:44.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2747" for this suite.

• [SLOW TEST:8.311 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":300,"skipped":5157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:25:44.923: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Feb 22 10:25:45.157: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-4826 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:25:45.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4826" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":301,"skipped":5184,"failed":0}

------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:25:45.322: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Feb 22 10:25:45.531: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 22 10:26:45.608: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:26:45.647: INFO: Starting informer...
STEP: Starting pods...
Feb 22 10:26:45.933: INFO: Pod1 is running on node2. Tainting Node
Feb 22 10:26:50.200: INFO: Pod2 is running on node2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 22 10:27:00.278: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 22 10:27:17.436: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:27:17.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8293" for this suite.

• [SLOW TEST:92.242 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":302,"skipped":5184,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:27:17.565: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:27:17.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8495" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":303,"skipped":5187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:27:18.039: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:27:18.250: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-39576799-dde8-447c-bbea-968fc4d18ae0" in namespace "security-context-test-3548" to be "Succeeded or Failed"
Feb 22 10:27:18.258: INFO: Pod "alpine-nnp-false-39576799-dde8-447c-bbea-968fc4d18ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.430666ms
Feb 22 10:27:20.271: INFO: Pod "alpine-nnp-false-39576799-dde8-447c-bbea-968fc4d18ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020293686s
Feb 22 10:27:22.279: INFO: Pod "alpine-nnp-false-39576799-dde8-447c-bbea-968fc4d18ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029018224s
Feb 22 10:27:24.295: INFO: Pod "alpine-nnp-false-39576799-dde8-447c-bbea-968fc4d18ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044179033s
Feb 22 10:27:26.304: INFO: Pod "alpine-nnp-false-39576799-dde8-447c-bbea-968fc4d18ae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.053346169s
Feb 22 10:27:26.304: INFO: Pod "alpine-nnp-false-39576799-dde8-447c-bbea-968fc4d18ae0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:27:26.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3548" for this suite.

• [SLOW TEST:8.325 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:27:26.366: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 22 10:27:36.743: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0222 10:27:36.742987      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 22 10:27:36.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8656" for this suite.

• [SLOW TEST:10.417 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":305,"skipped":5246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:27:36.783: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5014
STEP: creating service affinity-clusterip-transition in namespace services-5014
STEP: creating replication controller affinity-clusterip-transition in namespace services-5014
I0222 10:27:37.074931      19 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-5014, replica count: 3
I0222 10:27:40.128021      19 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 10:27:43.129011      19 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 10:27:43.163: INFO: Creating new exec pod
Feb 22 10:27:48.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-5014 exec execpod-affinitydq9dt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Feb 22 10:27:49.412: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 22 10:27:49.412: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:27:49.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-5014 exec execpod-affinitydq9dt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.1.144 80'
Feb 22 10:27:50.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.1.144 80\nConnection to 172.20.1.144 80 port [tcp/http] succeeded!\n"
Feb 22 10:27:50.129: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 22 10:27:50.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-5014 exec execpod-affinitydq9dt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.1.144:80/ ; done'
Feb 22 10:27:51.115: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n"
Feb 22 10:27:51.116: INFO: stdout: "\naffinity-clusterip-transition-6stcz\naffinity-clusterip-transition-6stcz\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-rh2sx\naffinity-clusterip-transition-rh2sx\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-rh2sx\naffinity-clusterip-transition-rh2sx\naffinity-clusterip-transition-6stcz\naffinity-clusterip-transition-6stcz\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-6stcz\naffinity-clusterip-transition-rh2sx\naffinity-clusterip-transition-6stcz\naffinity-clusterip-transition-qdrml"
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-6stcz
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-6stcz
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-rh2sx
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-rh2sx
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-rh2sx
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-rh2sx
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-6stcz
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-6stcz
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-6stcz
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-rh2sx
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-6stcz
Feb 22 10:27:51.116: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:51.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=services-5014 exec execpod-affinitydq9dt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.1.144:80/ ; done'
Feb 22 10:27:52.126: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.1.144:80/\n"
Feb 22 10:27:52.126: INFO: stdout: "\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml\naffinity-clusterip-transition-qdrml"
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Received response from host: affinity-clusterip-transition-qdrml
Feb 22 10:27:52.126: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5014, will wait for the garbage collector to delete the pods
Feb 22 10:27:52.537: INFO: Deleting ReplicationController affinity-clusterip-transition took: 68.626487ms
Feb 22 10:27:52.764: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 227.227864ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:27:57.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5014" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:20.714 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":306,"skipped":5276,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:27:57.498: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:27:57.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4472" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":307,"skipped":5277,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:27:58.025: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Feb 22 10:27:58.229: INFO: Waiting up to 5m0s for pod "var-expansion-15d047ea-ed2f-4986-a84b-984afe870e4e" in namespace "var-expansion-3207" to be "Succeeded or Failed"
Feb 22 10:27:58.240: INFO: Pod "var-expansion-15d047ea-ed2f-4986-a84b-984afe870e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.588751ms
Feb 22 10:28:00.281: INFO: Pod "var-expansion-15d047ea-ed2f-4986-a84b-984afe870e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051469343s
Feb 22 10:28:02.395: INFO: Pod "var-expansion-15d047ea-ed2f-4986-a84b-984afe870e4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.165895299s
STEP: Saw pod success
Feb 22 10:28:02.395: INFO: Pod "var-expansion-15d047ea-ed2f-4986-a84b-984afe870e4e" satisfied condition "Succeeded or Failed"
Feb 22 10:28:02.412: INFO: Trying to get logs from node node2 pod var-expansion-15d047ea-ed2f-4986-a84b-984afe870e4e container dapi-container: <nil>
STEP: delete the pod
Feb 22 10:28:02.605: INFO: Waiting for pod var-expansion-15d047ea-ed2f-4986-a84b-984afe870e4e to disappear
Feb 22 10:28:02.696: INFO: Pod var-expansion-15d047ea-ed2f-4986-a84b-984afe870e4e no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:28:02.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3207" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5297,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:28:02.915: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 10:28:03.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6165e6b4-8f78-4b16-a52b-a22e772ed57f" in namespace "downward-api-5448" to be "Succeeded or Failed"
Feb 22 10:28:03.328: INFO: Pod "downwardapi-volume-6165e6b4-8f78-4b16-a52b-a22e772ed57f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.522806ms
Feb 22 10:28:05.340: INFO: Pod "downwardapi-volume-6165e6b4-8f78-4b16-a52b-a22e772ed57f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032951228s
Feb 22 10:28:07.359: INFO: Pod "downwardapi-volume-6165e6b4-8f78-4b16-a52b-a22e772ed57f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051543421s
STEP: Saw pod success
Feb 22 10:28:07.359: INFO: Pod "downwardapi-volume-6165e6b4-8f78-4b16-a52b-a22e772ed57f" satisfied condition "Succeeded or Failed"
Feb 22 10:28:07.365: INFO: Trying to get logs from node node2 pod downwardapi-volume-6165e6b4-8f78-4b16-a52b-a22e772ed57f container client-container: <nil>
STEP: delete the pod
Feb 22 10:28:07.436: INFO: Waiting for pod downwardapi-volume-6165e6b4-8f78-4b16-a52b-a22e772ed57f to disappear
Feb 22 10:28:07.445: INFO: Pod downwardapi-volume-6165e6b4-8f78-4b16-a52b-a22e772ed57f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:28:07.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5448" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":309,"skipped":5298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:28:07.497: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 22 10:28:07.681: INFO: Waiting up to 5m0s for pod "pod-987befe7-0f22-43cd-948e-d01c8caf30c6" in namespace "emptydir-6806" to be "Succeeded or Failed"
Feb 22 10:28:07.703: INFO: Pod "pod-987befe7-0f22-43cd-948e-d01c8caf30c6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.9737ms
Feb 22 10:28:09.720: INFO: Pod "pod-987befe7-0f22-43cd-948e-d01c8caf30c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039436598s
Feb 22 10:28:11.747: INFO: Pod "pod-987befe7-0f22-43cd-948e-d01c8caf30c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065772256s
STEP: Saw pod success
Feb 22 10:28:11.747: INFO: Pod "pod-987befe7-0f22-43cd-948e-d01c8caf30c6" satisfied condition "Succeeded or Failed"
Feb 22 10:28:11.754: INFO: Trying to get logs from node node2 pod pod-987befe7-0f22-43cd-948e-d01c8caf30c6 container test-container: <nil>
STEP: delete the pod
Feb 22 10:28:11.864: INFO: Waiting for pod pod-987befe7-0f22-43cd-948e-d01c8caf30c6 to disappear
Feb 22 10:28:11.870: INFO: Pod pod-987befe7-0f22-43cd-948e-d01c8caf30c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:28:11.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6806" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:28:11.917: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-f8f2bb50-3a23-4ef9-9c22-e249d341d015
STEP: Creating a pod to test consume secrets
Feb 22 10:28:12.478: INFO: Waiting up to 5m0s for pod "pod-secrets-e36e5353-d336-4374-b356-ba4b43329d8d" in namespace "secrets-5639" to be "Succeeded or Failed"
Feb 22 10:28:12.507: INFO: Pod "pod-secrets-e36e5353-d336-4374-b356-ba4b43329d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 28.957168ms
Feb 22 10:28:14.521: INFO: Pod "pod-secrets-e36e5353-d336-4374-b356-ba4b43329d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042239785s
Feb 22 10:28:16.570: INFO: Pod "pod-secrets-e36e5353-d336-4374-b356-ba4b43329d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091429039s
STEP: Saw pod success
Feb 22 10:28:16.570: INFO: Pod "pod-secrets-e36e5353-d336-4374-b356-ba4b43329d8d" satisfied condition "Succeeded or Failed"
Feb 22 10:28:16.608: INFO: Trying to get logs from node node2 pod pod-secrets-e36e5353-d336-4374-b356-ba4b43329d8d container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:28:16.914: INFO: Waiting for pod pod-secrets-e36e5353-d336-4374-b356-ba4b43329d8d to disappear
Feb 22 10:28:16.929: INFO: Pod pod-secrets-e36e5353-d336-4374-b356-ba4b43329d8d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:28:16.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5639" for this suite.
STEP: Destroying namespace "secret-namespace-2616" for this suite.

• [SLOW TEST:5.151 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:28:17.069: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Feb 22 10:30:17.911: INFO: Successfully updated pod "var-expansion-e7cd1037-a5bc-47b4-9601-e2440b59993a"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Feb 22 10:30:19.941: INFO: Deleting pod "var-expansion-e7cd1037-a5bc-47b4-9601-e2440b59993a" in namespace "var-expansion-2779"
Feb 22 10:30:19.963: INFO: Wait up to 5m0s for pod "var-expansion-e7cd1037-a5bc-47b4-9601-e2440b59993a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:30:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2779" for this suite.

• [SLOW TEST:157.015 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":312,"skipped":5436,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:30:54.087: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:30:54.688: INFO: created pod
Feb 22 10:30:54.688: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1191" to be "Succeeded or Failed"
Feb 22 10:30:54.707: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.286312ms
Feb 22 10:30:56.728: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040824691s
Feb 22 10:30:58.797: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.109731255s
STEP: Saw pod success
Feb 22 10:30:58.797: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 22 10:31:28.799: INFO: polling logs
Feb 22 10:31:28.830: INFO: Pod logs: 
2022/02/22 10:30:57 OK: Got token
2022/02/22 10:30:57 validating with in-cluster discovery
2022/02/22 10:30:57 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/02/22 10:30:57 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1191:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1645526455, NotBefore:1645525855, IssuedAt:1645525855, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1191", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"7186601e-dc4f-49cc-b121-2ea6a3e2b64f"}}}
2022/02/22 10:30:57 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/02/22 10:30:57 OK: Validated signature on JWT
2022/02/22 10:30:57 OK: Got valid claims from token!
2022/02/22 10:30:57 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1191:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1645526455, NotBefore:1645525855, IssuedAt:1645525855, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1191", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"7186601e-dc4f-49cc-b121-2ea6a3e2b64f"}}}

Feb 22 10:31:28.830: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:31:28.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1191" for this suite.

• [SLOW TEST:34.805 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":313,"skipped":5456,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:31:28.891: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0222 10:32:09.815425      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 22 10:32:09.815: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 22 10:32:09.815: INFO: Deleting pod "simpletest.rc-2vkp8" in namespace "gc-4828"
Feb 22 10:32:09.999: INFO: Deleting pod "simpletest.rc-69mgw" in namespace "gc-4828"
Feb 22 10:32:10.063: INFO: Deleting pod "simpletest.rc-8rvfj" in namespace "gc-4828"
Feb 22 10:32:10.168: INFO: Deleting pod "simpletest.rc-dlqlk" in namespace "gc-4828"
Feb 22 10:32:10.369: INFO: Deleting pod "simpletest.rc-q8w7j" in namespace "gc-4828"
Feb 22 10:32:10.569: INFO: Deleting pod "simpletest.rc-qp478" in namespace "gc-4828"
Feb 22 10:32:10.808: INFO: Deleting pod "simpletest.rc-vxthj" in namespace "gc-4828"
Feb 22 10:32:11.048: INFO: Deleting pod "simpletest.rc-vzj4h" in namespace "gc-4828"
Feb 22 10:32:11.356: INFO: Deleting pod "simpletest.rc-wfb54" in namespace "gc-4828"
Feb 22 10:32:11.647: INFO: Deleting pod "simpletest.rc-z2kf2" in namespace "gc-4828"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:32:12.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4828" for this suite.

• [SLOW TEST:43.781 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":314,"skipped":5461,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:32:12.678: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-5296/secret-test-002d3d88-dbbf-47cd-8a18-cdea2114b0bf
STEP: Creating a pod to test consume secrets
Feb 22 10:32:14.591: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1" in namespace "secrets-5296" to be "Succeeded or Failed"
Feb 22 10:32:14.700: INFO: Pod "pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1": Phase="Pending", Reason="", readiness=false. Elapsed: 108.445406ms
Feb 22 10:32:17.174: INFO: Pod "pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.582263823s
Feb 22 10:32:19.472: INFO: Pod "pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881002738s
Feb 22 10:32:21.533: INFO: Pod "pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.94149074s
Feb 22 10:32:23.581: INFO: Pod "pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.9892603s
STEP: Saw pod success
Feb 22 10:32:23.581: INFO: Pod "pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1" satisfied condition "Succeeded or Failed"
Feb 22 10:32:23.700: INFO: Trying to get logs from node node2 pod pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1 container env-test: <nil>
STEP: delete the pod
Feb 22 10:32:24.247: INFO: Waiting for pod pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1 to disappear
Feb 22 10:32:24.283: INFO: Pod pod-configmaps-e8d19e87-529b-40cd-88a5-3d7861f569c1 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:32:24.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5296" for this suite.

• [SLOW TEST:11.675 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":5479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:32:24.349: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 10:32:24.697: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dfaec4c-43e7-4eed-9840-c7852c2eeec5" in namespace "downward-api-1315" to be "Succeeded or Failed"
Feb 22 10:32:24.703: INFO: Pod "downwardapi-volume-7dfaec4c-43e7-4eed-9840-c7852c2eeec5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.664374ms
Feb 22 10:32:26.718: INFO: Pod "downwardapi-volume-7dfaec4c-43e7-4eed-9840-c7852c2eeec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020548984s
Feb 22 10:32:28.738: INFO: Pod "downwardapi-volume-7dfaec4c-43e7-4eed-9840-c7852c2eeec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04083917s
STEP: Saw pod success
Feb 22 10:32:28.738: INFO: Pod "downwardapi-volume-7dfaec4c-43e7-4eed-9840-c7852c2eeec5" satisfied condition "Succeeded or Failed"
Feb 22 10:32:28.745: INFO: Trying to get logs from node node2 pod downwardapi-volume-7dfaec4c-43e7-4eed-9840-c7852c2eeec5 container client-container: <nil>
STEP: delete the pod
Feb 22 10:32:28.887: INFO: Waiting for pod downwardapi-volume-7dfaec4c-43e7-4eed-9840-c7852c2eeec5 to disappear
Feb 22 10:32:28.896: INFO: Pod downwardapi-volume-7dfaec4c-43e7-4eed-9840-c7852c2eeec5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:32:28.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1315" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":5501,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:32:28.975: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:32:29.171: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:32:36.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7721" for this suite.

• [SLOW TEST:7.195 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":317,"skipped":5508,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:32:36.171: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 22 10:32:36.335: INFO: Waiting up to 5m0s for pod "downward-api-2f72a9f0-e060-423e-a800-0ebd23519db5" in namespace "downward-api-1224" to be "Succeeded or Failed"
Feb 22 10:32:36.343: INFO: Pod "downward-api-2f72a9f0-e060-423e-a800-0ebd23519db5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.258263ms
Feb 22 10:32:38.367: INFO: Pod "downward-api-2f72a9f0-e060-423e-a800-0ebd23519db5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032234732s
Feb 22 10:32:40.378: INFO: Pod "downward-api-2f72a9f0-e060-423e-a800-0ebd23519db5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04312496s
STEP: Saw pod success
Feb 22 10:32:40.378: INFO: Pod "downward-api-2f72a9f0-e060-423e-a800-0ebd23519db5" satisfied condition "Succeeded or Failed"
Feb 22 10:32:40.384: INFO: Trying to get logs from node node2 pod downward-api-2f72a9f0-e060-423e-a800-0ebd23519db5 container dapi-container: <nil>
STEP: delete the pod
Feb 22 10:32:40.456: INFO: Waiting for pod downward-api-2f72a9f0-e060-423e-a800-0ebd23519db5 to disappear
Feb 22 10:32:40.464: INFO: Pod downward-api-2f72a9f0-e060-423e-a800-0ebd23519db5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:32:40.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1224" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":5512,"failed":0}

------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:32:40.501: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:32:44.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2304" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":319,"skipped":5512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:32:44.822: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:32:46.259: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 22 10:32:46.262: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 22 10:32:46.263: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 22 10:32:46.263: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 22 10:32:46.263: INFO: Checking APIGroup: apps
Feb 22 10:32:46.265: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 22 10:32:46.265: INFO: Versions found [{apps/v1 v1}]
Feb 22 10:32:46.265: INFO: apps/v1 matches apps/v1
Feb 22 10:32:46.265: INFO: Checking APIGroup: events.k8s.io
Feb 22 10:32:46.268: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 22 10:32:46.268: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Feb 22 10:32:46.268: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 22 10:32:46.268: INFO: Checking APIGroup: authentication.k8s.io
Feb 22 10:32:46.271: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 22 10:32:46.271: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 22 10:32:46.271: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 22 10:32:46.271: INFO: Checking APIGroup: authorization.k8s.io
Feb 22 10:32:46.273: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 22 10:32:46.273: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 22 10:32:46.273: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 22 10:32:46.273: INFO: Checking APIGroup: autoscaling
Feb 22 10:32:46.276: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Feb 22 10:32:46.276: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Feb 22 10:32:46.276: INFO: autoscaling/v1 matches autoscaling/v1
Feb 22 10:32:46.276: INFO: Checking APIGroup: batch
Feb 22 10:32:46.278: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 22 10:32:46.278: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Feb 22 10:32:46.278: INFO: batch/v1 matches batch/v1
Feb 22 10:32:46.278: INFO: Checking APIGroup: certificates.k8s.io
Feb 22 10:32:46.281: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 22 10:32:46.281: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 22 10:32:46.281: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 22 10:32:46.281: INFO: Checking APIGroup: networking.k8s.io
Feb 22 10:32:46.283: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 22 10:32:46.283: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 22 10:32:46.283: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 22 10:32:46.283: INFO: Checking APIGroup: policy
Feb 22 10:32:46.286: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 22 10:32:46.286: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Feb 22 10:32:46.286: INFO: policy/v1 matches policy/v1
Feb 22 10:32:46.286: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 22 10:32:46.292: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 22 10:32:46.292: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 22 10:32:46.292: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 22 10:32:46.292: INFO: Checking APIGroup: storage.k8s.io
Feb 22 10:32:46.295: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 22 10:32:46.295: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 22 10:32:46.295: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 22 10:32:46.295: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 22 10:32:46.298: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 22 10:32:46.298: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 22 10:32:46.298: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 22 10:32:46.298: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 22 10:32:46.302: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 22 10:32:46.302: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 22 10:32:46.302: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 22 10:32:46.302: INFO: Checking APIGroup: scheduling.k8s.io
Feb 22 10:32:46.307: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 22 10:32:46.307: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 22 10:32:46.307: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 22 10:32:46.307: INFO: Checking APIGroup: coordination.k8s.io
Feb 22 10:32:46.310: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 22 10:32:46.310: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 22 10:32:46.310: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 22 10:32:46.310: INFO: Checking APIGroup: node.k8s.io
Feb 22 10:32:46.313: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 22 10:32:46.313: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Feb 22 10:32:46.313: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 22 10:32:46.313: INFO: Checking APIGroup: discovery.k8s.io
Feb 22 10:32:46.316: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 22 10:32:46.316: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Feb 22 10:32:46.316: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 22 10:32:46.316: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 22 10:32:46.319: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Feb 22 10:32:46.319: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb 22 10:32:46.319: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Feb 22 10:32:46.319: INFO: Checking APIGroup: crd.projectcalico.org
Feb 22 10:32:46.321: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 22 10:32:46.321: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 22 10:32:46.321: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 22 10:32:46.321: INFO: Checking APIGroup: metrics.k8s.io
Feb 22 10:32:46.324: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 22 10:32:46.324: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 22 10:32:46.324: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:32:46.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1344" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":320,"skipped":5536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:32:46.351: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-28ef6a47-26cc-4256-b24f-9b07e8b486fa
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:32:46.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-538" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":321,"skipped":5559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:32:46.580: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:38:00.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8245" for this suite.

• [SLOW TEST:314.331 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":322,"skipped":5585,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:38:00.912: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:38:01.349: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 22 10:38:01.462: INFO: Number of nodes with available pods: 0
Feb 22 10:38:01.462: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 22 10:38:01.760: INFO: Number of nodes with available pods: 0
Feb 22 10:38:01.760: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:02.794: INFO: Number of nodes with available pods: 0
Feb 22 10:38:02.794: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:03.800: INFO: Number of nodes with available pods: 0
Feb 22 10:38:03.800: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:04.773: INFO: Number of nodes with available pods: 0
Feb 22 10:38:04.773: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:05.769: INFO: Number of nodes with available pods: 0
Feb 22 10:38:05.769: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:06.772: INFO: Number of nodes with available pods: 1
Feb 22 10:38:06.772: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 22 10:38:06.909: INFO: Number of nodes with available pods: 0
Feb 22 10:38:06.909: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 22 10:38:06.988: INFO: Number of nodes with available pods: 0
Feb 22 10:38:06.988: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:08.003: INFO: Number of nodes with available pods: 0
Feb 22 10:38:08.003: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:09.003: INFO: Number of nodes with available pods: 0
Feb 22 10:38:09.003: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:10.004: INFO: Number of nodes with available pods: 0
Feb 22 10:38:10.004: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:10.997: INFO: Number of nodes with available pods: 0
Feb 22 10:38:10.997: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:12.010: INFO: Number of nodes with available pods: 0
Feb 22 10:38:12.010: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:13.006: INFO: Number of nodes with available pods: 0
Feb 22 10:38:13.006: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:13.999: INFO: Number of nodes with available pods: 0
Feb 22 10:38:13.999: INFO: Node node2 is running more than one daemon pod
Feb 22 10:38:14.998: INFO: Number of nodes with available pods: 1
Feb 22 10:38:14.998: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6581, will wait for the garbage collector to delete the pods
Feb 22 10:38:15.105: INFO: Deleting DaemonSet.extensions daemon-set took: 29.275566ms
Feb 22 10:38:15.205: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.482942ms
Feb 22 10:38:18.841: INFO: Number of nodes with available pods: 0
Feb 22 10:38:18.841: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 10:38:18.848: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"68332"},"items":null}

Feb 22 10:38:18.855: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"68332"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:38:18.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6581" for this suite.

• [SLOW TEST:18.075 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":323,"skipped":5592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:38:18.988: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8998.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8998.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8998.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8998.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8998.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8998.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8998.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8998.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8998.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8998.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 4.22.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.22.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.22.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.22.4_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8998.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8998.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8998.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8998.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8998.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8998.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8998.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8998.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8998.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8998.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8998.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 4.22.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.22.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.22.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.22.4_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 10:38:23.558: INFO: Unable to read wheezy_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:23.601: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:23.609: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:23.619: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:23.720: INFO: Unable to read jessie_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:23.730: INFO: Unable to read jessie_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:23.742: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:23.752: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:23.832: INFO: Lookups using dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5 failed for: [wheezy_udp@dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_udp@dns-test-service.dns-8998.svc.cluster.local jessie_tcp@dns-test-service.dns-8998.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local]

Feb 22 10:38:28.852: INFO: Unable to read wheezy_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:28.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:28.906: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:28.919: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:28.994: INFO: Unable to read jessie_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:29.002: INFO: Unable to read jessie_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:29.012: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:29.022: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:29.107: INFO: Lookups using dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5 failed for: [wheezy_udp@dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_udp@dns-test-service.dns-8998.svc.cluster.local jessie_tcp@dns-test-service.dns-8998.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local]

Feb 22 10:38:33.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:33.888: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:33.901: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:33.916: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:33.983: INFO: Unable to read jessie_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:33.991: INFO: Unable to read jessie_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:34.001: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:34.012: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:34.074: INFO: Lookups using dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5 failed for: [wheezy_udp@dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_udp@dns-test-service.dns-8998.svc.cluster.local jessie_tcp@dns-test-service.dns-8998.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local]

Feb 22 10:38:38.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:38.889: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:38.903: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:38.917: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:39.022: INFO: Unable to read 172.20.22.4_udp@PTR from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: Get "https://172.20.0.1:443/api/v1/namespaces/dns-8998/pods/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5/proxy/results/172.20.22.4_udp@PTR": stream error: stream ID 2975; INTERNAL_ERROR; received from peer
Feb 22 10:38:39.140: INFO: Unable to read jessie_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:39.156: INFO: Unable to read jessie_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:39.169: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:39.179: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:39.235: INFO: Lookups using dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5 failed for: [wheezy_udp@dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local 172.20.22.4_udp@PTR jessie_udp@dns-test-service.dns-8998.svc.cluster.local jessie_tcp@dns-test-service.dns-8998.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local]

Feb 22 10:38:43.843: INFO: Unable to read wheezy_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:43.885: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:43.896: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:43.905: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:43.974: INFO: Unable to read jessie_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:43.982: INFO: Unable to read jessie_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:43.991: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:44.000: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:44.051: INFO: Lookups using dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5 failed for: [wheezy_udp@dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_udp@dns-test-service.dns-8998.svc.cluster.local jessie_tcp@dns-test-service.dns-8998.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local]

Feb 22 10:38:48.843: INFO: Unable to read wheezy_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:48.884: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:48.891: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:48.898: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:48.946: INFO: Unable to read jessie_udp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:48.952: INFO: Unable to read jessie_tcp@dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:48.958: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:48.964: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local from pod dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5: the server could not find the requested resource (get pods dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5)
Feb 22 10:38:49.007: INFO: Lookups using dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5 failed for: [wheezy_udp@dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@dns-test-service.dns-8998.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_udp@dns-test-service.dns-8998.svc.cluster.local jessie_tcp@dns-test-service.dns-8998.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8998.svc.cluster.local]

Feb 22 10:38:54.045: INFO: DNS probes using dns-8998/dns-test-e5d8ea47-4a35-41fb-9bfb-c83df25d34e5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:38:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8998" for this suite.

• [SLOW TEST:35.685 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":324,"skipped":5636,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:38:54.674: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:39:06.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4205" for this suite.

• [SLOW TEST:11.355 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":325,"skipped":5641,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:39:06.030: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Feb 22 10:39:06.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 create -f -'
Feb 22 10:39:08.490: INFO: stderr: ""
Feb 22 10:39:08.490: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 10:39:08.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 10:39:08.828: INFO: stderr: ""
Feb 22 10:39:08.828: INFO: stdout: "update-demo-nautilus-j8p22 update-demo-nautilus-x5pxt "
Feb 22 10:39:08.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get pods update-demo-nautilus-j8p22 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 10:39:09.096: INFO: stderr: ""
Feb 22 10:39:09.096: INFO: stdout: ""
Feb 22 10:39:09.096: INFO: update-demo-nautilus-j8p22 is created but not running
Feb 22 10:39:14.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 22 10:39:14.271: INFO: stderr: ""
Feb 22 10:39:14.271: INFO: stdout: "update-demo-nautilus-j8p22 update-demo-nautilus-x5pxt "
Feb 22 10:39:14.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get pods update-demo-nautilus-j8p22 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 10:39:14.435: INFO: stderr: ""
Feb 22 10:39:14.435: INFO: stdout: "true"
Feb 22 10:39:14.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get pods update-demo-nautilus-j8p22 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 22 10:39:14.570: INFO: stderr: ""
Feb 22 10:39:14.570: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 22 10:39:14.570: INFO: validating pod update-demo-nautilus-j8p22
Feb 22 10:39:14.659: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 10:39:14.659: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 10:39:14.659: INFO: update-demo-nautilus-j8p22 is verified up and running
Feb 22 10:39:14.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get pods update-demo-nautilus-x5pxt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 22 10:39:14.886: INFO: stderr: ""
Feb 22 10:39:14.886: INFO: stdout: "true"
Feb 22 10:39:14.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get pods update-demo-nautilus-x5pxt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 22 10:39:15.031: INFO: stderr: ""
Feb 22 10:39:15.031: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 22 10:39:15.031: INFO: validating pod update-demo-nautilus-x5pxt
Feb 22 10:39:15.121: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 10:39:15.121: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 10:39:15.121: INFO: update-demo-nautilus-x5pxt is verified up and running
STEP: using delete to clean up resources
Feb 22 10:39:15.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
Feb 22 10:39:15.317: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 10:39:15.317: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 22 10:39:15.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get rc,svc -l name=update-demo --no-headers'
Feb 22 10:39:15.745: INFO: stderr: "No resources found in kubectl-6580 namespace.\n"
Feb 22 10:39:15.745: INFO: stdout: ""
Feb 22 10:39:15.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-6580 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 10:39:16.064: INFO: stderr: ""
Feb 22 10:39:16.064: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:39:16.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6580" for this suite.

• [SLOW TEST:10.175 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":326,"skipped":5653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:39:16.207: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-343d565d-1a05-46ba-8249-4ffaff62ec48
STEP: Creating a pod to test consume secrets
Feb 22 10:39:16.765: INFO: Waiting up to 5m0s for pod "pod-secrets-69abd7ca-957c-4e35-811c-c2673fe13943" in namespace "secrets-7343" to be "Succeeded or Failed"
Feb 22 10:39:16.810: INFO: Pod "pod-secrets-69abd7ca-957c-4e35-811c-c2673fe13943": Phase="Pending", Reason="", readiness=false. Elapsed: 45.633991ms
Feb 22 10:39:18.835: INFO: Pod "pod-secrets-69abd7ca-957c-4e35-811c-c2673fe13943": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069995358s
Feb 22 10:39:20.846: INFO: Pod "pod-secrets-69abd7ca-957c-4e35-811c-c2673fe13943": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080787387s
STEP: Saw pod success
Feb 22 10:39:20.846: INFO: Pod "pod-secrets-69abd7ca-957c-4e35-811c-c2673fe13943" satisfied condition "Succeeded or Failed"
Feb 22 10:39:20.852: INFO: Trying to get logs from node node2 pod pod-secrets-69abd7ca-957c-4e35-811c-c2673fe13943 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:39:20.978: INFO: Waiting for pod pod-secrets-69abd7ca-957c-4e35-811c-c2673fe13943 to disappear
Feb 22 10:39:20.985: INFO: Pod pod-secrets-69abd7ca-957c-4e35-811c-c2673fe13943 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:39:20.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7343" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5698,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:39:21.024: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 10:39:21.406: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6602353d-fce7-4c50-9ba1-879237798cee" in namespace "projected-270" to be "Succeeded or Failed"
Feb 22 10:39:21.443: INFO: Pod "downwardapi-volume-6602353d-fce7-4c50-9ba1-879237798cee": Phase="Pending", Reason="", readiness=false. Elapsed: 37.061631ms
Feb 22 10:39:23.451: INFO: Pod "downwardapi-volume-6602353d-fce7-4c50-9ba1-879237798cee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045269171s
Feb 22 10:39:25.471: INFO: Pod "downwardapi-volume-6602353d-fce7-4c50-9ba1-879237798cee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065036959s
STEP: Saw pod success
Feb 22 10:39:25.471: INFO: Pod "downwardapi-volume-6602353d-fce7-4c50-9ba1-879237798cee" satisfied condition "Succeeded or Failed"
Feb 22 10:39:25.484: INFO: Trying to get logs from node node2 pod downwardapi-volume-6602353d-fce7-4c50-9ba1-879237798cee container client-container: <nil>
STEP: delete the pod
Feb 22 10:39:25.593: INFO: Waiting for pod downwardapi-volume-6602353d-fce7-4c50-9ba1-879237798cee to disappear
Feb 22 10:39:25.600: INFO: Pod downwardapi-volume-6602353d-fce7-4c50-9ba1-879237798cee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:39:25.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-270" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":328,"skipped":5730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:39:25.632: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Feb 22 10:39:25.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-714983222 --namespace=kubectl-266 api-versions'
Feb 22 10:39:26.010: INFO: stderr: ""
Feb 22 10:39:26.010: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:39:26.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-266" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":329,"skipped":5753,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:39:26.045: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-a17eb826-b638-45b9-b894-17615ab8f6eb
STEP: Creating a pod to test consume secrets
Feb 22 10:39:26.314: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b4af42f-6252-42b8-be0f-017154f5f1b8" in namespace "projected-8701" to be "Succeeded or Failed"
Feb 22 10:39:26.353: INFO: Pod "pod-projected-secrets-3b4af42f-6252-42b8-be0f-017154f5f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.247225ms
Feb 22 10:39:28.379: INFO: Pod "pod-projected-secrets-3b4af42f-6252-42b8-be0f-017154f5f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064239785s
Feb 22 10:39:30.387: INFO: Pod "pod-projected-secrets-3b4af42f-6252-42b8-be0f-017154f5f1b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072432825s
STEP: Saw pod success
Feb 22 10:39:30.387: INFO: Pod "pod-projected-secrets-3b4af42f-6252-42b8-be0f-017154f5f1b8" satisfied condition "Succeeded or Failed"
Feb 22 10:39:30.393: INFO: Trying to get logs from node node2 pod pod-projected-secrets-3b4af42f-6252-42b8-be0f-017154f5f1b8 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:39:30.461: INFO: Waiting for pod pod-projected-secrets-3b4af42f-6252-42b8-be0f-017154f5f1b8 to disappear
Feb 22 10:39:30.486: INFO: Pod pod-projected-secrets-3b4af42f-6252-42b8-be0f-017154f5f1b8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:39:30.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8701" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":5760,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:39:30.511: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-361019d1-c878-4017-9244-70a61eb735fc in namespace container-probe-629
Feb 22 10:39:34.862: INFO: Started pod liveness-361019d1-c878-4017-9244-70a61eb735fc in namespace container-probe-629
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 10:39:34.869: INFO: Initial restart count of pod liveness-361019d1-c878-4017-9244-70a61eb735fc is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:43:36.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-629" for this suite.

• [SLOW TEST:246.672 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":5770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:43:37.188: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 22 10:43:42.640: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:43:43.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8926" for this suite.

• [SLOW TEST:5.977 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":5834,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:43:43.166: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-6da68b77-7f2b-4d4c-a6ed-5cb7c9554590
STEP: Creating a pod to test consume configMaps
Feb 22 10:43:43.611: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-51014c9c-3883-44d9-b144-86b8737b3200" in namespace "projected-8932" to be "Succeeded or Failed"
Feb 22 10:43:43.656: INFO: Pod "pod-projected-configmaps-51014c9c-3883-44d9-b144-86b8737b3200": Phase="Pending", Reason="", readiness=false. Elapsed: 45.674292ms
Feb 22 10:43:45.700: INFO: Pod "pod-projected-configmaps-51014c9c-3883-44d9-b144-86b8737b3200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089702369s
Feb 22 10:43:47.747: INFO: Pod "pod-projected-configmaps-51014c9c-3883-44d9-b144-86b8737b3200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.135953336s
STEP: Saw pod success
Feb 22 10:43:47.747: INFO: Pod "pod-projected-configmaps-51014c9c-3883-44d9-b144-86b8737b3200" satisfied condition "Succeeded or Failed"
Feb 22 10:43:47.790: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-51014c9c-3883-44d9-b144-86b8737b3200 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 10:43:48.047: INFO: Waiting for pod pod-projected-configmaps-51014c9c-3883-44d9-b144-86b8737b3200 to disappear
Feb 22 10:43:48.091: INFO: Pod pod-projected-configmaps-51014c9c-3883-44d9-b144-86b8737b3200 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:43:48.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8932" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":5870,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:43:48.152: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 22 10:43:48.627: INFO: The status of Pod pod-update-fefa180b-3467-48a9-84af-70415fe6db4d is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:43:50.634: INFO: The status of Pod pod-update-fefa180b-3467-48a9-84af-70415fe6db4d is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:43:52.639: INFO: The status of Pod pod-update-fefa180b-3467-48a9-84af-70415fe6db4d is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 22 10:43:53.298: INFO: Successfully updated pod "pod-update-fefa180b-3467-48a9-84af-70415fe6db4d"
STEP: verifying the updated pod is in kubernetes
Feb 22 10:43:53.315: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:43:53.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5989" for this suite.

• [SLOW TEST:5.203 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":5872,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:43:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 22 10:43:53.579: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119" in namespace "downward-api-1293" to be "Succeeded or Failed"
Feb 22 10:43:53.763: INFO: Pod "downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119": Phase="Pending", Reason="", readiness=false. Elapsed: 183.329464ms
Feb 22 10:43:55.771: INFO: Pod "downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191276505s
Feb 22 10:43:57.787: INFO: Pod "downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119": Phase="Pending", Reason="", readiness=false. Elapsed: 4.207895808s
Feb 22 10:43:59.826: INFO: Pod "downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.247206706s
STEP: Saw pod success
Feb 22 10:43:59.827: INFO: Pod "downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119" satisfied condition "Succeeded or Failed"
Feb 22 10:43:59.837: INFO: Trying to get logs from node node2 pod downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119 container client-container: <nil>
STEP: delete the pod
Feb 22 10:44:00.045: INFO: Waiting for pod downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119 to disappear
Feb 22 10:44:00.055: INFO: Pod downwardapi-volume-21bbb78c-794f-4a5d-83b8-4a09d30f9119 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:44:00.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1293" for this suite.

• [SLOW TEST:6.808 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":5877,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:44:00.163: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-1615c4d1-9381-429d-b4f6-c271e0913137
STEP: Creating a pod to test consume secrets
Feb 22 10:44:00.707: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ccff68c-8b58-4755-9631-01d78c5816bd" in namespace "projected-2180" to be "Succeeded or Failed"
Feb 22 10:44:00.725: INFO: Pod "pod-projected-secrets-8ccff68c-8b58-4755-9631-01d78c5816bd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.608715ms
Feb 22 10:44:02.744: INFO: Pod "pod-projected-secrets-8ccff68c-8b58-4755-9631-01d78c5816bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037461907s
Feb 22 10:44:04.756: INFO: Pod "pod-projected-secrets-8ccff68c-8b58-4755-9631-01d78c5816bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049130332s
STEP: Saw pod success
Feb 22 10:44:04.756: INFO: Pod "pod-projected-secrets-8ccff68c-8b58-4755-9631-01d78c5816bd" satisfied condition "Succeeded or Failed"
Feb 22 10:44:04.764: INFO: Trying to get logs from node node2 pod pod-projected-secrets-8ccff68c-8b58-4755-9631-01d78c5816bd container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 10:44:04.822: INFO: Waiting for pod pod-projected-secrets-8ccff68c-8b58-4755-9631-01d78c5816bd to disappear
Feb 22 10:44:04.833: INFO: Pod pod-projected-secrets-8ccff68c-8b58-4755-9631-01d78c5816bd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:44:04.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2180" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":5890,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:44:04.886: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6309, will wait for the garbage collector to delete the pods
Feb 22 10:44:11.402: INFO: Deleting Job.batch foo took: 72.778069ms
Feb 22 10:44:11.603: INFO: Terminating Job.batch foo pods took: 201.021983ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:44:44.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6309" for this suite.

• [SLOW TEST:39.763 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":337,"skipped":5894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:44:44.649: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:44:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4813" for this suite.

• [SLOW TEST:13.601 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":338,"skipped":5930,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:44:58.251: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 22 10:45:00.460: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:45:00.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0222 10:45:00.460000      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-7694" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":339,"skipped":5945,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:45:00.521: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:45:01.129: INFO: Creating ReplicaSet my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca
Feb 22 10:45:01.321: INFO: Pod name my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca: Found 0 pods out of 1
Feb 22 10:45:06.373: INFO: Pod name my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca: Found 1 pods out of 1
Feb 22 10:45:06.373: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca" is running
Feb 22 10:45:08.413: INFO: Pod "my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca-8vpmn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-22 10:45:01 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-22 10:45:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-22 10:45:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-22 10:45:01 +0000 UTC Reason: Message:}])
Feb 22 10:45:08.413: INFO: Trying to dial the pod
Feb 22 10:45:13.545: INFO: Controller my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca: Got expected result from replica 1 [my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca-8vpmn]: "my-hostname-basic-c8b32162-e28c-4d87-996f-128b6ca796ca-8vpmn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:45:13.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-337" for this suite.

• [SLOW TEST:13.071 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":340,"skipped":5951,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:45:13.592: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:45:13.763: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Creating first CR 
Feb 22 10:45:16.519: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-22T10:45:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-22T10:45:16Z]] name:name1 resourceVersion:69608 uid:41dff154-b7cb-4d8e-b042-ab877b426406] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 22 10:45:26.563: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-22T10:45:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-22T10:45:26Z]] name:name2 resourceVersion:69640 uid:5019981c-46bc-48b0-838d-fd6505eaf6f5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 22 10:45:36.599: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-22T10:45:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-22T10:45:36Z]] name:name1 resourceVersion:69653 uid:41dff154-b7cb-4d8e-b042-ab877b426406] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 22 10:45:46.622: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-22T10:45:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-22T10:45:46Z]] name:name2 resourceVersion:69666 uid:5019981c-46bc-48b0-838d-fd6505eaf6f5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 22 10:45:56.663: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-22T10:45:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-22T10:45:36Z]] name:name1 resourceVersion:69680 uid:41dff154-b7cb-4d8e-b042-ab877b426406] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 22 10:46:06.708: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-22T10:45:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-22T10:45:46Z]] name:name2 resourceVersion:69694 uid:5019981c-46bc-48b0-838d-fd6505eaf6f5] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:46:17.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8022" for this suite.

• [SLOW TEST:63.758 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":341,"skipped":5952,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:46:17.351: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8349
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 22 10:46:17.597: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 22 10:46:17.930: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:46:19.985: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:46:21.998: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 22 10:46:23.943: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 10:46:25.946: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 10:46:27.942: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 10:46:29.940: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 10:46:31.940: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 10:46:33.946: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 10:46:35.937: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 10:46:38.039: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 22 10:46:39.945: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 22 10:46:39.958: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Feb 22 10:46:44.146: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Feb 22 10:46:44.146: INFO: Going to poll 172.21.40.135 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Feb 22 10:46:44.153: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.40.135:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8349 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 10:46:44.153: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 10:46:44.655: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 22 10:46:44.655: INFO: Going to poll 172.21.104.58 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Feb 22 10:46:44.665: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.104.58:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8349 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 22 10:46:44.665: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
Feb 22 10:46:45.234: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:46:45.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8349" for this suite.

• [SLOW TEST:27.918 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":342,"skipped":5971,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:46:45.269: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Feb 22 10:46:45.594: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Feb 22 10:46:45.747: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:46:45.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1790" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":343,"skipped":5987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:46:45.923: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-1823
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1823
STEP: Deleting pre-stop pod
Feb 22 10:47:01.371: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:47:01.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1823" for this suite.

• [SLOW TEST:15.580 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":344,"skipped":6032,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:47:01.503: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 22 10:47:01.714: INFO: Got root ca configmap in namespace "svcaccounts-2973"
Feb 22 10:47:01.777: INFO: Deleted root ca configmap in namespace "svcaccounts-2973"
STEP: waiting for a new root ca configmap created
Feb 22 10:47:02.290: INFO: Recreated root ca configmap in namespace "svcaccounts-2973"
Feb 22 10:47:02.310: INFO: Updated root ca configmap in namespace "svcaccounts-2973"
STEP: waiting for the root ca configmap reconciled
Feb 22 10:47:02.819: INFO: Reconciled root ca configmap in namespace "svcaccounts-2973"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:47:02.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2973" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":345,"skipped":6040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 22 10:47:02.847: INFO: >>> kubeConfig: /tmp/kubeconfig-714983222
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 22 10:47:15.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7327" for this suite.

• [SLOW TEST:13.086 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":346,"skipped":6072,"failed":0}
SSSSSSSSSSSSSSSFeb 22 10:47:15.935: INFO: Running AfterSuite actions on all nodes
Feb 22 10:47:15.935: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Feb 22 10:47:15.935: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Feb 22 10:47:15.935: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Feb 22 10:47:15.935: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Feb 22 10:47:15.935: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Feb 22 10:47:15.935: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Feb 22 10:47:15.935: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Feb 22 10:47:15.935: INFO: Running AfterSuite actions on node 1
Feb 22 10:47:15.935: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6087,"failed":0}

Ran 346 of 6433 Specs in 7040.273 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6087 Skipped
PASS

Ginkgo ran 1 suite in 1h57m26.447701133s
Test Suite Passed
