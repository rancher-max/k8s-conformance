I0406 02:56:43.205502      21 e2e.go:129] Starting e2e run "ecf983b6-3e7c-47bb-95e8-fca82b9cc807" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1649213803 - Will randomize all specs
Will run 346 of 6433 specs

Apr  6 02:56:44.695: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 02:56:44.699: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr  6 02:56:44.759: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr  6 02:56:44.823: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr  6 02:56:44.823: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Apr  6 02:56:44.823: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr  6 02:56:44.840: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Apr  6 02:56:44.840: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-vpc-block-csi-node' (0 seconds elapsed)
Apr  6 02:56:44.840: INFO: e2e test version: v1.22.5
Apr  6 02:56:44.845: INFO: kube-apiserver version: v1.22.5+5c84e52
Apr  6 02:56:44.845: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 02:56:44.860: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:56:44.860: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
W0406 02:56:45.023570      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Apr  6 02:56:45.023: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-5d820371-5c19-4b56-83d2-2b6a062e32e3
STEP: Creating a pod to test consume secrets
Apr  6 02:56:45.112: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab" in namespace "projected-1147" to be "Succeeded or Failed"
Apr  6 02:56:45.123: INFO: Pod "pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.743348ms
Apr  6 02:56:47.137: INFO: Pod "pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02465956s
Apr  6 02:56:49.151: INFO: Pod "pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039415191s
Apr  6 02:56:51.185: INFO: Pod "pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072997654s
Apr  6 02:56:53.198: INFO: Pod "pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.086510339s
STEP: Saw pod success
Apr  6 02:56:53.198: INFO: Pod "pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab" satisfied condition "Succeeded or Failed"
Apr  6 02:56:53.209: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 02:56:53.364: INFO: Waiting for pod pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab to disappear
Apr  6 02:56:53.374: INFO: Pod pod-projected-secrets-8d3b7b0e-0a95-47f7-9edb-03f7551654ab no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:56:53.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1147" for this suite.

• [SLOW TEST:8.549 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":33,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:56:53.409: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 02:56:53.604: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:56:56.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6435" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":2,"skipped":72,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:56:57.008: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr  6 02:56:57.244: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 02:57:57.444: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Apr  6 02:57:57.553: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr  6 02:57:57.588: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr  6 02:57:57.651: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr  6 02:57:57.700: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Apr  6 02:57:57.776: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Apr  6 02:57:57.822: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:58:12.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1634" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:75.260 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":3,"skipped":76,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:58:12.267: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Apr  6 02:58:22.545: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0406 02:58:22.545531      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:58:22.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1476" for this suite.

• [SLOW TEST:10.317 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":4,"skipped":77,"failed":0}
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:58:22.584: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr  6 02:58:22.940: INFO: starting watch
STEP: patching
STEP: updating
Apr  6 02:58:22.974: INFO: waiting for watch events with expected annotations
Apr  6 02:58:22.974: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:58:23.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5953" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":5,"skipped":77,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:58:23.108: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-f9zl
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 02:58:23.441: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f9zl" in namespace "subpath-2143" to be "Succeeded or Failed"
Apr  6 02:58:23.450: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Pending", Reason="", readiness=false. Elapsed: 9.630871ms
Apr  6 02:58:25.464: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 2.022859225s
Apr  6 02:58:27.482: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 4.041502061s
Apr  6 02:58:29.503: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 6.061969028s
Apr  6 02:58:31.534: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 8.092852225s
Apr  6 02:58:33.548: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 10.107680901s
Apr  6 02:58:35.560: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 12.118945867s
Apr  6 02:58:37.582: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 14.141394622s
Apr  6 02:58:39.597: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 16.155903933s
Apr  6 02:58:41.614: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 18.173192413s
Apr  6 02:58:43.633: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Running", Reason="", readiness=true. Elapsed: 20.192205006s
Apr  6 02:58:45.648: INFO: Pod "pod-subpath-test-configmap-f9zl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.207091742s
STEP: Saw pod success
Apr  6 02:58:45.648: INFO: Pod "pod-subpath-test-configmap-f9zl" satisfied condition "Succeeded or Failed"
Apr  6 02:58:45.660: INFO: Trying to get logs from node 10.241.0.102 pod pod-subpath-test-configmap-f9zl container test-container-subpath-configmap-f9zl: <nil>
STEP: delete the pod
Apr  6 02:58:45.749: INFO: Waiting for pod pod-subpath-test-configmap-f9zl to disappear
Apr  6 02:58:45.761: INFO: Pod pod-subpath-test-configmap-f9zl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f9zl
Apr  6 02:58:45.761: INFO: Deleting pod "pod-subpath-test-configmap-f9zl" in namespace "subpath-2143"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:58:45.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2143" for this suite.

• [SLOW TEST:22.694 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":6,"skipped":111,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:58:45.803: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 02:58:45.983: INFO: Creating deployment "webserver-deployment"
Apr  6 02:58:46.001: INFO: Waiting for observed generation 1
Apr  6 02:58:48.023: INFO: Waiting for all required pods to come up
Apr  6 02:58:48.035: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr  6 02:58:56.068: INFO: Waiting for deployment "webserver-deployment" to complete
Apr  6 02:58:56.087: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr  6 02:58:56.168: INFO: Updating deployment webserver-deployment
Apr  6 02:58:56.168: INFO: Waiting for observed generation 2
Apr  6 02:58:58.193: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr  6 02:58:58.203: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr  6 02:58:58.212: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr  6 02:58:58.237: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr  6 02:58:58.237: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr  6 02:58:58.265: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr  6 02:58:58.285: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr  6 02:58:58.285: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr  6 02:58:58.311: INFO: Updating deployment webserver-deployment
Apr  6 02:58:58.311: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr  6 02:58:58.331: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr  6 02:58:58.339: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr  6 02:58:58.357: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8006  a23bb341-e5be-4958-a144-41dbea95562e 55332 3 2022-04-06 02:58:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-04-06 02:58:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00400b358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-04-06 02:58:55 +0000 UTC,LastTransitionTime:2022-04-06 02:58:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-04-06 02:58:56 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr  6 02:58:58.371: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8006  581a5331-8cf1-4792-ac9c-d19e7fd393b6 55334 3 2022-04-06 02:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a23bb341-e5be-4958-a144-41dbea95562e 0xc00400a087 0xc00400a088}] []  [{kube-controller-manager Update apps/v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a23bb341-e5be-4958-a144-41dbea95562e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00400a128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 02:58:58.371: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr  6 02:58:58.371: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-8006  23d7b6d3-7728-4d5f-a21e-1b910c424523 55333 3 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a23bb341-e5be-4958-a144-41dbea95562e 0xc00400a187 0xc00400a188}] []  [{kube-controller-manager Update apps/v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a23bb341-e5be-4958-a144-41dbea95562e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 02:58:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00400a218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr  6 02:58:58.389: INFO: Pod "webserver-deployment-795d758f88-9mb8c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9mb8c webserver-deployment-795d758f88- deployment-8006  0683de82-e66c-4405-84c9-cdb7a4036387 55299 0 2022-04-06 02:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:bca75f94791de3e8411fad7af7da8e1e90e110e6d29eeca0007d21ef4d179b31 cni.projectcalico.org/podIP:172.17.77.60/32 cni.projectcalico.org/podIPs:172.17.77.60/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.60"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.60"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 581a5331-8cf1-4792-ac9c-d19e7fd393b6 0xc003f2e0e7 0xc003f2e0e8}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581a5331-8cf1-4792-ac9c-d19e7fd393b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7mmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7mmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.100,PodIP:,StartTime:2022-04-06 02:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.389: INFO: Pod "webserver-deployment-795d758f88-fcpkn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fcpkn webserver-deployment-795d758f88- deployment-8006  0b02fa4d-17b7-46ff-a522-d86d0d673e8a 55311 0 2022-04-06 02:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:6b0941a5838f7f36a9e9bb15679ddfdf4f48413debd372a09b9ec5dad7a1ddca cni.projectcalico.org/podIP:172.17.95.157/32 cni.projectcalico.org/podIPs:172.17.95.157/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.157"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.157"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 581a5331-8cf1-4792-ac9c-d19e7fd393b6 0xc003f2e377 0xc003f2e378}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581a5331-8cf1-4792-ac9c-d19e7fd393b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.95.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hb6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hb6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.101,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.101,PodIP:172.17.95.157,StartTime:2022-04-06 02:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.95.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.389: INFO: Pod "webserver-deployment-795d758f88-g2xcw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-g2xcw webserver-deployment-795d758f88- deployment-8006  54d1d111-7b04-449a-b9cc-94bf85eefb22 55284 0 2022-04-06 02:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:4b6940646a846a719a630ec13ab9c8983ad93b59f1992686a05ce38e1691a311 cni.projectcalico.org/podIP:172.17.77.59/32 cni.projectcalico.org/podIPs:172.17.77.59/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.59"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.59"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 581a5331-8cf1-4792-ac9c-d19e7fd393b6 0xc003f2e637 0xc003f2e638}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581a5331-8cf1-4792-ac9c-d19e7fd393b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6mn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6mn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.100,PodIP:,StartTime:2022-04-06 02:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.390: INFO: Pod "webserver-deployment-795d758f88-p5g4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-p5g4r webserver-deployment-795d758f88- deployment-8006  3d59d111-f930-4ba7-88cc-a2c040541099 55280 0 2022-04-06 02:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:66f4334cace491ea36a3642d25262dcb999ffb862262b462356db4f1031603b3 cni.projectcalico.org/podIP:172.17.96.110/32 cni.projectcalico.org/podIPs:172.17.96.110/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.110"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.110"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 581a5331-8cf1-4792-ac9c-d19e7fd393b6 0xc003f2e8c7 0xc003f2e8c8}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581a5331-8cf1-4792-ac9c-d19e7fd393b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6q7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6q7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.102,PodIP:,StartTime:2022-04-06 02:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.390: INFO: Pod "webserver-deployment-795d758f88-sf2dq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sf2dq webserver-deployment-795d758f88- deployment-8006  44ea0b13-783a-4fab-8d70-f7f772709f99 55297 0 2022-04-06 02:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:18ce1669630591a6f97794a9ee1734f30d80fa676b8e1fb287803ea9e04739ea cni.projectcalico.org/podIP:172.17.96.111/32 cni.projectcalico.org/podIPs:172.17.96.111/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.111"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.111"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 581a5331-8cf1-4792-ac9c-d19e7fd393b6 0xc003f2eb57 0xc003f2eb58}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581a5331-8cf1-4792-ac9c-d19e7fd393b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-04-06 02:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mzjr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mzjr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.102,PodIP:,StartTime:2022-04-06 02:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.390: INFO: Pod "webserver-deployment-847dcfb7fb-4mln4" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-4mln4 webserver-deployment-847dcfb7fb- deployment-8006  438e3496-8c83-42f3-ab82-41f8e121b79a 55119 0 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:c1f5f7b9c6d7a9b5927818207b47361a01ab9301c9b7fabbe0af43a4f0f2a115 cni.projectcalico.org/podIP:172.17.95.154/32 cni.projectcalico.org/podIPs:172.17.95.154/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.154"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.154"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003f2ede7 0xc003f2ede8}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 02:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.95.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cgvnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cgvnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.101,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.101,PodIP:172.17.95.154,StartTime:2022-04-06 02:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 02:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://be43df36baaf48e1630ae1c26689b1af22c11479ff42d155a84303fa3c289b76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.95.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.390: INFO: Pod "webserver-deployment-847dcfb7fb-8rvvv" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8rvvv webserver-deployment-847dcfb7fb- deployment-8006  fca456bb-5251-46a5-9dfa-bb04eb739495 55339 0 2022-04-06 02:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003f2f057 0xc003f2f058}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jq5lw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jq5lw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.390: INFO: Pod "webserver-deployment-847dcfb7fb-95khz" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-95khz webserver-deployment-847dcfb7fb- deployment-8006  4bc7d430-8f49-433d-b0a5-57f5cd5c6e88 55111 0 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:dc49c94cbbbd202294a2f1dc37f7a590750f577901fa9cc321e30192f1282b8a cni.projectcalico.org/podIP:172.17.95.155/32 cni.projectcalico.org/podIPs:172.17.95.155/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.155"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.155"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003f2f1f7 0xc003f2f1f8}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 02:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.95.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5h4mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5h4mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.101,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.101,PodIP:172.17.95.155,StartTime:2022-04-06 02:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 02:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://75282d2135b627d9835b733baec64371fbddb003d9086ba454c98e06ed70e839,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.95.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.390: INFO: Pod "webserver-deployment-847dcfb7fb-hblbb" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hblbb webserver-deployment-847dcfb7fb- deployment-8006  78762840-eb4c-40f0-9bb3-96add0cce4bc 55180 0 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:a3575a5f027b7f231499701687c872254b6731bae67ed69d28cd775742c8b853 cni.projectcalico.org/podIP:172.17.96.107/32 cni.projectcalico.org/podIPs:172.17.96.107/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.107"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.107"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003f2f487 0xc003f2f488}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 02:58:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.96.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzjm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzjm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.102,PodIP:172.17.96.107,StartTime:2022-04-06 02:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 02:58:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://2deae5ba4d1ac66f1ca3cbd6a83a3df04732626997d6dc55db669b33a3ee4738,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.96.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.391: INFO: Pod "webserver-deployment-847dcfb7fb-lp6m6" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lp6m6 webserver-deployment-847dcfb7fb- deployment-8006  4644db62-1d59-4eb5-ada4-d719e5d13274 55159 0 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:e356f29ce7b514f343964b2f0274e5f154a0fada09ee5fa048c8ff2cf788999d cni.projectcalico.org/podIP:172.17.77.57/32 cni.projectcalico.org/podIPs:172.17.77.57/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.57"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.57"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003f2f717 0xc003f2f718}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 02:58:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.77.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvh7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvh7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.100,PodIP:172.17.77.57,StartTime:2022-04-06 02:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 02:58:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://c241e83f8a3fc73d4a1943bc315d3df50e66a0c7eecf5789a9d378a755311c1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.77.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.391: INFO: Pod "webserver-deployment-847dcfb7fb-nrjtm" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nrjtm webserver-deployment-847dcfb7fb- deployment-8006  97217643-7a7b-4da2-ba42-36f6cddcffcf 55162 0 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:6d4f7920afa0af5609ef4ab76abbffd9da6aa070e82b6a45f1e37436c0c19c9d cni.projectcalico.org/podIP:172.17.77.58/32 cni.projectcalico.org/podIPs:172.17.77.58/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.58"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.58"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003f2f9a7 0xc003f2f9a8}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 02:58:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.77.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t57q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t57q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.100,PodIP:172.17.77.58,StartTime:2022-04-06 02:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 02:58:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://e4fef9402ec3c85051c81e254621171f923e84ef4740396a8c8ec502ba342e9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.77.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.391: INFO: Pod "webserver-deployment-847dcfb7fb-tj24d" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tj24d webserver-deployment-847dcfb7fb- deployment-8006  9635491a-43c7-4836-916e-c55e54bbd2f1 55166 0 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:7be07bd2106bf85fec3f73730a0d5ff3ad2028ab3b69c4b64c8871aab9552c11 cni.projectcalico.org/podIP:172.17.77.56/32 cni.projectcalico.org/podIPs:172.17.77.56/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.56"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.56"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003f2fc47 0xc003f2fc48}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 02:58:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.77.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp9g2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp9g2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.100,PodIP:172.17.77.56,StartTime:2022-04-06 02:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 02:58:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://6d0cff8e199c2a0292ad9f886f44064c13e36db776cc2f33ca83e2a87d532baf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.77.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.391: INFO: Pod "webserver-deployment-847dcfb7fb-vd2qb" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vd2qb webserver-deployment-847dcfb7fb- deployment-8006  c9ff3761-f4c4-4bed-8762-d23f0aea7196 55176 0 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:6921b67deb1ef40ede2c9e3380850dbc6f56720e6e36b38924ef0bfaf29c9460 cni.projectcalico.org/podIP:172.17.96.109/32 cni.projectcalico.org/podIPs:172.17.96.109/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.109"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.109"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003f2fed7 0xc003f2fed8}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 02:58:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.96.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btnj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btnj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.102,PodIP:172.17.96.109,StartTime:2022-04-06 02:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 02:58:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://7d94945706d4c223ff1b311fc3a266c59813c5cbcab093c8f26ddf75e2d9ef1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.96.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 02:58:58.391: INFO: Pod "webserver-deployment-847dcfb7fb-wbcds" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wbcds webserver-deployment-847dcfb7fb- deployment-8006  371f1c78-4565-482e-8ba2-11083ae26931 55113 0 2022-04-06 02:58:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:e3a1c8cca468a40936ec18b3cd443c35d830fedf42bc1c966dbaf4b2cee63ada cni.projectcalico.org/podIP:172.17.95.156/32 cni.projectcalico.org/podIPs:172.17.95.156/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.156"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.156"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 23d7b6d3-7728-4d5f-a21e-1b910c424523 0xc003e46167 0xc003e46168}] []  [{kube-controller-manager Update v1 2022-04-06 02:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23d7b6d3-7728-4d5f-a21e-1b910c424523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 02:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 02:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.95.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbdc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbdc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.101,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k84xl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 02:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.101,PodIP:172.17.95.156,StartTime:2022-04-06 02:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 02:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://84eba02839011ffabf4c616c19e67774de117b0013e2f5f837440b3560c0266c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.95.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:58:58.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8006" for this suite.

• [SLOW TEST:12.674 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":7,"skipped":122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:58:58.478: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Apr  6 02:58:58.805: INFO: created test-podtemplate-1
Apr  6 02:58:58.822: INFO: created test-podtemplate-2
Apr  6 02:58:58.841: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Apr  6 02:58:58.871: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Apr  6 02:58:58.912: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:58:58.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6596" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":8,"skipped":168,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:58:58.983: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Apr  6 02:58:59.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7879 create -f -'
Apr  6 02:59:02.857: INFO: stderr: ""
Apr  6 02:59:02.857: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr  6 02:59:03.867: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 02:59:03.867: INFO: Found 0 / 1
Apr  6 02:59:04.871: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 02:59:04.871: INFO: Found 0 / 1
Apr  6 02:59:05.868: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 02:59:05.868: INFO: Found 1 / 1
Apr  6 02:59:05.868: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr  6 02:59:05.878: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 02:59:05.878: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 02:59:05.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7879 patch pod agnhost-primary-b5lmb -p {"metadata":{"annotations":{"x":"y"}}}'
Apr  6 02:59:06.148: INFO: stderr: ""
Apr  6 02:59:06.148: INFO: stdout: "pod/agnhost-primary-b5lmb patched\n"
STEP: checking annotations
Apr  6 02:59:06.163: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 02:59:06.163: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:59:06.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7879" for this suite.

• [SLOW TEST:7.224 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":9,"skipped":169,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:59:06.207: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr  6 02:59:06.578: INFO: Waiting up to 5m0s for pod "pod-3a17b360-4ff9-4648-8af4-24f571b62def" in namespace "emptydir-9636" to be "Succeeded or Failed"
Apr  6 02:59:06.594: INFO: Pod "pod-3a17b360-4ff9-4648-8af4-24f571b62def": Phase="Pending", Reason="", readiness=false. Elapsed: 16.146112ms
Apr  6 02:59:08.609: INFO: Pod "pod-3a17b360-4ff9-4648-8af4-24f571b62def": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031077372s
STEP: Saw pod success
Apr  6 02:59:08.609: INFO: Pod "pod-3a17b360-4ff9-4648-8af4-24f571b62def" satisfied condition "Succeeded or Failed"
Apr  6 02:59:08.622: INFO: Trying to get logs from node 10.241.0.102 pod pod-3a17b360-4ff9-4648-8af4-24f571b62def container test-container: <nil>
STEP: delete the pod
Apr  6 02:59:08.695: INFO: Waiting for pod pod-3a17b360-4ff9-4648-8af4-24f571b62def to disappear
Apr  6 02:59:08.705: INFO: Pod pod-3a17b360-4ff9-4648-8af4-24f571b62def no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:59:08.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9636" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:59:08.741: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Apr  6 02:59:08.927: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:59:13.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1239" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":11,"skipped":244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:59:13.561: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:59:13.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6653" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":12,"skipped":290,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:59:14.015: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1359
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-1359
Apr  6 02:59:14.298: INFO: Found 0 stateful pods, waiting for 1
Apr  6 02:59:24.311: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Apr  6 02:59:24.354: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Apr  6 02:59:24.382: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Apr  6 02:59:24.389: INFO: Observed &StatefulSet event: ADDED
Apr  6 02:59:24.389: INFO: Found Statefulset ss in namespace statefulset-1359 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr  6 02:59:24.389: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Apr  6 02:59:24.389: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr  6 02:59:24.405: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Apr  6 02:59:24.411: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Apr  6 02:59:24.411: INFO: Deleting all statefulset in ns statefulset-1359
Apr  6 02:59:24.419: INFO: Scaling statefulset ss to 0
Apr  6 02:59:34.474: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 02:59:34.484: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 02:59:34.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1359" for this suite.

• [SLOW TEST:20.541 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":13,"skipped":297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 02:59:34.556: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr  6 02:59:34.707: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 02:59:43.993: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:00:17.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8416" for this suite.

• [SLOW TEST:42.703 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":14,"skipped":366,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:00:17.260: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Apr  6 03:00:57.572: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0406 03:00:57.572068      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 03:00:57.572: INFO: Deleting pod "simpletest.rc-6sgg6" in namespace "gc-8402"
Apr  6 03:00:57.614: INFO: Deleting pod "simpletest.rc-ckqj9" in namespace "gc-8402"
Apr  6 03:00:57.639: INFO: Deleting pod "simpletest.rc-h8lck" in namespace "gc-8402"
Apr  6 03:00:57.687: INFO: Deleting pod "simpletest.rc-j9n69" in namespace "gc-8402"
Apr  6 03:00:57.721: INFO: Deleting pod "simpletest.rc-jzltj" in namespace "gc-8402"
Apr  6 03:00:57.773: INFO: Deleting pod "simpletest.rc-kk2f2" in namespace "gc-8402"
Apr  6 03:00:57.802: INFO: Deleting pod "simpletest.rc-m2mf7" in namespace "gc-8402"
Apr  6 03:00:57.835: INFO: Deleting pod "simpletest.rc-nwqzx" in namespace "gc-8402"
Apr  6 03:00:57.866: INFO: Deleting pod "simpletest.rc-rzjvg" in namespace "gc-8402"
Apr  6 03:00:57.922: INFO: Deleting pod "simpletest.rc-srcjq" in namespace "gc-8402"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:00:57.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8402" for this suite.

• [SLOW TEST:40.716 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":15,"skipped":379,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:00:57.977: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Apr  6 03:00:58.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6927 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr  6 03:00:58.298: INFO: stderr: ""
Apr  6 03:00:58.298: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Apr  6 03:00:58.298: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr  6 03:00:58.298: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6927" to be "running and ready, or succeeded"
Apr  6 03:00:58.308: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.405418ms
Apr  6 03:01:00.319: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020724278s
Apr  6 03:01:02.329: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.030845286s
Apr  6 03:01:02.329: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr  6 03:01:02.329: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr  6 03:01:02.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6927 logs logs-generator logs-generator'
Apr  6 03:01:02.432: INFO: stderr: ""
Apr  6 03:01:02.432: INFO: stdout: "I0406 03:01:00.205151       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/rkf 454\nI0406 03:01:00.404858       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/n26p 595\nI0406 03:01:00.605636       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/rgn 390\nI0406 03:01:00.804962       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/jf6 551\nI0406 03:01:01.005341       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/qg8h 318\nI0406 03:01:01.205649       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/4qk 345\nI0406 03:01:01.405049       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/5nn 370\nI0406 03:01:01.605394       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vmxk 576\nI0406 03:01:01.804660       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/6bw4 227\nI0406 03:01:02.005001       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/z6rg 320\nI0406 03:01:02.205359       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/sb9x 324\nI0406 03:01:02.405693       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/lh5 399\n"
STEP: limiting log lines
Apr  6 03:01:02.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6927 logs logs-generator logs-generator --tail=1'
Apr  6 03:01:02.543: INFO: stderr: ""
Apr  6 03:01:02.543: INFO: stdout: "I0406 03:01:02.405693       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/lh5 399\n"
Apr  6 03:01:02.543: INFO: got output "I0406 03:01:02.405693       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/lh5 399\n"
STEP: limiting log bytes
Apr  6 03:01:02.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6927 logs logs-generator logs-generator --limit-bytes=1'
Apr  6 03:01:02.744: INFO: stderr: ""
Apr  6 03:01:02.744: INFO: stdout: "I"
Apr  6 03:01:02.744: INFO: got output "I"
STEP: exposing timestamps
Apr  6 03:01:02.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6927 logs logs-generator logs-generator --tail=1 --timestamps'
Apr  6 03:01:02.824: INFO: stderr: ""
Apr  6 03:01:02.824: INFO: stdout: "2022-04-05T22:01:02.805427793-05:00 I0406 03:01:02.805364       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/7pk 447\n"
Apr  6 03:01:02.824: INFO: got output "2022-04-05T22:01:02.805427793-05:00 I0406 03:01:02.805364       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/7pk 447\n"
STEP: restricting to a time range
Apr  6 03:01:05.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6927 logs logs-generator logs-generator --since=1s'
Apr  6 03:01:05.428: INFO: stderr: ""
Apr  6 03:01:05.428: INFO: stdout: "I0406 03:01:04.605708       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/5pk 396\nI0406 03:01:04.805112       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/sn5j 502\nI0406 03:01:05.005469       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/ngqz 383\nI0406 03:01:05.205033       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/ttsl 254\nI0406 03:01:05.405429       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/hfqc 539\n"
Apr  6 03:01:05.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6927 logs logs-generator logs-generator --since=24h'
Apr  6 03:01:05.535: INFO: stderr: ""
Apr  6 03:01:05.535: INFO: stdout: "I0406 03:01:00.205151       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/rkf 454\nI0406 03:01:00.404858       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/n26p 595\nI0406 03:01:00.605636       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/rgn 390\nI0406 03:01:00.804962       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/jf6 551\nI0406 03:01:01.005341       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/qg8h 318\nI0406 03:01:01.205649       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/4qk 345\nI0406 03:01:01.405049       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/5nn 370\nI0406 03:01:01.605394       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vmxk 576\nI0406 03:01:01.804660       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/6bw4 227\nI0406 03:01:02.005001       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/z6rg 320\nI0406 03:01:02.205359       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/sb9x 324\nI0406 03:01:02.405693       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/lh5 399\nI0406 03:01:02.605046       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/94d 497\nI0406 03:01:02.805364       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/7pk 447\nI0406 03:01:03.005694       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/spm7 421\nI0406 03:01:03.205034       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/dld 583\nI0406 03:01:03.405211       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/7hxz 413\nI0406 03:01:03.605662       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/qr9p 579\nI0406 03:01:03.805061       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/h9n 232\nI0406 03:01:04.005444       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/pj2 203\nI0406 03:01:04.204724       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/2km 321\nI0406 03:01:04.405229       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/cs7m 210\nI0406 03:01:04.605708       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/5pk 396\nI0406 03:01:04.805112       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/sn5j 502\nI0406 03:01:05.005469       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/ngqz 383\nI0406 03:01:05.205033       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/ttsl 254\nI0406 03:01:05.405429       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/hfqc 539\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Apr  6 03:01:05.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6927 delete pod logs-generator'
Apr  6 03:01:06.999: INFO: stderr: ""
Apr  6 03:01:06.999: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:01:06.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6927" for this suite.

• [SLOW TEST:9.094 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":16,"skipped":389,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:01:07.071: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-9254d623-6140-4c36-ada1-16ef38115e7c in namespace container-probe-6601
Apr  6 03:01:09.299: INFO: Started pod liveness-9254d623-6140-4c36-ada1-16ef38115e7c in namespace container-probe-6601
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 03:01:09.307: INFO: Initial restart count of pod liveness-9254d623-6140-4c36-ada1-16ef38115e7c is 0
Apr  6 03:01:29.450: INFO: Restart count of pod container-probe-6601/liveness-9254d623-6140-4c36-ada1-16ef38115e7c is now 1 (20.142092986s elapsed)
Apr  6 03:01:49.575: INFO: Restart count of pod container-probe-6601/liveness-9254d623-6140-4c36-ada1-16ef38115e7c is now 2 (40.267928533s elapsed)
Apr  6 03:02:09.740: INFO: Restart count of pod container-probe-6601/liveness-9254d623-6140-4c36-ada1-16ef38115e7c is now 3 (1m0.432415255s elapsed)
Apr  6 03:02:29.891: INFO: Restart count of pod container-probe-6601/liveness-9254d623-6140-4c36-ada1-16ef38115e7c is now 4 (1m20.583210524s elapsed)
Apr  6 03:03:32.371: INFO: Restart count of pod container-probe-6601/liveness-9254d623-6140-4c36-ada1-16ef38115e7c is now 5 (2m23.063104788s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:03:32.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6601" for this suite.

• [SLOW TEST:145.373 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":17,"skipped":405,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:03:32.444: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-838be0f1-5a62-4bc2-a16c-8d103149f07a
STEP: Creating a pod to test consume configMaps
Apr  6 03:03:32.786: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ca50844b-5edc-4b0c-83a9-d8f8ca1a8ee2" in namespace "projected-9776" to be "Succeeded or Failed"
Apr  6 03:03:32.798: INFO: Pod "pod-projected-configmaps-ca50844b-5edc-4b0c-83a9-d8f8ca1a8ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.623048ms
Apr  6 03:03:34.808: INFO: Pod "pod-projected-configmaps-ca50844b-5edc-4b0c-83a9-d8f8ca1a8ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022714187s
STEP: Saw pod success
Apr  6 03:03:34.808: INFO: Pod "pod-projected-configmaps-ca50844b-5edc-4b0c-83a9-d8f8ca1a8ee2" satisfied condition "Succeeded or Failed"
Apr  6 03:03:34.818: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-configmaps-ca50844b-5edc-4b0c-83a9-d8f8ca1a8ee2 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 03:03:34.883: INFO: Waiting for pod pod-projected-configmaps-ca50844b-5edc-4b0c-83a9-d8f8ca1a8ee2 to disappear
Apr  6 03:03:34.892: INFO: Pod pod-projected-configmaps-ca50844b-5edc-4b0c-83a9-d8f8ca1a8ee2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:03:34.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9776" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":414,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:03:34.915: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:03:35.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-8249 version'
Apr  6 03:03:35.177: INFO: stderr: ""
Apr  6 03:03:35.177: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5+5c84e52\", GitCommit:\"ce18cbe56f6e88a8fc0e06366afe113b415ad39b\", GitTreeState:\"clean\", BuildDate:\"2022-03-01T18:44:38Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:03:35.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8249" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":19,"skipped":425,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:03:35.197: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:03:35.412: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Creating first CR 
Apr  6 03:03:38.102: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-06T03:03:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-06T03:03:38Z]] name:name1 resourceVersion:58608 uid:7e1b44e2-568a-4646-8b6c-8fa3b5b57252] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr  6 03:03:48.127: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-06T03:03:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-06T03:03:48Z]] name:name2 resourceVersion:58750 uid:662025b6-0c2b-4628-9505-27094578e41e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr  6 03:03:58.151: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-06T03:03:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-06T03:03:58Z]] name:name1 resourceVersion:58904 uid:7e1b44e2-568a-4646-8b6c-8fa3b5b57252] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr  6 03:04:08.175: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-06T03:03:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-06T03:04:08Z]] name:name2 resourceVersion:59002 uid:662025b6-0c2b-4628-9505-27094578e41e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr  6 03:04:18.200: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-06T03:03:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-06T03:03:58Z]] name:name1 resourceVersion:59047 uid:7e1b44e2-568a-4646-8b6c-8fa3b5b57252] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr  6 03:04:28.229: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-06T03:03:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-06T03:04:08Z]] name:name2 resourceVersion:59117 uid:662025b6-0c2b-4628-9505-27094578e41e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:04:38.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9195" for this suite.

• [SLOW TEST:63.601 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":20,"skipped":438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:04:38.799: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:04:39.080: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9136a29-bb15-4b90-95f4-c63dd298a38b" in namespace "projected-7559" to be "Succeeded or Failed"
Apr  6 03:04:39.087: INFO: Pod "downwardapi-volume-b9136a29-bb15-4b90-95f4-c63dd298a38b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.794423ms
Apr  6 03:04:41.098: INFO: Pod "downwardapi-volume-b9136a29-bb15-4b90-95f4-c63dd298a38b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018005444s
STEP: Saw pod success
Apr  6 03:04:41.098: INFO: Pod "downwardapi-volume-b9136a29-bb15-4b90-95f4-c63dd298a38b" satisfied condition "Succeeded or Failed"
Apr  6 03:04:41.107: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-b9136a29-bb15-4b90-95f4-c63dd298a38b container client-container: <nil>
STEP: delete the pod
Apr  6 03:04:41.147: INFO: Waiting for pod downwardapi-volume-b9136a29-bb15-4b90-95f4-c63dd298a38b to disappear
Apr  6 03:04:41.155: INFO: Pod downwardapi-volume-b9136a29-bb15-4b90-95f4-c63dd298a38b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:04:41.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7559" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:04:41.175: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr  6 03:04:41.360: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 03:04:41.386: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 03:04:41.427: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.100 before test
Apr  6 03:04:41.492: INFO: calico-kube-controllers-b6474b6d6-v54x8 from calico-system started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr  6 03:04:41.492: INFO: calico-node-tnbgq from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:04:41.492: INFO: calico-typha-5fc4c7b7c7-rjrk4 from calico-system started at 2022-04-06 01:23:03 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 03:04:41.492: INFO: managed-storage-validation-webhooks-86b89bd6d-25bg6 from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Apr  6 03:04:41.492: INFO: managed-storage-validation-webhooks-86b89bd6d-7bfgh from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Apr  6 03:04:41.492: INFO: managed-storage-validation-webhooks-86b89bd6d-q5wr7 from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Apr  6 03:04:41.492: INFO: ibm-keepalived-watcher-c7h76 from kube-system started at 2022-04-06 01:20:27 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:04:41.492: INFO: ibm-master-proxy-static-10.241.0.100 from kube-system started at 2022-04-06 01:20:21 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:04:41.492: INFO: ibm-storage-metrics-agent-77bc4fb5c9-dck7l from kube-system started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Apr  6 03:04:41.492: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2022-04-06 01:25:08 +0000 UTC (6 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container csi-resizer ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:04:41.492: INFO: ibm-vpc-block-csi-node-n4fgh from kube-system started at 2022-04-06 01:20:27 +0000 UTC (4 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:04:41.492: INFO: cluster-node-tuning-operator-847fd957bd-kmksx from openshift-cluster-node-tuning-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: tuned-lxrqd from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:04:41.492: INFO: cluster-samples-operator-67d667cb6c-5hvmq from openshift-cluster-samples-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr  6 03:04:41.492: INFO: cluster-storage-operator-77cf9bb8c7-zwgsg from openshift-cluster-storage-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Apr  6 03:04:41.492: INFO: csi-snapshot-controller-operator-55d76bfc74-fxv2z from openshift-cluster-storage-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: console-operator-7c7f956448-rt9cz from openshift-console-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container console-operator ready: true, restart count 1
Apr  6 03:04:41.492: INFO: console-84457c4b7f-l4zfr from openshift-console started at 2022-04-06 01:38:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container console ready: true, restart count 0
Apr  6 03:04:41.492: INFO: dns-operator-8d8fb8787-xmhn7 from openshift-dns-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container dns-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: dns-default-gx9pd from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: node-resolver-8cqjf from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:04:41.492: INFO: cluster-image-registry-operator-6bcb795945-5qbrw from openshift-image-registry started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: node-ca-gvpjv from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:04:41.492: INFO: ingress-canary-jd288 from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:04:41.492: INFO: ingress-operator-58b79c98c4-9q2fc from openshift-ingress-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container ingress-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: openshift-kube-proxy-pqrfn from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: kube-storage-version-migrator-operator-5cfccbf8c7-c9h68 from openshift-kube-storage-version-migrator-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Apr  6 03:04:41.492: INFO: marketplace-operator-6cf6b95b7c-vjlnm from openshift-marketplace started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: cluster-monitoring-operator-6c8f74c5d5-9lsz4 from openshift-monitoring started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Apr  6 03:04:41.492: INFO: node-exporter-wbftb from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:04:41.492: INFO: multus-additional-cni-plugins-xpz8b from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:04:41.492: INFO: multus-admission-controller-qwnbg from openshift-multus started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:04:41.492: INFO: multus-fjdss from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:04:41.492: INFO: network-metrics-daemon-4m7nv from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:04:41.492: INFO: network-check-source-544bd4cb64-g5s5s from openshift-network-diagnostics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container check-endpoints ready: true, restart count 0
Apr  6 03:04:41.492: INFO: network-check-target-5lxc6 from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:04:41.492: INFO: network-operator-6c8789b55f-ntzrr from openshift-network-operator started at 2022-04-06 01:21:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container network-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: catalog-operator-699fc547c5-l5qkl from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container catalog-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: collect-profiles-27486870--1-qvmpv from openshift-operator-lifecycle-manager started at 2022-04-06 02:30:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:04:41.492: INFO: collect-profiles-27486885--1-4phn9 from openshift-operator-lifecycle-manager started at 2022-04-06 02:45:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:04:41.492: INFO: olm-operator-864d7cb959-4fbrf from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container olm-operator ready: true, restart count 0
Apr  6 03:04:41.492: INFO: package-server-manager-6f44bc74b7-c9pcp from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container package-server-manager ready: true, restart count 0
Apr  6 03:04:41.492: INFO: packageserver-77bb6bdcd6-hqmlj from openshift-operator-lifecycle-manager started at 2022-04-06 01:35:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container packageserver ready: true, restart count 0
Apr  6 03:04:41.492: INFO: metrics-b6fbdf747-4gdlt from openshift-roks-metrics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container metrics ready: true, restart count 2
Apr  6 03:04:41.492: INFO: push-gateway-f7897c967-v6cxg from openshift-roks-metrics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container push-gateway ready: true, restart count 0
Apr  6 03:04:41.492: INFO: service-ca-operator-77fd55df89-nkd65 from openshift-service-ca-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container service-ca-operator ready: true, restart count 1
Apr  6 03:04:41.492: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-htb29 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.492: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:04:41.492: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.101 before test
Apr  6 03:04:41.551: INFO: calico-node-cs6hj from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:04:41.551: INFO: calico-typha-5fc4c7b7c7-67lgl from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 03:04:41.551: INFO: ibm-keepalived-watcher-rppx4 from kube-system started at 2022-04-06 01:20:09 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:04:41.551: INFO: ibm-master-proxy-static-10.241.0.101 from kube-system started at 2022-04-06 01:20:00 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:04:41.551: INFO: ibm-vpc-block-csi-node-qh8rl from kube-system started at 2022-04-06 01:20:09 +0000 UTC (4 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:04:41.551: INFO: tuned-46gbx from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:04:41.551: INFO: csi-snapshot-controller-7ffc756fcb-xzddf from openshift-cluster-storage-operator started at 2022-04-06 01:36:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container snapshot-controller ready: true, restart count 0
Apr  6 03:04:41.551: INFO: csi-snapshot-webhook-574644677c-dmmpc from openshift-cluster-storage-operator started at 2022-04-06 01:36:15 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container webhook ready: true, restart count 0
Apr  6 03:04:41.551: INFO: console-84457c4b7f-8gp5t from openshift-console started at 2022-04-06 01:39:26 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container console ready: true, restart count 0
Apr  6 03:04:41.551: INFO: downloads-dbb5d5764-jhtbl from openshift-console started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container download-server ready: true, restart count 0
Apr  6 03:04:41.551: INFO: dns-default-v242c from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: node-resolver-lsbp7 from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:04:41.551: INFO: node-ca-ccn7d from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:04:41.551: INFO: ingress-canary-wklcl from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:04:41.551: INFO: router-default-79f7bb79b4-vnvq6 from openshift-ingress started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container router ready: true, restart count 0
Apr  6 03:04:41.551: INFO: openshift-kube-proxy-lsknc from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: community-operators-c2jdx from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:04:41.551: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: node-exporter-vx9fl from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:04:41.551: INFO: openshift-state-metrics-8dcbc5f76-cx6zj from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (3 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr  6 03:04:41.551: INFO: prometheus-adapter-54744554d8-95snn from openshift-monitoring started at 2022-04-06 01:39:52 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr  6 03:04:41.551: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-04-06 01:38:59 +0000 UTC (7 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container prometheus ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr  6 03:04:41.551: INFO: thanos-querier-6797965cb7-gjqm5 from openshift-monitoring started at 2022-04-06 01:38:55 +0000 UTC (5 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container thanos-query ready: true, restart count 0
Apr  6 03:04:41.551: INFO: multus-additional-cni-plugins-d5x4n from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:04:41.551: INFO: multus-admission-controller-6c4s6 from openshift-multus started at 2022-04-06 01:27:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:04:41.551: INFO: multus-cwlpj from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:04:41.551: INFO: network-metrics-daemon-hgtsb from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:04:41.551: INFO: network-check-target-rmb8p from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:04:41.551: INFO: sonobuoy from sonobuoy started at 2022-04-06 02:56:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 03:04:41.551: INFO: sonobuoy-e2e-job-9989f36257d44400 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container e2e ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:04:41.551: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-ffjt6 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:04:41.551: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.102 before test
Apr  6 03:04:41.604: INFO: calico-node-t786f from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:04:41.604: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-04-06 01:29:25 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Apr  6 03:04:41.604: INFO: ibm-keepalived-watcher-bv7pb from kube-system started at 2022-04-06 01:20:29 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:04:41.604: INFO: ibm-master-proxy-static-10.241.0.102 from kube-system started at 2022-04-06 01:20:23 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:04:41.604: INFO: ibm-vpc-block-csi-node-nk2wg from kube-system started at 2022-04-06 01:20:29 +0000 UTC (4 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:04:41.604: INFO: vpn-849cbbd4f5-zbslr from kube-system started at 2022-04-06 01:31:19 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container vpn ready: true, restart count 0
Apr  6 03:04:41.604: INFO: tuned-r57x6 from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:04:41.604: INFO: csi-snapshot-controller-7ffc756fcb-j85jv from openshift-cluster-storage-operator started at 2022-04-06 01:36:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container snapshot-controller ready: true, restart count 0
Apr  6 03:04:41.604: INFO: csi-snapshot-webhook-574644677c-gkf4n from openshift-cluster-storage-operator started at 2022-04-06 01:36:15 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container webhook ready: true, restart count 0
Apr  6 03:04:41.604: INFO: downloads-dbb5d5764-6ldtc from openshift-console started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container download-server ready: true, restart count 0
Apr  6 03:04:41.604: INFO: dns-default-zvb7x from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: node-resolver-9gtvm from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:04:41.604: INFO: image-registry-54fc9d45f8-c8xl9 from openshift-image-registry started at 2022-04-06 01:38:16 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container registry ready: true, restart count 0
Apr  6 03:04:41.604: INFO: node-ca-w42rx from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:04:41.604: INFO: ingress-canary-7dgmf from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:04:41.604: INFO: router-default-79f7bb79b4-txbvm from openshift-ingress started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container router ready: true, restart count 0
Apr  6 03:04:41.604: INFO: openshift-kube-proxy-9c8jc from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: migrator-c84bfd698-4gmq8 from openshift-kube-storage-version-migrator started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container migrator ready: true, restart count 0
Apr  6 03:04:41.604: INFO: certified-operators-k8bbt from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:04:41.604: INFO: redhat-marketplace-dhghg from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:04:41.604: INFO: redhat-operators-7266t from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:04:41.604: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: grafana-65d7ff4ff4-pjvw7 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container grafana ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: kube-state-metrics-c4c8f95d8-s5wlt from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (3 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 03:04:41.604: INFO: node-exporter-nnrpz from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:04:41.604: INFO: prometheus-adapter-54744554d8-v5q6n from openshift-monitoring started at 2022-04-06 01:39:52 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr  6 03:04:41.604: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-04-06 01:39:00 +0000 UTC (7 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container prometheus ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr  6 03:04:41.604: INFO: prometheus-operator-785898db99-xvm9b from openshift-monitoring started at 2022-04-06 01:36:07 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr  6 03:04:41.604: INFO: telemeter-client-bdc7d9995-gfbw6 from openshift-monitoring started at 2022-04-06 01:37:33 +0000 UTC (3 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container reload ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container telemeter-client ready: true, restart count 0
Apr  6 03:04:41.604: INFO: thanos-querier-6797965cb7-6fjfp from openshift-monitoring started at 2022-04-06 01:38:55 +0000 UTC (5 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container thanos-query ready: true, restart count 0
Apr  6 03:04:41.604: INFO: multus-additional-cni-plugins-v2thd from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:04:41.604: INFO: multus-admission-controller-n2fzx from openshift-multus started at 2022-04-06 01:25:10 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:04:41.604: INFO: multus-mhr5m from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:04:41.604: INFO: network-metrics-daemon-qt77t from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:04:41.604: INFO: network-check-target-m4pg9 from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:04:41.604: INFO: collect-profiles-27486900--1-xnsfw from openshift-operator-lifecycle-manager started at 2022-04-06 03:00:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:04:41.604: INFO: packageserver-77bb6bdcd6-2sng2 from openshift-operator-lifecycle-manager started at 2022-04-06 01:35:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container packageserver ready: true, restart count 0
Apr  6 03:04:41.604: INFO: service-ca-c77965566-b6wft from openshift-service-ca started at 2022-04-06 01:33:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container service-ca-controller ready: true, restart count 0
Apr  6 03:04:41.604: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-84flv from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:04:41.604: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:04:41.604: INFO: tigera-operator-5d4d8f956c-2mlcb from tigera-operator started at 2022-04-06 01:21:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:04:41.604: INFO: 	Container tigera-operator ready: true, restart count 3
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16e32fa6c49e9fc7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:04:42.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-873" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":22,"skipped":514,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:04:42.902: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Apr  6 03:04:43.110: INFO: Waiting up to 5m0s for pod "var-expansion-58bddfad-5444-47e4-aeba-71738da03506" in namespace "var-expansion-4183" to be "Succeeded or Failed"
Apr  6 03:04:43.119: INFO: Pod "var-expansion-58bddfad-5444-47e4-aeba-71738da03506": Phase="Pending", Reason="", readiness=false. Elapsed: 8.554861ms
Apr  6 03:04:45.144: INFO: Pod "var-expansion-58bddfad-5444-47e4-aeba-71738da03506": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033354187s
Apr  6 03:04:47.154: INFO: Pod "var-expansion-58bddfad-5444-47e4-aeba-71738da03506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043621654s
STEP: Saw pod success
Apr  6 03:04:47.154: INFO: Pod "var-expansion-58bddfad-5444-47e4-aeba-71738da03506" satisfied condition "Succeeded or Failed"
Apr  6 03:04:47.163: INFO: Trying to get logs from node 10.241.0.102 pod var-expansion-58bddfad-5444-47e4-aeba-71738da03506 container dapi-container: <nil>
STEP: delete the pod
Apr  6 03:04:47.235: INFO: Waiting for pod var-expansion-58bddfad-5444-47e4-aeba-71738da03506 to disappear
Apr  6 03:04:47.244: INFO: Pod var-expansion-58bddfad-5444-47e4-aeba-71738da03506 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:04:47.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4183" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":23,"skipped":530,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:04:47.265: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-389 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-389;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-389 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-389;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-389.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-389.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-389.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-389.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-389.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-389.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-389.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-389.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-389.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-389.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-389.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-389.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-389.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 66.159.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.159.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.159.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.159.66_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-389 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-389;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-389 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-389;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-389.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-389.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-389.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-389.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-389.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-389.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-389.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-389.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-389.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-389.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-389.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-389.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-389.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 66.159.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.159.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.159.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.159.66_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 03:05:01.631: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.642: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.678: INFO: Unable to read wheezy_udp@dns-test-service.dns-389 from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.690: INFO: Unable to read wheezy_tcp@dns-test-service.dns-389 from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.704: INFO: Unable to read wheezy_udp@dns-test-service.dns-389.svc from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.713: INFO: Unable to read wheezy_tcp@dns-test-service.dns-389.svc from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.723: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-389.svc from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.734: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-389.svc from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.833: INFO: Unable to read jessie_udp@dns-test-service from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.888: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.917: INFO: Unable to read jessie_udp@dns-test-service.dns-389 from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.949: INFO: Unable to read jessie_udp@dns-test-service.dns-389.svc from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.974: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-389.svc from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:01.990: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-389.svc from pod dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b: the server could not find the requested resource (get pods dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b)
Apr  6 03:05:02.109: INFO: Lookups using dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-389 wheezy_tcp@dns-test-service.dns-389 wheezy_udp@dns-test-service.dns-389.svc wheezy_tcp@dns-test-service.dns-389.svc wheezy_udp@_http._tcp.dns-test-service.dns-389.svc wheezy_tcp@_http._tcp.dns-test-service.dns-389.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-389 jessie_udp@dns-test-service.dns-389.svc jessie_udp@_http._tcp.dns-test-service.dns-389.svc jessie_tcp@_http._tcp.dns-test-service.dns-389.svc]

Apr  6 03:05:07.476: INFO: DNS probes using dns-389/dns-test-4a25fdb6-b39d-4445-a512-dceb75b0e83b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:05:07.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-389" for this suite.

• [SLOW TEST:20.366 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":24,"skipped":540,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:05:07.631: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-1eee7bdb-f691-4176-a808-051621b55d9c
STEP: Creating a pod to test consume secrets
Apr  6 03:05:07.985: INFO: Waiting up to 5m0s for pod "pod-secrets-3efeecc5-55e1-445f-b5a1-6ed8df7b1767" in namespace "secrets-6371" to be "Succeeded or Failed"
Apr  6 03:05:08.022: INFO: Pod "pod-secrets-3efeecc5-55e1-445f-b5a1-6ed8df7b1767": Phase="Pending", Reason="", readiness=false. Elapsed: 37.04486ms
Apr  6 03:05:10.033: INFO: Pod "pod-secrets-3efeecc5-55e1-445f-b5a1-6ed8df7b1767": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047954285s
Apr  6 03:05:12.047: INFO: Pod "pod-secrets-3efeecc5-55e1-445f-b5a1-6ed8df7b1767": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06115897s
STEP: Saw pod success
Apr  6 03:05:12.047: INFO: Pod "pod-secrets-3efeecc5-55e1-445f-b5a1-6ed8df7b1767" satisfied condition "Succeeded or Failed"
Apr  6 03:05:12.055: INFO: Trying to get logs from node 10.241.0.102 pod pod-secrets-3efeecc5-55e1-445f-b5a1-6ed8df7b1767 container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 03:05:12.106: INFO: Waiting for pod pod-secrets-3efeecc5-55e1-445f-b5a1-6ed8df7b1767 to disappear
Apr  6 03:05:12.121: INFO: Pod pod-secrets-3efeecc5-55e1-445f-b5a1-6ed8df7b1767 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:05:12.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6371" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":25,"skipped":541,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:05:12.142: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Apr  6 03:05:12.394: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:05:14.404: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Apr  6 03:05:14.446: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:05:16.458: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr  6 03:05:16.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 03:05:16.532: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  6 03:05:18.532: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 03:05:18.542: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  6 03:05:20.533: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 03:05:20.544: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:05:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4769" for this suite.

• [SLOW TEST:8.424 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":558,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:05:20.566: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr  6 03:05:20.874: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 03:06:21.073: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:06:21.111: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Apr  6 03:06:23.407: INFO: found a healthy node: 10.241.0.102
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:06:31.673: INFO: pods created so far: [1 1 1]
Apr  6 03:06:31.674: INFO: length of pods created so far: 3
Apr  6 03:06:37.732: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:06:44.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3973" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:06:44.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2652" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:84.492 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":27,"skipped":559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:06:45.058: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Apr  6 03:06:45.305: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:07:26.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2938" for this suite.

• [SLOW TEST:41.050 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":28,"skipped":592,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:07:26.108: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr  6 03:07:26.309: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6224  b24489c2-28a9-4b0a-88f9-5014aaa90b55 61097 0 2022-04-06 03:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-04-06 03:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 03:07:26.309: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6224  b24489c2-28a9-4b0a-88f9-5014aaa90b55 61100 0 2022-04-06 03:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-04-06 03:07:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr  6 03:07:26.341: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6224  b24489c2-28a9-4b0a-88f9-5014aaa90b55 61102 0 2022-04-06 03:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-04-06 03:07:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 03:07:26.341: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6224  b24489c2-28a9-4b0a-88f9-5014aaa90b55 61103 0 2022-04-06 03:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-04-06 03:07:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:07:26.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6224" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":29,"skipped":593,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:07:26.385: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:07:33.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6139" for this suite.

• [SLOW TEST:7.203 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":30,"skipped":597,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:07:33.589: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:07:33.787: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34d2e1d9-048a-4495-ba50-a1a4a01ad15a" in namespace "projected-5608" to be "Succeeded or Failed"
Apr  6 03:07:33.807: INFO: Pod "downwardapi-volume-34d2e1d9-048a-4495-ba50-a1a4a01ad15a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.951036ms
Apr  6 03:07:35.820: INFO: Pod "downwardapi-volume-34d2e1d9-048a-4495-ba50-a1a4a01ad15a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032902062s
STEP: Saw pod success
Apr  6 03:07:35.820: INFO: Pod "downwardapi-volume-34d2e1d9-048a-4495-ba50-a1a4a01ad15a" satisfied condition "Succeeded or Failed"
Apr  6 03:07:35.829: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-34d2e1d9-048a-4495-ba50-a1a4a01ad15a container client-container: <nil>
STEP: delete the pod
Apr  6 03:07:35.890: INFO: Waiting for pod downwardapi-volume-34d2e1d9-048a-4495-ba50-a1a4a01ad15a to disappear
Apr  6 03:07:35.898: INFO: Pod downwardapi-volume-34d2e1d9-048a-4495-ba50-a1a4a01ad15a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:07:35.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5608" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:07:35.919: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-vq4pj in namespace proxy-2653
I0406 03:07:36.138500      21 runners.go:190] Created replication controller with name: proxy-service-vq4pj, namespace: proxy-2653, replica count: 1
I0406 03:07:37.188764      21 runners.go:190] proxy-service-vq4pj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 03:07:38.189625      21 runners.go:190] proxy-service-vq4pj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 03:07:39.190310      21 runners.go:190] proxy-service-vq4pj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 03:07:39.197: INFO: setup took 3.106332235s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr  6 03:07:39.218: INFO: (0) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 21.079699ms)
Apr  6 03:07:39.220: INFO: (0) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 22.574567ms)
Apr  6 03:07:39.220: INFO: (0) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 22.711792ms)
Apr  6 03:07:39.220: INFO: (0) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 22.770277ms)
Apr  6 03:07:39.221: INFO: (0) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 23.891935ms)
Apr  6 03:07:39.221: INFO: (0) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 24.133098ms)
Apr  6 03:07:39.225: INFO: (0) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 28.039445ms)
Apr  6 03:07:39.225: INFO: (0) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 28.074523ms)
Apr  6 03:07:39.229: INFO: (0) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 31.944153ms)
Apr  6 03:07:39.229: INFO: (0) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 32.390096ms)
Apr  6 03:07:39.230: INFO: (0) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 32.351326ms)
Apr  6 03:07:39.230: INFO: (0) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 32.723465ms)
Apr  6 03:07:39.230: INFO: (0) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 32.696773ms)
Apr  6 03:07:39.231: INFO: (0) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 34.312761ms)
Apr  6 03:07:39.232: INFO: (0) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 34.551707ms)
Apr  6 03:07:39.232: INFO: (0) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 35.09879ms)
Apr  6 03:07:39.245: INFO: (1) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 12.533593ms)
Apr  6 03:07:39.246: INFO: (1) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 13.413168ms)
Apr  6 03:07:39.247: INFO: (1) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 14.135933ms)
Apr  6 03:07:39.247: INFO: (1) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 14.284835ms)
Apr  6 03:07:39.247: INFO: (1) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 14.716343ms)
Apr  6 03:07:39.249: INFO: (1) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 16.509953ms)
Apr  6 03:07:39.250: INFO: (1) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 17.282188ms)
Apr  6 03:07:39.250: INFO: (1) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 17.346426ms)
Apr  6 03:07:39.250: INFO: (1) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 17.602488ms)
Apr  6 03:07:39.250: INFO: (1) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 17.572611ms)
Apr  6 03:07:39.251: INFO: (1) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 18.664751ms)
Apr  6 03:07:39.253: INFO: (1) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 20.317289ms)
Apr  6 03:07:39.254: INFO: (1) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 21.170114ms)
Apr  6 03:07:39.254: INFO: (1) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 21.24277ms)
Apr  6 03:07:39.254: INFO: (1) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 21.60457ms)
Apr  6 03:07:39.254: INFO: (1) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 21.807682ms)
Apr  6 03:07:39.265: INFO: (2) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 10.672664ms)
Apr  6 03:07:39.269: INFO: (2) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 14.741741ms)
Apr  6 03:07:39.269: INFO: (2) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 14.768263ms)
Apr  6 03:07:39.270: INFO: (2) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.141947ms)
Apr  6 03:07:39.270: INFO: (2) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 15.497627ms)
Apr  6 03:07:39.271: INFO: (2) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 16.609552ms)
Apr  6 03:07:39.272: INFO: (2) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 17.172585ms)
Apr  6 03:07:39.272: INFO: (2) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 17.602376ms)
Apr  6 03:07:39.272: INFO: (2) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 17.689658ms)
Apr  6 03:07:39.272: INFO: (2) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 17.781473ms)
Apr  6 03:07:39.274: INFO: (2) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 18.964587ms)
Apr  6 03:07:39.274: INFO: (2) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 19.333426ms)
Apr  6 03:07:39.274: INFO: (2) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 19.617656ms)
Apr  6 03:07:39.274: INFO: (2) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 19.576031ms)
Apr  6 03:07:39.275: INFO: (2) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 20.101109ms)
Apr  6 03:07:39.275: INFO: (2) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 20.333919ms)
Apr  6 03:07:39.285: INFO: (3) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 9.948362ms)
Apr  6 03:07:39.293: INFO: (3) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 18.037402ms)
Apr  6 03:07:39.293: INFO: (3) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 18.009471ms)
Apr  6 03:07:39.295: INFO: (3) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 19.686407ms)
Apr  6 03:07:39.295: INFO: (3) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 19.55304ms)
Apr  6 03:07:39.295: INFO: (3) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 19.677283ms)
Apr  6 03:07:39.295: INFO: (3) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 19.762508ms)
Apr  6 03:07:39.295: INFO: (3) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 19.650467ms)
Apr  6 03:07:39.295: INFO: (3) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 19.513681ms)
Apr  6 03:07:39.295: INFO: (3) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 19.421265ms)
Apr  6 03:07:39.295: INFO: (3) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 19.51164ms)
Apr  6 03:07:39.296: INFO: (3) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 20.550904ms)
Apr  6 03:07:39.296: INFO: (3) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 21.240558ms)
Apr  6 03:07:39.297: INFO: (3) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 21.621362ms)
Apr  6 03:07:39.297: INFO: (3) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 21.944693ms)
Apr  6 03:07:39.297: INFO: (3) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 21.850857ms)
Apr  6 03:07:39.312: INFO: (4) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 14.563118ms)
Apr  6 03:07:39.312: INFO: (4) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 14.443286ms)
Apr  6 03:07:39.312: INFO: (4) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 14.952074ms)
Apr  6 03:07:39.312: INFO: (4) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 15.303348ms)
Apr  6 03:07:39.313: INFO: (4) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 15.637934ms)
Apr  6 03:07:39.315: INFO: (4) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 17.53906ms)
Apr  6 03:07:39.317: INFO: (4) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 19.576567ms)
Apr  6 03:07:39.317: INFO: (4) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 20.292054ms)
Apr  6 03:07:39.318: INFO: (4) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 20.325251ms)
Apr  6 03:07:39.318: INFO: (4) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 21.042987ms)
Apr  6 03:07:39.318: INFO: (4) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 21.10643ms)
Apr  6 03:07:39.319: INFO: (4) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 21.409853ms)
Apr  6 03:07:39.319: INFO: (4) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 21.53039ms)
Apr  6 03:07:39.320: INFO: (4) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 22.883598ms)
Apr  6 03:07:39.320: INFO: (4) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 23.050787ms)
Apr  6 03:07:39.321: INFO: (4) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 23.145341ms)
Apr  6 03:07:39.331: INFO: (5) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 9.931874ms)
Apr  6 03:07:39.337: INFO: (5) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 15.652684ms)
Apr  6 03:07:39.337: INFO: (5) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.616399ms)
Apr  6 03:07:39.337: INFO: (5) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.908692ms)
Apr  6 03:07:39.337: INFO: (5) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 15.916219ms)
Apr  6 03:07:39.338: INFO: (5) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 16.099248ms)
Apr  6 03:07:39.338: INFO: (5) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 16.783479ms)
Apr  6 03:07:39.338: INFO: (5) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 17.098322ms)
Apr  6 03:07:39.339: INFO: (5) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 18.066686ms)
Apr  6 03:07:39.339: INFO: (5) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 18.369657ms)
Apr  6 03:07:39.342: INFO: (5) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 20.974679ms)
Apr  6 03:07:39.343: INFO: (5) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 21.516224ms)
Apr  6 03:07:39.343: INFO: (5) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 21.926442ms)
Apr  6 03:07:39.343: INFO: (5) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 21.863499ms)
Apr  6 03:07:39.343: INFO: (5) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 22.017092ms)
Apr  6 03:07:39.343: INFO: (5) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 22.322307ms)
Apr  6 03:07:39.354: INFO: (6) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 10.864052ms)
Apr  6 03:07:39.360: INFO: (6) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 16.354143ms)
Apr  6 03:07:39.360: INFO: (6) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 16.486559ms)
Apr  6 03:07:39.361: INFO: (6) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 17.390504ms)
Apr  6 03:07:39.361: INFO: (6) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 17.463733ms)
Apr  6 03:07:39.361: INFO: (6) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 17.681249ms)
Apr  6 03:07:39.362: INFO: (6) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 17.739835ms)
Apr  6 03:07:39.362: INFO: (6) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 18.250522ms)
Apr  6 03:07:39.362: INFO: (6) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 18.396785ms)
Apr  6 03:07:39.362: INFO: (6) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 18.142472ms)
Apr  6 03:07:39.362: INFO: (6) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 18.128717ms)
Apr  6 03:07:39.362: INFO: (6) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 18.591548ms)
Apr  6 03:07:39.362: INFO: (6) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 19.055995ms)
Apr  6 03:07:39.363: INFO: (6) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 19.521907ms)
Apr  6 03:07:39.364: INFO: (6) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 20.0748ms)
Apr  6 03:07:39.365: INFO: (6) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 20.848814ms)
Apr  6 03:07:39.375: INFO: (7) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 10.45605ms)
Apr  6 03:07:39.378: INFO: (7) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 12.792959ms)
Apr  6 03:07:39.378: INFO: (7) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 12.777977ms)
Apr  6 03:07:39.379: INFO: (7) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 13.637992ms)
Apr  6 03:07:39.380: INFO: (7) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 14.735231ms)
Apr  6 03:07:39.380: INFO: (7) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 14.371289ms)
Apr  6 03:07:39.381: INFO: (7) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 15.435121ms)
Apr  6 03:07:39.381: INFO: (7) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 15.402571ms)
Apr  6 03:07:39.381: INFO: (7) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 15.629446ms)
Apr  6 03:07:39.381: INFO: (7) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 16.161642ms)
Apr  6 03:07:39.382: INFO: (7) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 17.028173ms)
Apr  6 03:07:39.382: INFO: (7) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 16.983398ms)
Apr  6 03:07:39.384: INFO: (7) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 18.762549ms)
Apr  6 03:07:39.384: INFO: (7) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 18.720225ms)
Apr  6 03:07:39.384: INFO: (7) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 19.207228ms)
Apr  6 03:07:39.385: INFO: (7) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 19.6103ms)
Apr  6 03:07:39.395: INFO: (8) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 10.277453ms)
Apr  6 03:07:39.400: INFO: (8) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.1043ms)
Apr  6 03:07:39.400: INFO: (8) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 15.267155ms)
Apr  6 03:07:39.401: INFO: (8) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 16.219049ms)
Apr  6 03:07:39.402: INFO: (8) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 17.139348ms)
Apr  6 03:07:39.402: INFO: (8) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 17.116313ms)
Apr  6 03:07:39.402: INFO: (8) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 17.152334ms)
Apr  6 03:07:39.402: INFO: (8) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 17.016945ms)
Apr  6 03:07:39.402: INFO: (8) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 17.381078ms)
Apr  6 03:07:39.402: INFO: (8) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 17.475336ms)
Apr  6 03:07:39.403: INFO: (8) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 18.263364ms)
Apr  6 03:07:39.403: INFO: (8) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 18.565784ms)
Apr  6 03:07:39.405: INFO: (8) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 20.459321ms)
Apr  6 03:07:39.406: INFO: (8) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 21.185815ms)
Apr  6 03:07:39.406: INFO: (8) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 21.350638ms)
Apr  6 03:07:39.406: INFO: (8) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 21.475388ms)
Apr  6 03:07:39.422: INFO: (9) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 15.102498ms)
Apr  6 03:07:39.422: INFO: (9) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 15.289409ms)
Apr  6 03:07:39.422: INFO: (9) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 15.364584ms)
Apr  6 03:07:39.423: INFO: (9) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 16.204542ms)
Apr  6 03:07:39.423: INFO: (9) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 16.215806ms)
Apr  6 03:07:39.423: INFO: (9) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 16.17621ms)
Apr  6 03:07:39.423: INFO: (9) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 16.213366ms)
Apr  6 03:07:39.423: INFO: (9) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 16.402597ms)
Apr  6 03:07:39.423: INFO: (9) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 16.434687ms)
Apr  6 03:07:39.423: INFO: (9) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 16.838713ms)
Apr  6 03:07:39.423: INFO: (9) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 17.172424ms)
Apr  6 03:07:39.426: INFO: (9) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 19.407207ms)
Apr  6 03:07:39.426: INFO: (9) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 20.03171ms)
Apr  6 03:07:39.427: INFO: (9) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 20.345881ms)
Apr  6 03:07:39.427: INFO: (9) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 20.442386ms)
Apr  6 03:07:39.427: INFO: (9) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 20.614114ms)
Apr  6 03:07:39.438: INFO: (10) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 10.841717ms)
Apr  6 03:07:39.442: INFO: (10) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 14.874191ms)
Apr  6 03:07:39.443: INFO: (10) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 15.278075ms)
Apr  6 03:07:39.443: INFO: (10) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 15.209925ms)
Apr  6 03:07:39.445: INFO: (10) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 18.017113ms)
Apr  6 03:07:39.445: INFO: (10) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 18.092004ms)
Apr  6 03:07:39.445: INFO: (10) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 18.179888ms)
Apr  6 03:07:39.445: INFO: (10) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 18.405599ms)
Apr  6 03:07:39.445: INFO: (10) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 18.117941ms)
Apr  6 03:07:39.446: INFO: (10) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 18.448024ms)
Apr  6 03:07:39.447: INFO: (10) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 19.619551ms)
Apr  6 03:07:39.447: INFO: (10) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 20.021786ms)
Apr  6 03:07:39.447: INFO: (10) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 20.110013ms)
Apr  6 03:07:39.448: INFO: (10) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 20.433297ms)
Apr  6 03:07:39.448: INFO: (10) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 21.051487ms)
Apr  6 03:07:39.448: INFO: (10) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 21.244429ms)
Apr  6 03:07:39.458: INFO: (11) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 9.677787ms)
Apr  6 03:07:39.463: INFO: (11) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 13.988008ms)
Apr  6 03:07:39.463: INFO: (11) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 14.321814ms)
Apr  6 03:07:39.464: INFO: (11) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 14.914365ms)
Apr  6 03:07:39.464: INFO: (11) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 15.647273ms)
Apr  6 03:07:39.464: INFO: (11) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 15.673709ms)
Apr  6 03:07:39.464: INFO: (11) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.84622ms)
Apr  6 03:07:39.464: INFO: (11) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 15.853666ms)
Apr  6 03:07:39.465: INFO: (11) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 16.235317ms)
Apr  6 03:07:39.466: INFO: (11) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 17.650549ms)
Apr  6 03:07:39.467: INFO: (11) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 18.581662ms)
Apr  6 03:07:39.469: INFO: (11) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 20.054623ms)
Apr  6 03:07:39.469: INFO: (11) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 20.624648ms)
Apr  6 03:07:39.469: INFO: (11) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 20.82356ms)
Apr  6 03:07:39.470: INFO: (11) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 20.986245ms)
Apr  6 03:07:39.470: INFO: (11) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 21.020473ms)
Apr  6 03:07:39.481: INFO: (12) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 11.639817ms)
Apr  6 03:07:39.485: INFO: (12) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 15.44137ms)
Apr  6 03:07:39.486: INFO: (12) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 16.332947ms)
Apr  6 03:07:39.487: INFO: (12) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 16.489702ms)
Apr  6 03:07:39.487: INFO: (12) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 16.813374ms)
Apr  6 03:07:39.488: INFO: (12) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 17.779148ms)
Apr  6 03:07:39.488: INFO: (12) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 18.199129ms)
Apr  6 03:07:39.488: INFO: (12) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 18.062309ms)
Apr  6 03:07:39.488: INFO: (12) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 18.337718ms)
Apr  6 03:07:39.488: INFO: (12) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 18.490671ms)
Apr  6 03:07:39.489: INFO: (12) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 19.110225ms)
Apr  6 03:07:39.489: INFO: (12) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 18.963357ms)
Apr  6 03:07:39.491: INFO: (12) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 20.646685ms)
Apr  6 03:07:39.492: INFO: (12) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 21.642095ms)
Apr  6 03:07:39.492: INFO: (12) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 22.028631ms)
Apr  6 03:07:39.492: INFO: (12) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 22.13338ms)
Apr  6 03:07:39.503: INFO: (13) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 10.692937ms)
Apr  6 03:07:39.506: INFO: (13) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 14.229522ms)
Apr  6 03:07:39.507: INFO: (13) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 14.301182ms)
Apr  6 03:07:39.508: INFO: (13) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.382292ms)
Apr  6 03:07:39.508: INFO: (13) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 15.551136ms)
Apr  6 03:07:39.508: INFO: (13) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 15.676142ms)
Apr  6 03:07:39.508: INFO: (13) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.916818ms)
Apr  6 03:07:39.508: INFO: (13) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 15.910544ms)
Apr  6 03:07:39.508: INFO: (13) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 15.921424ms)
Apr  6 03:07:39.508: INFO: (13) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 15.873075ms)
Apr  6 03:07:39.510: INFO: (13) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 17.754651ms)
Apr  6 03:07:39.510: INFO: (13) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 17.944186ms)
Apr  6 03:07:39.510: INFO: (13) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 18.123105ms)
Apr  6 03:07:39.511: INFO: (13) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 18.81983ms)
Apr  6 03:07:39.511: INFO: (13) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 19.086306ms)
Apr  6 03:07:39.517: INFO: (13) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 24.470734ms)
Apr  6 03:07:39.527: INFO: (14) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 10.144431ms)
Apr  6 03:07:39.533: INFO: (14) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 15.957886ms)
Apr  6 03:07:39.533: INFO: (14) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 16.340279ms)
Apr  6 03:07:39.533: INFO: (14) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 16.302513ms)
Apr  6 03:07:39.534: INFO: (14) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 16.813362ms)
Apr  6 03:07:39.535: INFO: (14) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 17.775331ms)
Apr  6 03:07:39.535: INFO: (14) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 17.919537ms)
Apr  6 03:07:39.535: INFO: (14) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 17.625919ms)
Apr  6 03:07:39.535: INFO: (14) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 17.781768ms)
Apr  6 03:07:39.535: INFO: (14) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 17.914865ms)
Apr  6 03:07:39.535: INFO: (14) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 17.971752ms)
Apr  6 03:07:39.538: INFO: (14) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 20.636474ms)
Apr  6 03:07:39.538: INFO: (14) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 21.515382ms)
Apr  6 03:07:39.539: INFO: (14) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 21.708947ms)
Apr  6 03:07:39.539: INFO: (14) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 21.733538ms)
Apr  6 03:07:39.539: INFO: (14) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 22.107638ms)
Apr  6 03:07:39.551: INFO: (15) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 11.189826ms)
Apr  6 03:07:39.555: INFO: (15) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 15.947562ms)
Apr  6 03:07:39.556: INFO: (15) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 16.314102ms)
Apr  6 03:07:39.565: INFO: (15) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 25.785864ms)
Apr  6 03:07:39.565: INFO: (15) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 25.902635ms)
Apr  6 03:07:39.566: INFO: (15) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 26.264149ms)
Apr  6 03:07:39.566: INFO: (15) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 26.645199ms)
Apr  6 03:07:39.566: INFO: (15) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 26.734834ms)
Apr  6 03:07:39.566: INFO: (15) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 26.763583ms)
Apr  6 03:07:39.566: INFO: (15) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 26.943011ms)
Apr  6 03:07:39.580: INFO: (15) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 40.121854ms)
Apr  6 03:07:39.580: INFO: (15) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 40.411018ms)
Apr  6 03:07:39.580: INFO: (15) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 40.633565ms)
Apr  6 03:07:39.580: INFO: (15) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 40.793328ms)
Apr  6 03:07:39.581: INFO: (15) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 41.174619ms)
Apr  6 03:07:39.581: INFO: (15) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 41.148257ms)
Apr  6 03:07:39.592: INFO: (16) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 11.594384ms)
Apr  6 03:07:39.595: INFO: (16) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 14.225094ms)
Apr  6 03:07:39.596: INFO: (16) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 15.224891ms)
Apr  6 03:07:39.596: INFO: (16) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.344142ms)
Apr  6 03:07:39.596: INFO: (16) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 15.484153ms)
Apr  6 03:07:39.596: INFO: (16) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 15.608065ms)
Apr  6 03:07:39.596: INFO: (16) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 15.574729ms)
Apr  6 03:07:39.597: INFO: (16) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 15.860478ms)
Apr  6 03:07:39.597: INFO: (16) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 16.419537ms)
Apr  6 03:07:39.597: INFO: (16) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 16.373568ms)
Apr  6 03:07:39.610: INFO: (16) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 28.973596ms)
Apr  6 03:07:39.610: INFO: (16) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 28.861337ms)
Apr  6 03:07:39.610: INFO: (16) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 28.919013ms)
Apr  6 03:07:39.610: INFO: (16) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 28.912347ms)
Apr  6 03:07:39.610: INFO: (16) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 28.890974ms)
Apr  6 03:07:39.610: INFO: (16) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 29.036602ms)
Apr  6 03:07:39.621: INFO: (17) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 11.471413ms)
Apr  6 03:07:39.622: INFO: (17) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 11.955035ms)
Apr  6 03:07:39.623: INFO: (17) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 13.277406ms)
Apr  6 03:07:39.624: INFO: (17) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 13.971185ms)
Apr  6 03:07:39.625: INFO: (17) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 15.219902ms)
Apr  6 03:07:39.625: INFO: (17) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 15.236368ms)
Apr  6 03:07:39.625: INFO: (17) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 15.466152ms)
Apr  6 03:07:39.626: INFO: (17) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 15.648557ms)
Apr  6 03:07:39.626: INFO: (17) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 15.714682ms)
Apr  6 03:07:39.626: INFO: (17) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 15.752307ms)
Apr  6 03:07:39.629: INFO: (17) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 18.814603ms)
Apr  6 03:07:39.629: INFO: (17) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 18.771486ms)
Apr  6 03:07:39.630: INFO: (17) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 19.906529ms)
Apr  6 03:07:39.630: INFO: (17) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 19.924533ms)
Apr  6 03:07:39.630: INFO: (17) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 20.25139ms)
Apr  6 03:07:39.630: INFO: (17) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 20.155747ms)
Apr  6 03:07:39.645: INFO: (18) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 14.774875ms)
Apr  6 03:07:39.646: INFO: (18) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 15.109752ms)
Apr  6 03:07:39.646: INFO: (18) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 15.249452ms)
Apr  6 03:07:39.646: INFO: (18) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 15.074445ms)
Apr  6 03:07:39.646: INFO: (18) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 15.098386ms)
Apr  6 03:07:39.646: INFO: (18) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 15.262376ms)
Apr  6 03:07:39.646: INFO: (18) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 15.416541ms)
Apr  6 03:07:39.647: INFO: (18) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 16.718629ms)
Apr  6 03:07:39.648: INFO: (18) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 17.386283ms)
Apr  6 03:07:39.648: INFO: (18) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 17.578358ms)
Apr  6 03:07:39.650: INFO: (18) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 19.132991ms)
Apr  6 03:07:39.651: INFO: (18) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 20.377478ms)
Apr  6 03:07:39.652: INFO: (18) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 22.055883ms)
Apr  6 03:07:39.652: INFO: (18) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 22.055721ms)
Apr  6 03:07:39.652: INFO: (18) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 22.056269ms)
Apr  6 03:07:39.653: INFO: (18) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 22.105676ms)
Apr  6 03:07:39.664: INFO: (19) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 11.391618ms)
Apr  6 03:07:39.672: INFO: (19) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs/proxy/rewriteme">test</a> (200; 18.794917ms)
Apr  6 03:07:39.672: INFO: (19) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">... (200; 19.155385ms)
Apr  6 03:07:39.672: INFO: (19) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 19.407846ms)
Apr  6 03:07:39.673: INFO: (19) /api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/proxy-service-vq4pj-6spgs:1080/proxy/rewriteme">test<... (200; 19.622305ms)
Apr  6 03:07:39.673: INFO: (19) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:162/proxy/: bar (200; 20.129814ms)
Apr  6 03:07:39.674: INFO: (19) /api/v1/namespaces/proxy-2653/pods/http:proxy-service-vq4pj-6spgs:160/proxy/: foo (200; 21.324041ms)
Apr  6 03:07:39.674: INFO: (19) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname2/proxy/: tls qux (200; 21.283533ms)
Apr  6 03:07:39.674: INFO: (19) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/: <a href="/api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:443/proxy/tlsrewritem... (200; 21.444076ms)
Apr  6 03:07:39.674: INFO: (19) /api/v1/namespaces/proxy-2653/services/https:proxy-service-vq4pj:tlsportname1/proxy/: tls baz (200; 21.314455ms)
Apr  6 03:07:39.674: INFO: (19) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:460/proxy/: tls baz (200; 21.28674ms)
Apr  6 03:07:39.674: INFO: (19) /api/v1/namespaces/proxy-2653/pods/https:proxy-service-vq4pj-6spgs:462/proxy/: tls qux (200; 21.275094ms)
Apr  6 03:07:39.675: INFO: (19) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname2/proxy/: bar (200; 21.917845ms)
Apr  6 03:07:39.676: INFO: (19) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname1/proxy/: foo (200; 22.8171ms)
Apr  6 03:07:39.676: INFO: (19) /api/v1/namespaces/proxy-2653/services/http:proxy-service-vq4pj:portname2/proxy/: bar (200; 23.234439ms)
Apr  6 03:07:39.676: INFO: (19) /api/v1/namespaces/proxy-2653/services/proxy-service-vq4pj:portname1/proxy/: foo (200; 23.371543ms)
STEP: deleting ReplicationController proxy-service-vq4pj in namespace proxy-2653, will wait for the garbage collector to delete the pods
Apr  6 03:07:39.747: INFO: Deleting ReplicationController proxy-service-vq4pj took: 11.859642ms
Apr  6 03:07:39.847: INFO: Terminating ReplicationController proxy-service-vq4pj pods took: 100.217465ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:07:43.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2653" for this suite.

• [SLOW TEST:7.155 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":32,"skipped":671,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:07:43.075: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:07:43.264: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:07:50.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7644" for this suite.

• [SLOW TEST:7.933 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":33,"skipped":676,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:07:51.007: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr  6 03:07:51.326: INFO: Waiting up to 5m0s for pod "pod-70e593b0-4f57-462c-9b56-7bc827d0e2bc" in namespace "emptydir-8953" to be "Succeeded or Failed"
Apr  6 03:07:51.394: INFO: Pod "pod-70e593b0-4f57-462c-9b56-7bc827d0e2bc": Phase="Pending", Reason="", readiness=false. Elapsed: 68.154564ms
Apr  6 03:07:53.402: INFO: Pod "pod-70e593b0-4f57-462c-9b56-7bc827d0e2bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076727989s
Apr  6 03:07:55.418: INFO: Pod "pod-70e593b0-4f57-462c-9b56-7bc827d0e2bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091752563s
STEP: Saw pod success
Apr  6 03:07:55.418: INFO: Pod "pod-70e593b0-4f57-462c-9b56-7bc827d0e2bc" satisfied condition "Succeeded or Failed"
Apr  6 03:07:55.426: INFO: Trying to get logs from node 10.241.0.102 pod pod-70e593b0-4f57-462c-9b56-7bc827d0e2bc container test-container: <nil>
STEP: delete the pod
Apr  6 03:07:55.468: INFO: Waiting for pod pod-70e593b0-4f57-462c-9b56-7bc827d0e2bc to disappear
Apr  6 03:07:55.480: INFO: Pod pod-70e593b0-4f57-462c-9b56-7bc827d0e2bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:07:55.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8953" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":697,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:07:55.507: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:07:55.891: INFO: Create a RollingUpdate DaemonSet
Apr  6 03:07:55.903: INFO: Check that daemon pods launch on every node of the cluster
Apr  6 03:07:55.925: INFO: Number of nodes with available pods: 0
Apr  6 03:07:55.925: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:07:56.975: INFO: Number of nodes with available pods: 0
Apr  6 03:07:56.975: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:07:57.945: INFO: Number of nodes with available pods: 2
Apr  6 03:07:57.945: INFO: Node 10.241.0.101 is running more than one daemon pod
Apr  6 03:07:58.973: INFO: Number of nodes with available pods: 3
Apr  6 03:07:58.973: INFO: Number of running nodes: 3, number of available pods: 3
Apr  6 03:07:58.973: INFO: Update the DaemonSet to trigger a rollout
Apr  6 03:07:59.061: INFO: Updating DaemonSet daemon-set
Apr  6 03:08:02.134: INFO: Roll back the DaemonSet before rollout is complete
Apr  6 03:08:02.183: INFO: Updating DaemonSet daemon-set
Apr  6 03:08:02.183: INFO: Make sure DaemonSet rollback is complete
Apr  6 03:08:02.195: INFO: Wrong image for pod: daemon-set-xwc9n. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Apr  6 03:08:02.195: INFO: Pod daemon-set-xwc9n is not available
Apr  6 03:08:05.355: INFO: Pod daemon-set-mk98w is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1632, will wait for the garbage collector to delete the pods
Apr  6 03:08:05.459: INFO: Deleting DaemonSet.extensions daemon-set took: 17.042542ms
Apr  6 03:08:05.559: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.66551ms
Apr  6 03:08:08.570: INFO: Number of nodes with available pods: 0
Apr  6 03:08:08.570: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 03:08:08.613: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61991"},"items":null}

Apr  6 03:08:08.622: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61991"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:08:08.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1632" for this suite.

• [SLOW TEST:13.239 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":35,"skipped":709,"failed":0}
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:08:08.746: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Apr  6 03:08:09.009: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Apr  6 03:08:09.094: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr  6 03:08:09.094: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Apr  6 03:08:09.149: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr  6 03:08:09.149: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Apr  6 03:08:09.201: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr  6 03:08:09.201: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Apr  6 03:08:16.516: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:08:16.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-909" for this suite.

• [SLOW TEST:8.127 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":36,"skipped":709,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:08:16.873: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:08:17.055: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr  6 03:08:27.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-6764 --namespace=crd-publish-openapi-6764 create -f -'
Apr  6 03:08:29.916: INFO: stderr: ""
Apr  6 03:08:29.916: INFO: stdout: "e2e-test-crd-publish-openapi-6741-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr  6 03:08:29.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-6764 --namespace=crd-publish-openapi-6764 delete e2e-test-crd-publish-openapi-6741-crds test-cr'
Apr  6 03:08:30.026: INFO: stderr: ""
Apr  6 03:08:30.026: INFO: stdout: "e2e-test-crd-publish-openapi-6741-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr  6 03:08:30.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-6764 --namespace=crd-publish-openapi-6764 apply -f -'
Apr  6 03:08:31.964: INFO: stderr: ""
Apr  6 03:08:31.964: INFO: stdout: "e2e-test-crd-publish-openapi-6741-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr  6 03:08:31.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-6764 --namespace=crd-publish-openapi-6764 delete e2e-test-crd-publish-openapi-6741-crds test-cr'
Apr  6 03:08:32.052: INFO: stderr: ""
Apr  6 03:08:32.052: INFO: stdout: "e2e-test-crd-publish-openapi-6741-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr  6 03:08:32.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-6764 explain e2e-test-crd-publish-openapi-6741-crds'
Apr  6 03:08:32.326: INFO: stderr: ""
Apr  6 03:08:32.326: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6741-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:08:41.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6764" for this suite.

• [SLOW TEST:24.433 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":37,"skipped":713,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:08:41.306: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:08:41.686: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7cea30b5-6881-4bcc-bc01-9ce1dc87d9d5" in namespace "security-context-test-5461" to be "Succeeded or Failed"
Apr  6 03:08:41.705: INFO: Pod "busybox-user-65534-7cea30b5-6881-4bcc-bc01-9ce1dc87d9d5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.449849ms
Apr  6 03:08:43.725: INFO: Pod "busybox-user-65534-7cea30b5-6881-4bcc-bc01-9ce1dc87d9d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038326184s
Apr  6 03:08:45.743: INFO: Pod "busybox-user-65534-7cea30b5-6881-4bcc-bc01-9ce1dc87d9d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056089254s
Apr  6 03:08:45.743: INFO: Pod "busybox-user-65534-7cea30b5-6881-4bcc-bc01-9ce1dc87d9d5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:08:45.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5461" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":717,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:08:45.772: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr  6 03:08:45.938: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 03:08:45.968: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 03:08:45.998: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.100 before test
Apr  6 03:08:46.063: INFO: calico-kube-controllers-b6474b6d6-v54x8 from calico-system started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr  6 03:08:46.063: INFO: calico-node-tnbgq from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:08:46.063: INFO: calico-typha-5fc4c7b7c7-rjrk4 from calico-system started at 2022-04-06 01:23:03 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 03:08:46.063: INFO: managed-storage-validation-webhooks-86b89bd6d-25bg6 from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Apr  6 03:08:46.063: INFO: managed-storage-validation-webhooks-86b89bd6d-7bfgh from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Apr  6 03:08:46.063: INFO: managed-storage-validation-webhooks-86b89bd6d-q5wr7 from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Apr  6 03:08:46.063: INFO: ibm-keepalived-watcher-c7h76 from kube-system started at 2022-04-06 01:20:27 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:08:46.063: INFO: ibm-master-proxy-static-10.241.0.100 from kube-system started at 2022-04-06 01:20:21 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:08:46.063: INFO: ibm-storage-metrics-agent-77bc4fb5c9-dck7l from kube-system started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Apr  6 03:08:46.063: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2022-04-06 01:25:08 +0000 UTC (6 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container csi-resizer ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:08:46.063: INFO: ibm-vpc-block-csi-node-n4fgh from kube-system started at 2022-04-06 01:20:27 +0000 UTC (4 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:08:46.063: INFO: cluster-node-tuning-operator-847fd957bd-kmksx from openshift-cluster-node-tuning-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: tuned-lxrqd from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:08:46.063: INFO: cluster-samples-operator-67d667cb6c-5hvmq from openshift-cluster-samples-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr  6 03:08:46.063: INFO: cluster-storage-operator-77cf9bb8c7-zwgsg from openshift-cluster-storage-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Apr  6 03:08:46.063: INFO: csi-snapshot-controller-operator-55d76bfc74-fxv2z from openshift-cluster-storage-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: console-operator-7c7f956448-rt9cz from openshift-console-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container console-operator ready: true, restart count 1
Apr  6 03:08:46.063: INFO: console-84457c4b7f-l4zfr from openshift-console started at 2022-04-06 01:38:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container console ready: true, restart count 0
Apr  6 03:08:46.063: INFO: dns-operator-8d8fb8787-xmhn7 from openshift-dns-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container dns-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: dns-default-gx9pd from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: node-resolver-8cqjf from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:08:46.063: INFO: cluster-image-registry-operator-6bcb795945-5qbrw from openshift-image-registry started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: node-ca-gvpjv from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:08:46.063: INFO: ingress-canary-jd288 from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:08:46.063: INFO: ingress-operator-58b79c98c4-9q2fc from openshift-ingress-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container ingress-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: openshift-kube-proxy-pqrfn from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: kube-storage-version-migrator-operator-5cfccbf8c7-c9h68 from openshift-kube-storage-version-migrator-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Apr  6 03:08:46.063: INFO: marketplace-operator-6cf6b95b7c-vjlnm from openshift-marketplace started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: cluster-monitoring-operator-6c8f74c5d5-9lsz4 from openshift-monitoring started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Apr  6 03:08:46.063: INFO: node-exporter-wbftb from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:08:46.063: INFO: multus-additional-cni-plugins-xpz8b from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:08:46.063: INFO: multus-admission-controller-qwnbg from openshift-multus started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:08:46.063: INFO: multus-fjdss from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:08:46.063: INFO: network-metrics-daemon-4m7nv from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:08:46.063: INFO: network-check-source-544bd4cb64-g5s5s from openshift-network-diagnostics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container check-endpoints ready: true, restart count 0
Apr  6 03:08:46.063: INFO: network-check-target-5lxc6 from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:08:46.063: INFO: network-operator-6c8789b55f-ntzrr from openshift-network-operator started at 2022-04-06 01:21:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container network-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: catalog-operator-699fc547c5-l5qkl from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container catalog-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: collect-profiles-27486870--1-qvmpv from openshift-operator-lifecycle-manager started at 2022-04-06 02:30:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:08:46.063: INFO: collect-profiles-27486885--1-4phn9 from openshift-operator-lifecycle-manager started at 2022-04-06 02:45:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:08:46.063: INFO: olm-operator-864d7cb959-4fbrf from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container olm-operator ready: true, restart count 0
Apr  6 03:08:46.063: INFO: package-server-manager-6f44bc74b7-c9pcp from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container package-server-manager ready: true, restart count 0
Apr  6 03:08:46.063: INFO: packageserver-77bb6bdcd6-hqmlj from openshift-operator-lifecycle-manager started at 2022-04-06 01:35:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container packageserver ready: true, restart count 0
Apr  6 03:08:46.063: INFO: metrics-b6fbdf747-4gdlt from openshift-roks-metrics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container metrics ready: true, restart count 2
Apr  6 03:08:46.063: INFO: push-gateway-f7897c967-v6cxg from openshift-roks-metrics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container push-gateway ready: true, restart count 0
Apr  6 03:08:46.063: INFO: service-ca-operator-77fd55df89-nkd65 from openshift-service-ca-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container service-ca-operator ready: true, restart count 1
Apr  6 03:08:46.063: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-htb29 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.063: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:08:46.063: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:08:46.064: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.101 before test
Apr  6 03:08:46.114: INFO: calico-node-cs6hj from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:08:46.114: INFO: calico-typha-5fc4c7b7c7-67lgl from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 03:08:46.114: INFO: ibm-keepalived-watcher-rppx4 from kube-system started at 2022-04-06 01:20:09 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:08:46.114: INFO: ibm-master-proxy-static-10.241.0.101 from kube-system started at 2022-04-06 01:20:00 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:08:46.114: INFO: ibm-vpc-block-csi-node-qh8rl from kube-system started at 2022-04-06 01:20:09 +0000 UTC (4 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:08:46.114: INFO: tuned-46gbx from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:08:46.114: INFO: csi-snapshot-controller-7ffc756fcb-xzddf from openshift-cluster-storage-operator started at 2022-04-06 01:36:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container snapshot-controller ready: true, restart count 0
Apr  6 03:08:46.114: INFO: csi-snapshot-webhook-574644677c-dmmpc from openshift-cluster-storage-operator started at 2022-04-06 01:36:15 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container webhook ready: true, restart count 0
Apr  6 03:08:46.114: INFO: console-84457c4b7f-8gp5t from openshift-console started at 2022-04-06 01:39:26 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container console ready: true, restart count 0
Apr  6 03:08:46.114: INFO: downloads-dbb5d5764-jhtbl from openshift-console started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container download-server ready: true, restart count 0
Apr  6 03:08:46.114: INFO: dns-default-v242c from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: node-resolver-lsbp7 from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:08:46.114: INFO: node-ca-ccn7d from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:08:46.114: INFO: ingress-canary-wklcl from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:08:46.114: INFO: router-default-79f7bb79b4-vnvq6 from openshift-ingress started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container router ready: true, restart count 0
Apr  6 03:08:46.114: INFO: openshift-kube-proxy-lsknc from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: community-operators-c2jdx from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:08:46.114: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: node-exporter-vx9fl from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:08:46.114: INFO: openshift-state-metrics-8dcbc5f76-cx6zj from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (3 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr  6 03:08:46.114: INFO: prometheus-adapter-54744554d8-95snn from openshift-monitoring started at 2022-04-06 01:39:52 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr  6 03:08:46.114: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-04-06 01:38:59 +0000 UTC (7 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container prometheus ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr  6 03:08:46.114: INFO: thanos-querier-6797965cb7-gjqm5 from openshift-monitoring started at 2022-04-06 01:38:55 +0000 UTC (5 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container thanos-query ready: true, restart count 0
Apr  6 03:08:46.114: INFO: multus-additional-cni-plugins-d5x4n from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:08:46.114: INFO: multus-admission-controller-6c4s6 from openshift-multus started at 2022-04-06 01:27:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:08:46.114: INFO: multus-cwlpj from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:08:46.114: INFO: network-metrics-daemon-hgtsb from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:08:46.114: INFO: network-check-target-rmb8p from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:08:46.114: INFO: sonobuoy from sonobuoy started at 2022-04-06 02:56:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 03:08:46.114: INFO: sonobuoy-e2e-job-9989f36257d44400 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container e2e ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:08:46.114: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-ffjt6 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:08:46.114: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.102 before test
Apr  6 03:08:46.174: INFO: calico-node-t786f from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.174: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:08:46.174: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-04-06 01:29:25 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.174: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Apr  6 03:08:46.174: INFO: ibm-keepalived-watcher-bv7pb from kube-system started at 2022-04-06 01:20:29 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.174: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:08:46.174: INFO: ibm-master-proxy-static-10.241.0.102 from kube-system started at 2022-04-06 01:20:23 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.174: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:08:46.174: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:08:46.174: INFO: ibm-vpc-block-csi-node-nk2wg from kube-system started at 2022-04-06 01:20:29 +0000 UTC (4 container statuses recorded)
Apr  6 03:08:46.174: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:08:46.174: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:08:46.174: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:08:46.174: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:08:46.174: INFO: vpn-849cbbd4f5-zbslr from kube-system started at 2022-04-06 01:31:19 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.174: INFO: 	Container vpn ready: true, restart count 0
Apr  6 03:08:46.174: INFO: tuned-r57x6 from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.174: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:08:46.174: INFO: csi-snapshot-controller-7ffc756fcb-j85jv from openshift-cluster-storage-operator started at 2022-04-06 01:36:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.174: INFO: 	Container snapshot-controller ready: true, restart count 0
Apr  6 03:08:46.175: INFO: csi-snapshot-webhook-574644677c-gkf4n from openshift-cluster-storage-operator started at 2022-04-06 01:36:15 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container webhook ready: true, restart count 0
Apr  6 03:08:46.175: INFO: downloads-dbb5d5764-6ldtc from openshift-console started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container download-server ready: true, restart count 0
Apr  6 03:08:46.175: INFO: dns-default-zvb7x from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: node-resolver-9gtvm from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:08:46.175: INFO: image-registry-54fc9d45f8-c8xl9 from openshift-image-registry started at 2022-04-06 01:38:16 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container registry ready: true, restart count 0
Apr  6 03:08:46.175: INFO: node-ca-w42rx from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:08:46.175: INFO: ingress-canary-7dgmf from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:08:46.175: INFO: router-default-79f7bb79b4-txbvm from openshift-ingress started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container router ready: true, restart count 0
Apr  6 03:08:46.175: INFO: openshift-kube-proxy-9c8jc from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: migrator-c84bfd698-4gmq8 from openshift-kube-storage-version-migrator started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container migrator ready: true, restart count 0
Apr  6 03:08:46.175: INFO: certified-operators-k8bbt from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:08:46.175: INFO: redhat-marketplace-dhghg from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:08:46.175: INFO: redhat-operators-7266t from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:08:46.175: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: grafana-65d7ff4ff4-pjvw7 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container grafana ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: kube-state-metrics-c4c8f95d8-s5wlt from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (3 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 03:08:46.175: INFO: node-exporter-nnrpz from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:08:46.175: INFO: prometheus-adapter-54744554d8-v5q6n from openshift-monitoring started at 2022-04-06 01:39:52 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr  6 03:08:46.175: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-04-06 01:39:00 +0000 UTC (7 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container prometheus ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr  6 03:08:46.175: INFO: prometheus-operator-785898db99-xvm9b from openshift-monitoring started at 2022-04-06 01:36:07 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr  6 03:08:46.175: INFO: telemeter-client-bdc7d9995-gfbw6 from openshift-monitoring started at 2022-04-06 01:37:33 +0000 UTC (3 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container reload ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container telemeter-client ready: true, restart count 0
Apr  6 03:08:46.175: INFO: thanos-querier-6797965cb7-6fjfp from openshift-monitoring started at 2022-04-06 01:38:55 +0000 UTC (5 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container thanos-query ready: true, restart count 0
Apr  6 03:08:46.175: INFO: multus-additional-cni-plugins-v2thd from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:08:46.175: INFO: multus-admission-controller-n2fzx from openshift-multus started at 2022-04-06 01:25:10 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:08:46.175: INFO: multus-mhr5m from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:08:46.175: INFO: network-metrics-daemon-qt77t from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:08:46.175: INFO: network-check-target-m4pg9 from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:08:46.175: INFO: collect-profiles-27486900--1-xnsfw from openshift-operator-lifecycle-manager started at 2022-04-06 03:00:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:08:46.175: INFO: packageserver-77bb6bdcd6-2sng2 from openshift-operator-lifecycle-manager started at 2022-04-06 01:35:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container packageserver ready: true, restart count 0
Apr  6 03:08:46.175: INFO: service-ca-c77965566-b6wft from openshift-service-ca started at 2022-04-06 01:33:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container service-ca-controller ready: true, restart count 0
Apr  6 03:08:46.175: INFO: busybox-user-65534-7cea30b5-6881-4bcc-bc01-9ce1dc87d9d5 from security-context-test-5461 started at 2022-04-06 03:08:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container busybox-user-65534-7cea30b5-6881-4bcc-bc01-9ce1dc87d9d5 ready: false, restart count 0
Apr  6 03:08:46.175: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-84flv from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:08:46.175: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:08:46.175: INFO: tigera-operator-5d4d8f956c-2mlcb from tigera-operator started at 2022-04-06 01:21:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:08:46.175: INFO: 	Container tigera-operator ready: true, restart count 3
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-eb613192-d61d-4cf2-a6cd-7b18c5124824 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.241.0.102 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-eb613192-d61d-4cf2-a6cd-7b18c5124824 off the node 10.241.0.102
STEP: verifying the node doesn't have the label kubernetes.io/e2e-eb613192-d61d-4cf2-a6cd-7b18c5124824
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:13:50.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6810" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.826 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":39,"skipped":727,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:13:50.598: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:13:50.806: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:13:51.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4442" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":40,"skipped":728,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:13:51.506: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Apr  6 03:13:51.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-5447 create -f -'
Apr  6 03:13:52.494: INFO: stderr: ""
Apr  6 03:13:52.494: INFO: stdout: "pod/pause created\n"
Apr  6 03:13:52.494: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr  6 03:13:52.494: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5447" to be "running and ready"
Apr  6 03:13:52.506: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.207579ms
Apr  6 03:13:54.530: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.03650714s
Apr  6 03:13:54.530: INFO: Pod "pause" satisfied condition "running and ready"
Apr  6 03:13:54.530: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Apr  6 03:13:54.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-5447 label pods pause testing-label=testing-label-value'
Apr  6 03:13:54.635: INFO: stderr: ""
Apr  6 03:13:54.635: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr  6 03:13:54.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-5447 get pod pause -L testing-label'
Apr  6 03:13:54.729: INFO: stderr: ""
Apr  6 03:13:54.729: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr  6 03:13:54.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-5447 label pods pause testing-label-'
Apr  6 03:13:54.978: INFO: stderr: ""
Apr  6 03:13:54.978: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr  6 03:13:54.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-5447 get pod pause -L testing-label'
Apr  6 03:13:55.054: INFO: stderr: ""
Apr  6 03:13:55.054: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Apr  6 03:13:55.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-5447 delete --grace-period=0 --force -f -'
Apr  6 03:13:55.176: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 03:13:55.176: INFO: stdout: "pod \"pause\" force deleted\n"
Apr  6 03:13:55.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-5447 get rc,svc -l name=pause --no-headers'
Apr  6 03:13:55.292: INFO: stderr: "No resources found in kubectl-5447 namespace.\n"
Apr  6 03:13:55.292: INFO: stdout: ""
Apr  6 03:13:55.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-5447 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 03:13:55.369: INFO: stderr: ""
Apr  6 03:13:55.369: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:13:55.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5447" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":41,"skipped":733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:13:55.398: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:14:07.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4394" for this suite.

• [SLOW TEST:11.681 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":42,"skipped":757,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:14:07.079: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Apr  6 03:14:07.340: INFO: Waiting up to 5m0s for pod "client-containers-54cc6b24-61b7-4769-acf5-fb14eeff480e" in namespace "containers-4841" to be "Succeeded or Failed"
Apr  6 03:14:07.355: INFO: Pod "client-containers-54cc6b24-61b7-4769-acf5-fb14eeff480e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.609579ms
Apr  6 03:14:09.367: INFO: Pod "client-containers-54cc6b24-61b7-4769-acf5-fb14eeff480e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026839424s
Apr  6 03:14:11.386: INFO: Pod "client-containers-54cc6b24-61b7-4769-acf5-fb14eeff480e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04543085s
STEP: Saw pod success
Apr  6 03:14:11.386: INFO: Pod "client-containers-54cc6b24-61b7-4769-acf5-fb14eeff480e" satisfied condition "Succeeded or Failed"
Apr  6 03:14:11.399: INFO: Trying to get logs from node 10.241.0.102 pod client-containers-54cc6b24-61b7-4769-acf5-fb14eeff480e container agnhost-container: <nil>
STEP: delete the pod
Apr  6 03:14:11.528: INFO: Waiting for pod client-containers-54cc6b24-61b7-4769-acf5-fb14eeff480e to disappear
Apr  6 03:14:11.541: INFO: Pod client-containers-54cc6b24-61b7-4769-acf5-fb14eeff480e no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:14:11.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4841" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":793,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:14:11.571: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr  6 03:14:11.827: INFO: Waiting up to 5m0s for pod "pod-1c8c2ca4-34a4-4e67-a2aa-0668d4e831b6" in namespace "emptydir-6732" to be "Succeeded or Failed"
Apr  6 03:14:11.838: INFO: Pod "pod-1c8c2ca4-34a4-4e67-a2aa-0668d4e831b6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.535613ms
Apr  6 03:14:13.849: INFO: Pod "pod-1c8c2ca4-34a4-4e67-a2aa-0668d4e831b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022712386s
Apr  6 03:14:15.865: INFO: Pod "pod-1c8c2ca4-34a4-4e67-a2aa-0668d4e831b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038359113s
STEP: Saw pod success
Apr  6 03:14:15.865: INFO: Pod "pod-1c8c2ca4-34a4-4e67-a2aa-0668d4e831b6" satisfied condition "Succeeded or Failed"
Apr  6 03:14:15.889: INFO: Trying to get logs from node 10.241.0.102 pod pod-1c8c2ca4-34a4-4e67-a2aa-0668d4e831b6 container test-container: <nil>
STEP: delete the pod
Apr  6 03:14:15.978: INFO: Waiting for pod pod-1c8c2ca4-34a4-4e67-a2aa-0668d4e831b6 to disappear
Apr  6 03:14:15.995: INFO: Pod pod-1c8c2ca4-34a4-4e67-a2aa-0668d4e831b6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:14:15.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6732" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":816,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:14:16.033: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Apr  6 03:14:16.325: INFO: Waiting up to 5m0s for pod "security-context-25006586-ebe4-4ae5-bce6-741ec93dd241" in namespace "security-context-4396" to be "Succeeded or Failed"
Apr  6 03:14:16.340: INFO: Pod "security-context-25006586-ebe4-4ae5-bce6-741ec93dd241": Phase="Pending", Reason="", readiness=false. Elapsed: 15.283939ms
Apr  6 03:14:18.354: INFO: Pod "security-context-25006586-ebe4-4ae5-bce6-741ec93dd241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029137563s
Apr  6 03:14:20.370: INFO: Pod "security-context-25006586-ebe4-4ae5-bce6-741ec93dd241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044830971s
STEP: Saw pod success
Apr  6 03:14:20.370: INFO: Pod "security-context-25006586-ebe4-4ae5-bce6-741ec93dd241" satisfied condition "Succeeded or Failed"
Apr  6 03:14:20.385: INFO: Trying to get logs from node 10.241.0.102 pod security-context-25006586-ebe4-4ae5-bce6-741ec93dd241 container test-container: <nil>
STEP: delete the pod
Apr  6 03:14:20.449: INFO: Waiting for pod security-context-25006586-ebe4-4ae5-bce6-741ec93dd241 to disappear
Apr  6 03:14:20.460: INFO: Pod security-context-25006586-ebe4-4ae5-bce6-741ec93dd241 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:14:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4396" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":45,"skipped":837,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:14:20.490: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-d0c2cbac-d97f-47f3-86ee-f27bc90206ab in namespace container-probe-1905
Apr  6 03:14:22.752: INFO: Started pod busybox-d0c2cbac-d97f-47f3-86ee-f27bc90206ab in namespace container-probe-1905
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 03:14:22.763: INFO: Initial restart count of pod busybox-d0c2cbac-d97f-47f3-86ee-f27bc90206ab is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:18:22.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1905" for this suite.

• [SLOW TEST:242.473 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:18:22.963: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:18:23.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7240" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":47,"skipped":867,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:18:23.227: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-30624415-f5a7-412c-835e-adde21b83006
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:18:23.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1546" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":48,"skipped":874,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:18:23.601: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-ff2w
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 03:18:24.002: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ff2w" in namespace "subpath-6746" to be "Succeeded or Failed"
Apr  6 03:18:24.014: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Pending", Reason="", readiness=false. Elapsed: 11.972823ms
Apr  6 03:18:26.029: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027181691s
Apr  6 03:18:28.041: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 4.039149059s
Apr  6 03:18:30.067: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 6.065093054s
Apr  6 03:18:32.088: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 8.085987168s
Apr  6 03:18:34.104: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 10.102329002s
Apr  6 03:18:36.125: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 12.123280274s
Apr  6 03:18:38.142: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 14.140732161s
Apr  6 03:18:40.188: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 16.186064254s
Apr  6 03:18:42.204: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 18.202224758s
Apr  6 03:18:44.222: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Running", Reason="", readiness=true. Elapsed: 20.22009131s
Apr  6 03:18:46.237: INFO: Pod "pod-subpath-test-downwardapi-ff2w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.234840743s
STEP: Saw pod success
Apr  6 03:18:46.237: INFO: Pod "pod-subpath-test-downwardapi-ff2w" satisfied condition "Succeeded or Failed"
Apr  6 03:18:46.249: INFO: Trying to get logs from node 10.241.0.102 pod pod-subpath-test-downwardapi-ff2w container test-container-subpath-downwardapi-ff2w: <nil>
STEP: delete the pod
Apr  6 03:18:46.373: INFO: Waiting for pod pod-subpath-test-downwardapi-ff2w to disappear
Apr  6 03:18:46.384: INFO: Pod pod-subpath-test-downwardapi-ff2w no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ff2w
Apr  6 03:18:46.384: INFO: Deleting pod "pod-subpath-test-downwardapi-ff2w" in namespace "subpath-6746"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:18:46.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6746" for this suite.

• [SLOW TEST:22.933 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":49,"skipped":878,"failed":0}
SS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:18:46.534: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr  6 03:18:50.908: INFO: &Pod{ObjectMeta:{send-events-a9cf9ee6-f9d2-4cb1-b08d-aac10005b02b  events-6217  49dac9f8-f9c6-4141-8c4f-cf8ef4d80f4a 66822 0 2022-04-06 03:18:46 +0000 UTC <nil> <nil> map[name:foo time:719447271] map[cni.projectcalico.org/containerID:66e8b4c404a2cf8f7bbde734a506254a544f54d1f0f18ea14b19df73a5af15f1 cni.projectcalico.org/podIP:172.17.96.120/32 cni.projectcalico.org/podIPs:172.17.96.120/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.120"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.120"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-04-06 03:18:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 03:18:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 03:18:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 03:18:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.96.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5cmqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5cmqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:18:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:18:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:18:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:18:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.102,PodIP:172.17.96.120,StartTime:2022-04-06 03:18:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 03:18:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://5db563b77d2596e9195be8123a7cebe0493de29249b4c86fd7252c2d90f53d9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.96.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr  6 03:18:52.928: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr  6 03:18:54.974: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:18:55.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6217" for this suite.

• [SLOW TEST:8.511 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":50,"skipped":880,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:18:55.046: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr  6 03:18:55.348: INFO: Pod name pod-release: Found 0 pods out of 1
Apr  6 03:19:00.364: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:19:01.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2785" for this suite.

• [SLOW TEST:6.503 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":51,"skipped":905,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:19:01.549: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9382
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9382
STEP: Waiting until pod test-pod will start running in namespace statefulset-9382
STEP: Creating statefulset with conflicting port in namespace statefulset-9382
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9382
Apr  6 03:19:06.056: INFO: Observed stateful pod in namespace: statefulset-9382, name: ss-0, uid: 80c7db92-5296-4ea6-a2ec-93f64e914eaa, status phase: Pending. Waiting for statefulset controller to delete.
Apr  6 03:19:06.101: INFO: Observed stateful pod in namespace: statefulset-9382, name: ss-0, uid: 80c7db92-5296-4ea6-a2ec-93f64e914eaa, status phase: Failed. Waiting for statefulset controller to delete.
Apr  6 03:19:06.129: INFO: Observed stateful pod in namespace: statefulset-9382, name: ss-0, uid: 80c7db92-5296-4ea6-a2ec-93f64e914eaa, status phase: Failed. Waiting for statefulset controller to delete.
Apr  6 03:19:06.145: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9382
STEP: Removing pod with conflicting port in namespace statefulset-9382
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9382 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Apr  6 03:19:10.262: INFO: Deleting all statefulset in ns statefulset-9382
Apr  6 03:19:10.275: INFO: Scaling statefulset ss to 0
Apr  6 03:19:20.339: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 03:19:20.350: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:19:20.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9382" for this suite.

• [SLOW TEST:18.925 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":52,"skipped":927,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:19:20.475: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:19:20.759: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr  6 03:19:20.789: INFO: Number of nodes with available pods: 0
Apr  6 03:19:20.789: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr  6 03:19:20.847: INFO: Number of nodes with available pods: 0
Apr  6 03:19:20.847: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:19:21.870: INFO: Number of nodes with available pods: 0
Apr  6 03:19:21.870: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:19:22.868: INFO: Number of nodes with available pods: 0
Apr  6 03:19:22.868: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:19:23.859: INFO: Number of nodes with available pods: 1
Apr  6 03:19:23.859: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr  6 03:19:23.952: INFO: Number of nodes with available pods: 1
Apr  6 03:19:23.952: INFO: Number of running nodes: 0, number of available pods: 1
Apr  6 03:19:24.971: INFO: Number of nodes with available pods: 0
Apr  6 03:19:24.971: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr  6 03:19:25.005: INFO: Number of nodes with available pods: 0
Apr  6 03:19:25.005: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:19:26.019: INFO: Number of nodes with available pods: 0
Apr  6 03:19:26.019: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:19:27.019: INFO: Number of nodes with available pods: 0
Apr  6 03:19:27.019: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:19:28.023: INFO: Number of nodes with available pods: 0
Apr  6 03:19:28.023: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:19:29.016: INFO: Number of nodes with available pods: 1
Apr  6 03:19:29.016: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9308, will wait for the garbage collector to delete the pods
Apr  6 03:19:29.110: INFO: Deleting DaemonSet.extensions daemon-set took: 17.457455ms
Apr  6 03:19:29.210: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.435721ms
Apr  6 03:19:31.835: INFO: Number of nodes with available pods: 0
Apr  6 03:19:31.835: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 03:19:31.853: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"67620"},"items":null}

Apr  6 03:19:31.878: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"67620"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:19:31.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9308" for this suite.

• [SLOW TEST:11.542 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":53,"skipped":935,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:19:32.017: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-dbe6c985-a141-444a-990a-e3a635732659 in namespace container-probe-7368
Apr  6 03:19:36.349: INFO: Started pod test-webserver-dbe6c985-a141-444a-990a-e3a635732659 in namespace container-probe-7368
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 03:19:36.364: INFO: Initial restart count of pod test-webserver-dbe6c985-a141-444a-990a-e3a635732659 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:23:36.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7368" for this suite.

• [SLOW TEST:244.510 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":950,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:23:36.527: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-dac44ff0-8d11-415c-8fc6-e42288a2c299
STEP: Creating a pod to test consume configMaps
Apr  6 03:23:36.786: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dbf3d8fd-6bed-4a62-bfe7-41ac9d6fe175" in namespace "projected-1083" to be "Succeeded or Failed"
Apr  6 03:23:36.799: INFO: Pod "pod-projected-configmaps-dbf3d8fd-6bed-4a62-bfe7-41ac9d6fe175": Phase="Pending", Reason="", readiness=false. Elapsed: 13.315832ms
Apr  6 03:23:38.816: INFO: Pod "pod-projected-configmaps-dbf3d8fd-6bed-4a62-bfe7-41ac9d6fe175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029998754s
Apr  6 03:23:40.837: INFO: Pod "pod-projected-configmaps-dbf3d8fd-6bed-4a62-bfe7-41ac9d6fe175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051709317s
STEP: Saw pod success
Apr  6 03:23:40.837: INFO: Pod "pod-projected-configmaps-dbf3d8fd-6bed-4a62-bfe7-41ac9d6fe175" satisfied condition "Succeeded or Failed"
Apr  6 03:23:40.852: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-configmaps-dbf3d8fd-6bed-4a62-bfe7-41ac9d6fe175 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 03:23:40.952: INFO: Waiting for pod pod-projected-configmaps-dbf3d8fd-6bed-4a62-bfe7-41ac9d6fe175 to disappear
Apr  6 03:23:40.966: INFO: Pod pod-projected-configmaps-dbf3d8fd-6bed-4a62-bfe7-41ac9d6fe175 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:23:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1083" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:23:41.003: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr  6 03:23:41.200: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:23:47.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8800" for this suite.

• [SLOW TEST:6.481 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":983,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:23:47.484: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9546
Apr  6 03:23:47.830: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:23:49.852: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Apr  6 03:23:49.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr  6 03:23:50.533: INFO: rc: 7
Apr  6 03:23:50.573: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr  6 03:23:50.588: INFO: Pod kube-proxy-mode-detector no longer exists
Apr  6 03:23:50.588: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-9546
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9546
I0406 03:23:50.646168      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9546, replica count: 3
I0406 03:23:53.696622      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 03:23:56.697528      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 03:23:59.698042      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 03:23:59.793: INFO: Creating new exec pod
Apr  6 03:24:04.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Apr  6 03:24:05.225: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Apr  6 03:24:05.225: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:24:05.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.89.219 80'
Apr  6 03:24:05.463: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.89.219 80\nConnection to 172.21.89.219 80 port [tcp/http] succeeded!\n"
Apr  6 03:24:05.463: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:24:05.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.102 31171'
Apr  6 03:24:05.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.102 31171\nConnection to 10.241.0.102 31171 port [tcp/*] succeeded!\n"
Apr  6 03:24:05.843: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:24:05.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.101 31171'
Apr  6 03:24:06.223: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.241.0.101 31171\nConnection to 10.241.0.101 31171 port [tcp/*] succeeded!\n"
Apr  6 03:24:06.223: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:24:06.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.241.0.100:31171/ ; done'
Apr  6 03:24:06.540: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n"
Apr  6 03:24:06.540: INFO: stdout: "\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj\naffinity-nodeport-timeout-ktjmj"
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Received response from host: affinity-nodeport-timeout-ktjmj
Apr  6 03:24:06.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.241.0.100:31171/'
Apr  6 03:24:06.802: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n"
Apr  6 03:24:06.802: INFO: stdout: "affinity-nodeport-timeout-ktjmj"
Apr  6 03:24:26.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.241.0.100:31171/'
Apr  6 03:24:27.551: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n"
Apr  6 03:24:27.551: INFO: stdout: "affinity-nodeport-timeout-ktjmj"
Apr  6 03:24:47.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.241.0.100:31171/'
Apr  6 03:24:47.839: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n"
Apr  6 03:24:47.839: INFO: stdout: "affinity-nodeport-timeout-ktjmj"
Apr  6 03:25:07.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9546 exec execpod-affinityvk2p2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.241.0.100:31171/'
Apr  6 03:25:08.266: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.241.0.100:31171/\n"
Apr  6 03:25:08.266: INFO: stdout: "affinity-nodeport-timeout-tmrlx"
Apr  6 03:25:08.266: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9546, will wait for the garbage collector to delete the pods
Apr  6 03:25:08.391: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 23.712027ms
Apr  6 03:25:08.591: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.471256ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:25:11.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9546" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:83.991 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":57,"skipped":987,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:25:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr  6 03:25:14.731: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:25:14.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9121" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:25:14.805: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Apr  6 03:25:17.396: INFO: running pods: 0 < 1
Apr  6 03:25:19.413: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:25:21.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9816" for this suite.

• [SLOW TEST:6.724 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":59,"skipped":1031,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:25:21.529: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Apr  6 03:25:21.746: INFO: Waiting up to 5m0s for pod "client-containers-8192c32d-0615-4ee1-a423-595f54624b78" in namespace "containers-8534" to be "Succeeded or Failed"
Apr  6 03:25:21.759: INFO: Pod "client-containers-8192c32d-0615-4ee1-a423-595f54624b78": Phase="Pending", Reason="", readiness=false. Elapsed: 12.826692ms
Apr  6 03:25:23.772: INFO: Pod "client-containers-8192c32d-0615-4ee1-a423-595f54624b78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026217298s
STEP: Saw pod success
Apr  6 03:25:23.772: INFO: Pod "client-containers-8192c32d-0615-4ee1-a423-595f54624b78" satisfied condition "Succeeded or Failed"
Apr  6 03:25:23.784: INFO: Trying to get logs from node 10.241.0.101 pod client-containers-8192c32d-0615-4ee1-a423-595f54624b78 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 03:25:23.880: INFO: Waiting for pod client-containers-8192c32d-0615-4ee1-a423-595f54624b78 to disappear
Apr  6 03:25:23.895: INFO: Pod client-containers-8192c32d-0615-4ee1-a423-595f54624b78 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:25:23.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8534" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":60,"skipped":1067,"failed":0}

------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:25:23.930: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:25:24.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1424" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":61,"skipped":1067,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:25:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-56b431fe-da58-495a-8e53-e4f0888d0f22 in namespace container-probe-1634
Apr  6 03:25:28.351: INFO: Started pod busybox-56b431fe-da58-495a-8e53-e4f0888d0f22 in namespace container-probe-1634
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 03:25:28.363: INFO: Initial restart count of pod busybox-56b431fe-da58-495a-8e53-e4f0888d0f22 is 0
Apr  6 03:26:16.760: INFO: Restart count of pod container-probe-1634/busybox-56b431fe-da58-495a-8e53-e4f0888d0f22 is now 1 (48.396163884s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:26:16.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1634" for this suite.

• [SLOW TEST:52.698 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":62,"skipped":1086,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:26:16.841: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:26:17.057: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr  6 03:26:22.069: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  6 03:26:22.069: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr  6 03:26:22.132: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5498  136ced38-d53c-4585-971c-d8629580943d 70838 1 2022-04-06 03:26:22 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-04-06 03:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ca1848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr  6 03:26:22.142: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Apr  6 03:26:22.142: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr  6 03:26:22.142: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5498  7807acb5-c4bf-4200-80a1-78465b3613fa 70839 1 2022-04-06 03:26:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 136ced38-d53c-4585-971c-d8629580943d 0xc001fe8dc7 0xc001fe8dc8}] []  [{e2e.test Update apps/v1 2022-04-06 03:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:26:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-04-06 03:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"136ced38-d53c-4585-971c-d8629580943d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001fe8e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 03:26:22.170: INFO: Pod "test-cleanup-controller-lxgs4" is available:
&Pod{ObjectMeta:{test-cleanup-controller-lxgs4 test-cleanup-controller- deployment-5498  779d6665-11ee-4109-848a-8cbf382d3d2b 70815 0 2022-04-06 03:26:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:630852d2a9217ae55f4e982d9801277a3b756b1bb8fce13315561a557ed9c2a6 cni.projectcalico.org/podIP:172.17.77.45/32 cni.projectcalico.org/podIPs:172.17.77.45/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.45"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.45"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 7807acb5-c4bf-4200-80a1-78465b3613fa 0xc0036c51e7 0xc0036c51e8}] []  [{calico Update v1 2022-04-06 03:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-06 03:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7807acb5-c4bf-4200-80a1-78465b3613fa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-04-06 03:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.77.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-04-06 03:26:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wn86h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wn86h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wnj4j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:26:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:26:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.100,PodIP:172.17.77.45,StartTime:2022-04-06 03:26:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 03:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://2817d02fbfd76ab7e2e05307c99a6572afe090e8a731069c427c686ba4494214,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.77.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:26:22.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5498" for this suite.

• [SLOW TEST:5.361 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":63,"skipped":1091,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:26:22.202: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:26:22.394: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:26:23.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5552" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":64,"skipped":1112,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:26:23.480: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Apr  6 03:26:23.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7128 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr  6 03:26:23.929: INFO: stderr: ""
Apr  6 03:26:23.929: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr  6 03:26:28.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7128 get pod e2e-test-httpd-pod -o json'
Apr  6 03:26:29.050: INFO: stderr: ""
Apr  6 03:26:29.050: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"eda920202082a104db91983fc27f1dc5884c2cd40f44fe7d0ca6f76e4dcb9f13\",\n            \"cni.projectcalico.org/podIP\": \"172.17.95.170/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.17.95.170/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.17.95.170\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.17.95.170\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2022-04-06T03:26:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7128\",\n        \"resourceVersion\": \"70990\",\n        \"uid\": \"28776ee9-2bec-45ac-83f6-03f76b6ea584\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-bwv6m\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-q4mgg\"\n            }\n        ],\n        \"nodeName\": \"10.241.0.101\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c37,c34\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-bwv6m\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-04-06T03:26:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-04-06T03:26:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-04-06T03:26:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-04-06T03:26:24Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://d9e62eb2b50298bceab07793ec5097b49d73d150a4b0e16bef015e7d3d2d0092\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-04-06T03:26:25Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.241.0.101\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.17.95.170\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.17.95.170\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-04-06T03:26:24Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr  6 03:26:29.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7128 replace -f -'
Apr  6 03:26:32.039: INFO: stderr: ""
Apr  6 03:26:32.039: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Apr  6 03:26:32.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7128 delete pods e2e-test-httpd-pod'
Apr  6 03:26:34.668: INFO: stderr: ""
Apr  6 03:26:34.668: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:26:34.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7128" for this suite.

• [SLOW TEST:11.227 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":65,"skipped":1170,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:26:34.708: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr  6 03:26:34.944: INFO: Waiting up to 5m0s for pod "downward-api-b0340608-5deb-4e3e-a8e7-9095b088374e" in namespace "downward-api-4530" to be "Succeeded or Failed"
Apr  6 03:26:34.958: INFO: Pod "downward-api-b0340608-5deb-4e3e-a8e7-9095b088374e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.524844ms
Apr  6 03:26:36.972: INFO: Pod "downward-api-b0340608-5deb-4e3e-a8e7-9095b088374e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027652594s
Apr  6 03:26:38.991: INFO: Pod "downward-api-b0340608-5deb-4e3e-a8e7-9095b088374e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046850072s
STEP: Saw pod success
Apr  6 03:26:38.991: INFO: Pod "downward-api-b0340608-5deb-4e3e-a8e7-9095b088374e" satisfied condition "Succeeded or Failed"
Apr  6 03:26:39.006: INFO: Trying to get logs from node 10.241.0.102 pod downward-api-b0340608-5deb-4e3e-a8e7-9095b088374e container dapi-container: <nil>
STEP: delete the pod
Apr  6 03:26:39.100: INFO: Waiting for pod downward-api-b0340608-5deb-4e3e-a8e7-9095b088374e to disappear
Apr  6 03:26:39.115: INFO: Pod downward-api-b0340608-5deb-4e3e-a8e7-9095b088374e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:26:39.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4530" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1173,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:26:39.145: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr  6 03:26:39.370: INFO: Waiting up to 5m0s for pod "pod-4b98e8c9-1346-4a6d-80e2-03616b12deda" in namespace "emptydir-2" to be "Succeeded or Failed"
Apr  6 03:26:39.393: INFO: Pod "pod-4b98e8c9-1346-4a6d-80e2-03616b12deda": Phase="Pending", Reason="", readiness=false. Elapsed: 23.474883ms
Apr  6 03:26:41.410: INFO: Pod "pod-4b98e8c9-1346-4a6d-80e2-03616b12deda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040563034s
Apr  6 03:26:43.430: INFO: Pod "pod-4b98e8c9-1346-4a6d-80e2-03616b12deda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05967164s
STEP: Saw pod success
Apr  6 03:26:43.430: INFO: Pod "pod-4b98e8c9-1346-4a6d-80e2-03616b12deda" satisfied condition "Succeeded or Failed"
Apr  6 03:26:43.441: INFO: Trying to get logs from node 10.241.0.101 pod pod-4b98e8c9-1346-4a6d-80e2-03616b12deda container test-container: <nil>
STEP: delete the pod
Apr  6 03:26:43.528: INFO: Waiting for pod pod-4b98e8c9-1346-4a6d-80e2-03616b12deda to disappear
Apr  6 03:26:43.540: INFO: Pod pod-4b98e8c9-1346-4a6d-80e2-03616b12deda no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:26:43.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":67,"skipped":1193,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:26:43.578: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr  6 03:26:43.876: INFO: Waiting up to 5m0s for pod "downward-api-96bb7658-4ce7-4ecd-b20c-848c48cf6e19" in namespace "downward-api-8390" to be "Succeeded or Failed"
Apr  6 03:26:43.895: INFO: Pod "downward-api-96bb7658-4ce7-4ecd-b20c-848c48cf6e19": Phase="Pending", Reason="", readiness=false. Elapsed: 19.725904ms
Apr  6 03:26:45.913: INFO: Pod "downward-api-96bb7658-4ce7-4ecd-b20c-848c48cf6e19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037711736s
Apr  6 03:26:47.936: INFO: Pod "downward-api-96bb7658-4ce7-4ecd-b20c-848c48cf6e19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060479923s
STEP: Saw pod success
Apr  6 03:26:47.936: INFO: Pod "downward-api-96bb7658-4ce7-4ecd-b20c-848c48cf6e19" satisfied condition "Succeeded or Failed"
Apr  6 03:26:47.964: INFO: Trying to get logs from node 10.241.0.102 pod downward-api-96bb7658-4ce7-4ecd-b20c-848c48cf6e19 container dapi-container: <nil>
STEP: delete the pod
Apr  6 03:26:48.089: INFO: Waiting for pod downward-api-96bb7658-4ce7-4ecd-b20c-848c48cf6e19 to disappear
Apr  6 03:26:48.106: INFO: Pod downward-api-96bb7658-4ce7-4ecd-b20c-848c48cf6e19 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:26:48.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8390" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1197,"failed":0}
SS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:26:48.146: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-93695624-0e09-43f1-947b-c1bfda44d7a5
STEP: Creating a pod to test consume secrets
Apr  6 03:26:48.475: INFO: Waiting up to 5m0s for pod "pod-secrets-03763530-025c-47d4-ac78-607d3f259d2d" in namespace "secrets-3398" to be "Succeeded or Failed"
Apr  6 03:26:48.486: INFO: Pod "pod-secrets-03763530-025c-47d4-ac78-607d3f259d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.508953ms
Apr  6 03:26:50.499: INFO: Pod "pod-secrets-03763530-025c-47d4-ac78-607d3f259d2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024299522s
STEP: Saw pod success
Apr  6 03:26:50.499: INFO: Pod "pod-secrets-03763530-025c-47d4-ac78-607d3f259d2d" satisfied condition "Succeeded or Failed"
Apr  6 03:26:50.511: INFO: Trying to get logs from node 10.241.0.102 pod pod-secrets-03763530-025c-47d4-ac78-607d3f259d2d container secret-env-test: <nil>
STEP: delete the pod
Apr  6 03:26:50.576: INFO: Waiting for pod pod-secrets-03763530-025c-47d4-ac78-607d3f259d2d to disappear
Apr  6 03:26:50.588: INFO: Pod pod-secrets-03763530-025c-47d4-ac78-607d3f259d2d no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:26:50.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3398" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1199,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:26:50.619: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:26:51.642: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 03:26:53.673: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812411, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812411, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812411, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812411, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:26:56.706: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:27:07.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8126" for this suite.
STEP: Destroying namespace "webhook-8126-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.870 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":70,"skipped":1205,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:27:07.491: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4195
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:27:07.790: INFO: Found 0 stateful pods, waiting for 1
Apr  6 03:27:17.806: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Apr  6 03:27:17.896: INFO: Found 1 stateful pods, waiting for 2
Apr  6 03:27:27.916: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:27:27.916: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Apr  6 03:27:28.005: INFO: Deleting all statefulset in ns statefulset-4195
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:27:28.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4195" for this suite.

• [SLOW TEST:20.576 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":71,"skipped":1210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:27:28.067: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1191
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-1191
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1191
Apr  6 03:27:28.263: INFO: Found 0 stateful pods, waiting for 1
Apr  6 03:27:38.278: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr  6 03:27:38.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-1191 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 03:27:38.602: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 03:27:38.602: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 03:27:38.602: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 03:27:38.618: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  6 03:27:48.636: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 03:27:48.636: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 03:27:48.703: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr  6 03:27:48.703: INFO: ss-0  10.241.0.101  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:28 +0000 UTC  }]
Apr  6 03:27:48.703: INFO: 
Apr  6 03:27:48.703: INFO: StatefulSet ss has not reached scale 3, at 1
Apr  6 03:27:49.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986706215s
Apr  6 03:27:50.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971442515s
Apr  6 03:27:51.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953468783s
Apr  6 03:27:52.774: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.936125845s
Apr  6 03:27:53.788: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.915961783s
Apr  6 03:27:54.803: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.902596746s
Apr  6 03:27:55.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.886127395s
Apr  6 03:27:56.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.872015235s
Apr  6 03:27:57.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 854.377043ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1191
Apr  6 03:27:58.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-1191 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 03:27:59.059: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 03:27:59.059: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 03:27:59.059: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 03:27:59.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-1191 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 03:27:59.370: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  6 03:27:59.370: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 03:27:59.370: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 03:27:59.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-1191 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 03:27:59.602: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  6 03:27:59.602: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 03:27:59.602: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 03:27:59.616: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:27:59.616: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:27:59.616: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr  6 03:27:59.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-1191 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 03:27:59.843: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 03:27:59.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 03:27:59.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 03:27:59.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-1191 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 03:28:00.067: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 03:28:00.067: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 03:28:00.067: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 03:28:00.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-1191 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 03:28:00.361: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 03:28:00.361: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 03:28:00.361: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 03:28:00.361: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 03:28:00.379: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr  6 03:28:10.406: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 03:28:10.406: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 03:28:10.406: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 03:28:10.471: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr  6 03:28:10.471: INFO: ss-0  10.241.0.101  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:28 +0000 UTC  }]
Apr  6 03:28:10.471: INFO: ss-1  10.241.0.100  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  }]
Apr  6 03:28:10.471: INFO: ss-2  10.241.0.102  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  }]
Apr  6 03:28:10.471: INFO: 
Apr  6 03:28:10.471: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  6 03:28:11.490: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr  6 03:28:11.490: INFO: ss-0  10.241.0.101  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:28 +0000 UTC  }]
Apr  6 03:28:11.490: INFO: ss-1  10.241.0.100  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  }]
Apr  6 03:28:11.490: INFO: ss-2  10.241.0.102  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  }]
Apr  6 03:28:11.490: INFO: 
Apr  6 03:28:11.490: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  6 03:28:12.504: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr  6 03:28:12.504: INFO: ss-1  10.241.0.100  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  }]
Apr  6 03:28:12.504: INFO: ss-2  10.241.0.102  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:28:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 03:27:48 +0000 UTC  }]
Apr  6 03:28:12.504: INFO: 
Apr  6 03:28:12.504: INFO: StatefulSet ss has not reached scale 0, at 2
Apr  6 03:28:13.517: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.935333133s
Apr  6 03:28:14.530: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.92277856s
Apr  6 03:28:15.546: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.910000676s
Apr  6 03:28:16.561: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.892780905s
Apr  6 03:28:17.577: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.878693567s
Apr  6 03:28:18.589: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.862336678s
Apr  6 03:28:19.602: INFO: Verifying statefulset ss doesn't scale past 0 for another 850.69711ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1191
Apr  6 03:28:20.619: INFO: Scaling statefulset ss to 0
Apr  6 03:28:20.661: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Apr  6 03:28:20.669: INFO: Deleting all statefulset in ns statefulset-1191
Apr  6 03:28:20.682: INFO: Scaling statefulset ss to 0
Apr  6 03:28:20.733: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 03:28:20.745: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:28:20.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1191" for this suite.

• [SLOW TEST:52.769 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":72,"skipped":1252,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:28:20.837: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-96ac3efd-6cbe-4517-8641-d096e10114b4
STEP: Creating a pod to test consume configMaps
Apr  6 03:28:21.124: INFO: Waiting up to 5m0s for pod "pod-configmaps-69d20a78-e998-49ec-96ca-e93c66ecebe2" in namespace "configmap-1319" to be "Succeeded or Failed"
Apr  6 03:28:21.135: INFO: Pod "pod-configmaps-69d20a78-e998-49ec-96ca-e93c66ecebe2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.780893ms
Apr  6 03:28:23.153: INFO: Pod "pod-configmaps-69d20a78-e998-49ec-96ca-e93c66ecebe2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029419895s
STEP: Saw pod success
Apr  6 03:28:23.153: INFO: Pod "pod-configmaps-69d20a78-e998-49ec-96ca-e93c66ecebe2" satisfied condition "Succeeded or Failed"
Apr  6 03:28:23.169: INFO: Trying to get logs from node 10.241.0.101 pod pod-configmaps-69d20a78-e998-49ec-96ca-e93c66ecebe2 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 03:28:23.277: INFO: Waiting for pod pod-configmaps-69d20a78-e998-49ec-96ca-e93c66ecebe2 to disappear
Apr  6 03:28:23.289: INFO: Pod pod-configmaps-69d20a78-e998-49ec-96ca-e93c66ecebe2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:28:23.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1319" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1252,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:28:23.318: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:28:23.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-577677f6-1ce2-4a84-8ae7-09e55dc51fd2" in namespace "projected-7965" to be "Succeeded or Failed"
Apr  6 03:28:23.660: INFO: Pod "downwardapi-volume-577677f6-1ce2-4a84-8ae7-09e55dc51fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.660757ms
Apr  6 03:28:25.674: INFO: Pod "downwardapi-volume-577677f6-1ce2-4a84-8ae7-09e55dc51fd2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028865207s
Apr  6 03:28:27.686: INFO: Pod "downwardapi-volume-577677f6-1ce2-4a84-8ae7-09e55dc51fd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041122018s
STEP: Saw pod success
Apr  6 03:28:27.686: INFO: Pod "downwardapi-volume-577677f6-1ce2-4a84-8ae7-09e55dc51fd2" satisfied condition "Succeeded or Failed"
Apr  6 03:28:27.703: INFO: Trying to get logs from node 10.241.0.101 pod downwardapi-volume-577677f6-1ce2-4a84-8ae7-09e55dc51fd2 container client-container: <nil>
STEP: delete the pod
Apr  6 03:28:27.782: INFO: Waiting for pod downwardapi-volume-577677f6-1ce2-4a84-8ae7-09e55dc51fd2 to disappear
Apr  6 03:28:27.796: INFO: Pod downwardapi-volume-577677f6-1ce2-4a84-8ae7-09e55dc51fd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:28:27.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7965" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1257,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:28:27.828: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:28:32.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4088" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1267,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:28:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:28:34.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6256" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1282,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:28:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:28:51.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9775" for this suite.

• [SLOW TEST:17.402 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":77,"skipped":1284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:28:51.912: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Apr  6 03:28:54.254: INFO: running pods: 0 < 3
Apr  6 03:28:56.275: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:28:58.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6063" for this suite.

• [SLOW TEST:6.403 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":78,"skipped":1316,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:28:58.315: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Apr  6 03:28:58.601: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:29:00.615: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:01.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8090" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":79,"skipped":1332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:01.740: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:29:04.041: INFO: Deleting pod "var-expansion-5ddced48-1eab-46fb-b27b-399ca83fbb75" in namespace "var-expansion-237"
Apr  6 03:29:04.060: INFO: Wait up to 5m0s for pod "var-expansion-5ddced48-1eab-46fb-b27b-399ca83fbb75" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:08.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-237" for this suite.

• [SLOW TEST:6.377 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":80,"skipped":1371,"failed":0}
SSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:08.117: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr  6 03:29:08.386: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr  6 03:29:08.398: INFO: starting watch
STEP: patching
STEP: updating
Apr  6 03:29:08.444: INFO: waiting for watch events with expected annotations
Apr  6 03:29:08.444: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:08.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5985" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":81,"skipped":1376,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:08.660: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:29:08.804: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c85e0669-d23a-479e-b18d-34dcb348f6fe
STEP: Creating the pod
Apr  6 03:29:08.889: INFO: The status of Pod pod-projected-configmaps-634552ff-6dc9-48ce-8c6d-945f96a8ffd5 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:29:10.908: INFO: The status of Pod pod-projected-configmaps-634552ff-6dc9-48ce-8c6d-945f96a8ffd5 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-c85e0669-d23a-479e-b18d-34dcb348f6fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:13.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4131" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":82,"skipped":1392,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:13.113: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:29:13.367: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:29:15.381: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:17.383: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:19.380: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:21.381: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:23.380: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:25.383: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:27.404: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:29.385: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:31.383: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:33.382: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = false)
Apr  6 03:29:35.391: INFO: The status of Pod test-webserver-6cb85859-f5a2-429e-af7d-0629697efbab is Running (Ready = true)
Apr  6 03:29:35.403: INFO: Container started at 2022-04-06 03:29:14 +0000 UTC, pod became ready at 2022-04-06 03:29:33 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:35.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-373" for this suite.

• [SLOW TEST:22.340 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1397,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:35.453: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:29:35.737: INFO: Creating ReplicaSet my-hostname-basic-2e174340-acc7-49a6-8b01-5a45b25f99fb
Apr  6 03:29:35.791: INFO: Pod name my-hostname-basic-2e174340-acc7-49a6-8b01-5a45b25f99fb: Found 0 pods out of 1
Apr  6 03:29:40.818: INFO: Pod name my-hostname-basic-2e174340-acc7-49a6-8b01-5a45b25f99fb: Found 1 pods out of 1
Apr  6 03:29:40.818: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2e174340-acc7-49a6-8b01-5a45b25f99fb" is running
Apr  6 03:29:40.829: INFO: Pod "my-hostname-basic-2e174340-acc7-49a6-8b01-5a45b25f99fb-fl9wd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-06 03:29:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-06 03:29:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-06 03:29:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-06 03:29:35 +0000 UTC Reason: Message:}])
Apr  6 03:29:40.829: INFO: Trying to dial the pod
Apr  6 03:29:45.973: INFO: Controller my-hostname-basic-2e174340-acc7-49a6-8b01-5a45b25f99fb: Got expected result from replica 1 [my-hostname-basic-2e174340-acc7-49a6-8b01-5a45b25f99fb-fl9wd]: "my-hostname-basic-2e174340-acc7-49a6-8b01-5a45b25f99fb-fl9wd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:45.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-426" for this suite.

• [SLOW TEST:10.555 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":84,"skipped":1407,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:46.008: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:46.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6118" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":85,"skipped":1417,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:46.397: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:29:47.191: INFO: Checking APIGroup: apiregistration.k8s.io
Apr  6 03:29:47.195: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr  6 03:29:47.195: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Apr  6 03:29:47.195: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr  6 03:29:47.195: INFO: Checking APIGroup: apps
Apr  6 03:29:47.199: INFO: PreferredVersion.GroupVersion: apps/v1
Apr  6 03:29:47.199: INFO: Versions found [{apps/v1 v1}]
Apr  6 03:29:47.199: INFO: apps/v1 matches apps/v1
Apr  6 03:29:47.199: INFO: Checking APIGroup: events.k8s.io
Apr  6 03:29:47.202: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr  6 03:29:47.202: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Apr  6 03:29:47.202: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr  6 03:29:47.202: INFO: Checking APIGroup: authentication.k8s.io
Apr  6 03:29:47.208: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr  6 03:29:47.208: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Apr  6 03:29:47.208: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr  6 03:29:47.208: INFO: Checking APIGroup: authorization.k8s.io
Apr  6 03:29:47.212: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr  6 03:29:47.212: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Apr  6 03:29:47.212: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr  6 03:29:47.212: INFO: Checking APIGroup: autoscaling
Apr  6 03:29:47.218: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Apr  6 03:29:47.218: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Apr  6 03:29:47.218: INFO: autoscaling/v1 matches autoscaling/v1
Apr  6 03:29:47.218: INFO: Checking APIGroup: batch
Apr  6 03:29:47.222: INFO: PreferredVersion.GroupVersion: batch/v1
Apr  6 03:29:47.222: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Apr  6 03:29:47.222: INFO: batch/v1 matches batch/v1
Apr  6 03:29:47.222: INFO: Checking APIGroup: certificates.k8s.io
Apr  6 03:29:47.226: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr  6 03:29:47.226: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Apr  6 03:29:47.226: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr  6 03:29:47.226: INFO: Checking APIGroup: networking.k8s.io
Apr  6 03:29:47.230: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr  6 03:29:47.230: INFO: Versions found [{networking.k8s.io/v1 v1}]
Apr  6 03:29:47.230: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr  6 03:29:47.230: INFO: Checking APIGroup: policy
Apr  6 03:29:47.234: INFO: PreferredVersion.GroupVersion: policy/v1
Apr  6 03:29:47.234: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Apr  6 03:29:47.234: INFO: policy/v1 matches policy/v1
Apr  6 03:29:47.234: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr  6 03:29:47.239: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr  6 03:29:47.239: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Apr  6 03:29:47.239: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr  6 03:29:47.239: INFO: Checking APIGroup: storage.k8s.io
Apr  6 03:29:47.244: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr  6 03:29:47.244: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr  6 03:29:47.244: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr  6 03:29:47.244: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr  6 03:29:47.250: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr  6 03:29:47.250: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Apr  6 03:29:47.250: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr  6 03:29:47.250: INFO: Checking APIGroup: apiextensions.k8s.io
Apr  6 03:29:47.253: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr  6 03:29:47.253: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Apr  6 03:29:47.253: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr  6 03:29:47.253: INFO: Checking APIGroup: scheduling.k8s.io
Apr  6 03:29:47.257: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr  6 03:29:47.257: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Apr  6 03:29:47.257: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr  6 03:29:47.257: INFO: Checking APIGroup: coordination.k8s.io
Apr  6 03:29:47.262: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr  6 03:29:47.262: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Apr  6 03:29:47.262: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr  6 03:29:47.262: INFO: Checking APIGroup: node.k8s.io
Apr  6 03:29:47.266: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr  6 03:29:47.266: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Apr  6 03:29:47.266: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr  6 03:29:47.266: INFO: Checking APIGroup: discovery.k8s.io
Apr  6 03:29:47.271: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Apr  6 03:29:47.271: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Apr  6 03:29:47.271: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Apr  6 03:29:47.271: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr  6 03:29:47.275: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Apr  6 03:29:47.275: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Apr  6 03:29:47.275: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Apr  6 03:29:47.275: INFO: Checking APIGroup: apps.openshift.io
Apr  6 03:29:47.278: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Apr  6 03:29:47.278: INFO: Versions found [{apps.openshift.io/v1 v1}]
Apr  6 03:29:47.278: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Apr  6 03:29:47.278: INFO: Checking APIGroup: authorization.openshift.io
Apr  6 03:29:47.283: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Apr  6 03:29:47.283: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Apr  6 03:29:47.283: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Apr  6 03:29:47.283: INFO: Checking APIGroup: build.openshift.io
Apr  6 03:29:47.288: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Apr  6 03:29:47.288: INFO: Versions found [{build.openshift.io/v1 v1}]
Apr  6 03:29:47.288: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Apr  6 03:29:47.288: INFO: Checking APIGroup: image.openshift.io
Apr  6 03:29:47.292: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Apr  6 03:29:47.292: INFO: Versions found [{image.openshift.io/v1 v1}]
Apr  6 03:29:47.292: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Apr  6 03:29:47.292: INFO: Checking APIGroup: oauth.openshift.io
Apr  6 03:29:47.296: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Apr  6 03:29:47.296: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Apr  6 03:29:47.296: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Apr  6 03:29:47.296: INFO: Checking APIGroup: project.openshift.io
Apr  6 03:29:47.299: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Apr  6 03:29:47.300: INFO: Versions found [{project.openshift.io/v1 v1}]
Apr  6 03:29:47.300: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Apr  6 03:29:47.300: INFO: Checking APIGroup: quota.openshift.io
Apr  6 03:29:47.303: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Apr  6 03:29:47.303: INFO: Versions found [{quota.openshift.io/v1 v1}]
Apr  6 03:29:47.303: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Apr  6 03:29:47.303: INFO: Checking APIGroup: route.openshift.io
Apr  6 03:29:47.306: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Apr  6 03:29:47.306: INFO: Versions found [{route.openshift.io/v1 v1}]
Apr  6 03:29:47.306: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Apr  6 03:29:47.306: INFO: Checking APIGroup: security.openshift.io
Apr  6 03:29:47.311: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Apr  6 03:29:47.311: INFO: Versions found [{security.openshift.io/v1 v1}]
Apr  6 03:29:47.311: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Apr  6 03:29:47.311: INFO: Checking APIGroup: template.openshift.io
Apr  6 03:29:47.317: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Apr  6 03:29:47.317: INFO: Versions found [{template.openshift.io/v1 v1}]
Apr  6 03:29:47.317: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Apr  6 03:29:47.317: INFO: Checking APIGroup: user.openshift.io
Apr  6 03:29:47.321: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Apr  6 03:29:47.321: INFO: Versions found [{user.openshift.io/v1 v1}]
Apr  6 03:29:47.321: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Apr  6 03:29:47.321: INFO: Checking APIGroup: packages.operators.coreos.com
Apr  6 03:29:47.326: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Apr  6 03:29:47.326: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Apr  6 03:29:47.326: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Apr  6 03:29:47.326: INFO: Checking APIGroup: config.openshift.io
Apr  6 03:29:47.330: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Apr  6 03:29:47.330: INFO: Versions found [{config.openshift.io/v1 v1}]
Apr  6 03:29:47.330: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Apr  6 03:29:47.330: INFO: Checking APIGroup: operator.openshift.io
Apr  6 03:29:47.336: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Apr  6 03:29:47.336: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Apr  6 03:29:47.336: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Apr  6 03:29:47.336: INFO: Checking APIGroup: apiserver.openshift.io
Apr  6 03:29:47.341: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Apr  6 03:29:47.341: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Apr  6 03:29:47.341: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Apr  6 03:29:47.341: INFO: Checking APIGroup: cloudcredential.openshift.io
Apr  6 03:29:47.346: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Apr  6 03:29:47.346: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Apr  6 03:29:47.346: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Apr  6 03:29:47.346: INFO: Checking APIGroup: console.openshift.io
Apr  6 03:29:47.350: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Apr  6 03:29:47.350: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Apr  6 03:29:47.350: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Apr  6 03:29:47.350: INFO: Checking APIGroup: crd.projectcalico.org
Apr  6 03:29:47.355: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Apr  6 03:29:47.355: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Apr  6 03:29:47.355: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Apr  6 03:29:47.355: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Apr  6 03:29:47.360: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Apr  6 03:29:47.360: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Apr  6 03:29:47.360: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Apr  6 03:29:47.360: INFO: Checking APIGroup: ingress.operator.openshift.io
Apr  6 03:29:47.365: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Apr  6 03:29:47.365: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Apr  6 03:29:47.365: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Apr  6 03:29:47.365: INFO: Checking APIGroup: k8s.cni.cncf.io
Apr  6 03:29:47.369: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Apr  6 03:29:47.369: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Apr  6 03:29:47.369: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Apr  6 03:29:47.369: INFO: Checking APIGroup: machineconfiguration.openshift.io
Apr  6 03:29:47.374: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Apr  6 03:29:47.374: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Apr  6 03:29:47.374: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Apr  6 03:29:47.374: INFO: Checking APIGroup: monitoring.coreos.com
Apr  6 03:29:47.378: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Apr  6 03:29:47.378: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Apr  6 03:29:47.378: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Apr  6 03:29:47.378: INFO: Checking APIGroup: network.operator.openshift.io
Apr  6 03:29:47.381: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Apr  6 03:29:47.381: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Apr  6 03:29:47.381: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Apr  6 03:29:47.381: INFO: Checking APIGroup: operator.tigera.io
Apr  6 03:29:47.386: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Apr  6 03:29:47.386: INFO: Versions found [{operator.tigera.io/v1 v1}]
Apr  6 03:29:47.386: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Apr  6 03:29:47.386: INFO: Checking APIGroup: operators.coreos.com
Apr  6 03:29:47.390: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Apr  6 03:29:47.390: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Apr  6 03:29:47.390: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Apr  6 03:29:47.390: INFO: Checking APIGroup: samples.operator.openshift.io
Apr  6 03:29:47.394: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Apr  6 03:29:47.394: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Apr  6 03:29:47.394: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Apr  6 03:29:47.394: INFO: Checking APIGroup: security.internal.openshift.io
Apr  6 03:29:47.398: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Apr  6 03:29:47.398: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Apr  6 03:29:47.398: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Apr  6 03:29:47.398: INFO: Checking APIGroup: snapshot.storage.k8s.io
Apr  6 03:29:47.403: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Apr  6 03:29:47.403: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Apr  6 03:29:47.403: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Apr  6 03:29:47.403: INFO: Checking APIGroup: tuned.openshift.io
Apr  6 03:29:47.406: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Apr  6 03:29:47.406: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Apr  6 03:29:47.406: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Apr  6 03:29:47.406: INFO: Checking APIGroup: controlplane.operator.openshift.io
Apr  6 03:29:47.412: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Apr  6 03:29:47.412: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Apr  6 03:29:47.412: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Apr  6 03:29:47.412: INFO: Checking APIGroup: ibm.com
Apr  6 03:29:47.423: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Apr  6 03:29:47.423: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Apr  6 03:29:47.423: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Apr  6 03:29:47.423: INFO: Checking APIGroup: migration.k8s.io
Apr  6 03:29:47.428: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Apr  6 03:29:47.428: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Apr  6 03:29:47.428: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Apr  6 03:29:47.428: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Apr  6 03:29:47.433: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Apr  6 03:29:47.433: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Apr  6 03:29:47.433: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Apr  6 03:29:47.433: INFO: Checking APIGroup: helm.openshift.io
Apr  6 03:29:47.442: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Apr  6 03:29:47.442: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Apr  6 03:29:47.442: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Apr  6 03:29:47.442: INFO: Checking APIGroup: metrics.k8s.io
Apr  6 03:29:47.493: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Apr  6 03:29:47.493: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Apr  6 03:29:47.493: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:47.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3137" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":86,"skipped":1425,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:47.613: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9029.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9029.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9029.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9029.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9029.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9029.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 03:29:52.126: INFO: DNS probes using dns-9029/dns-test-ba3b39ba-98ce-4790-8728-6f5b1424d29e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:52.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9029" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":87,"skipped":1426,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:52.208: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:29:52.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-223d5060-f01a-41fd-9981-11447d2bacc0" in namespace "downward-api-347" to be "Succeeded or Failed"
Apr  6 03:29:52.480: INFO: Pod "downwardapi-volume-223d5060-f01a-41fd-9981-11447d2bacc0": Phase="Pending", Reason="", readiness=false. Elapsed: 31.874128ms
Apr  6 03:29:54.533: INFO: Pod "downwardapi-volume-223d5060-f01a-41fd-9981-11447d2bacc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.08573198s
STEP: Saw pod success
Apr  6 03:29:54.533: INFO: Pod "downwardapi-volume-223d5060-f01a-41fd-9981-11447d2bacc0" satisfied condition "Succeeded or Failed"
Apr  6 03:29:54.545: INFO: Trying to get logs from node 10.241.0.101 pod downwardapi-volume-223d5060-f01a-41fd-9981-11447d2bacc0 container client-container: <nil>
STEP: delete the pod
Apr  6 03:29:54.659: INFO: Waiting for pod downwardapi-volume-223d5060-f01a-41fd-9981-11447d2bacc0 to disappear
Apr  6 03:29:54.670: INFO: Pod downwardapi-volume-223d5060-f01a-41fd-9981-11447d2bacc0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:54.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-347" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1427,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:54.700: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-e43fa2f5-e468-405e-aa06-ad2e0e4581a9
STEP: Creating a pod to test consume secrets
Apr  6 03:29:54.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e649039e-33ee-4dd6-a335-365d4ed992ff" in namespace "projected-4830" to be "Succeeded or Failed"
Apr  6 03:29:54.998: INFO: Pod "pod-projected-secrets-e649039e-33ee-4dd6-a335-365d4ed992ff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.465675ms
Apr  6 03:29:57.012: INFO: Pod "pod-projected-secrets-e649039e-33ee-4dd6-a335-365d4ed992ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026334005s
STEP: Saw pod success
Apr  6 03:29:57.012: INFO: Pod "pod-projected-secrets-e649039e-33ee-4dd6-a335-365d4ed992ff" satisfied condition "Succeeded or Failed"
Apr  6 03:29:57.024: INFO: Trying to get logs from node 10.241.0.100 pod pod-projected-secrets-e649039e-33ee-4dd6-a335-365d4ed992ff container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 03:29:57.213: INFO: Waiting for pod pod-projected-secrets-e649039e-33ee-4dd6-a335-365d4ed992ff to disappear
Apr  6 03:29:57.228: INFO: Pod pod-projected-secrets-e649039e-33ee-4dd6-a335-365d4ed992ff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:57.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4830" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:57.318: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr  6 03:29:57.693: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr  6 03:29:57.739: INFO: starting watch
STEP: patching
STEP: updating
Apr  6 03:29:57.825: INFO: waiting for watch events with expected annotations
Apr  6 03:29:57.825: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:29:58.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1180" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":90,"skipped":1476,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:29:58.045: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-760675c2-75c2-4dad-8f38-34b919911167
STEP: Creating a pod to test consume configMaps
Apr  6 03:29:58.373: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4648897-e7ee-42fd-b327-ba118ffcfa77" in namespace "configmap-6544" to be "Succeeded or Failed"
Apr  6 03:29:58.408: INFO: Pod "pod-configmaps-d4648897-e7ee-42fd-b327-ba118ffcfa77": Phase="Pending", Reason="", readiness=false. Elapsed: 34.842206ms
Apr  6 03:30:00.423: INFO: Pod "pod-configmaps-d4648897-e7ee-42fd-b327-ba118ffcfa77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050027092s
Apr  6 03:30:02.445: INFO: Pod "pod-configmaps-d4648897-e7ee-42fd-b327-ba118ffcfa77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072171778s
STEP: Saw pod success
Apr  6 03:30:02.445: INFO: Pod "pod-configmaps-d4648897-e7ee-42fd-b327-ba118ffcfa77" satisfied condition "Succeeded or Failed"
Apr  6 03:30:02.465: INFO: Trying to get logs from node 10.241.0.100 pod pod-configmaps-d4648897-e7ee-42fd-b327-ba118ffcfa77 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 03:30:02.563: INFO: Waiting for pod pod-configmaps-d4648897-e7ee-42fd-b327-ba118ffcfa77 to disappear
Apr  6 03:30:02.583: INFO: Pod pod-configmaps-d4648897-e7ee-42fd-b327-ba118ffcfa77 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:02.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6544" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:02.675: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:30:03.175: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d0d110a6-3b34-4cab-9fcc-e78d258e0aaa", Controller:(*bool)(0xc005b29e62), BlockOwnerDeletion:(*bool)(0xc005b29e63)}}
Apr  6 03:30:03.193: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b36be4d1-a91e-4e19-a8c5-601feb5f3a38", Controller:(*bool)(0xc000911126), BlockOwnerDeletion:(*bool)(0xc000911127)}}
Apr  6 03:30:03.231: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"aa96a1f0-58b5-4318-815f-2cca1607b374", Controller:(*bool)(0xc000911456), BlockOwnerDeletion:(*bool)(0xc000911457)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:08.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5112" for this suite.

• [SLOW TEST:5.655 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":92,"skipped":1547,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:08.330: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:09.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2766" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":93,"skipped":1558,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:09.959: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:30:10.738: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 03:30:12.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812610, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812610, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812610, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812610, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:30:15.845: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:15.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8477" for this suite.
STEP: Destroying namespace "webhook-8477-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.181 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":94,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:16.142: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:30:16.564: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr  6 03:30:18.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812616, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812616, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812616, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812616, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:30:21.650: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:30:21.660: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:25.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4879" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.176 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":95,"skipped":1599,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:25.318: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:30:25.562: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3293
I0406 03:30:25.590610      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3293, replica count: 1
I0406 03:30:26.641569      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 03:30:27.641958      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 03:30:28.642373      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 03:30:28.772: INFO: Created: latency-svc-n5sdt
Apr  6 03:30:28.795: INFO: Got endpoints: latency-svc-n5sdt [52.493207ms]
Apr  6 03:30:28.825: INFO: Created: latency-svc-f9qfc
Apr  6 03:30:28.861: INFO: Got endpoints: latency-svc-f9qfc [65.965913ms]
Apr  6 03:30:28.871: INFO: Created: latency-svc-kqbpv
Apr  6 03:30:28.884: INFO: Created: latency-svc-q47p8
Apr  6 03:30:28.890: INFO: Got endpoints: latency-svc-kqbpv [95.259941ms]
Apr  6 03:30:28.896: INFO: Created: latency-svc-plcfm
Apr  6 03:30:28.902: INFO: Got endpoints: latency-svc-q47p8 [106.64028ms]
Apr  6 03:30:28.914: INFO: Created: latency-svc-9bf7b
Apr  6 03:30:28.917: INFO: Got endpoints: latency-svc-plcfm [121.91242ms]
Apr  6 03:30:28.927: INFO: Created: latency-svc-l54d2
Apr  6 03:30:28.931: INFO: Got endpoints: latency-svc-9bf7b [136.191154ms]
Apr  6 03:30:28.937: INFO: Created: latency-svc-szdmk
Apr  6 03:30:29.005: INFO: Got endpoints: latency-svc-l54d2 [210.32483ms]
Apr  6 03:30:29.005: INFO: Got endpoints: latency-svc-szdmk [210.62755ms]
Apr  6 03:30:29.009: INFO: Created: latency-svc-4p8vz
Apr  6 03:30:29.023: INFO: Got endpoints: latency-svc-4p8vz [228.120101ms]
Apr  6 03:30:29.032: INFO: Created: latency-svc-5wg5s
Apr  6 03:30:29.044: INFO: Created: latency-svc-ltv4l
Apr  6 03:30:29.047: INFO: Got endpoints: latency-svc-5wg5s [252.018214ms]
Apr  6 03:30:29.057: INFO: Created: latency-svc-wp7n7
Apr  6 03:30:29.061: INFO: Got endpoints: latency-svc-ltv4l [265.617904ms]
Apr  6 03:30:29.096: INFO: Created: latency-svc-ptdzj
Apr  6 03:30:29.097: INFO: Got endpoints: latency-svc-wp7n7 [301.67992ms]
Apr  6 03:30:29.103: INFO: Created: latency-svc-dh6js
Apr  6 03:30:29.114: INFO: Created: latency-svc-nt4mk
Apr  6 03:30:29.121: INFO: Got endpoints: latency-svc-ptdzj [325.212743ms]
Apr  6 03:30:29.123: INFO: Got endpoints: latency-svc-dh6js [327.141766ms]
Apr  6 03:30:29.128: INFO: Got endpoints: latency-svc-nt4mk [332.932184ms]
Apr  6 03:30:29.156: INFO: Created: latency-svc-sgggj
Apr  6 03:30:29.169: INFO: Created: latency-svc-54kts
Apr  6 03:30:29.173: INFO: Got endpoints: latency-svc-sgggj [377.346236ms]
Apr  6 03:30:29.189: INFO: Got endpoints: latency-svc-54kts [328.521554ms]
Apr  6 03:30:29.202: INFO: Created: latency-svc-6t4rv
Apr  6 03:30:29.249: INFO: Got endpoints: latency-svc-6t4rv [358.888818ms]
Apr  6 03:30:29.251: INFO: Created: latency-svc-5xt9k
Apr  6 03:30:29.292: INFO: Created: latency-svc-786cs
Apr  6 03:30:29.297: INFO: Got endpoints: latency-svc-5xt9k [394.966188ms]
Apr  6 03:30:29.327: INFO: Created: latency-svc-b82n9
Apr  6 03:30:29.332: INFO: Got endpoints: latency-svc-786cs [414.488614ms]
Apr  6 03:30:29.338: INFO: Created: latency-svc-vmhx4
Apr  6 03:30:29.341: INFO: Got endpoints: latency-svc-b82n9 [410.198268ms]
Apr  6 03:30:29.350: INFO: Created: latency-svc-rgnw7
Apr  6 03:30:29.353: INFO: Got endpoints: latency-svc-vmhx4 [347.18305ms]
Apr  6 03:30:29.360: INFO: Created: latency-svc-rtdtx
Apr  6 03:30:29.366: INFO: Got endpoints: latency-svc-rgnw7 [360.511669ms]
Apr  6 03:30:29.375: INFO: Created: latency-svc-rdq2j
Apr  6 03:30:29.376: INFO: Got endpoints: latency-svc-rtdtx [352.701837ms]
Apr  6 03:30:29.384: INFO: Created: latency-svc-k2xn9
Apr  6 03:30:29.395: INFO: Got endpoints: latency-svc-rdq2j [348.008534ms]
Apr  6 03:30:29.404: INFO: Created: latency-svc-98qhc
Apr  6 03:30:29.424: INFO: Got endpoints: latency-svc-k2xn9 [362.444168ms]
Apr  6 03:30:29.424: INFO: Got endpoints: latency-svc-98qhc [327.223341ms]
Apr  6 03:30:29.429: INFO: Created: latency-svc-z6k8w
Apr  6 03:30:29.438: INFO: Created: latency-svc-r4xv2
Apr  6 03:30:29.449: INFO: Got endpoints: latency-svc-z6k8w [328.672378ms]
Apr  6 03:30:29.459: INFO: Created: latency-svc-jzgss
Apr  6 03:30:29.459: INFO: Got endpoints: latency-svc-r4xv2 [336.46927ms]
Apr  6 03:30:29.475: INFO: Created: latency-svc-29cqp
Apr  6 03:30:29.477: INFO: Got endpoints: latency-svc-jzgss [348.582107ms]
Apr  6 03:30:29.496: INFO: Created: latency-svc-t6nmg
Apr  6 03:30:29.498: INFO: Got endpoints: latency-svc-29cqp [324.768997ms]
Apr  6 03:30:29.504: INFO: Created: latency-svc-cl4gg
Apr  6 03:30:29.516: INFO: Got endpoints: latency-svc-t6nmg [326.904051ms]
Apr  6 03:30:29.517: INFO: Created: latency-svc-9qzht
Apr  6 03:30:29.518: INFO: Got endpoints: latency-svc-cl4gg [268.984803ms]
Apr  6 03:30:29.540: INFO: Created: latency-svc-qqqfx
Apr  6 03:30:29.545: INFO: Got endpoints: latency-svc-9qzht [247.642286ms]
Apr  6 03:30:29.550: INFO: Created: latency-svc-jd4mq
Apr  6 03:30:29.561: INFO: Got endpoints: latency-svc-qqqfx [228.940985ms]
Apr  6 03:30:29.575: INFO: Got endpoints: latency-svc-jd4mq [233.125916ms]
Apr  6 03:30:29.575: INFO: Created: latency-svc-wllcj
Apr  6 03:30:29.581: INFO: Created: latency-svc-tkzl8
Apr  6 03:30:29.588: INFO: Got endpoints: latency-svc-wllcj [235.573925ms]
Apr  6 03:30:29.594: INFO: Created: latency-svc-6gq98
Apr  6 03:30:29.597: INFO: Got endpoints: latency-svc-tkzl8 [230.186128ms]
Apr  6 03:30:29.607: INFO: Created: latency-svc-rvt6k
Apr  6 03:30:29.614: INFO: Got endpoints: latency-svc-6gq98 [238.018309ms]
Apr  6 03:30:29.623: INFO: Got endpoints: latency-svc-rvt6k [228.096531ms]
Apr  6 03:30:29.627: INFO: Created: latency-svc-kq7gc
Apr  6 03:30:29.630: INFO: Created: latency-svc-cdmmm
Apr  6 03:30:29.640: INFO: Got endpoints: latency-svc-kq7gc [216.732392ms]
Apr  6 03:30:29.646: INFO: Created: latency-svc-2r46w
Apr  6 03:30:29.653: INFO: Got endpoints: latency-svc-cdmmm [228.190611ms]
Apr  6 03:30:29.660: INFO: Created: latency-svc-j4xwk
Apr  6 03:30:29.662: INFO: Got endpoints: latency-svc-2r46w [212.087569ms]
Apr  6 03:30:29.671: INFO: Created: latency-svc-4m5pm
Apr  6 03:30:29.675: INFO: Got endpoints: latency-svc-j4xwk [215.930879ms]
Apr  6 03:30:29.681: INFO: Created: latency-svc-b9j86
Apr  6 03:30:29.690: INFO: Got endpoints: latency-svc-4m5pm [212.552634ms]
Apr  6 03:30:29.691: INFO: Created: latency-svc-k42s6
Apr  6 03:30:29.695: INFO: Got endpoints: latency-svc-b9j86 [197.496244ms]
Apr  6 03:30:29.702: INFO: Created: latency-svc-xj5rv
Apr  6 03:30:29.709: INFO: Got endpoints: latency-svc-k42s6 [192.266508ms]
Apr  6 03:30:29.718: INFO: Got endpoints: latency-svc-xj5rv [200.214156ms]
Apr  6 03:30:29.721: INFO: Created: latency-svc-m5spf
Apr  6 03:30:29.734: INFO: Created: latency-svc-lw2gn
Apr  6 03:30:29.739: INFO: Got endpoints: latency-svc-m5spf [194.479697ms]
Apr  6 03:30:29.745: INFO: Created: latency-svc-vb7xx
Apr  6 03:30:29.755: INFO: Got endpoints: latency-svc-lw2gn [194.175326ms]
Apr  6 03:30:29.759: INFO: Got endpoints: latency-svc-vb7xx [183.973975ms]
Apr  6 03:30:29.760: INFO: Created: latency-svc-6jmjr
Apr  6 03:30:29.769: INFO: Created: latency-svc-wlsph
Apr  6 03:30:29.776: INFO: Got endpoints: latency-svc-6jmjr [187.884312ms]
Apr  6 03:30:29.779: INFO: Created: latency-svc-pk87c
Apr  6 03:30:29.784: INFO: Got endpoints: latency-svc-wlsph [187.225417ms]
Apr  6 03:30:29.786: INFO: Created: latency-svc-pc8sj
Apr  6 03:30:29.792: INFO: Got endpoints: latency-svc-pk87c [178.102224ms]
Apr  6 03:30:29.798: INFO: Created: latency-svc-g5gsf
Apr  6 03:30:29.801: INFO: Got endpoints: latency-svc-pc8sj [177.94098ms]
Apr  6 03:30:29.810: INFO: Created: latency-svc-xfhgf
Apr  6 03:30:29.814: INFO: Got endpoints: latency-svc-g5gsf [173.83112ms]
Apr  6 03:30:29.821: INFO: Created: latency-svc-kftnb
Apr  6 03:30:29.826: INFO: Got endpoints: latency-svc-xfhgf [173.02433ms]
Apr  6 03:30:29.829: INFO: Created: latency-svc-fdklh
Apr  6 03:30:29.838: INFO: Created: latency-svc-kcjfr
Apr  6 03:30:29.839: INFO: Got endpoints: latency-svc-kftnb [177.004269ms]
Apr  6 03:30:29.844: INFO: Got endpoints: latency-svc-fdklh [169.035875ms]
Apr  6 03:30:29.852: INFO: Created: latency-svc-9zc6t
Apr  6 03:30:29.888: INFO: Got endpoints: latency-svc-kcjfr [198.462391ms]
Apr  6 03:30:29.890: INFO: Created: latency-svc-7jz87
Apr  6 03:30:29.891: INFO: Got endpoints: latency-svc-9zc6t [196.015324ms]
Apr  6 03:30:29.903: INFO: Created: latency-svc-llh6t
Apr  6 03:30:29.908: INFO: Got endpoints: latency-svc-7jz87 [199.610362ms]
Apr  6 03:30:29.914: INFO: Created: latency-svc-dp7sc
Apr  6 03:30:29.918: INFO: Got endpoints: latency-svc-llh6t [199.691072ms]
Apr  6 03:30:29.923: INFO: Created: latency-svc-hnjlp
Apr  6 03:30:29.931: INFO: Created: latency-svc-c7qmm
Apr  6 03:30:29.931: INFO: Got endpoints: latency-svc-dp7sc [192.401028ms]
Apr  6 03:30:29.938: INFO: Got endpoints: latency-svc-hnjlp [183.521026ms]
Apr  6 03:30:29.942: INFO: Created: latency-svc-c8q4k
Apr  6 03:30:29.945: INFO: Got endpoints: latency-svc-c7qmm [186.594394ms]
Apr  6 03:30:29.954: INFO: Created: latency-svc-qftff
Apr  6 03:30:29.957: INFO: Got endpoints: latency-svc-c8q4k [180.976466ms]
Apr  6 03:30:29.963: INFO: Created: latency-svc-ph79l
Apr  6 03:30:29.971: INFO: Got endpoints: latency-svc-qftff [187.091655ms]
Apr  6 03:30:29.974: INFO: Created: latency-svc-qq9wf
Apr  6 03:30:29.982: INFO: Got endpoints: latency-svc-ph79l [189.656581ms]
Apr  6 03:30:29.991: INFO: Got endpoints: latency-svc-qq9wf [189.10499ms]
Apr  6 03:30:29.991: INFO: Created: latency-svc-m9dpq
Apr  6 03:30:30.005: INFO: Created: latency-svc-vvkbl
Apr  6 03:30:30.014: INFO: Got endpoints: latency-svc-m9dpq [199.841064ms]
Apr  6 03:30:30.016: INFO: Created: latency-svc-k4ljp
Apr  6 03:30:30.026: INFO: Got endpoints: latency-svc-vvkbl [200.33635ms]
Apr  6 03:30:30.027: INFO: Created: latency-svc-7hw85
Apr  6 03:30:30.033: INFO: Got endpoints: latency-svc-k4ljp [194.544037ms]
Apr  6 03:30:30.041: INFO: Created: latency-svc-xwmcf
Apr  6 03:30:30.075: INFO: Got endpoints: latency-svc-xwmcf [186.81586ms]
Apr  6 03:30:30.075: INFO: Got endpoints: latency-svc-7hw85 [230.648804ms]
Apr  6 03:30:30.078: INFO: Created: latency-svc-2p24f
Apr  6 03:30:30.091: INFO: Created: latency-svc-xx9s7
Apr  6 03:30:30.094: INFO: Got endpoints: latency-svc-2p24f [202.941724ms]
Apr  6 03:30:30.104: INFO: Created: latency-svc-75h5j
Apr  6 03:30:30.107: INFO: Got endpoints: latency-svc-xx9s7 [198.294297ms]
Apr  6 03:30:30.117: INFO: Created: latency-svc-8l68s
Apr  6 03:30:30.125: INFO: Got endpoints: latency-svc-75h5j [207.119124ms]
Apr  6 03:30:30.125: INFO: Created: latency-svc-k4z75
Apr  6 03:30:30.127: INFO: Got endpoints: latency-svc-8l68s [195.370526ms]
Apr  6 03:30:30.162: INFO: Created: latency-svc-dhw87
Apr  6 03:30:30.165: INFO: Got endpoints: latency-svc-k4z75 [226.872178ms]
Apr  6 03:30:30.172: INFO: Created: latency-svc-6t6qr
Apr  6 03:30:30.183: INFO: Created: latency-svc-7p8kt
Apr  6 03:30:30.190: INFO: Got endpoints: latency-svc-dhw87 [245.00793ms]
Apr  6 03:30:30.197: INFO: Got endpoints: latency-svc-6t6qr [240.080448ms]
Apr  6 03:30:30.198: INFO: Created: latency-svc-tbsg4
Apr  6 03:30:30.206: INFO: Created: latency-svc-qxhx6
Apr  6 03:30:30.206: INFO: Got endpoints: latency-svc-7p8kt [235.108709ms]
Apr  6 03:30:30.218: INFO: Created: latency-svc-pv84k
Apr  6 03:30:30.220: INFO: Got endpoints: latency-svc-tbsg4 [238.270194ms]
Apr  6 03:30:30.231: INFO: Got endpoints: latency-svc-qxhx6 [240.11487ms]
Apr  6 03:30:30.241: INFO: Created: latency-svc-clgt9
Apr  6 03:30:30.247: INFO: Created: latency-svc-w2nhs
Apr  6 03:30:30.247: INFO: Got endpoints: latency-svc-pv84k [232.853391ms]
Apr  6 03:30:30.264: INFO: Created: latency-svc-qdwcf
Apr  6 03:30:30.269: INFO: Got endpoints: latency-svc-clgt9 [242.498082ms]
Apr  6 03:30:30.270: INFO: Created: latency-svc-brntk
Apr  6 03:30:30.275: INFO: Got endpoints: latency-svc-w2nhs [242.006178ms]
Apr  6 03:30:30.276: INFO: Got endpoints: latency-svc-qdwcf [200.723152ms]
Apr  6 03:30:30.288: INFO: Got endpoints: latency-svc-brntk [40.760803ms]
Apr  6 03:30:30.288: INFO: Created: latency-svc-cz5kv
Apr  6 03:30:30.292: INFO: Created: latency-svc-4kqlm
Apr  6 03:30:30.299: INFO: Got endpoints: latency-svc-cz5kv [223.747654ms]
Apr  6 03:30:30.304: INFO: Created: latency-svc-xpmpn
Apr  6 03:30:30.307: INFO: Got endpoints: latency-svc-4kqlm [212.522195ms]
Apr  6 03:30:30.317: INFO: Created: latency-svc-wkxfg
Apr  6 03:30:30.328: INFO: Got endpoints: latency-svc-xpmpn [221.928492ms]
Apr  6 03:30:30.337: INFO: Got endpoints: latency-svc-wkxfg [212.024271ms]
Apr  6 03:30:30.369: INFO: Created: latency-svc-jt5js
Apr  6 03:30:30.378: INFO: Created: latency-svc-wlw56
Apr  6 03:30:30.387: INFO: Got endpoints: latency-svc-jt5js [259.735085ms]
Apr  6 03:30:30.387: INFO: Created: latency-svc-pf7rz
Apr  6 03:30:30.396: INFO: Got endpoints: latency-svc-wlw56 [231.014725ms]
Apr  6 03:30:30.399: INFO: Created: latency-svc-wllxd
Apr  6 03:30:30.399: INFO: Got endpoints: latency-svc-pf7rz [209.09818ms]
Apr  6 03:30:30.408: INFO: Created: latency-svc-wj5j7
Apr  6 03:30:30.416: INFO: Got endpoints: latency-svc-wllxd [218.150326ms]
Apr  6 03:30:30.417: INFO: Created: latency-svc-qfcr4
Apr  6 03:30:30.426: INFO: Got endpoints: latency-svc-wj5j7 [220.235293ms]
Apr  6 03:30:30.426: INFO: Created: latency-svc-sjbj4
Apr  6 03:30:30.431: INFO: Got endpoints: latency-svc-qfcr4 [211.099953ms]
Apr  6 03:30:30.437: INFO: Got endpoints: latency-svc-sjbj4 [206.349688ms]
Apr  6 03:30:30.438: INFO: Created: latency-svc-nsjpz
Apr  6 03:30:30.450: INFO: Got endpoints: latency-svc-nsjpz [181.113604ms]
Apr  6 03:30:30.452: INFO: Created: latency-svc-tm6wj
Apr  6 03:30:30.461: INFO: Created: latency-svc-7h2lr
Apr  6 03:30:30.470: INFO: Created: latency-svc-s29bj
Apr  6 03:30:30.472: INFO: Got endpoints: latency-svc-tm6wj [196.256971ms]
Apr  6 03:30:30.476: INFO: Got endpoints: latency-svc-7h2lr [200.224427ms]
Apr  6 03:30:30.479: INFO: Created: latency-svc-fsgxx
Apr  6 03:30:30.487: INFO: Got endpoints: latency-svc-s29bj [198.880049ms]
Apr  6 03:30:30.490: INFO: Created: latency-svc-bsb67
Apr  6 03:30:30.494: INFO: Got endpoints: latency-svc-fsgxx [194.805894ms]
Apr  6 03:30:30.500: INFO: Created: latency-svc-nrm6j
Apr  6 03:30:30.507: INFO: Got endpoints: latency-svc-bsb67 [200.446888ms]
Apr  6 03:30:30.509: INFO: Created: latency-svc-f9r9w
Apr  6 03:30:30.512: INFO: Got endpoints: latency-svc-nrm6j [183.566606ms]
Apr  6 03:30:30.523: INFO: Created: latency-svc-shngj
Apr  6 03:30:30.525: INFO: Got endpoints: latency-svc-f9r9w [187.370942ms]
Apr  6 03:30:30.531: INFO: Created: latency-svc-jsjzb
Apr  6 03:30:30.541: INFO: Created: latency-svc-9qbfh
Apr  6 03:30:30.542: INFO: Got endpoints: latency-svc-shngj [155.536183ms]
Apr  6 03:30:30.546: INFO: Got endpoints: latency-svc-jsjzb [149.95191ms]
Apr  6 03:30:30.550: INFO: Created: latency-svc-pqlz4
Apr  6 03:30:30.557: INFO: Got endpoints: latency-svc-9qbfh [157.263361ms]
Apr  6 03:30:30.560: INFO: Got endpoints: latency-svc-pqlz4 [144.776536ms]
Apr  6 03:30:30.562: INFO: Created: latency-svc-95hdb
Apr  6 03:30:30.572: INFO: Created: latency-svc-gkkwg
Apr  6 03:30:30.584: INFO: Created: latency-svc-55x2t
Apr  6 03:30:30.588: INFO: Got endpoints: latency-svc-95hdb [161.217157ms]
Apr  6 03:30:30.595: INFO: Got endpoints: latency-svc-gkkwg [163.485955ms]
Apr  6 03:30:30.598: INFO: Created: latency-svc-p9wzq
Apr  6 03:30:30.604: INFO: Got endpoints: latency-svc-55x2t [166.535058ms]
Apr  6 03:30:30.610: INFO: Created: latency-svc-h92m8
Apr  6 03:30:30.611: INFO: Got endpoints: latency-svc-p9wzq [161.139945ms]
Apr  6 03:30:30.621: INFO: Created: latency-svc-4d7zz
Apr  6 03:30:30.640: INFO: Got endpoints: latency-svc-h92m8 [168.868198ms]
Apr  6 03:30:30.643: INFO: Created: latency-svc-rsr9h
Apr  6 03:30:30.656: INFO: Got endpoints: latency-svc-4d7zz [180.392919ms]
Apr  6 03:30:30.658: INFO: Created: latency-svc-rqwnj
Apr  6 03:30:30.661: INFO: Got endpoints: latency-svc-rsr9h [174.214683ms]
Apr  6 03:30:30.671: INFO: Got endpoints: latency-svc-rqwnj [177.483252ms]
Apr  6 03:30:30.674: INFO: Created: latency-svc-dkf96
Apr  6 03:30:30.683: INFO: Created: latency-svc-psmkt
Apr  6 03:30:30.698: INFO: Created: latency-svc-gtp6h
Apr  6 03:30:30.698: INFO: Got endpoints: latency-svc-dkf96 [190.433463ms]
Apr  6 03:30:30.699: INFO: Got endpoints: latency-svc-psmkt [186.527897ms]
Apr  6 03:30:30.702: INFO: Created: latency-svc-vgqj4
Apr  6 03:30:30.712: INFO: Got endpoints: latency-svc-gtp6h [187.606989ms]
Apr  6 03:30:30.721: INFO: Created: latency-svc-jhdxc
Apr  6 03:30:30.724: INFO: Got endpoints: latency-svc-vgqj4 [181.635536ms]
Apr  6 03:30:30.734: INFO: Got endpoints: latency-svc-jhdxc [187.544495ms]
Apr  6 03:30:30.734: INFO: Created: latency-svc-w74g7
Apr  6 03:30:30.746: INFO: Created: latency-svc-g7sbg
Apr  6 03:30:30.754: INFO: Got endpoints: latency-svc-w74g7 [197.129693ms]
Apr  6 03:30:30.757: INFO: Created: latency-svc-v99hk
Apr  6 03:30:30.765: INFO: Got endpoints: latency-svc-g7sbg [205.099501ms]
Apr  6 03:30:30.770: INFO: Got endpoints: latency-svc-v99hk [182.727443ms]
Apr  6 03:30:30.772: INFO: Created: latency-svc-qd4vq
Apr  6 03:30:30.784: INFO: Created: latency-svc-9qrqp
Apr  6 03:30:30.785: INFO: Got endpoints: latency-svc-qd4vq [189.94271ms]
Apr  6 03:30:30.821: INFO: Created: latency-svc-cvgfj
Apr  6 03:30:30.824: INFO: Got endpoints: latency-svc-9qrqp [219.856259ms]
Apr  6 03:30:30.828: INFO: Created: latency-svc-9x9mq
Apr  6 03:30:30.841: INFO: Got endpoints: latency-svc-cvgfj [230.300167ms]
Apr  6 03:30:30.843: INFO: Created: latency-svc-pzlp4
Apr  6 03:30:30.852: INFO: Got endpoints: latency-svc-9x9mq [211.38169ms]
Apr  6 03:30:30.855: INFO: Created: latency-svc-7fvnb
Apr  6 03:30:30.867: INFO: Created: latency-svc-64d5b
Apr  6 03:30:30.875: INFO: Created: latency-svc-h2qld
Apr  6 03:30:30.876: INFO: Got endpoints: latency-svc-pzlp4 [220.083643ms]
Apr  6 03:30:30.889: INFO: Got endpoints: latency-svc-7fvnb [228.461644ms]
Apr  6 03:30:30.894: INFO: Got endpoints: latency-svc-64d5b [222.477504ms]
Apr  6 03:30:30.900: INFO: Got endpoints: latency-svc-h2qld [202.003884ms]
Apr  6 03:30:30.900: INFO: Created: latency-svc-kqsqj
Apr  6 03:30:30.933: INFO: Got endpoints: latency-svc-kqsqj [234.211797ms]
Apr  6 03:30:30.938: INFO: Created: latency-svc-bp76j
Apr  6 03:30:30.949: INFO: Created: latency-svc-6488g
Apr  6 03:30:30.956: INFO: Got endpoints: latency-svc-bp76j [243.654656ms]
Apr  6 03:30:30.958: INFO: Created: latency-svc-4m96m
Apr  6 03:30:30.968: INFO: Got endpoints: latency-svc-6488g [244.282658ms]
Apr  6 03:30:30.969: INFO: Created: latency-svc-rsjml
Apr  6 03:30:30.974: INFO: Got endpoints: latency-svc-4m96m [239.672278ms]
Apr  6 03:30:30.982: INFO: Created: latency-svc-4xcg2
Apr  6 03:30:30.984: INFO: Got endpoints: latency-svc-rsjml [229.888899ms]
Apr  6 03:30:30.991: INFO: Created: latency-svc-kpjpr
Apr  6 03:30:30.998: INFO: Got endpoints: latency-svc-4xcg2 [232.382633ms]
Apr  6 03:30:31.000: INFO: Created: latency-svc-gjmcn
Apr  6 03:30:31.007: INFO: Got endpoints: latency-svc-kpjpr [236.967585ms]
Apr  6 03:30:31.010: INFO: Created: latency-svc-d6vjq
Apr  6 03:30:31.015: INFO: Got endpoints: latency-svc-gjmcn [230.204969ms]
Apr  6 03:30:31.021: INFO: Created: latency-svc-4n57k
Apr  6 03:30:31.025: INFO: Got endpoints: latency-svc-d6vjq [201.704051ms]
Apr  6 03:30:31.030: INFO: Created: latency-svc-p9mxf
Apr  6 03:30:31.036: INFO: Got endpoints: latency-svc-4n57k [195.163672ms]
Apr  6 03:30:31.037: INFO: Created: latency-svc-m2cl5
Apr  6 03:30:31.052: INFO: Created: latency-svc-bwpc5
Apr  6 03:30:31.058: INFO: Got endpoints: latency-svc-p9mxf [206.078966ms]
Apr  6 03:30:31.060: INFO: Created: latency-svc-2bq4m
Apr  6 03:30:31.064: INFO: Got endpoints: latency-svc-m2cl5 [187.463459ms]
Apr  6 03:30:31.066: INFO: Got endpoints: latency-svc-bwpc5 [176.282535ms]
Apr  6 03:30:31.070: INFO: Created: latency-svc-pfhvg
Apr  6 03:30:31.081: INFO: Got endpoints: latency-svc-2bq4m [187.332557ms]
Apr  6 03:30:31.083: INFO: Got endpoints: latency-svc-pfhvg [183.333109ms]
Apr  6 03:30:31.085: INFO: Created: latency-svc-f5ckg
Apr  6 03:30:31.097: INFO: Created: latency-svc-88pff
Apr  6 03:30:31.097: INFO: Got endpoints: latency-svc-f5ckg [163.914893ms]
Apr  6 03:30:31.106: INFO: Created: latency-svc-vphtw
Apr  6 03:30:31.109: INFO: Got endpoints: latency-svc-88pff [152.422912ms]
Apr  6 03:30:31.114: INFO: Created: latency-svc-dnmn4
Apr  6 03:30:31.120: INFO: Got endpoints: latency-svc-vphtw [151.45123ms]
Apr  6 03:30:31.124: INFO: Created: latency-svc-9gxxj
Apr  6 03:30:31.130: INFO: Created: latency-svc-6ff2f
Apr  6 03:30:31.131: INFO: Got endpoints: latency-svc-dnmn4 [157.653887ms]
Apr  6 03:30:31.138: INFO: Got endpoints: latency-svc-9gxxj [153.913133ms]
Apr  6 03:30:31.140: INFO: Created: latency-svc-vklbg
Apr  6 03:30:31.146: INFO: Got endpoints: latency-svc-6ff2f [148.56503ms]
Apr  6 03:30:31.151: INFO: Created: latency-svc-pgqml
Apr  6 03:30:31.155: INFO: Got endpoints: latency-svc-vklbg [147.643827ms]
Apr  6 03:30:31.160: INFO: Created: latency-svc-894hw
Apr  6 03:30:31.168: INFO: Got endpoints: latency-svc-pgqml [152.280344ms]
Apr  6 03:30:31.169: INFO: Created: latency-svc-rlmvm
Apr  6 03:30:31.175: INFO: Got endpoints: latency-svc-894hw [149.239617ms]
Apr  6 03:30:31.179: INFO: Created: latency-svc-cf655
Apr  6 03:30:31.182: INFO: Got endpoints: latency-svc-rlmvm [145.98574ms]
Apr  6 03:30:31.192: INFO: Created: latency-svc-8s85b
Apr  6 03:30:31.199: INFO: Got endpoints: latency-svc-cf655 [141.43133ms]
Apr  6 03:30:31.209: INFO: Created: latency-svc-4khfn
Apr  6 03:30:31.212: INFO: Got endpoints: latency-svc-8s85b [146.706396ms]
Apr  6 03:30:31.215: INFO: Created: latency-svc-ffsnx
Apr  6 03:30:31.225: INFO: Got endpoints: latency-svc-4khfn [161.227284ms]
Apr  6 03:30:31.228: INFO: Created: latency-svc-d6b5l
Apr  6 03:30:31.237: INFO: Got endpoints: latency-svc-ffsnx [155.85776ms]
Apr  6 03:30:31.238: INFO: Created: latency-svc-h5lgp
Apr  6 03:30:31.241: INFO: Got endpoints: latency-svc-d6b5l [157.958758ms]
Apr  6 03:30:31.247: INFO: Created: latency-svc-2mkpj
Apr  6 03:30:31.251: INFO: Got endpoints: latency-svc-h5lgp [154.071047ms]
Apr  6 03:30:31.257: INFO: Created: latency-svc-nxbft
Apr  6 03:30:31.257: INFO: Got endpoints: latency-svc-2mkpj [148.819037ms]
Apr  6 03:30:31.269: INFO: Created: latency-svc-g5hjr
Apr  6 03:30:31.272: INFO: Got endpoints: latency-svc-nxbft [152.663084ms]
Apr  6 03:30:31.278: INFO: Created: latency-svc-t47qx
Apr  6 03:30:31.283: INFO: Got endpoints: latency-svc-g5hjr [151.477928ms]
Apr  6 03:30:31.291: INFO: Created: latency-svc-hlsdb
Apr  6 03:30:31.293: INFO: Got endpoints: latency-svc-t47qx [155.183022ms]
Apr  6 03:30:31.301: INFO: Created: latency-svc-q24hk
Apr  6 03:30:31.306: INFO: Got endpoints: latency-svc-hlsdb [159.489599ms]
Apr  6 03:30:31.311: INFO: Created: latency-svc-rd5z4
Apr  6 03:30:31.315: INFO: Got endpoints: latency-svc-q24hk [159.656008ms]
Apr  6 03:30:31.318: INFO: Created: latency-svc-ngrpl
Apr  6 03:30:31.324: INFO: Got endpoints: latency-svc-rd5z4 [156.729679ms]
Apr  6 03:30:31.329: INFO: Created: latency-svc-rv6kb
Apr  6 03:30:31.332: INFO: Got endpoints: latency-svc-ngrpl [157.282763ms]
Apr  6 03:30:31.341: INFO: Created: latency-svc-jvpv9
Apr  6 03:30:31.348: INFO: Got endpoints: latency-svc-rv6kb [165.413094ms]
Apr  6 03:30:31.350: INFO: Created: latency-svc-b88p2
Apr  6 03:30:31.355: INFO: Got endpoints: latency-svc-jvpv9 [155.354293ms]
Apr  6 03:30:31.361: INFO: Created: latency-svc-6xmh6
Apr  6 03:30:31.368: INFO: Got endpoints: latency-svc-b88p2 [155.573329ms]
Apr  6 03:30:31.370: INFO: Created: latency-svc-kjfz8
Apr  6 03:30:31.375: INFO: Got endpoints: latency-svc-6xmh6 [150.285513ms]
Apr  6 03:30:31.382: INFO: Created: latency-svc-sg4kz
Apr  6 03:30:31.383: INFO: Got endpoints: latency-svc-kjfz8 [145.740845ms]
Apr  6 03:30:31.395: INFO: Created: latency-svc-2k8w7
Apr  6 03:30:31.398: INFO: Got endpoints: latency-svc-sg4kz [156.811264ms]
Apr  6 03:30:31.403: INFO: Created: latency-svc-vmw9x
Apr  6 03:30:31.409: INFO: Got endpoints: latency-svc-2k8w7 [158.305969ms]
Apr  6 03:30:31.413: INFO: Created: latency-svc-4t6gl
Apr  6 03:30:31.419: INFO: Got endpoints: latency-svc-vmw9x [161.361441ms]
Apr  6 03:30:31.420: INFO: Created: latency-svc-mn4dk
Apr  6 03:30:31.426: INFO: Got endpoints: latency-svc-4t6gl [153.464766ms]
Apr  6 03:30:31.432: INFO: Created: latency-svc-dc8nr
Apr  6 03:30:31.438: INFO: Got endpoints: latency-svc-mn4dk [155.609129ms]
Apr  6 03:30:31.444: INFO: Created: latency-svc-s8zn7
Apr  6 03:30:31.445: INFO: Got endpoints: latency-svc-dc8nr [151.734616ms]
Apr  6 03:30:31.449: INFO: Created: latency-svc-kgxmh
Apr  6 03:30:31.456: INFO: Got endpoints: latency-svc-s8zn7 [149.578636ms]
Apr  6 03:30:31.459: INFO: Created: latency-svc-rp98t
Apr  6 03:30:31.462: INFO: Got endpoints: latency-svc-kgxmh [147.054012ms]
Apr  6 03:30:31.471: INFO: Created: latency-svc-87mm9
Apr  6 03:30:31.474: INFO: Got endpoints: latency-svc-rp98t [149.818305ms]
Apr  6 03:30:31.480: INFO: Created: latency-svc-d5gkx
Apr  6 03:30:31.489: INFO: Got endpoints: latency-svc-87mm9 [157.259118ms]
Apr  6 03:30:31.490: INFO: Created: latency-svc-k8fmn
Apr  6 03:30:31.495: INFO: Got endpoints: latency-svc-d5gkx [147.11333ms]
Apr  6 03:30:31.503: INFO: Created: latency-svc-dpqfb
Apr  6 03:30:31.504: INFO: Got endpoints: latency-svc-k8fmn [149.307556ms]
Apr  6 03:30:31.514: INFO: Created: latency-svc-mvs5c
Apr  6 03:30:31.518: INFO: Got endpoints: latency-svc-dpqfb [149.7553ms]
Apr  6 03:30:31.521: INFO: Created: latency-svc-vp6h5
Apr  6 03:30:31.527: INFO: Got endpoints: latency-svc-mvs5c [151.337539ms]
Apr  6 03:30:31.530: INFO: Created: latency-svc-jsxfd
Apr  6 03:30:31.538: INFO: Got endpoints: latency-svc-vp6h5 [155.580582ms]
Apr  6 03:30:31.545: INFO: Created: latency-svc-rjxlc
Apr  6 03:30:31.547: INFO: Got endpoints: latency-svc-jsxfd [148.874349ms]
Apr  6 03:30:31.553: INFO: Created: latency-svc-twwqx
Apr  6 03:30:31.563: INFO: Got endpoints: latency-svc-rjxlc [153.568539ms]
Apr  6 03:30:31.564: INFO: Created: latency-svc-5zzp2
Apr  6 03:30:31.571: INFO: Got endpoints: latency-svc-twwqx [152.347281ms]
Apr  6 03:30:31.574: INFO: Created: latency-svc-jkphg
Apr  6 03:30:31.579: INFO: Got endpoints: latency-svc-5zzp2 [153.420787ms]
Apr  6 03:30:31.585: INFO: Got endpoints: latency-svc-jkphg [146.900812ms]
Apr  6 03:30:31.585: INFO: Latencies: [40.760803ms 65.965913ms 95.259941ms 106.64028ms 121.91242ms 136.191154ms 141.43133ms 144.776536ms 145.740845ms 145.98574ms 146.706396ms 146.900812ms 147.054012ms 147.11333ms 147.643827ms 148.56503ms 148.819037ms 148.874349ms 149.239617ms 149.307556ms 149.578636ms 149.7553ms 149.818305ms 149.95191ms 150.285513ms 151.337539ms 151.45123ms 151.477928ms 151.734616ms 152.280344ms 152.347281ms 152.422912ms 152.663084ms 153.420787ms 153.464766ms 153.568539ms 153.913133ms 154.071047ms 155.183022ms 155.354293ms 155.536183ms 155.573329ms 155.580582ms 155.609129ms 155.85776ms 156.729679ms 156.811264ms 157.259118ms 157.263361ms 157.282763ms 157.653887ms 157.958758ms 158.305969ms 159.489599ms 159.656008ms 161.139945ms 161.217157ms 161.227284ms 161.361441ms 163.485955ms 163.914893ms 165.413094ms 166.535058ms 168.868198ms 169.035875ms 173.02433ms 173.83112ms 174.214683ms 176.282535ms 177.004269ms 177.483252ms 177.94098ms 178.102224ms 180.392919ms 180.976466ms 181.113604ms 181.635536ms 182.727443ms 183.333109ms 183.521026ms 183.566606ms 183.973975ms 186.527897ms 186.594394ms 186.81586ms 187.091655ms 187.225417ms 187.332557ms 187.370942ms 187.463459ms 187.544495ms 187.606989ms 187.884312ms 189.10499ms 189.656581ms 189.94271ms 190.433463ms 192.266508ms 192.401028ms 194.175326ms 194.479697ms 194.544037ms 194.805894ms 195.163672ms 195.370526ms 196.015324ms 196.256971ms 197.129693ms 197.496244ms 198.294297ms 198.462391ms 198.880049ms 199.610362ms 199.691072ms 199.841064ms 200.214156ms 200.224427ms 200.33635ms 200.446888ms 200.723152ms 201.704051ms 202.003884ms 202.941724ms 205.099501ms 206.078966ms 206.349688ms 207.119124ms 209.09818ms 210.32483ms 210.62755ms 211.099953ms 211.38169ms 212.024271ms 212.087569ms 212.522195ms 212.552634ms 215.930879ms 216.732392ms 218.150326ms 219.856259ms 220.083643ms 220.235293ms 221.928492ms 222.477504ms 223.747654ms 226.872178ms 228.096531ms 228.120101ms 228.190611ms 228.461644ms 228.940985ms 229.888899ms 230.186128ms 230.204969ms 230.300167ms 230.648804ms 231.014725ms 232.382633ms 232.853391ms 233.125916ms 234.211797ms 235.108709ms 235.573925ms 236.967585ms 238.018309ms 238.270194ms 239.672278ms 240.080448ms 240.11487ms 242.006178ms 242.498082ms 243.654656ms 244.282658ms 245.00793ms 247.642286ms 252.018214ms 259.735085ms 265.617904ms 268.984803ms 301.67992ms 324.768997ms 325.212743ms 326.904051ms 327.141766ms 327.223341ms 328.521554ms 328.672378ms 332.932184ms 336.46927ms 347.18305ms 348.008534ms 348.582107ms 352.701837ms 358.888818ms 360.511669ms 362.444168ms 377.346236ms 394.966188ms 410.198268ms 414.488614ms]
Apr  6 03:30:31.586: INFO: 50 %ile: 194.479697ms
Apr  6 03:30:31.586: INFO: 90 %ile: 324.768997ms
Apr  6 03:30:31.586: INFO: 99 %ile: 410.198268ms
Apr  6 03:30:31.586: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:31.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3293" for this suite.

• [SLOW TEST:6.301 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":96,"skipped":1605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:31.619: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:30:31.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6793 create -f -'
Apr  6 03:30:34.979: INFO: stderr: ""
Apr  6 03:30:34.979: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr  6 03:30:34.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6793 create -f -'
Apr  6 03:30:38.166: INFO: stderr: ""
Apr  6 03:30:38.166: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr  6 03:30:39.190: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 03:30:39.190: INFO: Found 1 / 1
Apr  6 03:30:39.190: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  6 03:30:39.218: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 03:30:39.218: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 03:30:39.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6793 describe pod agnhost-primary-rgfdn'
Apr  6 03:30:39.313: INFO: stderr: ""
Apr  6 03:30:39.314: INFO: stdout: "Name:         agnhost-primary-rgfdn\nNamespace:    kubectl-6793\nPriority:     0\nNode:         10.241.0.100/10.241.0.100\nStart Time:   Wed, 06 Apr 2022 03:30:35 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 1b2e7588097e59d7e869291927c33f4997cb397e5636dfb16ac7f5f197a431ed\n              cni.projectcalico.org/podIP: 172.17.77.52/32\n              cni.projectcalico.org/podIPs: 172.17.77.52/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.17.77.52\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.17.77.52\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.17.77.52\nIPs:\n  IP:           172.17.77.52\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://1791f1ffe62b6b34a9308e98165ff7b4b654ae1d8fa6150091e44fe8322e5f40\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 06 Apr 2022 03:30:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xchbq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xchbq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       4s    default-scheduler  Successfully assigned kubectl-6793/agnhost-primary-rgfdn to 10.241.0.100\n  Normal  AddedInterface  4s    multus             Add eth0 [172.17.77.52/32] from k8s-pod-network\n  Normal  Pulled          3s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created         3s    kubelet            Created container agnhost-primary\n  Normal  Started         3s    kubelet            Started container agnhost-primary\n"
Apr  6 03:30:39.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6793 describe rc agnhost-primary'
Apr  6 03:30:39.429: INFO: stderr: ""
Apr  6 03:30:39.429: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6793\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-rgfdn\n"
Apr  6 03:30:39.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6793 describe service agnhost-primary'
Apr  6 03:30:39.527: INFO: stderr: ""
Apr  6 03:30:39.527: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6793\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.199.96\nIPs:               172.21.199.96\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.17.77.52:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr  6 03:30:39.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6793 describe node 10.241.0.100'
Apr  6 03:30:39.874: INFO: stderr: ""
Apr  6 03:30:39.875: INFO: stdout: "Name:               10.241.0.100\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=bx2.4x16\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east\n                    failure-domain.beta.kubernetes.io/zone=us-east-1\n                    ibm-cloud.kubernetes.io/iaas-provider=g2\n                    ibm-cloud.kubernetes.io/instance-id=0757_7e599397-3b7c-4e5f-b587-9ecced05ca1c\n                    ibm-cloud.kubernetes.io/internal-ip=10.241.0.100\n                    ibm-cloud.kubernetes.io/machine-type=bx2.4x16\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=us-east\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/subnet-id=0757-565e0141-1821-4115-aa7d-d8b64ce4caf1\n                    ibm-cloud.kubernetes.io/worker-id=kube-c96ea4nw0kj6a3ob37b0-kubee2epvgx-default-00000395\n                    ibm-cloud.kubernetes.io/worker-pool-id=c96ea4nw0kj6a3ob37b0-8e0e677\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.9.25_1532_openshift\n                    ibm-cloud.kubernetes.io/zone=us-east-1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.241.0.100\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=bx2.4x16\n                    node.openshift.io/os_id=rhel\n                    topology.kubernetes.io/region=us-east\n                    topology.kubernetes.io/zone=us-east-1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"vpc.block.csi.ibm.io\":\"kube-c96ea4nw0kj6a3ob37b0-kubee2epvgx-default-00000395\"}\n                    projectcalico.org/IPv4Address: 10.241.0.100/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.17.77.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 06 Apr 2022 01:20:27 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.241.0.100\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 06 Apr 2022 03:30:37 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 06 Apr 2022 01:28:24 +0000   Wed, 06 Apr 2022 01:28:24 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 06 Apr 2022 03:29:30 +0000   Wed, 06 Apr 2022 01:20:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 06 Apr 2022 03:29:30 +0000   Wed, 06 Apr 2022 01:20:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 06 Apr 2022 03:29:30 +0000   Wed, 06 Apr 2022 01:20:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 06 Apr 2022 03:29:30 +0000   Wed, 06 Apr 2022 01:25:08 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.241.0.100\n  ExternalIP:  10.241.0.100\n  Hostname:    10.241.0.100\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102048096Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16266120Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93525038945\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13489032Ki\n  pods:               110\nSystem Info:\n  Machine ID:                                       7e5993973b7c4e5fb5879ecced05ca1c\n  System UUID:                                      7E599397-3B7C-4E5F-B587-9ECCED05CA1C\n  Boot ID:                                          285bce29-97a0-4d9c-9a67-d2c49d3760b6\n  Kernel Version:                                   3.10.0-1160.59.1.el7.x86_64\n  OS Image:                                         Red Hat\n  Operating System:                                 linux\n  Architecture:                                     amd64\n  Container Runtime Version:                        cri-o://1.22.2-3.rhaos4.9.gitb030be8.el7\n  Kubelet Version:                                  v1.22.5+5c84e52\n  Kube-Proxy Version:                               v1.22.5+5c84e52\nPodCIDR:                                            172.17.65.0/24\nPodCIDRs:                                           172.17.65.0/24\nProviderID:                                         ibm://68010fd8df4f467681ddec1e065d7a48///c96ea4nw0kj6a3ob37b0/kube-c96ea4nw0kj6a3ob37b0-kubee2epvgx-default-00000395\nNon-terminated Pods:                                (47 in total)\n  Namespace                                         Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                         ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                                     calico-kube-controllers-b6474b6d6-v54x8                    10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      127m\n  calico-system                                     calico-node-tnbgq                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         127m\n  calico-system                                     calico-typha-5fc4c7b7c7-rjrk4                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         127m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-86b89bd6d-25bg6        10m (0%)      100m (2%)   10Mi (0%)        100Mi (0%)     140m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-86b89bd6d-7bfgh        10m (0%)      100m (2%)   10Mi (0%)        100Mi (0%)     140m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-86b89bd6d-q5wr7        10m (0%)      100m (2%)   10Mi (0%)        100Mi (0%)     140m\n  kube-system                                       ibm-keepalived-watcher-c7h76                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         130m\n  kube-system                                       ibm-master-proxy-static-10.241.0.100                       26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      128m\n  kube-system                                       ibm-storage-metrics-agent-77bc4fb5c9-dck7l                 50m (1%)      500m (12%)  100Mi (0%)       500Mi (3%)     140m\n  kube-system                                       ibm-vpc-block-csi-controller-0                             138m (3%)     552m (14%)  275Mi (2%)       1100Mi (8%)    134m\n  kube-system                                       ibm-vpc-block-csi-node-n4fgh                               48m (1%)      192m (4%)   110Mi (0%)       440Mi (3%)     130m\n  kubectl-6793                                      agnhost-primary-rgfdn                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         5s\n  openshift-cluster-node-tuning-operator            cluster-node-tuning-operator-847fd957bd-kmksx              10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         145m\n  openshift-cluster-node-tuning-operator            tuned-lxrqd                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         112m\n  openshift-cluster-samples-operator                cluster-samples-operator-67d667cb6c-5hvmq                  20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         145m\n  openshift-cluster-storage-operator                cluster-storage-operator-77cf9bb8c7-zwgsg                  10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         145m\n  openshift-cluster-storage-operator                csi-snapshot-controller-operator-55d76bfc74-fxv2z          10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         145m\n  openshift-console-operator                        console-operator-7c7f956448-rt9cz                          10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         145m\n  openshift-console                                 console-84457c4b7f-l4zfr                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         111m\n  openshift-dns-operator                            dns-operator-8d8fb8787-xmhn7                               20m (0%)      0 (0%)      69Mi (0%)        0 (0%)         145m\n  openshift-dns                                     dns-default-gx9pd                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         112m\n  openshift-dns                                     node-resolver-8cqjf                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         112m\n  openshift-image-registry                          cluster-image-registry-operator-6bcb795945-5qbrw           10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         145m\n  openshift-image-registry                          node-ca-gvpjv                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         112m\n  openshift-ingress-canary                          ingress-canary-jd288                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         112m\n  openshift-ingress-operator                        ingress-operator-58b79c98c4-9q2fc                          20m (0%)      0 (0%)      96Mi (0%)        0 (0%)         145m\n  openshift-kube-proxy                              openshift-kube-proxy-pqrfn                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         129m\n  openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-5cfccbf8c7-c9h68    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         145m\n  openshift-marketplace                             marketplace-operator-6cf6b95b7c-vjlnm                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         145m\n  openshift-monitoring                              alertmanager-main-2                                        8m (0%)       0 (0%)      105Mi (0%)       0 (0%)         111m\n  openshift-monitoring                              cluster-monitoring-operator-6c8f74c5d5-9lsz4               11m (0%)      0 (0%)      95Mi (0%)        0 (0%)         145m\n  openshift-monitoring                              node-exporter-wbftb                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         113m\n  openshift-multus                                  multus-additional-cni-plugins-xpz8b                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         129m\n  openshift-multus                                  multus-admission-controller-qwnbg                          20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         125m\n  openshift-multus                                  multus-fjdss                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         129m\n  openshift-multus                                  network-metrics-daemon-4m7nv                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         129m\n  openshift-network-diagnostics                     network-check-source-544bd4cb64-g5s5s                      10m (0%)      0 (0%)      40Mi (0%)        0 (0%)         129m\n  openshift-network-diagnostics                     network-check-target-5lxc6                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         129m\n  openshift-network-operator                        network-operator-6c8789b55f-ntzrr                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         145m\n  openshift-operator-lifecycle-manager              catalog-operator-699fc547c5-l5qkl                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         145m\n  openshift-operator-lifecycle-manager              olm-operator-864d7cb959-4fbrf                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         145m\n  openshift-operator-lifecycle-manager              package-server-manager-6f44bc74b7-c9pcp                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         145m\n  openshift-operator-lifecycle-manager              packageserver-77bb6bdcd6-hqmlj                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         115m\n  openshift-roks-metrics                            metrics-b6fbdf747-4gdlt                                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         145m\n  openshift-roks-metrics                            push-gateway-f7897c967-v6cxg                               10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         145m\n  openshift-service-ca-operator                     service-ca-operator-77fd55df89-nkd65                       10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         145m\n  sonobuoy                                          sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-htb29    0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1340m (34%)      1844m (47%)\n  memory             3101203Ki (22%)  6041888Ki (44%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From        Message\n  ----    ------                   ----                 ----        -------\n  Normal  Starting                 128m                 kube-proxy  \n  Normal  Starting                 130m                 kubelet     Starting kubelet.\n  Normal  NodeAllocatableEnforced  130m                 kubelet     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  130m (x8 over 130m)  kubelet     Node 10.241.0.100 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    130m (x8 over 130m)  kubelet     Node 10.241.0.100 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     130m (x7 over 130m)  kubelet     Node 10.241.0.100 status is now: NodeHasSufficientPID\n"
Apr  6 03:30:39.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6793 describe namespace kubectl-6793'
Apr  6 03:30:39.984: INFO: stderr: ""
Apr  6 03:30:39.985: INFO: stdout: "Name:         kubectl-6793\nLabels:       e2e-framework=kubectl\n              e2e-run=ecf983b6-3e7c-47bb-95e8-fca82b9cc807\n              kubernetes.io/metadata.name=kubectl-6793\nAnnotations:  openshift.io/sa.scc.mcs: s0:c42,c19\n              openshift.io/sa.scc.supplemental-groups: 1001760000/10000\n              openshift.io/sa.scc.uid-range: 1001760000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:39.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6793" for this suite.

• [SLOW TEST:8.397 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1094
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":97,"skipped":1636,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:40.016: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Apr  6 03:30:40.230: INFO: Waiting up to 5m0s for pod "var-expansion-ffe6ebb2-7f0a-441f-9120-a327e73712bf" in namespace "var-expansion-1847" to be "Succeeded or Failed"
Apr  6 03:30:40.241: INFO: Pod "var-expansion-ffe6ebb2-7f0a-441f-9120-a327e73712bf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.993014ms
Apr  6 03:30:42.256: INFO: Pod "var-expansion-ffe6ebb2-7f0a-441f-9120-a327e73712bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026251657s
STEP: Saw pod success
Apr  6 03:30:42.256: INFO: Pod "var-expansion-ffe6ebb2-7f0a-441f-9120-a327e73712bf" satisfied condition "Succeeded or Failed"
Apr  6 03:30:42.272: INFO: Trying to get logs from node 10.241.0.102 pod var-expansion-ffe6ebb2-7f0a-441f-9120-a327e73712bf container dapi-container: <nil>
STEP: delete the pod
Apr  6 03:30:42.407: INFO: Waiting for pod var-expansion-ffe6ebb2-7f0a-441f-9120-a327e73712bf to disappear
Apr  6 03:30:42.421: INFO: Pod var-expansion-ffe6ebb2-7f0a-441f-9120-a327e73712bf no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:42.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1847" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":98,"skipped":1642,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:42.450: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1446
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1446
I0406 03:30:42.807815      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1446, replica count: 2
Apr  6 03:30:45.859: INFO: Creating new exec pod
I0406 03:30:45.859656      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 03:30:50.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-1446 exec execpodks2ll -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr  6 03:30:51.179: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 03:30:51.179: INFO: stdout: "externalname-service-8dl6c"
Apr  6 03:30:51.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-1446 exec execpodks2ll -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.227.214 80'
Apr  6 03:30:51.421: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.227.214 80\nConnection to 172.21.227.214 80 port [tcp/http] succeeded!\n"
Apr  6 03:30:51.421: INFO: stdout: ""
Apr  6 03:30:52.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-1446 exec execpodks2ll -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.227.214 80'
Apr  6 03:30:52.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.227.214 80\nConnection to 172.21.227.214 80 port [tcp/http] succeeded!\n"
Apr  6 03:30:52.682: INFO: stdout: ""
Apr  6 03:30:53.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-1446 exec execpodks2ll -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.227.214 80'
Apr  6 03:30:53.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.227.214 80\nConnection to 172.21.227.214 80 port [tcp/http] succeeded!\n"
Apr  6 03:30:53.683: INFO: stdout: ""
Apr  6 03:30:54.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-1446 exec execpodks2ll -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.227.214 80'
Apr  6 03:30:54.693: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.227.214 80\nConnection to 172.21.227.214 80 port [tcp/http] succeeded!\n"
Apr  6 03:30:54.693: INFO: stdout: ""
Apr  6 03:30:55.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-1446 exec execpodks2ll -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.227.214 80'
Apr  6 03:30:55.658: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.227.214 80\nConnection to 172.21.227.214 80 port [tcp/http] succeeded!\n"
Apr  6 03:30:55.658: INFO: stdout: "externalname-service-8dl6c"
Apr  6 03:30:55.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-1446 exec execpodks2ll -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.102 31848'
Apr  6 03:30:55.876: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.102 31848\nConnection to 10.241.0.102 31848 port [tcp/*] succeeded!\n"
Apr  6 03:30:55.876: INFO: stdout: "externalname-service-8dl6c"
Apr  6 03:30:55.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-1446 exec execpodks2ll -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.100 31848'
Apr  6 03:30:56.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.100 31848\nConnection to 10.241.0.100 31848 port [tcp/*] succeeded!\n"
Apr  6 03:30:56.131: INFO: stdout: "externalname-service-6drhl"
Apr  6 03:30:56.131: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:30:56.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1446" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.756 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":99,"skipped":1642,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:30:56.205: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:30:56.418: INFO: created pod
Apr  6 03:30:56.418: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9500" to be "Succeeded or Failed"
Apr  6 03:30:56.430: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.086285ms
Apr  6 03:30:58.442: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024434541s
STEP: Saw pod success
Apr  6 03:30:58.442: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Apr  6 03:31:28.443: INFO: polling logs
Apr  6 03:31:28.467: INFO: Pod logs: 
2022/04/06 03:30:57 OK: Got token
2022/04/06 03:30:57 validating with in-cluster discovery
2022/04/06 03:30:57 OK: got issuer https://kubernetes.default.svc
2022/04/06 03:30:57 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-9500:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1649216456, NotBefore:1649215856, IssuedAt:1649215856, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9500", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0cc738e2-ae14-43b7-aaa2-f32925df4879"}}}
2022/04/06 03:30:57 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
2022/04/06 03:30:57 OK: Validated signature on JWT
2022/04/06 03:30:57 OK: Got valid claims from token!
2022/04/06 03:30:57 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-9500:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1649216456, NotBefore:1649215856, IssuedAt:1649215856, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9500", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0cc738e2-ae14-43b7-aaa2-f32925df4879"}}}

Apr  6 03:31:28.467: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:31:28.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9500" for this suite.

• [SLOW TEST:32.321 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":100,"skipped":1652,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:31:28.527: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:31:28.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20df7d17-e09f-4ca1-91ca-4e7f1ee1d454" in namespace "downward-api-2256" to be "Succeeded or Failed"
Apr  6 03:31:28.777: INFO: Pod "downwardapi-volume-20df7d17-e09f-4ca1-91ca-4e7f1ee1d454": Phase="Pending", Reason="", readiness=false. Elapsed: 12.445482ms
Apr  6 03:31:30.793: INFO: Pod "downwardapi-volume-20df7d17-e09f-4ca1-91ca-4e7f1ee1d454": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028739246s
STEP: Saw pod success
Apr  6 03:31:30.793: INFO: Pod "downwardapi-volume-20df7d17-e09f-4ca1-91ca-4e7f1ee1d454" satisfied condition "Succeeded or Failed"
Apr  6 03:31:30.804: INFO: Trying to get logs from node 10.241.0.100 pod downwardapi-volume-20df7d17-e09f-4ca1-91ca-4e7f1ee1d454 container client-container: <nil>
STEP: delete the pod
Apr  6 03:31:30.863: INFO: Waiting for pod downwardapi-volume-20df7d17-e09f-4ca1-91ca-4e7f1ee1d454 to disappear
Apr  6 03:31:30.874: INFO: Pod downwardapi-volume-20df7d17-e09f-4ca1-91ca-4e7f1ee1d454 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:31:30.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2256" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1662,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:31:30.901: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-43aa33df-7fcd-4aeb-8ccd-0d81603c94e2
Apr  6 03:31:31.097: INFO: Pod name my-hostname-basic-43aa33df-7fcd-4aeb-8ccd-0d81603c94e2: Found 0 pods out of 1
Apr  6 03:31:36.110: INFO: Pod name my-hostname-basic-43aa33df-7fcd-4aeb-8ccd-0d81603c94e2: Found 1 pods out of 1
Apr  6 03:31:36.110: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-43aa33df-7fcd-4aeb-8ccd-0d81603c94e2" are running
Apr  6 03:31:36.122: INFO: Pod "my-hostname-basic-43aa33df-7fcd-4aeb-8ccd-0d81603c94e2-c9xdx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-06 03:31:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-06 03:31:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-06 03:31:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-06 03:31:31 +0000 UTC Reason: Message:}])
Apr  6 03:31:36.122: INFO: Trying to dial the pod
Apr  6 03:31:41.184: INFO: Controller my-hostname-basic-43aa33df-7fcd-4aeb-8ccd-0d81603c94e2: Got expected result from replica 1 [my-hostname-basic-43aa33df-7fcd-4aeb-8ccd-0d81603c94e2-c9xdx]: "my-hostname-basic-43aa33df-7fcd-4aeb-8ccd-0d81603c94e2-c9xdx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:31:41.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-311" for this suite.

• [SLOW TEST:10.331 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":102,"skipped":1669,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:31:41.233: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:31:41.827: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:31:44.886: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:31:57.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7208" for this suite.
STEP: Destroying namespace "webhook-7208-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.386 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":103,"skipped":1688,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:31:57.619: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-f57f834b-fd89-45b4-8ede-6f01f97d2e6c
STEP: Creating a pod to test consume secrets
Apr  6 03:31:57.835: INFO: Waiting up to 5m0s for pod "pod-secrets-3acaccd3-d09d-4af4-ae52-75ac01984cc7" in namespace "secrets-2337" to be "Succeeded or Failed"
Apr  6 03:31:57.850: INFO: Pod "pod-secrets-3acaccd3-d09d-4af4-ae52-75ac01984cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.512582ms
Apr  6 03:31:59.865: INFO: Pod "pod-secrets-3acaccd3-d09d-4af4-ae52-75ac01984cc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029824841s
STEP: Saw pod success
Apr  6 03:31:59.865: INFO: Pod "pod-secrets-3acaccd3-d09d-4af4-ae52-75ac01984cc7" satisfied condition "Succeeded or Failed"
Apr  6 03:31:59.875: INFO: Trying to get logs from node 10.241.0.100 pod pod-secrets-3acaccd3-d09d-4af4-ae52-75ac01984cc7 container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 03:31:59.940: INFO: Waiting for pod pod-secrets-3acaccd3-d09d-4af4-ae52-75ac01984cc7 to disappear
Apr  6 03:31:59.955: INFO: Pod pod-secrets-3acaccd3-d09d-4af4-ae52-75ac01984cc7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:31:59.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2337" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:31:59.986: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8052
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8052
STEP: creating replication controller externalsvc in namespace services-8052
I0406 03:32:00.205089      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8052, replica count: 2
I0406 03:32:03.256639      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr  6 03:32:03.309: INFO: Creating new exec pod
Apr  6 03:32:07.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-8052 exec execpodw2gpb -- /bin/sh -x -c nslookup clusterip-service.services-8052.svc.cluster.local'
Apr  6 03:32:07.591: INFO: stderr: "+ nslookup clusterip-service.services-8052.svc.cluster.local\n"
Apr  6 03:32:07.591: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-8052.svc.cluster.local\tcanonical name = externalsvc.services-8052.svc.cluster.local.\nName:\texternalsvc.services-8052.svc.cluster.local\nAddress: 172.21.125.176\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8052, will wait for the garbage collector to delete the pods
Apr  6 03:32:07.681: INFO: Deleting ReplicationController externalsvc took: 25.184359ms
Apr  6 03:32:07.781: INFO: Terminating ReplicationController externalsvc pods took: 100.241384ms
Apr  6 03:32:10.822: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:32:10.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8052" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.890 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":105,"skipped":1731,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:32:10.876: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1680.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1680.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 03:32:15.299: INFO: DNS probes using dns-1680/dns-test-00f784e4-2305-478d-bf20-eace67ec5146 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:32:15.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1680" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":106,"skipped":1736,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:32:15.375: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:32:15.572: INFO: The status of Pod busybox-host-aliases56f8abd6-e048-4197-88c4-b12cbf46b2ab is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:32:17.586: INFO: The status of Pod busybox-host-aliases56f8abd6-e048-4197-88c4-b12cbf46b2ab is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:32:17.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6176" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":1741,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:32:17.672: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5412
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5412
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5412
Apr  6 03:32:17.903: INFO: Found 0 stateful pods, waiting for 1
Apr  6 03:32:27.921: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr  6 03:32:27.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-5412 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 03:32:28.204: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 03:32:28.204: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 03:32:28.204: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 03:32:28.219: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  6 03:32:38.234: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 03:32:38.234: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 03:32:38.294: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999975s
Apr  6 03:32:39.309: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.984584939s
Apr  6 03:32:40.329: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.971016183s
Apr  6 03:32:41.346: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.95070004s
Apr  6 03:32:42.363: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.933914616s
Apr  6 03:32:43.378: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.916511789s
Apr  6 03:32:44.395: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.901585647s
Apr  6 03:32:45.424: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.883660877s
Apr  6 03:32:46.438: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.855780855s
Apr  6 03:32:47.451: INFO: Verifying statefulset ss doesn't scale past 1 for another 841.60026ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5412
Apr  6 03:32:48.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-5412 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 03:32:48.750: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 03:32:48.750: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 03:32:48.750: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 03:32:48.763: INFO: Found 1 stateful pods, waiting for 3
Apr  6 03:32:58.780: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:32:58.780: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:32:58.780: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr  6 03:32:58.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-5412 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 03:32:59.025: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 03:32:59.025: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 03:32:59.025: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 03:32:59.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-5412 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 03:32:59.266: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 03:32:59.266: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 03:32:59.266: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 03:32:59.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-5412 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 03:32:59.514: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 03:32:59.514: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 03:32:59.514: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 03:32:59.514: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 03:32:59.529: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr  6 03:33:09.562: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 03:33:09.562: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 03:33:09.562: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 03:33:09.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999979s
Apr  6 03:33:10.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984573475s
Apr  6 03:33:11.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971584512s
Apr  6 03:33:12.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.956877219s
Apr  6 03:33:13.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.939739763s
Apr  6 03:33:14.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.924583442s
Apr  6 03:33:15.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.910963445s
Apr  6 03:33:16.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.896492981s
Apr  6 03:33:17.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.876977785s
Apr  6 03:33:18.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 862.032314ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5412
Apr  6 03:33:19.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-5412 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 03:33:20.126: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 03:33:20.126: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 03:33:20.126: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 03:33:20.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-5412 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 03:33:20.431: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 03:33:20.431: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 03:33:20.431: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 03:33:20.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-5412 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 03:33:20.681: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 03:33:20.681: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 03:33:20.681: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 03:33:20.681: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Apr  6 03:33:30.744: INFO: Deleting all statefulset in ns statefulset-5412
Apr  6 03:33:30.757: INFO: Scaling statefulset ss to 0
Apr  6 03:33:30.811: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 03:33:30.823: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:33:30.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5412" for this suite.

• [SLOW TEST:73.263 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":108,"skipped":1753,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:33:30.935: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:33:47.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8396" for this suite.

• [SLOW TEST:16.700 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":109,"skipped":1755,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:33:47.636: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:33:47.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27aed9cb-ecda-47f0-9b91-311044801fe0" in namespace "downward-api-9013" to be "Succeeded or Failed"
Apr  6 03:33:47.863: INFO: Pod "downwardapi-volume-27aed9cb-ecda-47f0-9b91-311044801fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.251146ms
Apr  6 03:33:49.887: INFO: Pod "downwardapi-volume-27aed9cb-ecda-47f0-9b91-311044801fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03726835s
Apr  6 03:33:51.905: INFO: Pod "downwardapi-volume-27aed9cb-ecda-47f0-9b91-311044801fe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055314207s
STEP: Saw pod success
Apr  6 03:33:51.905: INFO: Pod "downwardapi-volume-27aed9cb-ecda-47f0-9b91-311044801fe0" satisfied condition "Succeeded or Failed"
Apr  6 03:33:51.917: INFO: Trying to get logs from node 10.241.0.100 pod downwardapi-volume-27aed9cb-ecda-47f0-9b91-311044801fe0 container client-container: <nil>
STEP: delete the pod
Apr  6 03:33:52.008: INFO: Waiting for pod downwardapi-volume-27aed9cb-ecda-47f0-9b91-311044801fe0 to disappear
Apr  6 03:33:52.021: INFO: Pod downwardapi-volume-27aed9cb-ecda-47f0-9b91-311044801fe0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:33:52.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9013" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":1773,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:33:52.049: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:00.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9754" for this suite.

• [SLOW TEST:68.381 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":111,"skipped":1777,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:00.431: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:35:00.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-279f0ac9-d500-4501-b691-a59b88e33971" in namespace "downward-api-7019" to be "Succeeded or Failed"
Apr  6 03:35:00.701: INFO: Pod "downwardapi-volume-279f0ac9-d500-4501-b691-a59b88e33971": Phase="Pending", Reason="", readiness=false. Elapsed: 10.588094ms
Apr  6 03:35:02.715: INFO: Pod "downwardapi-volume-279f0ac9-d500-4501-b691-a59b88e33971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024545937s
STEP: Saw pod success
Apr  6 03:35:02.715: INFO: Pod "downwardapi-volume-279f0ac9-d500-4501-b691-a59b88e33971" satisfied condition "Succeeded or Failed"
Apr  6 03:35:02.731: INFO: Trying to get logs from node 10.241.0.101 pod downwardapi-volume-279f0ac9-d500-4501-b691-a59b88e33971 container client-container: <nil>
STEP: delete the pod
Apr  6 03:35:02.881: INFO: Waiting for pod downwardapi-volume-279f0ac9-d500-4501-b691-a59b88e33971 to disappear
Apr  6 03:35:02.893: INFO: Pod downwardapi-volume-279f0ac9-d500-4501-b691-a59b88e33971 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:02.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7019" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":1782,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Apr  6 03:35:03.203: INFO: Found Service test-service-6t45d in namespace services-4533 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Apr  6 03:35:03.203: INFO: Service test-service-6t45d created
STEP: Getting /status
Apr  6 03:35:03.214: INFO: Service test-service-6t45d has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Apr  6 03:35:03.238: INFO: observed Service test-service-6t45d in namespace services-4533 with annotations: map[] & LoadBalancer: {[]}
Apr  6 03:35:03.238: INFO: Found Service test-service-6t45d in namespace services-4533 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Apr  6 03:35:03.238: INFO: Service test-service-6t45d has service status patched
STEP: updating the ServiceStatus
Apr  6 03:35:03.287: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Apr  6 03:35:03.292: INFO: Observed Service test-service-6t45d in namespace services-4533 with annotations: map[] & Conditions: {[]}
Apr  6 03:35:03.292: INFO: Observed event: &Service{ObjectMeta:{test-service-6t45d  services-4533  2b59f1aa-4023-400d-b027-15bdb19128fd 81356 0 2022-04-06 03:35:03 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-04-06 03:35:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-04-06 03:35:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.124.252,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.124.252],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Apr  6 03:35:03.293: INFO: Found Service test-service-6t45d in namespace services-4533 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr  6 03:35:03.293: INFO: Service test-service-6t45d has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Apr  6 03:35:03.320: INFO: observed Service test-service-6t45d in namespace services-4533 with labels: map[test-service-static:true]
Apr  6 03:35:03.320: INFO: observed Service test-service-6t45d in namespace services-4533 with labels: map[test-service-static:true]
Apr  6 03:35:03.321: INFO: observed Service test-service-6t45d in namespace services-4533 with labels: map[test-service-static:true]
Apr  6 03:35:03.321: INFO: Found Service test-service-6t45d in namespace services-4533 with labels: map[test-service:patched test-service-static:true]
Apr  6 03:35:03.321: INFO: Service test-service-6t45d patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Apr  6 03:35:03.407: INFO: Observed event: ADDED
Apr  6 03:35:03.408: INFO: Observed event: MODIFIED
Apr  6 03:35:03.408: INFO: Observed event: MODIFIED
Apr  6 03:35:03.408: INFO: Observed event: MODIFIED
Apr  6 03:35:03.409: INFO: Found Service test-service-6t45d in namespace services-4533 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Apr  6 03:35:03.409: INFO: Service test-service-6t45d deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:03.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4533" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":113,"skipped":1791,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:03.443: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:35:03.677: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-22ec2b57-8d07-49f3-baff-0a46afd0a9ed
STEP: Creating the pod
Apr  6 03:35:03.796: INFO: The status of Pod pod-configmaps-14a25806-ae86-4279-b52a-1d819199a15e is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:35:05.827: INFO: The status of Pod pod-configmaps-14a25806-ae86-4279-b52a-1d819199a15e is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-22ec2b57-8d07-49f3-baff-0a46afd0a9ed
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:07.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4082" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":1801,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:08.042: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Apr  6 03:35:08.274: INFO: Waiting up to 5m0s for pod "var-expansion-347c5f78-5dc0-49e8-b967-b912c4002671" in namespace "var-expansion-5409" to be "Succeeded or Failed"
Apr  6 03:35:08.291: INFO: Pod "var-expansion-347c5f78-5dc0-49e8-b967-b912c4002671": Phase="Pending", Reason="", readiness=false. Elapsed: 17.035617ms
Apr  6 03:35:10.309: INFO: Pod "var-expansion-347c5f78-5dc0-49e8-b967-b912c4002671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034465296s
STEP: Saw pod success
Apr  6 03:35:10.309: INFO: Pod "var-expansion-347c5f78-5dc0-49e8-b967-b912c4002671" satisfied condition "Succeeded or Failed"
Apr  6 03:35:10.319: INFO: Trying to get logs from node 10.241.0.102 pod var-expansion-347c5f78-5dc0-49e8-b967-b912c4002671 container dapi-container: <nil>
STEP: delete the pod
Apr  6 03:35:10.400: INFO: Waiting for pod var-expansion-347c5f78-5dc0-49e8-b967-b912c4002671 to disappear
Apr  6 03:35:10.411: INFO: Pod var-expansion-347c5f78-5dc0-49e8-b967-b912c4002671 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:10.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5409" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":115,"skipped":1809,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:10.458: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:35:10.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f363d29-8fd4-4b3d-9f1b-571790cdbe4d" in namespace "projected-9681" to be "Succeeded or Failed"
Apr  6 03:35:10.679: INFO: Pod "downwardapi-volume-8f363d29-8fd4-4b3d-9f1b-571790cdbe4d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.399302ms
Apr  6 03:35:12.696: INFO: Pod "downwardapi-volume-8f363d29-8fd4-4b3d-9f1b-571790cdbe4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.029358412s
Apr  6 03:35:14.711: INFO: Pod "downwardapi-volume-8f363d29-8fd4-4b3d-9f1b-571790cdbe4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043733143s
STEP: Saw pod success
Apr  6 03:35:14.711: INFO: Pod "downwardapi-volume-8f363d29-8fd4-4b3d-9f1b-571790cdbe4d" satisfied condition "Succeeded or Failed"
Apr  6 03:35:14.726: INFO: Trying to get logs from node 10.241.0.100 pod downwardapi-volume-8f363d29-8fd4-4b3d-9f1b-571790cdbe4d container client-container: <nil>
STEP: delete the pod
Apr  6 03:35:14.815: INFO: Waiting for pod downwardapi-volume-8f363d29-8fd4-4b3d-9f1b-571790cdbe4d to disappear
Apr  6 03:35:14.832: INFO: Pod downwardapi-volume-8f363d29-8fd4-4b3d-9f1b-571790cdbe4d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:14.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9681" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":1811,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:14.861: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:17.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7453" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":117,"skipped":1829,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:17.171: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:33.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5917" for this suite.

• [SLOW TEST:16.574 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":118,"skipped":1854,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:33.745: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr  6 03:35:33.943: INFO: Waiting up to 5m0s for pod "pod-59f9645b-f4df-45ad-898e-7d0b5808d8eb" in namespace "emptydir-515" to be "Succeeded or Failed"
Apr  6 03:35:33.958: INFO: Pod "pod-59f9645b-f4df-45ad-898e-7d0b5808d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.033262ms
Apr  6 03:35:35.981: INFO: Pod "pod-59f9645b-f4df-45ad-898e-7d0b5808d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038107102s
Apr  6 03:35:37.997: INFO: Pod "pod-59f9645b-f4df-45ad-898e-7d0b5808d8eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054160245s
STEP: Saw pod success
Apr  6 03:35:37.997: INFO: Pod "pod-59f9645b-f4df-45ad-898e-7d0b5808d8eb" satisfied condition "Succeeded or Failed"
Apr  6 03:35:38.018: INFO: Trying to get logs from node 10.241.0.101 pod pod-59f9645b-f4df-45ad-898e-7d0b5808d8eb container test-container: <nil>
STEP: delete the pod
Apr  6 03:35:38.107: INFO: Waiting for pod pod-59f9645b-f4df-45ad-898e-7d0b5808d8eb to disappear
Apr  6 03:35:38.119: INFO: Pod pod-59f9645b-f4df-45ad-898e-7d0b5808d8eb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:38.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-515" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":1854,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:38.145: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr  6 03:35:42.973: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1186 pod-service-account-4dd2a5d4-ad34-47cb-8a2c-5057e2729ea3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr  6 03:35:43.522: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1186 pod-service-account-4dd2a5d4-ad34-47cb-8a2c-5057e2729ea3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr  6 03:35:43.788: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1186 pod-service-account-4dd2a5d4-ad34-47cb-8a2c-5057e2729ea3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:44.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1186" for this suite.

• [SLOW TEST:5.967 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":120,"skipped":1867,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:44.113: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:35:44.786: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr  6 03:35:46.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812944, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812944, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812945, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784812944, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:35:49.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:50.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3356" for this suite.
STEP: Destroying namespace "webhook-3356-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.189 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":121,"skipped":1868,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:50.302: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-b4567089-ba2b-4542-9b17-8f29f983e0d7
STEP: Creating a pod to test consume configMaps
Apr  6 03:35:50.597: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-60830f5d-f0ec-4240-abb6-e82f19176835" in namespace "projected-4722" to be "Succeeded or Failed"
Apr  6 03:35:50.608: INFO: Pod "pod-projected-configmaps-60830f5d-f0ec-4240-abb6-e82f19176835": Phase="Pending", Reason="", readiness=false. Elapsed: 11.387769ms
Apr  6 03:35:52.621: INFO: Pod "pod-projected-configmaps-60830f5d-f0ec-4240-abb6-e82f19176835": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024536292s
STEP: Saw pod success
Apr  6 03:35:52.621: INFO: Pod "pod-projected-configmaps-60830f5d-f0ec-4240-abb6-e82f19176835" satisfied condition "Succeeded or Failed"
Apr  6 03:35:52.637: INFO: Trying to get logs from node 10.241.0.101 pod pod-projected-configmaps-60830f5d-f0ec-4240-abb6-e82f19176835 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 03:35:52.697: INFO: Waiting for pod pod-projected-configmaps-60830f5d-f0ec-4240-abb6-e82f19176835 to disappear
Apr  6 03:35:52.716: INFO: Pod pod-projected-configmaps-60830f5d-f0ec-4240-abb6-e82f19176835 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:35:52.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4722" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":1878,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:35:52.749: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2306
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Apr  6 03:35:53.000: INFO: Found 0 stateful pods, waiting for 3
Apr  6 03:36:03.019: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:36:03.019: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:36:03.019: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Apr  6 03:36:03.098: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr  6 03:36:13.203: INFO: Updating stateful set ss2
Apr  6 03:36:13.238: INFO: Waiting for Pod statefulset-2306/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Apr  6 03:36:23.347: INFO: Found 1 stateful pods, waiting for 3
Apr  6 03:36:33.366: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:36:33.366: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 03:36:33.366: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr  6 03:36:33.438: INFO: Updating stateful set ss2
Apr  6 03:36:33.465: INFO: Waiting for Pod statefulset-2306/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Apr  6 03:36:43.535: INFO: Updating stateful set ss2
Apr  6 03:36:43.559: INFO: Waiting for StatefulSet statefulset-2306/ss2 to complete update
Apr  6 03:36:43.559: INFO: Waiting for Pod statefulset-2306/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Apr  6 03:36:53.586: INFO: Deleting all statefulset in ns statefulset-2306
Apr  6 03:36:53.600: INFO: Scaling statefulset ss2 to 0
Apr  6 03:37:03.665: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 03:37:03.680: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:03.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2306" for this suite.

• [SLOW TEST:71.020 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":123,"skipped":1897,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:03.770: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Apr  6 03:37:04.083: INFO: The status of Pod pod-update-activedeadlineseconds-e2195518-9e3c-496e-a5f0-fe193321eb99 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:37:06.095: INFO: The status of Pod pod-update-activedeadlineseconds-e2195518-9e3c-496e-a5f0-fe193321eb99 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:37:08.100: INFO: The status of Pod pod-update-activedeadlineseconds-e2195518-9e3c-496e-a5f0-fe193321eb99 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr  6 03:37:08.702: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e2195518-9e3c-496e-a5f0-fe193321eb99"
Apr  6 03:37:08.702: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e2195518-9e3c-496e-a5f0-fe193321eb99" in namespace "pods-270" to be "terminated due to deadline exceeded"
Apr  6 03:37:08.713: INFO: Pod "pod-update-activedeadlineseconds-e2195518-9e3c-496e-a5f0-fe193321eb99": Phase="Running", Reason="", readiness=true. Elapsed: 10.924552ms
Apr  6 03:37:10.726: INFO: Pod "pod-update-activedeadlineseconds-e2195518-9e3c-496e-a5f0-fe193321eb99": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 2.024021899s
Apr  6 03:37:10.726: INFO: Pod "pod-update-activedeadlineseconds-e2195518-9e3c-496e-a5f0-fe193321eb99" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:10.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-270" for this suite.

• [SLOW TEST:6.985 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":124,"skipped":1899,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:10.755: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6219.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6219.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6219.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6219.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 03:37:15.102: INFO: DNS probes using dns-test-ec0c9360-35d9-4990-913c-af7ad21eb2c9 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6219.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6219.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6219.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6219.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 03:37:19.322: INFO: DNS probes using dns-test-d51203c4-369b-4b8f-b9cf-6bc291c7fe40 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6219.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6219.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6219.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6219.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 03:37:23.528: INFO: DNS probes using dns-test-70fe9242-4cfb-43ab-b3ee-fd542812c206 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:23.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6219" for this suite.

• [SLOW TEST:12.878 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":125,"skipped":1917,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:23.633: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Apr  6 03:37:23.798: INFO: created test-event-1
Apr  6 03:37:23.812: INFO: created test-event-2
Apr  6 03:37:23.826: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Apr  6 03:37:23.835: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Apr  6 03:37:23.914: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:23.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8692" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":126,"skipped":1921,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4488
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4488
I0406 03:37:24.201932      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4488, replica count: 2
Apr  6 03:37:27.253: INFO: Creating new exec pod
I0406 03:37:27.253865      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 03:37:30.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-4488 exec execpodmm5qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr  6 03:37:30.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 03:37:30.587: INFO: stdout: ""
Apr  6 03:37:31.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-4488 exec execpodmm5qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr  6 03:37:32.001: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 03:37:32.001: INFO: stdout: ""
Apr  6 03:37:32.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-4488 exec execpodmm5qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr  6 03:37:32.874: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 03:37:32.874: INFO: stdout: "externalname-service-8n27z"
Apr  6 03:37:32.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-4488 exec execpodmm5qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.183.163 80'
Apr  6 03:37:33.127: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.183.163 80\nConnection to 172.21.183.163 80 port [tcp/http] succeeded!\n"
Apr  6 03:37:33.127: INFO: stdout: "externalname-service-9wbpl"
Apr  6 03:37:33.127: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:33.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4488" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.239 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":127,"skipped":1939,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:33.191: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:33.341: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1240
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:39.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-675" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:39.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1240" for this suite.

• [SLOW TEST:6.536 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":128,"skipped":1948,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:39.727: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-5256/configmap-test-7a125b65-2a5f-4a64-9799-4ce572e6e755
STEP: Creating a pod to test consume configMaps
Apr  6 03:37:39.965: INFO: Waiting up to 5m0s for pod "pod-configmaps-ffe5aab4-a2d9-4c6f-be90-55088d788932" in namespace "configmap-5256" to be "Succeeded or Failed"
Apr  6 03:37:39.975: INFO: Pod "pod-configmaps-ffe5aab4-a2d9-4c6f-be90-55088d788932": Phase="Pending", Reason="", readiness=false. Elapsed: 10.32226ms
Apr  6 03:37:41.990: INFO: Pod "pod-configmaps-ffe5aab4-a2d9-4c6f-be90-55088d788932": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025211824s
Apr  6 03:37:44.006: INFO: Pod "pod-configmaps-ffe5aab4-a2d9-4c6f-be90-55088d788932": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040807656s
STEP: Saw pod success
Apr  6 03:37:44.006: INFO: Pod "pod-configmaps-ffe5aab4-a2d9-4c6f-be90-55088d788932" satisfied condition "Succeeded or Failed"
Apr  6 03:37:44.018: INFO: Trying to get logs from node 10.241.0.102 pod pod-configmaps-ffe5aab4-a2d9-4c6f-be90-55088d788932 container env-test: <nil>
STEP: delete the pod
Apr  6 03:37:44.115: INFO: Waiting for pod pod-configmaps-ffe5aab4-a2d9-4c6f-be90-55088d788932 to disappear
Apr  6 03:37:44.127: INFO: Pod pod-configmaps-ffe5aab4-a2d9-4c6f-be90-55088d788932 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:44.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5256" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":1950,"failed":0}

------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:44.157: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Apr  6 03:37:44.380: INFO: The status of Pod pod-hostip-499a5e5a-7a33-41f4-be33-30032bb04ed8 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:37:46.394: INFO: The status of Pod pod-hostip-499a5e5a-7a33-41f4-be33-30032bb04ed8 is Running (Ready = true)
Apr  6 03:37:46.419: INFO: Pod pod-hostip-499a5e5a-7a33-41f4-be33-30032bb04ed8 has hostIP: 10.241.0.102
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:46.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-514" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":1950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:46.449: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:47.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5485" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":131,"skipped":1984,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:47.120: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Apr  6 03:37:47.393: INFO: created test-pod-1
Apr  6 03:37:47.443: INFO: created test-pod-2
Apr  6 03:37:47.482: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Apr  6 03:37:47.608: INFO: Pod quantity 3 is different from expected quantity 0
Apr  6 03:37:48.620: INFO: Pod quantity 3 is different from expected quantity 0
Apr  6 03:37:49.629: INFO: Pod quantity 3 is different from expected quantity 0
Apr  6 03:37:50.626: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:37:51.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2568" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":132,"skipped":1990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:37:51.648: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr  6 03:37:56.481: INFO: Successfully updated pod "adopt-release--1-c4thr"
STEP: Checking that the Job readopts the Pod
Apr  6 03:37:56.481: INFO: Waiting up to 15m0s for pod "adopt-release--1-c4thr" in namespace "job-4279" to be "adopted"
Apr  6 03:37:56.492: INFO: Pod "adopt-release--1-c4thr": Phase="Running", Reason="", readiness=true. Elapsed: 11.187694ms
Apr  6 03:37:58.505: INFO: Pod "adopt-release--1-c4thr": Phase="Running", Reason="", readiness=true. Elapsed: 2.024092687s
Apr  6 03:37:58.505: INFO: Pod "adopt-release--1-c4thr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr  6 03:37:59.059: INFO: Successfully updated pod "adopt-release--1-c4thr"
STEP: Checking that the Job releases the Pod
Apr  6 03:37:59.059: INFO: Waiting up to 15m0s for pod "adopt-release--1-c4thr" in namespace "job-4279" to be "released"
Apr  6 03:37:59.070: INFO: Pod "adopt-release--1-c4thr": Phase="Running", Reason="", readiness=true. Elapsed: 10.134348ms
Apr  6 03:38:01.092: INFO: Pod "adopt-release--1-c4thr": Phase="Running", Reason="", readiness=true. Elapsed: 2.032360914s
Apr  6 03:38:01.092: INFO: Pod "adopt-release--1-c4thr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:38:01.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4279" for this suite.

• [SLOW TEST:9.473 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":133,"skipped":2024,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:38:01.121: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:38:01.286: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr  6 03:38:01.344: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  6 03:38:06.359: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  6 03:38:06.359: INFO: Creating deployment "test-rolling-update-deployment"
Apr  6 03:38:06.376: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr  6 03:38:06.394: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr  6 03:38:08.416: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr  6 03:38:08.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813086, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813086, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813086, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813086, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 03:38:10.440: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr  6 03:38:10.498: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-657  e0d93d4b-b3a2-4bed-9fa2-e54791165bcf 84962 1 2022-04-06 03:38:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-04-06 03:38:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003415a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-04-06 03:38:06 +0000 UTC,LastTransitionTime:2022-04-06 03:38:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2022-04-06 03:38:09 +0000 UTC,LastTransitionTime:2022-04-06 03:38:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 03:38:10.510: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-657  224eac26-db15-46bf-b579-9b7490fdd2e5 84952 1 2022-04-06 03:38:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e0d93d4b-b3a2-4bed-9fa2-e54791165bcf 0xc001447877 0xc001447878}] []  [{kube-controller-manager Update apps/v1 2022-04-06 03:38:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0d93d4b-b3a2-4bed-9fa2-e54791165bcf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:38:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001447928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 03:38:10.510: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr  6 03:38:10.510: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-657  9b05cd5d-6362-48e3-b708-ff0fa1ff385b 84961 2 2022-04-06 03:38:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e0d93d4b-b3a2-4bed-9fa2-e54791165bcf 0xc001447747 0xc001447748}] []  [{e2e.test Update apps/v1 2022-04-06 03:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0d93d4b-b3a2-4bed-9fa2-e54791165bcf\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:38:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001447808 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 03:38:10.522: INFO: Pod "test-rolling-update-deployment-585b757574-6c87h" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-6c87h test-rolling-update-deployment-585b757574- deployment-657  4f00bca3-abd4-4248-bc59-20c42f71a098 84951 0 2022-04-06 03:38:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/containerID:92e00ccd3606a2f9473d27e783875238d6e7e7c74db237189bad330c0a468301 cni.projectcalico.org/podIP:172.17.77.38/32 cni.projectcalico.org/podIPs:172.17.77.38/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.38"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.77.38"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 224eac26-db15-46bf-b579-9b7490fdd2e5 0xc001447d87 0xc001447d88}] []  [{kube-controller-manager Update v1 2022-04-06 03:38:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"224eac26-db15-46bf-b579-9b7490fdd2e5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 03:38:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 03:38:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 03:38:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.77.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjb6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjb6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c45,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lxsfh,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:38:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:38:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:38:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:38:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.100,PodIP:172.17.77.38,StartTime:2022-04-06 03:38:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 03:38:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://6b26d9b2959c60b416d349e319c2187b822378f3fba6c38959687f6235986723,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.77.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:38:10.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-657" for this suite.

• [SLOW TEST:9.428 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":134,"skipped":2038,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:38:10.549: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:38:11.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8002" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":135,"skipped":2047,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:38:11.123: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:38:11.339: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-cbc23bfd-a30b-4280-89fb-a2ca94e67b95
STEP: Creating configMap with name cm-test-opt-upd-7dd46b64-04f1-4059-aba4-6eea4d24d1f9
STEP: Creating the pod
Apr  6 03:38:11.522: INFO: The status of Pod pod-configmaps-0745f3c7-78cc-4761-a4b3-7b5b3fd282cb is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:38:13.538: INFO: The status of Pod pod-configmaps-0745f3c7-78cc-4761-a4b3-7b5b3fd282cb is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:38:15.538: INFO: The status of Pod pod-configmaps-0745f3c7-78cc-4761-a4b3-7b5b3fd282cb is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-cbc23bfd-a30b-4280-89fb-a2ca94e67b95
STEP: Updating configmap cm-test-opt-upd-7dd46b64-04f1-4059-aba4-6eea4d24d1f9
STEP: Creating configMap with name cm-test-opt-create-8c0ada62-98fb-4c19-a868-a445b31bfd48
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:39:29.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5743" for this suite.

• [SLOW TEST:78.000 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":136,"skipped":2063,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:39:29.123: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Apr  6 03:39:29.981: INFO: created pod pod-service-account-defaultsa
Apr  6 03:39:29.981: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr  6 03:39:30.020: INFO: created pod pod-service-account-mountsa
Apr  6 03:39:30.020: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr  6 03:39:30.055: INFO: created pod pod-service-account-nomountsa
Apr  6 03:39:30.055: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr  6 03:39:30.087: INFO: created pod pod-service-account-defaultsa-mountspec
Apr  6 03:39:30.087: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr  6 03:39:30.121: INFO: created pod pod-service-account-mountsa-mountspec
Apr  6 03:39:30.121: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr  6 03:39:30.158: INFO: created pod pod-service-account-nomountsa-mountspec
Apr  6 03:39:30.158: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr  6 03:39:30.192: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr  6 03:39:30.192: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr  6 03:39:30.231: INFO: created pod pod-service-account-mountsa-nomountspec
Apr  6 03:39:30.231: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr  6 03:39:30.267: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr  6 03:39:30.267: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:39:30.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1628" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":137,"skipped":2079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:39:30.298: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Apr  6 03:39:30.583: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:39:32.603: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Apr  6 03:39:32.668: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:39:34.682: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr  6 03:39:34.758: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 03:39:34.768: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 03:39:36.769: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 03:39:36.783: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 03:39:38.769: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 03:39:38.783: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:39:38.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2989" for this suite.

• [SLOW TEST:8.519 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2162,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:39:38.817: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:39:39.404: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:39:42.479: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:39:42.491: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:39:45.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9144" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.278 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":139,"skipped":2180,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:39:46.095: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-pxcs
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 03:39:46.364: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pxcs" in namespace "subpath-8854" to be "Succeeded or Failed"
Apr  6 03:39:46.379: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Pending", Reason="", readiness=false. Elapsed: 15.868138ms
Apr  6 03:39:48.391: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 2.027159708s
Apr  6 03:39:50.404: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 4.040035442s
Apr  6 03:39:52.420: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 6.056241106s
Apr  6 03:39:54.432: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 8.068915596s
Apr  6 03:39:56.446: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 10.08296224s
Apr  6 03:39:58.465: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 12.101732557s
Apr  6 03:40:00.493: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 14.129748413s
Apr  6 03:40:02.512: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 16.148534274s
Apr  6 03:40:04.531: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 18.167562688s
Apr  6 03:40:06.547: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Running", Reason="", readiness=true. Elapsed: 20.183088284s
Apr  6 03:40:08.560: INFO: Pod "pod-subpath-test-secret-pxcs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.196380383s
STEP: Saw pod success
Apr  6 03:40:08.560: INFO: Pod "pod-subpath-test-secret-pxcs" satisfied condition "Succeeded or Failed"
Apr  6 03:40:08.572: INFO: Trying to get logs from node 10.241.0.101 pod pod-subpath-test-secret-pxcs container test-container-subpath-secret-pxcs: <nil>
STEP: delete the pod
Apr  6 03:40:08.663: INFO: Waiting for pod pod-subpath-test-secret-pxcs to disappear
Apr  6 03:40:08.674: INFO: Pod pod-subpath-test-secret-pxcs no longer exists
STEP: Deleting pod pod-subpath-test-secret-pxcs
Apr  6 03:40:08.674: INFO: Deleting pod "pod-subpath-test-secret-pxcs" in namespace "subpath-8854"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:08.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8854" for this suite.

• [SLOW TEST:22.623 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":140,"skipped":2189,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:08.718: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-a14a4254-39e4-4544-a302-a494048b4779
STEP: Creating a pod to test consume secrets
Apr  6 03:40:08.997: INFO: Waiting up to 5m0s for pod "pod-secrets-bad0568e-1afa-4974-b9ba-33f72fc49191" in namespace "secrets-8854" to be "Succeeded or Failed"
Apr  6 03:40:09.015: INFO: Pod "pod-secrets-bad0568e-1afa-4974-b9ba-33f72fc49191": Phase="Pending", Reason="", readiness=false. Elapsed: 18.449801ms
Apr  6 03:40:11.027: INFO: Pod "pod-secrets-bad0568e-1afa-4974-b9ba-33f72fc49191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030080851s
Apr  6 03:40:13.038: INFO: Pod "pod-secrets-bad0568e-1afa-4974-b9ba-33f72fc49191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041178723s
STEP: Saw pod success
Apr  6 03:40:13.038: INFO: Pod "pod-secrets-bad0568e-1afa-4974-b9ba-33f72fc49191" satisfied condition "Succeeded or Failed"
Apr  6 03:40:13.050: INFO: Trying to get logs from node 10.241.0.101 pod pod-secrets-bad0568e-1afa-4974-b9ba-33f72fc49191 container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 03:40:13.171: INFO: Waiting for pod pod-secrets-bad0568e-1afa-4974-b9ba-33f72fc49191 to disappear
Apr  6 03:40:13.185: INFO: Pod pod-secrets-bad0568e-1afa-4974-b9ba-33f72fc49191 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:13.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8854" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2197,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:13.217: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-191
STEP: creating service affinity-nodeport in namespace services-191
STEP: creating replication controller affinity-nodeport in namespace services-191
I0406 03:40:13.450073      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-191, replica count: 3
I0406 03:40:16.500569      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 03:40:16.561: INFO: Creating new exec pod
Apr  6 03:40:19.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-191 exec execpod-affinityxq5hd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Apr  6 03:40:20.227: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr  6 03:40:20.227: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:40:20.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-191 exec execpod-affinityxq5hd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.135.218 80'
Apr  6 03:40:20.875: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.135.218 80\nConnection to 172.21.135.218 80 port [tcp/http] succeeded!\n"
Apr  6 03:40:20.875: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:40:20.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-191 exec execpod-affinityxq5hd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.101 30036'
Apr  6 03:40:21.155: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.101 30036\nConnection to 10.241.0.101 30036 port [tcp/*] succeeded!\n"
Apr  6 03:40:21.155: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:40:21.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-191 exec execpod-affinityxq5hd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.100 30036'
Apr  6 03:40:21.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.100 30036\nConnection to 10.241.0.100 30036 port [tcp/*] succeeded!\n"
Apr  6 03:40:21.382: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:40:21.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-191 exec execpod-affinityxq5hd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.241.0.100:30036/ ; done'
Apr  6 03:40:21.656: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30036/\n"
Apr  6 03:40:21.656: INFO: stdout: "\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x\naffinity-nodeport-vdw6x"
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Received response from host: affinity-nodeport-vdw6x
Apr  6 03:40:21.656: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-191, will wait for the garbage collector to delete the pods
Apr  6 03:40:21.804: INFO: Deleting ReplicationController affinity-nodeport took: 35.141829ms
Apr  6 03:40:22.004: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.421611ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:25.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-191" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.869 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":142,"skipped":2243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:25.086: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:40:25.577: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 03:40:27.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813225, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813225, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813225, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813225, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:40:30.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:30.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3715" for this suite.
STEP: Destroying namespace "webhook-3715-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.091 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":143,"skipped":2274,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:31.177: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr  6 03:40:31.479: INFO: Waiting up to 5m0s for pod "pod-a6bfd023-6ac5-4000-9c12-6f75cce56163" in namespace "emptydir-6403" to be "Succeeded or Failed"
Apr  6 03:40:31.515: INFO: Pod "pod-a6bfd023-6ac5-4000-9c12-6f75cce56163": Phase="Pending", Reason="", readiness=false. Elapsed: 36.156582ms
Apr  6 03:40:33.527: INFO: Pod "pod-a6bfd023-6ac5-4000-9c12-6f75cce56163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04805654s
STEP: Saw pod success
Apr  6 03:40:33.527: INFO: Pod "pod-a6bfd023-6ac5-4000-9c12-6f75cce56163" satisfied condition "Succeeded or Failed"
Apr  6 03:40:33.538: INFO: Trying to get logs from node 10.241.0.101 pod pod-a6bfd023-6ac5-4000-9c12-6f75cce56163 container test-container: <nil>
STEP: delete the pod
Apr  6 03:40:33.610: INFO: Waiting for pod pod-a6bfd023-6ac5-4000-9c12-6f75cce56163 to disappear
Apr  6 03:40:33.621: INFO: Pod pod-a6bfd023-6ac5-4000-9c12-6f75cce56163 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:33.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6403" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":144,"skipped":2282,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:33.952: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Apr  6 03:40:34.465: INFO: The status of Pod pod-update-06be3d31-166f-4b85-9ed8-88e4d380c45c is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:40:36.482: INFO: The status of Pod pod-update-06be3d31-166f-4b85-9ed8-88e4d380c45c is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:40:38.476: INFO: The status of Pod pod-update-06be3d31-166f-4b85-9ed8-88e4d380c45c is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr  6 03:40:39.347: INFO: Successfully updated pod "pod-update-06be3d31-166f-4b85-9ed8-88e4d380c45c"
STEP: verifying the updated pod is in kubernetes
Apr  6 03:40:39.378: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:39.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9223" for this suite.

• [SLOW TEST:5.466 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:39.418: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr  6 03:40:39.791: INFO: Waiting up to 5m0s for pod "pod-a60d22c8-cfdc-4218-84ff-315c670b8621" in namespace "emptydir-6493" to be "Succeeded or Failed"
Apr  6 03:40:39.822: INFO: Pod "pod-a60d22c8-cfdc-4218-84ff-315c670b8621": Phase="Pending", Reason="", readiness=false. Elapsed: 31.785644ms
Apr  6 03:40:41.842: INFO: Pod "pod-a60d22c8-cfdc-4218-84ff-315c670b8621": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051301344s
STEP: Saw pod success
Apr  6 03:40:41.842: INFO: Pod "pod-a60d22c8-cfdc-4218-84ff-315c670b8621" satisfied condition "Succeeded or Failed"
Apr  6 03:40:41.853: INFO: Trying to get logs from node 10.241.0.101 pod pod-a60d22c8-cfdc-4218-84ff-315c670b8621 container test-container: <nil>
STEP: delete the pod
Apr  6 03:40:41.938: INFO: Waiting for pod pod-a60d22c8-cfdc-4218-84ff-315c670b8621 to disappear
Apr  6 03:40:41.949: INFO: Pod pod-a60d22c8-cfdc-4218-84ff-315c670b8621 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6493" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2345,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:41.978: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:42.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6061" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":147,"skipped":2357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:42.912: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr  6 03:40:43.272: INFO: Waiting up to 5m0s for pod "pod-4f477332-44f4-40f7-84d2-3026b4618ffc" in namespace "emptydir-7372" to be "Succeeded or Failed"
Apr  6 03:40:43.282: INFO: Pod "pod-4f477332-44f4-40f7-84d2-3026b4618ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.976416ms
Apr  6 03:40:45.297: INFO: Pod "pod-4f477332-44f4-40f7-84d2-3026b4618ffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025619028s
STEP: Saw pod success
Apr  6 03:40:45.297: INFO: Pod "pod-4f477332-44f4-40f7-84d2-3026b4618ffc" satisfied condition "Succeeded or Failed"
Apr  6 03:40:45.309: INFO: Trying to get logs from node 10.241.0.101 pod pod-4f477332-44f4-40f7-84d2-3026b4618ffc container test-container: <nil>
STEP: delete the pod
Apr  6 03:40:45.367: INFO: Waiting for pod pod-4f477332-44f4-40f7-84d2-3026b4618ffc to disappear
Apr  6 03:40:45.383: INFO: Pod pod-4f477332-44f4-40f7-84d2-3026b4618ffc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:45.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7372" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2406,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:40:45.411: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7120.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7120.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7120.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7120.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7120.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7120.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 03:40:59.871: INFO: DNS probes using dns-7120/dns-test-a3037320-b561-4455-b0a3-0c18728d6db8 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:40:59.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7120" for this suite.

• [SLOW TEST:14.821 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":149,"skipped":2418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:41:00.233: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3965.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3965.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3965.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3965.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3965.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3965.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3965.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3965.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3965.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3965.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 03:41:04.653: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local from pod dns-3965/dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7: the server could not find the requested resource (get pods dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7)
Apr  6 03:41:04.694: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3965.svc.cluster.local from pod dns-3965/dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7: the server could not find the requested resource (get pods dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7)
Apr  6 03:41:04.782: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local from pod dns-3965/dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7: the server could not find the requested resource (get pods dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7)
Apr  6 03:41:04.799: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local from pod dns-3965/dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7: the server could not find the requested resource (get pods dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7)
Apr  6 03:41:04.817: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3965.svc.cluster.local from pod dns-3965/dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7: the server could not find the requested resource (get pods dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7)
Apr  6 03:41:04.835: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3965.svc.cluster.local from pod dns-3965/dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7: the server could not find the requested resource (get pods dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7)
Apr  6 03:41:04.892: INFO: Lookups using dns-3965/dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3965.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3965.svc.cluster.local jessie_udp@dns-test-service-2.dns-3965.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3965.svc.cluster.local]

Apr  6 03:41:10.116: INFO: DNS probes using dns-3965/dns-test-1b3f2e4d-4c09-4864-aaf0-34f37e4812a7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:41:10.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3965" for this suite.

• [SLOW TEST:9.992 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":150,"skipped":2463,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:41:10.225: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Apr  6 03:41:10.440: INFO: Waiting up to 5m0s for pod "security-context-e5de5da6-a4b2-4bca-9698-6595070a5603" in namespace "security-context-5791" to be "Succeeded or Failed"
Apr  6 03:41:10.452: INFO: Pod "security-context-e5de5da6-a4b2-4bca-9698-6595070a5603": Phase="Pending", Reason="", readiness=false. Elapsed: 12.365324ms
Apr  6 03:41:12.465: INFO: Pod "security-context-e5de5da6-a4b2-4bca-9698-6595070a5603": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025494501s
Apr  6 03:41:14.481: INFO: Pod "security-context-e5de5da6-a4b2-4bca-9698-6595070a5603": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041226507s
STEP: Saw pod success
Apr  6 03:41:14.481: INFO: Pod "security-context-e5de5da6-a4b2-4bca-9698-6595070a5603" satisfied condition "Succeeded or Failed"
Apr  6 03:41:14.494: INFO: Trying to get logs from node 10.241.0.102 pod security-context-e5de5da6-a4b2-4bca-9698-6595070a5603 container test-container: <nil>
STEP: delete the pod
Apr  6 03:41:14.587: INFO: Waiting for pod security-context-e5de5da6-a4b2-4bca-9698-6595070a5603 to disappear
Apr  6 03:41:14.597: INFO: Pod security-context-e5de5da6-a4b2-4bca-9698-6595070a5603 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:41:14.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5791" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":151,"skipped":2472,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:41:14.637: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:41:18.928: INFO: Deleting pod "var-expansion-c4a77ea8-20cb-442d-8f8f-f28477e9f24c" in namespace "var-expansion-3080"
Apr  6 03:41:18.957: INFO: Wait up to 5m0s for pod "var-expansion-c4a77ea8-20cb-442d-8f8f-f28477e9f24c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:41:20.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3080" for this suite.

• [SLOW TEST:6.411 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":152,"skipped":2487,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:41:21.048: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-5185
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5185 to expose endpoints map[]
Apr  6 03:41:21.313: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr  6 03:41:22.370: INFO: successfully validated that service multi-endpoint-test in namespace services-5185 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5185
Apr  6 03:41:22.428: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:41:24.443: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:41:26.440: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5185 to expose endpoints map[pod1:[100]]
Apr  6 03:41:26.487: INFO: successfully validated that service multi-endpoint-test in namespace services-5185 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5185
Apr  6 03:41:26.536: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:41:28.550: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:41:30.550: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5185 to expose endpoints map[pod1:[100] pod2:[101]]
Apr  6 03:41:30.614: INFO: successfully validated that service multi-endpoint-test in namespace services-5185 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Apr  6 03:41:30.614: INFO: Creating new exec pod
Apr  6 03:41:35.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5185 exec execpodjxv2t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Apr  6 03:41:35.915: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Apr  6 03:41:35.915: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:41:35.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5185 exec execpodjxv2t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.48.128 80'
Apr  6 03:41:36.142: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.48.128 80\nConnection to 172.21.48.128 80 port [tcp/http] succeeded!\n"
Apr  6 03:41:36.142: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:41:36.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5185 exec execpodjxv2t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Apr  6 03:41:36.371: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Apr  6 03:41:36.371: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:41:36.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5185 exec execpodjxv2t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.48.128 81'
Apr  6 03:41:36.621: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.48.128 81\nConnection to 172.21.48.128 81 port [tcp/*] succeeded!\n"
Apr  6 03:41:36.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5185
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5185 to expose endpoints map[pod2:[101]]
Apr  6 03:41:37.704: INFO: successfully validated that service multi-endpoint-test in namespace services-5185 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5185
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5185 to expose endpoints map[]
Apr  6 03:41:38.807: INFO: successfully validated that service multi-endpoint-test in namespace services-5185 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:41:38.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5185" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:17.851 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":153,"skipped":2493,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:41:38.899: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr  6 03:41:41.225: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:41:41.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9824" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":154,"skipped":2509,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:41:41.344: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0406 03:41:47.716616      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 03:41:47.716: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:41:47.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3072" for this suite.

• [SLOW TEST:6.421 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":155,"skipped":2509,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:41:47.773: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-c7e14338-4a78-43f8-a56f-41736f3ac5ed
STEP: Creating a pod to test consume configMaps
Apr  6 03:41:48.022: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4424214-5a91-4d0d-a0ba-a7e3c34acd26" in namespace "configmap-2164" to be "Succeeded or Failed"
Apr  6 03:41:48.034: INFO: Pod "pod-configmaps-e4424214-5a91-4d0d-a0ba-a7e3c34acd26": Phase="Pending", Reason="", readiness=false. Elapsed: 12.908212ms
Apr  6 03:41:50.050: INFO: Pod "pod-configmaps-e4424214-5a91-4d0d-a0ba-a7e3c34acd26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028849137s
Apr  6 03:41:52.065: INFO: Pod "pod-configmaps-e4424214-5a91-4d0d-a0ba-a7e3c34acd26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043334758s
STEP: Saw pod success
Apr  6 03:41:52.065: INFO: Pod "pod-configmaps-e4424214-5a91-4d0d-a0ba-a7e3c34acd26" satisfied condition "Succeeded or Failed"
Apr  6 03:41:52.075: INFO: Trying to get logs from node 10.241.0.102 pod pod-configmaps-e4424214-5a91-4d0d-a0ba-a7e3c34acd26 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 03:41:52.142: INFO: Waiting for pod pod-configmaps-e4424214-5a91-4d0d-a0ba-a7e3c34acd26 to disappear
Apr  6 03:41:52.154: INFO: Pod pod-configmaps-e4424214-5a91-4d0d-a0ba-a7e3c34acd26 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:41:52.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2164" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":156,"skipped":2530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:41:52.203: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Apr  6 03:41:52.423: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:41:54.436: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:41:56.443: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Apr  6 03:41:56.515: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:41:58.530: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr  6 03:41:58.544: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:58.544: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:41:58.716: INFO: Exec stderr: ""
Apr  6 03:41:58.716: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:58.716: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:41:58.898: INFO: Exec stderr: ""
Apr  6 03:41:58.898: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:58.898: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:41:59.125: INFO: Exec stderr: ""
Apr  6 03:41:59.125: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:59.125: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:41:59.298: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr  6 03:41:59.298: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:59.298: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:41:59.482: INFO: Exec stderr: ""
Apr  6 03:41:59.482: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:59.482: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:41:59.649: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr  6 03:41:59.649: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:59.649: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:41:59.786: INFO: Exec stderr: ""
Apr  6 03:41:59.786: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:59.786: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:41:59.953: INFO: Exec stderr: ""
Apr  6 03:41:59.953: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:41:59.953: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:42:00.161: INFO: Exec stderr: ""
Apr  6 03:42:00.161: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3538 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:42:00.161: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:42:00.309: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:42:00.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3538" for this suite.

• [SLOW TEST:8.168 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2554,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:42:00.371: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  6 03:42:00.691: INFO: Number of nodes with available pods: 0
Apr  6 03:42:00.691: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:42:01.726: INFO: Number of nodes with available pods: 0
Apr  6 03:42:01.726: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:42:02.719: INFO: Number of nodes with available pods: 0
Apr  6 03:42:02.719: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 03:42:03.719: INFO: Number of nodes with available pods: 3
Apr  6 03:42:03.719: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Getting /status
Apr  6 03:42:03.737: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Apr  6 03:42:03.769: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Apr  6 03:42:03.776: INFO: Observed &DaemonSet event: ADDED
Apr  6 03:42:03.776: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.780: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.780: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.780: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.780: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.780: INFO: Found daemon set daemon-set in namespace daemonsets-6658 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr  6 03:42:03.780: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Apr  6 03:42:03.806: INFO: Observed &DaemonSet event: ADDED
Apr  6 03:42:03.806: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.806: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.806: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.807: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.807: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.807: INFO: Observed daemon set daemon-set in namespace daemonsets-6658 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr  6 03:42:03.807: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 03:42:03.807: INFO: Found daemon set daemon-set in namespace daemonsets-6658 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Apr  6 03:42:03.807: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6658, will wait for the garbage collector to delete the pods
Apr  6 03:42:03.897: INFO: Deleting DaemonSet.extensions daemon-set took: 17.364569ms
Apr  6 03:42:03.998: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.531803ms
Apr  6 03:42:07.413: INFO: Number of nodes with available pods: 0
Apr  6 03:42:07.413: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 03:42:07.421: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89254"},"items":null}

Apr  6 03:42:07.432: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89254"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:42:07.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6658" for this suite.

• [SLOW TEST:7.127 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":158,"skipped":2557,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:42:07.498: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:07.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5091" for this suite.

• [SLOW TEST:60.277 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":2583,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:07.776: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-9509/secret-test-1d0a0701-3910-4eb8-b420-a3301e6a50d4
STEP: Creating a pod to test consume secrets
Apr  6 03:43:08.114: INFO: Waiting up to 5m0s for pod "pod-configmaps-de4b4ffe-e017-4d70-ad4b-77e797a434bc" in namespace "secrets-9509" to be "Succeeded or Failed"
Apr  6 03:43:08.127: INFO: Pod "pod-configmaps-de4b4ffe-e017-4d70-ad4b-77e797a434bc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.061626ms
Apr  6 03:43:10.141: INFO: Pod "pod-configmaps-de4b4ffe-e017-4d70-ad4b-77e797a434bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02658091s
Apr  6 03:43:12.157: INFO: Pod "pod-configmaps-de4b4ffe-e017-4d70-ad4b-77e797a434bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043371774s
STEP: Saw pod success
Apr  6 03:43:12.157: INFO: Pod "pod-configmaps-de4b4ffe-e017-4d70-ad4b-77e797a434bc" satisfied condition "Succeeded or Failed"
Apr  6 03:43:12.171: INFO: Trying to get logs from node 10.241.0.102 pod pod-configmaps-de4b4ffe-e017-4d70-ad4b-77e797a434bc container env-test: <nil>
STEP: delete the pod
Apr  6 03:43:12.332: INFO: Waiting for pod pod-configmaps-de4b4ffe-e017-4d70-ad4b-77e797a434bc to disappear
Apr  6 03:43:12.343: INFO: Pod pod-configmaps-de4b4ffe-e017-4d70-ad4b-77e797a434bc no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:12.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9509" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":2602,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:12.372: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr  6 03:43:12.645: INFO: Waiting up to 5m0s for pod "pod-feaf321e-ef48-4c6f-8d7d-7e0a8bb4cbcb" in namespace "emptydir-2157" to be "Succeeded or Failed"
Apr  6 03:43:12.655: INFO: Pod "pod-feaf321e-ef48-4c6f-8d7d-7e0a8bb4cbcb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.984543ms
Apr  6 03:43:14.675: INFO: Pod "pod-feaf321e-ef48-4c6f-8d7d-7e0a8bb4cbcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030404001s
STEP: Saw pod success
Apr  6 03:43:14.675: INFO: Pod "pod-feaf321e-ef48-4c6f-8d7d-7e0a8bb4cbcb" satisfied condition "Succeeded or Failed"
Apr  6 03:43:14.686: INFO: Trying to get logs from node 10.241.0.102 pod pod-feaf321e-ef48-4c6f-8d7d-7e0a8bb4cbcb container test-container: <nil>
STEP: delete the pod
Apr  6 03:43:14.743: INFO: Waiting for pod pod-feaf321e-ef48-4c6f-8d7d-7e0a8bb4cbcb to disappear
Apr  6 03:43:14.755: INFO: Pod pod-feaf321e-ef48-4c6f-8d7d-7e0a8bb4cbcb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:14.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2157" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":2609,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:14.789: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:43:15.381: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 03:43:17.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813395, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813395, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813395, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813395, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:43:20.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:20.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9493" for this suite.
STEP: Destroying namespace "webhook-9493-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.117 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":162,"skipped":2621,"failed":0}
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:20.907: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr  6 03:43:21.144: INFO: Waiting up to 5m0s for pod "downward-api-e9954e26-6e65-4e1f-a5b8-ef4349ca1bb1" in namespace "downward-api-2511" to be "Succeeded or Failed"
Apr  6 03:43:21.153: INFO: Pod "downward-api-e9954e26-6e65-4e1f-a5b8-ef4349ca1bb1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.324151ms
Apr  6 03:43:23.190: INFO: Pod "downward-api-e9954e26-6e65-4e1f-a5b8-ef4349ca1bb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045878059s
Apr  6 03:43:25.210: INFO: Pod "downward-api-e9954e26-6e65-4e1f-a5b8-ef4349ca1bb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066027568s
STEP: Saw pod success
Apr  6 03:43:25.210: INFO: Pod "downward-api-e9954e26-6e65-4e1f-a5b8-ef4349ca1bb1" satisfied condition "Succeeded or Failed"
Apr  6 03:43:25.247: INFO: Trying to get logs from node 10.241.0.102 pod downward-api-e9954e26-6e65-4e1f-a5b8-ef4349ca1bb1 container dapi-container: <nil>
STEP: delete the pod
Apr  6 03:43:25.326: INFO: Waiting for pod downward-api-e9954e26-6e65-4e1f-a5b8-ef4349ca1bb1 to disappear
Apr  6 03:43:25.339: INFO: Pod downward-api-e9954e26-6e65-4e1f-a5b8-ef4349ca1bb1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:25.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2511" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":2621,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:25.387: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-6009
STEP: creating replication controller nodeport-test in namespace services-6009
I0406 03:43:25.632594      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6009, replica count: 2
Apr  6 03:43:28.683: INFO: Creating new exec pod
I0406 03:43:28.683830      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 03:43:31.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6009 exec execpodb75xm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Apr  6 03:43:32.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr  6 03:43:32.286: INFO: stdout: "nodeport-test-qzfwr"
Apr  6 03:43:32.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6009 exec execpodb75xm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.136.169 80'
Apr  6 03:43:32.734: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.136.169 80\nConnection to 172.21.136.169 80 port [tcp/http] succeeded!\n"
Apr  6 03:43:32.734: INFO: stdout: "nodeport-test-h6nqd"
Apr  6 03:43:32.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6009 exec execpodb75xm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.102 30759'
Apr  6 03:43:33.048: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.102 30759\nConnection to 10.241.0.102 30759 port [tcp/*] succeeded!\n"
Apr  6 03:43:33.048: INFO: stdout: ""
Apr  6 03:43:34.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6009 exec execpodb75xm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.102 30759'
Apr  6 03:43:34.315: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.102 30759\nConnection to 10.241.0.102 30759 port [tcp/*] succeeded!\n"
Apr  6 03:43:34.315: INFO: stdout: "nodeport-test-h6nqd"
Apr  6 03:43:34.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6009 exec execpodb75xm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.100 30759'
Apr  6 03:43:34.609: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.100 30759\nConnection to 10.241.0.100 30759 port [tcp/*] succeeded!\n"
Apr  6 03:43:34.609: INFO: stdout: "nodeport-test-h6nqd"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:34.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6009" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.253 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":164,"skipped":2626,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:34.640: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:43:34.982: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  6 03:43:39.995: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Apr  6 03:43:40.027: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Apr  6 03:43:40.059: INFO: observed ReplicaSet test-rs in namespace replicaset-3878 with ReadyReplicas 1, AvailableReplicas 1
Apr  6 03:43:40.101: INFO: observed ReplicaSet test-rs in namespace replicaset-3878 with ReadyReplicas 1, AvailableReplicas 1
Apr  6 03:43:40.154: INFO: observed ReplicaSet test-rs in namespace replicaset-3878 with ReadyReplicas 1, AvailableReplicas 1
Apr  6 03:43:40.172: INFO: observed ReplicaSet test-rs in namespace replicaset-3878 with ReadyReplicas 1, AvailableReplicas 1
Apr  6 03:43:42.040: INFO: observed ReplicaSet test-rs in namespace replicaset-3878 with ReadyReplicas 2, AvailableReplicas 2
Apr  6 03:43:42.742: INFO: observed Replicaset test-rs in namespace replicaset-3878 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:42.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3878" for this suite.

• [SLOW TEST:8.136 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":165,"skipped":2667,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:42.777: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:43:43.045: INFO: Waiting up to 5m0s for pod "downwardapi-volume-852e8fe2-3d26-4727-a660-9550392b8d4c" in namespace "downward-api-9685" to be "Succeeded or Failed"
Apr  6 03:43:43.077: INFO: Pod "downwardapi-volume-852e8fe2-3d26-4727-a660-9550392b8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.671995ms
Apr  6 03:43:45.092: INFO: Pod "downwardapi-volume-852e8fe2-3d26-4727-a660-9550392b8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046278625s
Apr  6 03:43:47.103: INFO: Pod "downwardapi-volume-852e8fe2-3d26-4727-a660-9550392b8d4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057657181s
STEP: Saw pod success
Apr  6 03:43:47.103: INFO: Pod "downwardapi-volume-852e8fe2-3d26-4727-a660-9550392b8d4c" satisfied condition "Succeeded or Failed"
Apr  6 03:43:47.116: INFO: Trying to get logs from node 10.241.0.100 pod downwardapi-volume-852e8fe2-3d26-4727-a660-9550392b8d4c container client-container: <nil>
STEP: delete the pod
Apr  6 03:43:47.235: INFO: Waiting for pod downwardapi-volume-852e8fe2-3d26-4727-a660-9550392b8d4c to disappear
Apr  6 03:43:47.246: INFO: Pod downwardapi-volume-852e8fe2-3d26-4727-a660-9550392b8d4c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:47.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9685" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":2671,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:47.276: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Apr  6 03:43:47.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1831 create -f -'
Apr  6 03:43:50.533: INFO: stderr: ""
Apr  6 03:43:50.533: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Apr  6 03:43:50.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1831 diff -f -'
Apr  6 03:43:50.988: INFO: rc: 1
Apr  6 03:43:50.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1831 delete -f -'
Apr  6 03:43:51.098: INFO: stderr: ""
Apr  6 03:43:51.098: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:51.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1831" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":167,"skipped":2679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:51.133: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Apr  6 03:43:51.373: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Apr  6 03:43:53.416: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Apr  6 03:43:55.459: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:43:57.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3097" for this suite.

• [SLOW TEST:6.413 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":168,"skipped":2724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:43:57.547: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:43:58.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 03:44:00.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813438, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813438, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813438, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784813438, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:44:03.307: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:44:04.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9489" for this suite.
STEP: Destroying namespace "webhook-9489-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.742 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":169,"skipped":2804,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:44:04.289: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Apr  6 03:44:04.555: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:44:06.575: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:44:08.570: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Apr  6 03:44:08.627: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:44:10.649: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Apr  6 03:44:10.681: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 03:44:10.692: INFO: Pod pod-with-prestop-http-hook still exists
Apr  6 03:44:12.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 03:44:12.711: INFO: Pod pod-with-prestop-http-hook still exists
Apr  6 03:44:14.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 03:44:14.711: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:44:14.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3671" for this suite.

• [SLOW TEST:10.474 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":170,"skipped":2833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:44:14.764: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:44:14.905: INFO: Creating pod...
Apr  6 03:44:14.971: INFO: Pod Quantity: 1 Status: Pending
Apr  6 03:44:15.991: INFO: Pod Quantity: 1 Status: Pending
Apr  6 03:44:16.985: INFO: Pod Status: Running
Apr  6 03:44:16.985: INFO: Creating service...
Apr  6 03:44:17.009: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/pods/agnhost/proxy/some/path/with/DELETE
Apr  6 03:44:17.028: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr  6 03:44:17.028: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/pods/agnhost/proxy/some/path/with/GET
Apr  6 03:44:17.043: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr  6 03:44:17.043: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/pods/agnhost/proxy/some/path/with/HEAD
Apr  6 03:44:17.067: INFO: http.Client request:HEAD | StatusCode:200
Apr  6 03:44:17.067: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/pods/agnhost/proxy/some/path/with/OPTIONS
Apr  6 03:44:17.082: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr  6 03:44:17.083: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/pods/agnhost/proxy/some/path/with/PATCH
Apr  6 03:44:17.101: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr  6 03:44:17.101: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/pods/agnhost/proxy/some/path/with/POST
Apr  6 03:44:17.119: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr  6 03:44:17.119: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/pods/agnhost/proxy/some/path/with/PUT
Apr  6 03:44:17.133: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr  6 03:44:17.133: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/services/test-service/proxy/some/path/with/DELETE
Apr  6 03:44:17.152: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr  6 03:44:17.152: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/services/test-service/proxy/some/path/with/GET
Apr  6 03:44:17.173: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr  6 03:44:17.173: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/services/test-service/proxy/some/path/with/HEAD
Apr  6 03:44:17.195: INFO: http.Client request:HEAD | StatusCode:200
Apr  6 03:44:17.195: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/services/test-service/proxy/some/path/with/OPTIONS
Apr  6 03:44:17.215: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr  6 03:44:17.215: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/services/test-service/proxy/some/path/with/PATCH
Apr  6 03:44:17.238: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr  6 03:44:17.238: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/services/test-service/proxy/some/path/with/POST
Apr  6 03:44:17.259: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr  6 03:44:17.259: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7678/services/test-service/proxy/some/path/with/PUT
Apr  6 03:44:17.280: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:44:17.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7678" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":171,"skipped":2862,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:44:17.315: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Apr  6 03:44:17.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7213 cluster-info'
Apr  6 03:44:17.554: INFO: stderr: ""
Apr  6 03:44:17.555: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:44:17.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7213" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":172,"skipped":2867,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:44:17.586: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:44:26.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-291" for this suite.
STEP: Destroying namespace "nsdeletetest-8979" for this suite.
Apr  6 03:44:26.253: INFO: Namespace nsdeletetest-8979 was already deleted
STEP: Destroying namespace "nsdeletetest-4008" for this suite.

• [SLOW TEST:8.683 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":173,"skipped":2871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:44:26.269: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Apr  6 03:44:46.836: INFO: EndpointSlice for Service endpointslice-4761/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:44:56.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4761" for this suite.

• [SLOW TEST:30.629 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":174,"skipped":2893,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:44:56.898: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:45:16.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3433" for this suite.
STEP: Destroying namespace "nsdeletetest-37" for this suite.
Apr  6 03:45:16.863: INFO: Namespace nsdeletetest-37 was already deleted
STEP: Destroying namespace "nsdeletetest-9250" for this suite.

• [SLOW TEST:19.981 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":175,"skipped":2905,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:45:16.880: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:45:17.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a4c7244-36ef-4f8c-8f2d-c9f9676a92f1" in namespace "downward-api-7729" to be "Succeeded or Failed"
Apr  6 03:45:17.117: INFO: Pod "downwardapi-volume-7a4c7244-36ef-4f8c-8f2d-c9f9676a92f1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.510186ms
Apr  6 03:45:19.143: INFO: Pod "downwardapi-volume-7a4c7244-36ef-4f8c-8f2d-c9f9676a92f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038352334s
Apr  6 03:45:21.158: INFO: Pod "downwardapi-volume-7a4c7244-36ef-4f8c-8f2d-c9f9676a92f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052806353s
STEP: Saw pod success
Apr  6 03:45:21.158: INFO: Pod "downwardapi-volume-7a4c7244-36ef-4f8c-8f2d-c9f9676a92f1" satisfied condition "Succeeded or Failed"
Apr  6 03:45:21.169: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-7a4c7244-36ef-4f8c-8f2d-c9f9676a92f1 container client-container: <nil>
STEP: delete the pod
Apr  6 03:45:21.223: INFO: Waiting for pod downwardapi-volume-7a4c7244-36ef-4f8c-8f2d-c9f9676a92f1 to disappear
Apr  6 03:45:21.235: INFO: Pod downwardapi-volume-7a4c7244-36ef-4f8c-8f2d-c9f9676a92f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:45:21.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7729" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":176,"skipped":2909,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:45:21.267: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:01.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2502" for this suite.

• [SLOW TEST:100.269 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":177,"skipped":2922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:01.538: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr  6 03:47:01.774: INFO: Waiting up to 5m0s for pod "pod-7ad714ec-4cb2-4746-8c98-733b391047c0" in namespace "emptydir-9436" to be "Succeeded or Failed"
Apr  6 03:47:01.784: INFO: Pod "pod-7ad714ec-4cb2-4746-8c98-733b391047c0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.836022ms
Apr  6 03:47:03.798: INFO: Pod "pod-7ad714ec-4cb2-4746-8c98-733b391047c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024244806s
STEP: Saw pod success
Apr  6 03:47:03.798: INFO: Pod "pod-7ad714ec-4cb2-4746-8c98-733b391047c0" satisfied condition "Succeeded or Failed"
Apr  6 03:47:03.812: INFO: Trying to get logs from node 10.241.0.101 pod pod-7ad714ec-4cb2-4746-8c98-733b391047c0 container test-container: <nil>
STEP: delete the pod
Apr  6 03:47:03.978: INFO: Waiting for pod pod-7ad714ec-4cb2-4746-8c98-733b391047c0 to disappear
Apr  6 03:47:03.993: INFO: Pod pod-7ad714ec-4cb2-4746-8c98-733b391047c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:03.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9436" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":178,"skipped":2979,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:04.045: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-8314aa9b-1a0b-4eec-87aa-8ead0938ee58
STEP: Creating a pod to test consume configMaps
Apr  6 03:47:04.308: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5c5a6c1d-ec77-4eeb-90ca-074edbf8202b" in namespace "projected-4301" to be "Succeeded or Failed"
Apr  6 03:47:04.322: INFO: Pod "pod-projected-configmaps-5c5a6c1d-ec77-4eeb-90ca-074edbf8202b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.304345ms
Apr  6 03:47:06.339: INFO: Pod "pod-projected-configmaps-5c5a6c1d-ec77-4eeb-90ca-074edbf8202b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030599132s
Apr  6 03:47:08.356: INFO: Pod "pod-projected-configmaps-5c5a6c1d-ec77-4eeb-90ca-074edbf8202b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047390477s
STEP: Saw pod success
Apr  6 03:47:08.356: INFO: Pod "pod-projected-configmaps-5c5a6c1d-ec77-4eeb-90ca-074edbf8202b" satisfied condition "Succeeded or Failed"
Apr  6 03:47:08.367: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-configmaps-5c5a6c1d-ec77-4eeb-90ca-074edbf8202b container agnhost-container: <nil>
STEP: delete the pod
Apr  6 03:47:08.455: INFO: Waiting for pod pod-projected-configmaps-5c5a6c1d-ec77-4eeb-90ca-074edbf8202b to disappear
Apr  6 03:47:08.472: INFO: Pod pod-projected-configmaps-5c5a6c1d-ec77-4eeb-90ca-074edbf8202b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:08.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4301" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":2988,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:08.498: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr  6 03:47:08.650: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 03:47:08.680: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 03:47:08.698: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.100 before test
Apr  6 03:47:08.769: INFO: calico-kube-controllers-b6474b6d6-v54x8 from calico-system started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr  6 03:47:08.769: INFO: calico-node-tnbgq from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:47:08.769: INFO: calico-typha-5fc4c7b7c7-rjrk4 from calico-system started at 2022-04-06 01:23:03 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 03:47:08.769: INFO: replace-27486947--1-ljtdf from cronjob-2502 started at 2022-04-06 03:47:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container c ready: true, restart count 0
Apr  6 03:47:08.769: INFO: managed-storage-validation-webhooks-86b89bd6d-25bg6 from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Apr  6 03:47:08.769: INFO: managed-storage-validation-webhooks-86b89bd6d-7bfgh from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Apr  6 03:47:08.769: INFO: managed-storage-validation-webhooks-86b89bd6d-q5wr7 from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Apr  6 03:47:08.769: INFO: ibm-keepalived-watcher-c7h76 from kube-system started at 2022-04-06 01:20:27 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:47:08.769: INFO: ibm-master-proxy-static-10.241.0.100 from kube-system started at 2022-04-06 01:20:21 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:47:08.769: INFO: ibm-storage-metrics-agent-77bc4fb5c9-dck7l from kube-system started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Apr  6 03:47:08.769: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2022-04-06 01:25:08 +0000 UTC (6 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container csi-resizer ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:47:08.769: INFO: ibm-vpc-block-csi-node-n4fgh from kube-system started at 2022-04-06 01:20:27 +0000 UTC (4 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:47:08.769: INFO: cluster-node-tuning-operator-847fd957bd-kmksx from openshift-cluster-node-tuning-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: tuned-lxrqd from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:47:08.769: INFO: cluster-samples-operator-67d667cb6c-5hvmq from openshift-cluster-samples-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr  6 03:47:08.769: INFO: cluster-storage-operator-77cf9bb8c7-zwgsg from openshift-cluster-storage-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Apr  6 03:47:08.769: INFO: csi-snapshot-controller-operator-55d76bfc74-fxv2z from openshift-cluster-storage-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: console-operator-7c7f956448-rt9cz from openshift-console-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container console-operator ready: true, restart count 1
Apr  6 03:47:08.769: INFO: console-84457c4b7f-l4zfr from openshift-console started at 2022-04-06 01:38:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container console ready: true, restart count 0
Apr  6 03:47:08.769: INFO: dns-operator-8d8fb8787-xmhn7 from openshift-dns-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container dns-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: dns-default-gx9pd from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: node-resolver-8cqjf from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:47:08.769: INFO: cluster-image-registry-operator-6bcb795945-5qbrw from openshift-image-registry started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: node-ca-gvpjv from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:47:08.769: INFO: ingress-canary-jd288 from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:47:08.769: INFO: ingress-operator-58b79c98c4-9q2fc from openshift-ingress-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container ingress-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: openshift-kube-proxy-pqrfn from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: kube-storage-version-migrator-operator-5cfccbf8c7-c9h68 from openshift-kube-storage-version-migrator-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Apr  6 03:47:08.769: INFO: marketplace-operator-6cf6b95b7c-vjlnm from openshift-marketplace started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: cluster-monitoring-operator-6c8f74c5d5-9lsz4 from openshift-monitoring started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Apr  6 03:47:08.769: INFO: node-exporter-wbftb from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:47:08.769: INFO: multus-additional-cni-plugins-xpz8b from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:47:08.769: INFO: multus-admission-controller-qwnbg from openshift-multus started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:47:08.769: INFO: multus-fjdss from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:47:08.769: INFO: network-metrics-daemon-4m7nv from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:47:08.769: INFO: network-check-source-544bd4cb64-g5s5s from openshift-network-diagnostics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container check-endpoints ready: true, restart count 0
Apr  6 03:47:08.769: INFO: network-check-target-5lxc6 from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:47:08.769: INFO: network-operator-6c8789b55f-ntzrr from openshift-network-operator started at 2022-04-06 01:21:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container network-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: catalog-operator-699fc547c5-l5qkl from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container catalog-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: collect-profiles-27486915--1-x4vm4 from openshift-operator-lifecycle-manager started at 2022-04-06 03:15:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:47:08.769: INFO: collect-profiles-27486945--1-z6wnl from openshift-operator-lifecycle-manager started at 2022-04-06 03:45:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:47:08.769: INFO: olm-operator-864d7cb959-4fbrf from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container olm-operator ready: true, restart count 0
Apr  6 03:47:08.769: INFO: package-server-manager-6f44bc74b7-c9pcp from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container package-server-manager ready: true, restart count 0
Apr  6 03:47:08.769: INFO: packageserver-77bb6bdcd6-hqmlj from openshift-operator-lifecycle-manager started at 2022-04-06 01:35:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container packageserver ready: true, restart count 0
Apr  6 03:47:08.769: INFO: metrics-b6fbdf747-4gdlt from openshift-roks-metrics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container metrics ready: true, restart count 2
Apr  6 03:47:08.769: INFO: push-gateway-f7897c967-v6cxg from openshift-roks-metrics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container push-gateway ready: true, restart count 0
Apr  6 03:47:08.769: INFO: service-ca-operator-77fd55df89-nkd65 from openshift-service-ca-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container service-ca-operator ready: true, restart count 1
Apr  6 03:47:08.769: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-htb29 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.769: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:47:08.769: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.101 before test
Apr  6 03:47:08.832: INFO: calico-node-cs6hj from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:47:08.832: INFO: calico-typha-5fc4c7b7c7-67lgl from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 03:47:08.832: INFO: ibm-keepalived-watcher-rppx4 from kube-system started at 2022-04-06 01:20:09 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:47:08.832: INFO: ibm-master-proxy-static-10.241.0.101 from kube-system started at 2022-04-06 01:20:00 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:47:08.832: INFO: ibm-vpc-block-csi-node-qh8rl from kube-system started at 2022-04-06 01:20:09 +0000 UTC (4 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:47:08.832: INFO: tuned-46gbx from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:47:08.832: INFO: csi-snapshot-controller-7ffc756fcb-xzddf from openshift-cluster-storage-operator started at 2022-04-06 01:36:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container snapshot-controller ready: true, restart count 0
Apr  6 03:47:08.832: INFO: csi-snapshot-webhook-574644677c-dmmpc from openshift-cluster-storage-operator started at 2022-04-06 01:36:15 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container webhook ready: true, restart count 0
Apr  6 03:47:08.832: INFO: console-84457c4b7f-8gp5t from openshift-console started at 2022-04-06 01:39:26 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container console ready: true, restart count 0
Apr  6 03:47:08.832: INFO: downloads-dbb5d5764-jhtbl from openshift-console started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container download-server ready: true, restart count 0
Apr  6 03:47:08.832: INFO: dns-default-v242c from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: node-resolver-lsbp7 from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:47:08.832: INFO: node-ca-ccn7d from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:47:08.832: INFO: ingress-canary-wklcl from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:47:08.832: INFO: router-default-79f7bb79b4-vnvq6 from openshift-ingress started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container router ready: true, restart count 0
Apr  6 03:47:08.832: INFO: openshift-kube-proxy-lsknc from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: community-operators-c2jdx from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:47:08.832: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: node-exporter-vx9fl from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:47:08.832: INFO: openshift-state-metrics-8dcbc5f76-cx6zj from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (3 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr  6 03:47:08.832: INFO: prometheus-adapter-54744554d8-95snn from openshift-monitoring started at 2022-04-06 01:39:52 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr  6 03:47:08.832: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-04-06 01:38:59 +0000 UTC (7 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container prometheus ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr  6 03:47:08.832: INFO: thanos-querier-6797965cb7-gjqm5 from openshift-monitoring started at 2022-04-06 01:38:55 +0000 UTC (5 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container thanos-query ready: true, restart count 0
Apr  6 03:47:08.832: INFO: multus-additional-cni-plugins-d5x4n from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:47:08.832: INFO: multus-admission-controller-6c4s6 from openshift-multus started at 2022-04-06 01:27:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:47:08.832: INFO: multus-cwlpj from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:47:08.832: INFO: network-metrics-daemon-hgtsb from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:47:08.832: INFO: network-check-target-rmb8p from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:47:08.832: INFO: sonobuoy from sonobuoy started at 2022-04-06 02:56:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 03:47:08.832: INFO: sonobuoy-e2e-job-9989f36257d44400 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container e2e ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:47:08.832: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-ffjt6 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.832: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:47:08.832: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.102 before test
Apr  6 03:47:08.898: INFO: calico-node-t786f from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 03:47:08.898: INFO: replace-27486946--1-7qqgs from cronjob-2502 started at 2022-04-06 03:46:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container c ready: true, restart count 0
Apr  6 03:47:08.898: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-04-06 01:29:25 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Apr  6 03:47:08.898: INFO: ibm-keepalived-watcher-bv7pb from kube-system started at 2022-04-06 01:20:29 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 03:47:08.898: INFO: ibm-master-proxy-static-10.241.0.102 from kube-system started at 2022-04-06 01:20:23 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container pause ready: true, restart count 0
Apr  6 03:47:08.898: INFO: ibm-vpc-block-csi-node-nk2wg from kube-system started at 2022-04-06 01:20:29 +0000 UTC (4 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 03:47:08.898: INFO: vpn-849cbbd4f5-zbslr from kube-system started at 2022-04-06 01:31:19 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container vpn ready: true, restart count 0
Apr  6 03:47:08.898: INFO: tuned-r57x6 from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container tuned ready: true, restart count 0
Apr  6 03:47:08.898: INFO: csi-snapshot-controller-7ffc756fcb-j85jv from openshift-cluster-storage-operator started at 2022-04-06 01:36:18 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container snapshot-controller ready: true, restart count 0
Apr  6 03:47:08.898: INFO: csi-snapshot-webhook-574644677c-gkf4n from openshift-cluster-storage-operator started at 2022-04-06 01:36:15 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container webhook ready: true, restart count 0
Apr  6 03:47:08.898: INFO: downloads-dbb5d5764-6ldtc from openshift-console started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container download-server ready: true, restart count 0
Apr  6 03:47:08.898: INFO: dns-default-zvb7x from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container dns ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: node-resolver-9gtvm from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 03:47:08.898: INFO: image-registry-54fc9d45f8-c8xl9 from openshift-image-registry started at 2022-04-06 01:38:16 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container registry ready: true, restart count 0
Apr  6 03:47:08.898: INFO: node-ca-w42rx from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 03:47:08.898: INFO: ingress-canary-7dgmf from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 03:47:08.898: INFO: router-default-79f7bb79b4-txbvm from openshift-ingress started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container router ready: true, restart count 0
Apr  6 03:47:08.898: INFO: openshift-kube-proxy-9c8jc from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: migrator-c84bfd698-4gmq8 from openshift-kube-storage-version-migrator started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container migrator ready: true, restart count 0
Apr  6 03:47:08.898: INFO: certified-operators-k8bbt from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:47:08.898: INFO: redhat-marketplace-dhghg from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:47:08.898: INFO: redhat-operators-7266t from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 03:47:08.898: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: grafana-65d7ff4ff4-pjvw7 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container grafana ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: kube-state-metrics-c4c8f95d8-s5wlt from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (3 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 03:47:08.898: INFO: node-exporter-nnrpz from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 03:47:08.898: INFO: prometheus-adapter-54744554d8-v5q6n from openshift-monitoring started at 2022-04-06 01:39:52 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr  6 03:47:08.898: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-04-06 01:39:00 +0000 UTC (7 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container prometheus ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr  6 03:47:08.898: INFO: prometheus-operator-785898db99-xvm9b from openshift-monitoring started at 2022-04-06 01:36:07 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr  6 03:47:08.898: INFO: telemeter-client-bdc7d9995-gfbw6 from openshift-monitoring started at 2022-04-06 01:37:33 +0000 UTC (3 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container reload ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container telemeter-client ready: true, restart count 0
Apr  6 03:47:08.898: INFO: thanos-querier-6797965cb7-6fjfp from openshift-monitoring started at 2022-04-06 01:38:55 +0000 UTC (5 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container thanos-query ready: true, restart count 0
Apr  6 03:47:08.898: INFO: multus-additional-cni-plugins-v2thd from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 03:47:08.898: INFO: multus-admission-controller-n2fzx from openshift-multus started at 2022-04-06 01:25:10 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 03:47:08.898: INFO: multus-mhr5m from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 03:47:08.898: INFO: network-metrics-daemon-qt77t from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 03:47:08.898: INFO: network-check-target-m4pg9 from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 03:47:08.898: INFO: collect-profiles-27486930--1-4mwfp from openshift-operator-lifecycle-manager started at 2022-04-06 03:30:00 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 03:47:08.898: INFO: packageserver-77bb6bdcd6-2sng2 from openshift-operator-lifecycle-manager started at 2022-04-06 01:35:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container packageserver ready: true, restart count 0
Apr  6 03:47:08.898: INFO: service-ca-c77965566-b6wft from openshift-service-ca started at 2022-04-06 01:33:31 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container service-ca-controller ready: true, restart count 0
Apr  6 03:47:08.898: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-84flv from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 03:47:08.898: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 03:47:08.898: INFO: tigera-operator-5d4d8f956c-2mlcb from tigera-operator started at 2022-04-06 01:21:14 +0000 UTC (1 container statuses recorded)
Apr  6 03:47:08.898: INFO: 	Container tigera-operator ready: true, restart count 3
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-733228d3-e7d7-42e9-8c60-909abddcead7 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-733228d3-e7d7-42e9-8c60-909abddcead7 off the node 10.241.0.102
STEP: verifying the node doesn't have the label kubernetes.io/e2e-733228d3-e7d7-42e9-8c60-909abddcead7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:13.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7782" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":180,"skipped":2994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:13.219: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Apr  6 03:47:13.459: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6449  5248160d-4b37-4379-a994-6a3026fec695 93556 0 2022-04-06 03:47:13 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-04-06 03:47:13 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq9dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq9dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 03:47:13.472: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:47:15.492: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:47:17.490: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Apr  6 03:47:17.490: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6449 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:47:17.490: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Verifying customized DNS server is configured on pod...
Apr  6 03:47:17.680: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6449 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:47:17.680: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:47:17.813: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:17.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6449" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":181,"skipped":3016,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:17.890: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:47:18.052: INFO: Creating simple deployment test-new-deployment
Apr  6 03:47:18.098: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr  6 03:47:20.215: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8953  c503d841-881f-415a-bfa1-d712f98bae85 93724 3 2022-04-06 03:47:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-04-06 03:47:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:47:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027fc7d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-04-06 03:47:19 +0000 UTC,LastTransitionTime:2022-04-06 03:47:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-04-06 03:47:19 +0000 UTC,LastTransitionTime:2022-04-06 03:47:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 03:47:20.225: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-8953  6731fc4a-b6a1-4095-bdeb-337c43642784 93727 3 2022-04-06 03:47:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c503d841-881f-415a-bfa1-d712f98bae85 0xc003b72777 0xc003b72778}] []  [{kube-controller-manager Update apps/v1 2022-04-06 03:47:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c503d841-881f-415a-bfa1-d712f98bae85\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:47:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b72898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 03:47:20.307: INFO: Pod "test-new-deployment-847dcfb7fb-m7wst" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-m7wst test-new-deployment-847dcfb7fb- deployment-8953  0c4f83f2-733e-4da9-a2a6-b85bb98349f4 93728 0 2022-04-06 03:47:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 6731fc4a-b6a1-4095-bdeb-337c43642784 0xc003b72f27 0xc003b72f28}] []  [{kube-controller-manager Update v1 2022-04-06 03:47:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6731fc4a-b6a1-4095-bdeb-337c43642784\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47mkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47mkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-brsgb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:47:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 03:47:20.307: INFO: Pod "test-new-deployment-847dcfb7fb-x458v" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-x458v test-new-deployment-847dcfb7fb- deployment-8953  d88d3421-235a-452b-b01d-90b22252f755 93717 0 2022-04-06 03:47:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:6fc3ba7b18bbbcc146c91c71bffa15ce02ded503eba70023b96079038dbf8811 cni.projectcalico.org/podIP:172.17.95.176/32 cni.projectcalico.org/podIPs:172.17.95.176/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.176"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.95.176"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 6731fc4a-b6a1-4095-bdeb-337c43642784 0xc003b73237 0xc003b73238}] []  [{calico Update v1 2022-04-06 03:47:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-06 03:47:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6731fc4a-b6a1-4095-bdeb-337c43642784\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-04-06 03:47:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.95.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-04-06 03:47:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgc8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgc8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.101,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-brsgb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:47:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:47:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:47:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.101,PodIP:172.17.95.176,StartTime:2022-04-06 03:47:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 03:47:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://8461791cdf5a41f794a53b45486c2eba43f5680e868c39f90dbaa7b6b4e09379,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.95.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:20.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8953" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":182,"skipped":3031,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:20.335: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Apr  6 03:47:20.571: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:25.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7428" for this suite.

• [SLOW TEST:5.436 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":183,"skipped":3046,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:25.771: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3320
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  6 03:47:26.048: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 03:47:26.248: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:47:28.260: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:30.262: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:32.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:34.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:36.262: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:38.266: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:40.265: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:42.266: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:44.261: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:47:46.261: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr  6 03:47:46.289: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr  6 03:47:46.322: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr  6 03:47:50.532: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr  6 03:47:50.532: INFO: Going to poll 172.17.77.56 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr  6 03:47:50.542: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.77.56 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3320 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:47:50.542: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:47:51.969: INFO: Found all 1 expected endpoints: [netserver-0]
Apr  6 03:47:51.969: INFO: Going to poll 172.17.95.177 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr  6 03:47:51.982: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.95.177 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3320 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:47:51.982: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:47:53.149: INFO: Found all 1 expected endpoints: [netserver-1]
Apr  6 03:47:53.149: INFO: Going to poll 172.17.96.99 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr  6 03:47:53.163: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.96.99 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3320 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:47:53.163: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:47:54.342: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:54.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3320" for this suite.

• [SLOW TEST:28.609 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3062,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:54.381: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:47:54.578: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-cdc0debb-3e9d-42e2-b4f2-b5fa16966838" in namespace "security-context-test-356" to be "Succeeded or Failed"
Apr  6 03:47:54.588: INFO: Pod "busybox-readonly-false-cdc0debb-3e9d-42e2-b4f2-b5fa16966838": Phase="Pending", Reason="", readiness=false. Elapsed: 10.312992ms
Apr  6 03:47:56.606: INFO: Pod "busybox-readonly-false-cdc0debb-3e9d-42e2-b4f2-b5fa16966838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027967433s
Apr  6 03:47:56.606: INFO: Pod "busybox-readonly-false-cdc0debb-3e9d-42e2-b4f2-b5fa16966838" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:47:56.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-356" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:47:56.636: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Apr  6 03:47:56.794: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 03:48:56.998: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:48:57.017: INFO: Starting informer...
STEP: Starting pod...
Apr  6 03:48:57.286: INFO: Pod is running on 10.241.0.102. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Apr  6 03:48:57.334: INFO: Pod wasn't evicted. Proceeding
Apr  6 03:48:57.334: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Apr  6 03:50:12.378: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:50:12.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-223" for this suite.

• [SLOW TEST:135.782 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":186,"skipped":3099,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:50:12.418: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Apr  6 03:50:12.580: INFO: namespace kubectl-3085
Apr  6 03:50:12.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-3085 create -f -'
Apr  6 03:50:13.269: INFO: stderr: ""
Apr  6 03:50:13.269: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr  6 03:50:14.284: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 03:50:14.284: INFO: Found 0 / 1
Apr  6 03:50:15.285: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 03:50:15.285: INFO: Found 0 / 1
Apr  6 03:50:16.289: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 03:50:16.289: INFO: Found 1 / 1
Apr  6 03:50:16.290: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  6 03:50:16.301: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 03:50:16.301: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 03:50:16.301: INFO: wait on agnhost-primary startup in kubectl-3085 
Apr  6 03:50:16.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-3085 logs agnhost-primary-28264 agnhost-primary'
Apr  6 03:50:16.430: INFO: stderr: ""
Apr  6 03:50:16.430: INFO: stdout: "Paused\n"
STEP: exposing RC
Apr  6 03:50:16.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-3085 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr  6 03:50:16.555: INFO: stderr: ""
Apr  6 03:50:16.555: INFO: stdout: "service/rm2 exposed\n"
Apr  6 03:50:16.565: INFO: Service rm2 in namespace kubectl-3085 found.
STEP: exposing service
Apr  6 03:50:18.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-3085 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr  6 03:50:18.699: INFO: stderr: ""
Apr  6 03:50:18.699: INFO: stdout: "service/rm3 exposed\n"
Apr  6 03:50:18.709: INFO: Service rm3 in namespace kubectl-3085 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:50:20.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3085" for this suite.

• [SLOW TEST:8.343 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":187,"skipped":3103,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:50:20.761: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-bnn6
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 03:50:21.118: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bnn6" in namespace "subpath-4764" to be "Succeeded or Failed"
Apr  6 03:50:21.140: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.270632ms
Apr  6 03:50:23.152: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034275508s
Apr  6 03:50:25.171: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 4.052834165s
Apr  6 03:50:27.205: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 6.086751904s
Apr  6 03:50:29.220: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 8.102482539s
Apr  6 03:50:31.234: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 10.116472778s
Apr  6 03:50:33.250: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 12.131894754s
Apr  6 03:50:35.266: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 14.148173745s
Apr  6 03:50:37.282: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 16.164185416s
Apr  6 03:50:39.300: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 18.182225659s
Apr  6 03:50:41.315: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 20.196956809s
Apr  6 03:50:43.332: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Running", Reason="", readiness=true. Elapsed: 22.214435371s
Apr  6 03:50:45.351: INFO: Pod "pod-subpath-test-projected-bnn6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.232717s
STEP: Saw pod success
Apr  6 03:50:45.351: INFO: Pod "pod-subpath-test-projected-bnn6" satisfied condition "Succeeded or Failed"
Apr  6 03:50:45.362: INFO: Trying to get logs from node 10.241.0.102 pod pod-subpath-test-projected-bnn6 container test-container-subpath-projected-bnn6: <nil>
STEP: delete the pod
Apr  6 03:50:45.438: INFO: Waiting for pod pod-subpath-test-projected-bnn6 to disappear
Apr  6 03:50:45.458: INFO: Pod pod-subpath-test-projected-bnn6 no longer exists
STEP: Deleting pod pod-subpath-test-projected-bnn6
Apr  6 03:50:45.458: INFO: Deleting pod "pod-subpath-test-projected-bnn6" in namespace "subpath-4764"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:50:45.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4764" for this suite.

• [SLOW TEST:24.742 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":188,"skipped":3120,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:50:45.503: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr  6 03:50:46.851: INFO: Pod name wrapped-volume-race-465c90d7-7829-4fe8-8dc3-7ed959e09647: Found 0 pods out of 5
Apr  6 03:50:51.875: INFO: Pod name wrapped-volume-race-465c90d7-7829-4fe8-8dc3-7ed959e09647: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-465c90d7-7829-4fe8-8dc3-7ed959e09647 in namespace emptydir-wrapper-2244, will wait for the garbage collector to delete the pods
Apr  6 03:50:52.087: INFO: Deleting ReplicationController wrapped-volume-race-465c90d7-7829-4fe8-8dc3-7ed959e09647 took: 31.649543ms
Apr  6 03:50:52.488: INFO: Terminating ReplicationController wrapped-volume-race-465c90d7-7829-4fe8-8dc3-7ed959e09647 pods took: 400.688691ms
STEP: Creating RC which spawns configmap-volume pods
Apr  6 03:50:56.152: INFO: Pod name wrapped-volume-race-cd7383b6-f290-4882-9182-ad475cf88258: Found 0 pods out of 5
Apr  6 03:51:01.195: INFO: Pod name wrapped-volume-race-cd7383b6-f290-4882-9182-ad475cf88258: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cd7383b6-f290-4882-9182-ad475cf88258 in namespace emptydir-wrapper-2244, will wait for the garbage collector to delete the pods
Apr  6 03:51:01.358: INFO: Deleting ReplicationController wrapped-volume-race-cd7383b6-f290-4882-9182-ad475cf88258 took: 24.531019ms
Apr  6 03:51:01.460: INFO: Terminating ReplicationController wrapped-volume-race-cd7383b6-f290-4882-9182-ad475cf88258 pods took: 101.331545ms
STEP: Creating RC which spawns configmap-volume pods
Apr  6 03:51:04.321: INFO: Pod name wrapped-volume-race-7c5dae2a-26ac-48ee-b776-044673222ad3: Found 0 pods out of 5
Apr  6 03:51:09.346: INFO: Pod name wrapped-volume-race-7c5dae2a-26ac-48ee-b776-044673222ad3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7c5dae2a-26ac-48ee-b776-044673222ad3 in namespace emptydir-wrapper-2244, will wait for the garbage collector to delete the pods
Apr  6 03:51:09.513: INFO: Deleting ReplicationController wrapped-volume-race-7c5dae2a-26ac-48ee-b776-044673222ad3 took: 30.786204ms
Apr  6 03:51:09.614: INFO: Terminating ReplicationController wrapped-volume-race-7c5dae2a-26ac-48ee-b776-044673222ad3 pods took: 100.732067ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:51:14.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2244" for this suite.

• [SLOW TEST:29.462 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":189,"skipped":3121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:51:14.966: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-453ce3c7-59d1-4d9b-836d-11525a15ab8c
STEP: Creating secret with name secret-projected-all-test-volume-d082f057-cb88-437a-9d50-a33fba947db4
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr  6 03:51:15.227: INFO: Waiting up to 5m0s for pod "projected-volume-2b2c7a65-3047-43cd-9abc-bbf4747ec190" in namespace "projected-575" to be "Succeeded or Failed"
Apr  6 03:51:15.241: INFO: Pod "projected-volume-2b2c7a65-3047-43cd-9abc-bbf4747ec190": Phase="Pending", Reason="", readiness=false. Elapsed: 14.257367ms
Apr  6 03:51:17.256: INFO: Pod "projected-volume-2b2c7a65-3047-43cd-9abc-bbf4747ec190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02906681s
STEP: Saw pod success
Apr  6 03:51:17.256: INFO: Pod "projected-volume-2b2c7a65-3047-43cd-9abc-bbf4747ec190" satisfied condition "Succeeded or Failed"
Apr  6 03:51:17.269: INFO: Trying to get logs from node 10.241.0.102 pod projected-volume-2b2c7a65-3047-43cd-9abc-bbf4747ec190 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr  6 03:51:17.355: INFO: Waiting for pod projected-volume-2b2c7a65-3047-43cd-9abc-bbf4747ec190 to disappear
Apr  6 03:51:17.369: INFO: Pod projected-volume-2b2c7a65-3047-43cd-9abc-bbf4747ec190 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:51:17.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-575" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:51:17.401: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Apr  6 03:53:18.339: INFO: Successfully updated pod "var-expansion-db340886-4ebc-4f3d-aea8-b7e0adfdbfdf"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Apr  6 03:53:20.365: INFO: Deleting pod "var-expansion-db340886-4ebc-4f3d-aea8-b7e0adfdbfdf" in namespace "var-expansion-351"
Apr  6 03:53:20.396: INFO: Wait up to 5m0s for pod "var-expansion-db340886-4ebc-4f3d-aea8-b7e0adfdbfdf" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:53:52.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-351" for this suite.

• [SLOW TEST:155.048 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":191,"skipped":3176,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:53:52.449: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr  6 03:53:52.706: INFO: Waiting up to 5m0s for pod "downward-api-12039cc4-68c5-4f78-b3c1-527254fae07c" in namespace "downward-api-173" to be "Succeeded or Failed"
Apr  6 03:53:52.718: INFO: Pod "downward-api-12039cc4-68c5-4f78-b3c1-527254fae07c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.430261ms
Apr  6 03:53:54.737: INFO: Pod "downward-api-12039cc4-68c5-4f78-b3c1-527254fae07c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030455237s
STEP: Saw pod success
Apr  6 03:53:54.737: INFO: Pod "downward-api-12039cc4-68c5-4f78-b3c1-527254fae07c" satisfied condition "Succeeded or Failed"
Apr  6 03:53:54.748: INFO: Trying to get logs from node 10.241.0.102 pod downward-api-12039cc4-68c5-4f78-b3c1-527254fae07c container dapi-container: <nil>
STEP: delete the pod
Apr  6 03:53:54.825: INFO: Waiting for pod downward-api-12039cc4-68c5-4f78-b3c1-527254fae07c to disappear
Apr  6 03:53:54.838: INFO: Pod downward-api-12039cc4-68c5-4f78-b3c1-527254fae07c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:53:54.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-173" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:53:54.868: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Apr  6 03:53:55.110: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  6 03:54:00.124: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Apr  6 03:54:00.133: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Apr  6 03:54:00.155: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Apr  6 03:54:00.159: INFO: Observed &ReplicaSet event: ADDED
Apr  6 03:54:00.159: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 03:54:00.159: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 03:54:00.159: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 03:54:00.159: INFO: Found replicaset test-rs in namespace replicaset-1026 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr  6 03:54:00.159: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Apr  6 03:54:00.159: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr  6 03:54:00.192: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Apr  6 03:54:00.197: INFO: Observed &ReplicaSet event: ADDED
Apr  6 03:54:00.197: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 03:54:00.197: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 03:54:00.198: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 03:54:00.198: INFO: Observed replicaset test-rs in namespace replicaset-1026 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr  6 03:54:00.198: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 03:54:00.198: INFO: Found replicaset test-rs in namespace replicaset-1026 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Apr  6 03:54:00.198: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:54:00.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1026" for this suite.

• [SLOW TEST:5.358 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":193,"skipped":3224,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:54:00.228: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Apr  6 03:54:00.439: INFO: Waiting up to 5m0s for pod "pod-652b606b-f6fd-4863-a283-71c99960ee2f" in namespace "emptydir-3776" to be "Succeeded or Failed"
Apr  6 03:54:00.468: INFO: Pod "pod-652b606b-f6fd-4863-a283-71c99960ee2f": Phase="Pending", Reason="", readiness=false. Elapsed: 29.370488ms
Apr  6 03:54:02.501: INFO: Pod "pod-652b606b-f6fd-4863-a283-71c99960ee2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061740943s
Apr  6 03:54:04.518: INFO: Pod "pod-652b606b-f6fd-4863-a283-71c99960ee2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078883041s
STEP: Saw pod success
Apr  6 03:54:04.518: INFO: Pod "pod-652b606b-f6fd-4863-a283-71c99960ee2f" satisfied condition "Succeeded or Failed"
Apr  6 03:54:04.531: INFO: Trying to get logs from node 10.241.0.102 pod pod-652b606b-f6fd-4863-a283-71c99960ee2f container test-container: <nil>
STEP: delete the pod
Apr  6 03:54:04.608: INFO: Waiting for pod pod-652b606b-f6fd-4863-a283-71c99960ee2f to disappear
Apr  6 03:54:04.619: INFO: Pod pod-652b606b-f6fd-4863-a283-71c99960ee2f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:54:04.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3776" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3232,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:54:04.647: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr  6 03:54:04.916: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  ee06a030-98be-4308-8de3-0d903dacd06e 99001 0 2022-04-06 03:54:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-06 03:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 03:54:04.916: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  ee06a030-98be-4308-8de3-0d903dacd06e 99005 0 2022-04-06 03:54:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-06 03:54:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 03:54:04.916: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  ee06a030-98be-4308-8de3-0d903dacd06e 99006 0 2022-04-06 03:54:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-06 03:54:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr  6 03:54:15.043: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  ee06a030-98be-4308-8de3-0d903dacd06e 99121 0 2022-04-06 03:54:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-06 03:54:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 03:54:15.043: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  ee06a030-98be-4308-8de3-0d903dacd06e 99122 0 2022-04-06 03:54:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-06 03:54:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 03:54:15.043: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  ee06a030-98be-4308-8de3-0d903dacd06e 99123 0 2022-04-06 03:54:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-06 03:54:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:54:15.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2119" for this suite.

• [SLOW TEST:10.425 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":195,"skipped":3237,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:54:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-lvsj
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 03:54:15.368: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lvsj" in namespace "subpath-7382" to be "Succeeded or Failed"
Apr  6 03:54:15.382: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Pending", Reason="", readiness=false. Elapsed: 13.958347ms
Apr  6 03:54:17.402: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034365704s
Apr  6 03:54:19.421: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 4.052783285s
Apr  6 03:54:21.437: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 6.068973672s
Apr  6 03:54:23.451: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 8.082614449s
Apr  6 03:54:25.469: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 10.100705304s
Apr  6 03:54:27.489: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 12.121079241s
Apr  6 03:54:29.504: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 14.135621327s
Apr  6 03:54:31.525: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 16.156686844s
Apr  6 03:54:33.541: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 18.172427707s
Apr  6 03:54:35.558: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 20.189472377s
Apr  6 03:54:37.591: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Running", Reason="", readiness=true. Elapsed: 22.223087635s
Apr  6 03:54:39.603: INFO: Pod "pod-subpath-test-configmap-lvsj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.234581936s
STEP: Saw pod success
Apr  6 03:54:39.603: INFO: Pod "pod-subpath-test-configmap-lvsj" satisfied condition "Succeeded or Failed"
Apr  6 03:54:39.617: INFO: Trying to get logs from node 10.241.0.102 pod pod-subpath-test-configmap-lvsj container test-container-subpath-configmap-lvsj: <nil>
STEP: delete the pod
Apr  6 03:54:39.722: INFO: Waiting for pod pod-subpath-test-configmap-lvsj to disappear
Apr  6 03:54:39.735: INFO: Pod pod-subpath-test-configmap-lvsj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lvsj
Apr  6 03:54:39.735: INFO: Deleting pod "pod-subpath-test-configmap-lvsj" in namespace "subpath-7382"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:54:39.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7382" for this suite.

• [SLOW TEST:24.733 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":196,"skipped":3257,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:54:39.806: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:54:45.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4501" for this suite.

• [SLOW TEST:5.426 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":197,"skipped":3262,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:54:45.232: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:54:45.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 03:54:47.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814085, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814085, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814085, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814085, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:54:50.869: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:54:50.879: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6354-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:54:54.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2154" for this suite.
STEP: Destroying namespace "webhook-2154-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.097 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":198,"skipped":3274,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:54:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-5641ebe4-8732-4859-9cc4-d864aa5f8917
STEP: Creating a pod to test consume secrets
Apr  6 03:54:54.533: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-57de1897-4176-41a9-b8ee-59be102798a5" in namespace "projected-1812" to be "Succeeded or Failed"
Apr  6 03:54:54.545: INFO: Pod "pod-projected-secrets-57de1897-4176-41a9-b8ee-59be102798a5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.46974ms
Apr  6 03:54:56.557: INFO: Pod "pod-projected-secrets-57de1897-4176-41a9-b8ee-59be102798a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023539646s
Apr  6 03:54:58.573: INFO: Pod "pod-projected-secrets-57de1897-4176-41a9-b8ee-59be102798a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040168403s
STEP: Saw pod success
Apr  6 03:54:58.573: INFO: Pod "pod-projected-secrets-57de1897-4176-41a9-b8ee-59be102798a5" satisfied condition "Succeeded or Failed"
Apr  6 03:54:58.584: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-secrets-57de1897-4176-41a9-b8ee-59be102798a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 03:54:58.638: INFO: Waiting for pod pod-projected-secrets-57de1897-4176-41a9-b8ee-59be102798a5 to disappear
Apr  6 03:54:58.651: INFO: Pod pod-projected-secrets-57de1897-4176-41a9-b8ee-59be102798a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:54:58.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1812" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3287,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:54:58.679: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:54:58.916: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-853e9ddd-c185-46be-8bcc-dbee727111b8" in namespace "security-context-test-436" to be "Succeeded or Failed"
Apr  6 03:54:58.930: INFO: Pod "busybox-privileged-false-853e9ddd-c185-46be-8bcc-dbee727111b8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.85386ms
Apr  6 03:55:00.943: INFO: Pod "busybox-privileged-false-853e9ddd-c185-46be-8bcc-dbee727111b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027160883s
Apr  6 03:55:00.943: INFO: Pod "busybox-privileged-false-853e9ddd-c185-46be-8bcc-dbee727111b8" satisfied condition "Succeeded or Failed"
Apr  6 03:55:00.964: INFO: Got logs for pod "busybox-privileged-false-853e9ddd-c185-46be-8bcc-dbee727111b8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:55:00.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-436" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3296,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:55:00.989: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:55:01.200: INFO: The status of Pod busybox-readonly-fsd8f399fb-c3ff-4ef8-a694-2a8d4b4c6ee1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:55:03.216: INFO: The status of Pod busybox-readonly-fsd8f399fb-c3ff-4ef8-a694-2a8d4b4c6ee1 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:55:03.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2169" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3301,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:55:03.314: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Apr  6 03:55:07.811: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3 PodName:var-expansion-ae8bbedc-569d-49f3-963d-df2cd840293f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:55:07.811: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: test for file in mounted path
Apr  6 03:55:07.979: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3 PodName:var-expansion-ae8bbedc-569d-49f3-963d-df2cd840293f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:55:07.979: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: updating the annotation value
Apr  6 03:55:08.735: INFO: Successfully updated pod "var-expansion-ae8bbedc-569d-49f3-963d-df2cd840293f"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Apr  6 03:55:08.747: INFO: Deleting pod "var-expansion-ae8bbedc-569d-49f3-963d-df2cd840293f" in namespace "var-expansion-3"
Apr  6 03:55:08.795: INFO: Wait up to 5m0s for pod "var-expansion-ae8bbedc-569d-49f3-963d-df2cd840293f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:55:40.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3" for this suite.

• [SLOW TEST:37.536 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":202,"skipped":3312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:55:40.851: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:55:41.508: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr  6 03:55:43.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814141, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814141, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814141, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814141, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:55:46.604: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:55:47.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1823" for this suite.
STEP: Destroying namespace "webhook-1823-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.616 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":203,"skipped":3363,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:55:47.466: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Apr  6 03:55:47.708: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:55:49.722: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:55:51.732: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Apr  6 03:55:51.797: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:55:53.815: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:55:55.814: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Apr  6 03:55:55.847: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 03:55:55.858: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 03:55:57.859: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 03:55:57.889: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 03:55:59.859: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 03:55:59.876: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:55:59.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8980" for this suite.

• [SLOW TEST:12.531 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:55:59.998: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6944
STEP: creating service affinity-nodeport-transition in namespace services-6944
STEP: creating replication controller affinity-nodeport-transition in namespace services-6944
I0406 03:56:00.333620      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-6944, replica count: 3
I0406 03:56:03.384965      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 03:56:03.417: INFO: Creating new exec pod
Apr  6 03:56:06.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6944 exec execpod-affinityjhf8n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Apr  6 03:56:06.854: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr  6 03:56:06.854: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:56:06.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6944 exec execpod-affinityjhf8n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.62.179 80'
Apr  6 03:56:07.164: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.62.179 80\nConnection to 172.21.62.179 80 port [tcp/http] succeeded!\n"
Apr  6 03:56:07.164: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:56:07.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6944 exec execpod-affinityjhf8n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.100 30436'
Apr  6 03:56:07.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.100 30436\nConnection to 10.241.0.100 30436 port [tcp/*] succeeded!\n"
Apr  6 03:56:07.440: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:56:07.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6944 exec execpod-affinityjhf8n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.241.0.102 30436'
Apr  6 03:56:07.709: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.241.0.102 30436\nConnection to 10.241.0.102 30436 port [tcp/*] succeeded!\n"
Apr  6 03:56:07.710: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 03:56:07.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6944 exec execpod-affinityjhf8n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.241.0.100:30436/ ; done'
Apr  6 03:56:08.077: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n"
Apr  6 03:56:08.077: INFO: stdout: "\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-vxnrb\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-vxnrb\naffinity-nodeport-transition-vxnrb\naffinity-nodeport-transition-cfphc\naffinity-nodeport-transition-vxnrb\naffinity-nodeport-transition-vxnrb\naffinity-nodeport-transition-cfphc\naffinity-nodeport-transition-vxnrb\naffinity-nodeport-transition-cfphc\naffinity-nodeport-transition-vxnrb\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp"
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-vxnrb
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-vxnrb
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-vxnrb
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-cfphc
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-vxnrb
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-vxnrb
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-cfphc
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-vxnrb
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-cfphc
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-vxnrb
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.077: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-6944 exec execpod-affinityjhf8n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.241.0.100:30436/ ; done'
Apr  6 03:56:08.435: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.241.0.100:30436/\n"
Apr  6 03:56:08.436: INFO: stdout: "\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp\naffinity-nodeport-transition-jm8zp"
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Received response from host: affinity-nodeport-transition-jm8zp
Apr  6 03:56:08.436: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6944, will wait for the garbage collector to delete the pods
Apr  6 03:56:08.557: INFO: Deleting ReplicationController affinity-nodeport-transition took: 22.253536ms
Apr  6 03:56:08.657: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.194793ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:56:11.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6944" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.750 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":205,"skipped":3423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:56:11.749: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 03:56:12.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 03:56:15.658: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:56:15.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5681" for this suite.
STEP: Destroying namespace "webhook-5681-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":206,"skipped":3446,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:56:15.840: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr  6 03:56:16.078: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 03:57:16.291: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:16.325: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:57:16.594: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Apr  6 03:57:16.609: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:16.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8519" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:16.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2185" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.105 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":207,"skipped":3455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:16.945: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Apr  6 03:57:17.167: INFO: Waiting up to 5m0s for pod "var-expansion-017d413d-811a-48e4-93ad-2e5923a18a04" in namespace "var-expansion-9681" to be "Succeeded or Failed"
Apr  6 03:57:17.179: INFO: Pod "var-expansion-017d413d-811a-48e4-93ad-2e5923a18a04": Phase="Pending", Reason="", readiness=false. Elapsed: 12.110395ms
Apr  6 03:57:19.194: INFO: Pod "var-expansion-017d413d-811a-48e4-93ad-2e5923a18a04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027324275s
Apr  6 03:57:21.210: INFO: Pod "var-expansion-017d413d-811a-48e4-93ad-2e5923a18a04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043361134s
STEP: Saw pod success
Apr  6 03:57:21.210: INFO: Pod "var-expansion-017d413d-811a-48e4-93ad-2e5923a18a04" satisfied condition "Succeeded or Failed"
Apr  6 03:57:21.221: INFO: Trying to get logs from node 10.241.0.102 pod var-expansion-017d413d-811a-48e4-93ad-2e5923a18a04 container dapi-container: <nil>
STEP: delete the pod
Apr  6 03:57:21.296: INFO: Waiting for pod var-expansion-017d413d-811a-48e4-93ad-2e5923a18a04 to disappear
Apr  6 03:57:21.310: INFO: Pod var-expansion-017d413d-811a-48e4-93ad-2e5923a18a04 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:21.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9681" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":208,"skipped":3490,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:21.342: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:57:21.593: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eebcf80b-dfd7-4149-adfa-31152891d14e" in namespace "projected-7082" to be "Succeeded or Failed"
Apr  6 03:57:21.627: INFO: Pod "downwardapi-volume-eebcf80b-dfd7-4149-adfa-31152891d14e": Phase="Pending", Reason="", readiness=false. Elapsed: 33.86963ms
Apr  6 03:57:23.639: INFO: Pod "downwardapi-volume-eebcf80b-dfd7-4149-adfa-31152891d14e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045792003s
STEP: Saw pod success
Apr  6 03:57:23.639: INFO: Pod "downwardapi-volume-eebcf80b-dfd7-4149-adfa-31152891d14e" satisfied condition "Succeeded or Failed"
Apr  6 03:57:23.652: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-eebcf80b-dfd7-4149-adfa-31152891d14e container client-container: <nil>
STEP: delete the pod
Apr  6 03:57:23.711: INFO: Waiting for pod downwardapi-volume-eebcf80b-dfd7-4149-adfa-31152891d14e to disappear
Apr  6 03:57:23.737: INFO: Pod downwardapi-volume-eebcf80b-dfd7-4149-adfa-31152891d14e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:23.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7082" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":3494,"failed":0}

------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Apr  6 03:57:24.071: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:24.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3284" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":210,"skipped":3494,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:24.231: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Apr  6 03:57:24.510: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  6 03:57:29.524: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:29.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9765" for this suite.

• [SLOW TEST:5.391 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":211,"skipped":3499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr  6 03:57:31.930: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:31.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4558" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":212,"skipped":3531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:32.030: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 03:57:32.254: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f79b4038-55d1-459b-b213-94e2fc57d14e" in namespace "downward-api-1321" to be "Succeeded or Failed"
Apr  6 03:57:32.274: INFO: Pod "downwardapi-volume-f79b4038-55d1-459b-b213-94e2fc57d14e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.43728ms
Apr  6 03:57:34.291: INFO: Pod "downwardapi-volume-f79b4038-55d1-459b-b213-94e2fc57d14e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037508828s
Apr  6 03:57:36.304: INFO: Pod "downwardapi-volume-f79b4038-55d1-459b-b213-94e2fc57d14e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050014332s
STEP: Saw pod success
Apr  6 03:57:36.304: INFO: Pod "downwardapi-volume-f79b4038-55d1-459b-b213-94e2fc57d14e" satisfied condition "Succeeded or Failed"
Apr  6 03:57:36.314: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-f79b4038-55d1-459b-b213-94e2fc57d14e container client-container: <nil>
STEP: delete the pod
Apr  6 03:57:36.360: INFO: Waiting for pod downwardapi-volume-f79b4038-55d1-459b-b213-94e2fc57d14e to disappear
Apr  6 03:57:36.370: INFO: Pod downwardapi-volume-f79b4038-55d1-459b-b213-94e2fc57d14e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:36.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1321" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":3555,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:36.396: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr  6 03:57:36.649: INFO: Waiting up to 5m0s for pod "pod-427bd173-4353-453a-825a-157e4ffdfc6f" in namespace "emptydir-6322" to be "Succeeded or Failed"
Apr  6 03:57:36.660: INFO: Pod "pod-427bd173-4353-453a-825a-157e4ffdfc6f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.425478ms
Apr  6 03:57:38.674: INFO: Pod "pod-427bd173-4353-453a-825a-157e4ffdfc6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02565639s
STEP: Saw pod success
Apr  6 03:57:38.674: INFO: Pod "pod-427bd173-4353-453a-825a-157e4ffdfc6f" satisfied condition "Succeeded or Failed"
Apr  6 03:57:38.686: INFO: Trying to get logs from node 10.241.0.102 pod pod-427bd173-4353-453a-825a-157e4ffdfc6f container test-container: <nil>
STEP: delete the pod
Apr  6 03:57:38.755: INFO: Waiting for pod pod-427bd173-4353-453a-825a-157e4ffdfc6f to disappear
Apr  6 03:57:38.767: INFO: Pod pod-427bd173-4353-453a-825a-157e4ffdfc6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6322" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":3595,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:38.792: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Apr  6 03:57:38.999: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr  6 03:57:38.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 create -f -'
Apr  6 03:57:42.119: INFO: stderr: ""
Apr  6 03:57:42.119: INFO: stdout: "service/agnhost-replica created\n"
Apr  6 03:57:42.119: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr  6 03:57:42.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 create -f -'
Apr  6 03:57:45.558: INFO: stderr: ""
Apr  6 03:57:45.558: INFO: stdout: "service/agnhost-primary created\n"
Apr  6 03:57:45.559: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr  6 03:57:45.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 create -f -'
Apr  6 03:57:45.871: INFO: stderr: ""
Apr  6 03:57:45.871: INFO: stdout: "service/frontend created\n"
Apr  6 03:57:45.871: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr  6 03:57:45.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 create -f -'
Apr  6 03:57:46.158: INFO: stderr: ""
Apr  6 03:57:46.158: INFO: stdout: "deployment.apps/frontend created\n"
Apr  6 03:57:46.158: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr  6 03:57:46.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 create -f -'
Apr  6 03:57:46.443: INFO: stderr: ""
Apr  6 03:57:46.443: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr  6 03:57:46.443: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr  6 03:57:46.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 create -f -'
Apr  6 03:57:46.793: INFO: stderr: ""
Apr  6 03:57:46.793: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Apr  6 03:57:46.793: INFO: Waiting for all frontend pods to be Running.
Apr  6 03:57:51.846: INFO: Waiting for frontend to serve content.
Apr  6 03:57:51.943: INFO: Trying to add a new entry to the guestbook.
Apr  6 03:57:52.004: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr  6 03:57:52.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 delete --grace-period=0 --force -f -'
Apr  6 03:57:52.271: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 03:57:52.271: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 03:57:52.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 delete --grace-period=0 --force -f -'
Apr  6 03:57:52.410: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 03:57:52.410: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 03:57:52.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 delete --grace-period=0 --force -f -'
Apr  6 03:57:52.600: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 03:57:52.600: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 03:57:52.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 delete --grace-period=0 --force -f -'
Apr  6 03:57:52.700: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 03:57:52.700: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 03:57:52.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 delete --grace-period=0 --force -f -'
Apr  6 03:57:52.784: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 03:57:52.784: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 03:57:52.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-7895 delete --grace-period=0 --force -f -'
Apr  6 03:57:52.860: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 03:57:52.860: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:52.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7895" for this suite.

• [SLOW TEST:14.094 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":215,"skipped":3608,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:52.886: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Apr  6 03:57:57.155: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4857 PodName:pod-sharedvolume-7b794b07-2de1-4129-81a2-319205e83054 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:57:57.155: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:57:57.312: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:57:57.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4857" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":216,"skipped":3613,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:57:57.366: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 03:57:57.516: INFO: Creating deployment "test-recreate-deployment"
Apr  6 03:57:57.550: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr  6 03:57:57.568: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr  6 03:57:59.594: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr  6 03:57:59.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814277, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814277, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814277, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814277, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 03:58:01.615: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr  6 03:58:01.644: INFO: Updating deployment test-recreate-deployment
Apr  6 03:58:01.644: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr  6 03:58:01.863: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2268  b9cdbd9c-51e3-49e6-8fe1-46d196f21f27 103106 2 2022-04-06 03:57:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-04-06 03:58:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:58:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f5d928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-04-06 03:58:01 +0000 UTC,LastTransitionTime:2022-04-06 03:58:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-04-06 03:58:01 +0000 UTC,LastTransitionTime:2022-04-06 03:57:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr  6 03:58:01.873: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-2268  dba5abb6-14bd-4c3e-a803-9179ddf300a6 103104 1 2022-04-06 03:58:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b9cdbd9c-51e3-49e6-8fe1-46d196f21f27 0xc001f5de00 0xc001f5de01}] []  [{kube-controller-manager Update apps/v1 2022-04-06 03:58:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9cdbd9c-51e3-49e6-8fe1-46d196f21f27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:58:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f5de98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 03:58:01.873: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr  6 03:58:01.873: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-2268  d763c7b9-2faa-4e9b-b1e9-2335d6a73b1d 103094 2 2022-04-06 03:57:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b9cdbd9c-51e3-49e6-8fe1-46d196f21f27 0xc001f5dce7 0xc001f5dce8}] []  [{kube-controller-manager Update apps/v1 2022-04-06 03:57:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9cdbd9c-51e3-49e6-8fe1-46d196f21f27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 03:58:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f5dd98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 03:58:01.885: INFO: Pod "test-recreate-deployment-85d47dcb4-xhg27" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-xhg27 test-recreate-deployment-85d47dcb4- deployment-2268  6a381652-4cdc-4524-acbb-f8f457a15067 103109 0 2022-04-06 03:58:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 dba5abb6-14bd-4c3e-a803-9179ddf300a6 0xc003414317 0xc003414318}] []  [{kube-controller-manager Update v1 2022-04-06 03:58:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dba5abb6-14bd-4c3e-a803-9179ddf300a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-04-06 03:58:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpczv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpczv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c56,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-22fqr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:58:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:58:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:58:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 03:58:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.102,PodIP:,StartTime:2022-04-06 03:58:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:58:01.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2268" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":217,"skipped":3617,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:58:01.919: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8888
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  6 03:58:02.092: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 03:58:02.292: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:58:04.307: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:06.308: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:08.310: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:10.308: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:12.307: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:14.305: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:16.306: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:18.309: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:20.312: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 03:58:22.310: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr  6 03:58:22.337: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr  6 03:58:22.366: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr  6 03:58:24.461: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr  6 03:58:24.461: INFO: Breadth first check of 172.17.77.45 on host 10.241.0.100...
Apr  6 03:58:24.473: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.96.74:9080/dial?request=hostname&protocol=udp&host=172.17.77.45&port=8081&tries=1'] Namespace:pod-network-test-8888 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:58:24.473: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:58:24.606: INFO: Waiting for responses: map[]
Apr  6 03:58:24.606: INFO: reached 172.17.77.45 after 0/1 tries
Apr  6 03:58:24.606: INFO: Breadth first check of 172.17.95.190 on host 10.241.0.101...
Apr  6 03:58:24.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.96.74:9080/dial?request=hostname&protocol=udp&host=172.17.95.190&port=8081&tries=1'] Namespace:pod-network-test-8888 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:58:24.622: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:58:24.768: INFO: Waiting for responses: map[]
Apr  6 03:58:24.768: INFO: reached 172.17.95.190 after 0/1 tries
Apr  6 03:58:24.768: INFO: Breadth first check of 172.17.96.87 on host 10.241.0.102...
Apr  6 03:58:24.783: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.96.74:9080/dial?request=hostname&protocol=udp&host=172.17.96.87&port=8081&tries=1'] Namespace:pod-network-test-8888 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 03:58:24.783: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 03:58:24.954: INFO: Waiting for responses: map[]
Apr  6 03:58:24.954: INFO: reached 172.17.96.87 after 0/1 tries
Apr  6 03:58:24.954: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:58:24.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8888" for this suite.

• [SLOW TEST:23.071 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":3627,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:58:24.990: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:58:53.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2697" for this suite.

• [SLOW TEST:28.397 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":219,"skipped":3629,"failed":0}
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:58:53.386: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Apr  6 03:58:53.841: INFO: The status of Pod labelsupdatecf6ae478-0078-4951-98a7-4c80b8591ff5 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:58:55.862: INFO: The status of Pod labelsupdatecf6ae478-0078-4951-98a7-4c80b8591ff5 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 03:58:57.856: INFO: The status of Pod labelsupdatecf6ae478-0078-4951-98a7-4c80b8591ff5 is Running (Ready = true)
Apr  6 03:58:58.457: INFO: Successfully updated pod "labelsupdatecf6ae478-0078-4951-98a7-4c80b8591ff5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 03:59:00.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7480" for this suite.

• [SLOW TEST:7.148 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":220,"skipped":3629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 03:59:00.534: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-b96e9c48-35c2-4b8c-b9a6-e1d84a645443 in namespace container-probe-2437
Apr  6 03:59:04.791: INFO: Started pod liveness-b96e9c48-35c2-4b8c-b9a6-e1d84a645443 in namespace container-probe-2437
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 03:59:04.805: INFO: Initial restart count of pod liveness-b96e9c48-35c2-4b8c-b9a6-e1d84a645443 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:03:05.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2437" for this suite.

• [SLOW TEST:244.723 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":3651,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:03:05.258: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 04:03:05.854: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 04:03:07.888: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814585, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814585, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814585, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814585, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 04:03:10.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:03:10.942: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6375-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:03:14.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5894" for this suite.
STEP: Destroying namespace "webhook-5894-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.421 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":222,"skipped":3672,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:03:14.679: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:03:14.894: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-0cfe3fc5-d1fa-423f-b073-1fae3456c3e0
STEP: Creating configMap with name cm-test-opt-upd-c776f827-22d6-4369-a9dd-28e4aa3b0334
STEP: Creating the pod
Apr  6 04:03:15.123: INFO: The status of Pod pod-projected-configmaps-6f7f24f3-c1cc-4e8f-b5ed-251ceac55090 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:03:17.158: INFO: The status of Pod pod-projected-configmaps-6f7f24f3-c1cc-4e8f-b5ed-251ceac55090 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:03:19.141: INFO: The status of Pod pod-projected-configmaps-6f7f24f3-c1cc-4e8f-b5ed-251ceac55090 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-0cfe3fc5-d1fa-423f-b073-1fae3456c3e0
STEP: Updating configmap cm-test-opt-upd-c776f827-22d6-4369-a9dd-28e4aa3b0334
STEP: Creating configMap with name cm-test-opt-create-f4e5c066-dcd6-4efc-a4b9-421b7f86911b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:04:24.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8199" for this suite.

• [SLOW TEST:70.001 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":3687,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:04:24.681: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr  6 04:04:25.397: INFO: starting watch
STEP: patching
STEP: updating
Apr  6 04:04:25.443: INFO: waiting for watch events with expected annotations
Apr  6 04:04:25.443: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:04:25.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3953" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":224,"skipped":3705,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:04:25.698: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:04:30.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1245" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":225,"skipped":3719,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:04:30.051: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:04:41.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5481" for this suite.

• [SLOW TEST:11.391 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":226,"skipped":3719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:04:41.442: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Apr  6 04:04:41.676: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Apr  6 04:04:41.761: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:04:41.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4527" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":227,"skipped":3764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:04:41.895: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:04:42.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-692" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":228,"skipped":3801,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:04:42.266: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:04:42.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3385" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":229,"skipped":3817,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:04:42.471: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:05:17.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1592" for this suite.

• [SLOW TEST:35.248 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":3837,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:05:17.719: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:05:17.898: INFO: Got root ca configmap in namespace "svcaccounts-4688"
Apr  6 04:05:17.920: INFO: Deleted root ca configmap in namespace "svcaccounts-4688"
STEP: waiting for a new root ca configmap created
Apr  6 04:05:18.436: INFO: Recreated root ca configmap in namespace "svcaccounts-4688"
Apr  6 04:05:18.455: INFO: Updated root ca configmap in namespace "svcaccounts-4688"
STEP: waiting for the root ca configmap reconciled
Apr  6 04:05:18.970: INFO: Reconciled root ca configmap in namespace "svcaccounts-4688"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:05:18.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4688" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":231,"skipped":3841,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:05:19.003: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:05:19.235: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr  6 04:05:24.252: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  6 04:05:24.252: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr  6 04:05:26.270: INFO: Creating deployment "test-rollover-deployment"
Apr  6 04:05:26.300: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr  6 04:05:28.325: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr  6 04:05:28.353: INFO: Ensure that both replica sets have 1 created replica
Apr  6 04:05:28.385: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr  6 04:05:28.423: INFO: Updating deployment test-rollover-deployment
Apr  6 04:05:28.423: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr  6 04:05:30.449: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr  6 04:05:30.470: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr  6 04:05:30.489: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 04:05:30.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814728, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:05:32.516: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 04:05:32.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814730, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:05:34.514: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 04:05:34.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814730, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:05:36.515: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 04:05:36.515: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814730, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:05:38.515: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 04:05:38.515: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814730, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:05:40.518: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 04:05:40.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814730, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784814726, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:05:42.512: INFO: 
Apr  6 04:05:42.512: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr  6 04:05:42.541: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3039  e985666e-1931-481d-88f6-907d5f26e864 107503 2 2022-04-06 04:05:26 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-04-06 04:05:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 04:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034e8468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-04-06 04:05:26 +0000 UTC,LastTransitionTime:2022-04-06 04:05:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2022-04-06 04:05:40 +0000 UTC,LastTransitionTime:2022-04-06 04:05:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 04:05:42.551: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-3039  23b5fcc8-f4b0-49ee-8a06-3aef907f9c94 107492 2 2022-04-06 04:05:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e985666e-1931-481d-88f6-907d5f26e864 0xc0034e8a60 0xc0034e8a61}] []  [{kube-controller-manager Update apps/v1 2022-04-06 04:05:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e985666e-1931-481d-88f6-907d5f26e864\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 04:05:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034e8af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 04:05:42.551: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr  6 04:05:42.551: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3039  56f9e286-3b1f-42fb-b627-84c22f1de2b6 107502 2 2022-04-06 04:05:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e985666e-1931-481d-88f6-907d5f26e864 0xc0034e8807 0xc0034e8808}] []  [{e2e.test Update apps/v1 2022-04-06 04:05:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 04:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e985666e-1931-481d-88f6-907d5f26e864\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-04-06 04:05:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0034e88c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 04:05:42.551: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-3039  265a64dc-120c-4f68-987f-52fa49b37753 107401 2 2022-04-06 04:05:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e985666e-1931-481d-88f6-907d5f26e864 0xc0034e8937 0xc0034e8938}] []  [{kube-controller-manager Update apps/v1 2022-04-06 04:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e985666e-1931-481d-88f6-907d5f26e864\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 04:05:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034e89f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 04:05:42.564: INFO: Pod "test-rollover-deployment-98c5f4599-4mzn7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-4mzn7 test-rollover-deployment-98c5f4599- deployment-3039  03a80e97-3f19-443e-8fe1-16991d2bba4e 107433 0 2022-04-06 04:05:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/containerID:ad71434dce8a5abd3bfc395c7e5ff0487935676efc10a2b98b0cb8dedef51e37 cni.projectcalico.org/podIP:172.17.96.125/32 cni.projectcalico.org/podIPs:172.17.96.125/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.125"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.125"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 23b5fcc8-f4b0-49ee-8a06-3aef907f9c94 0xc005c42f07 0xc005c42f08}] []  [{kube-controller-manager Update v1 2022-04-06 04:05:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23b5fcc8-f4b0-49ee-8a06-3aef907f9c94\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 04:05:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 04:05:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 04:05:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.96.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtn4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtn4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-t92sr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 04:05:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 04:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 04:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 04:05:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.102,PodIP:172.17.96.125,StartTime:2022-04-06 04:05:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 04:05:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://ffb32e7ef61bc4517f99d17428f40f7084e13108c3613add02758c78af705865,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.96.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:05:42.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3039" for this suite.

• [SLOW TEST:23.597 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":232,"skipped":3853,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:05:42.600: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4248.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4248.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4248.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4248.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4248.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4248.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4248.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 206.224.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.224.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.224.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.224.206_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4248.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4248.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4248.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4248.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4248.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4248.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4248.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 206.224.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.224.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.224.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.224.206_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 04:05:47.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-4248.svc.cluster.local from pod dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8: the server could not find the requested resource (get pods dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8)
Apr  6 04:05:47.054: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4248.svc.cluster.local from pod dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8: the server could not find the requested resource (get pods dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8)
Apr  6 04:05:47.071: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local from pod dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8: the server could not find the requested resource (get pods dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8)
Apr  6 04:05:47.097: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local from pod dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8: the server could not find the requested resource (get pods dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8)
Apr  6 04:05:47.237: INFO: Unable to read jessie_udp@dns-test-service.dns-4248.svc.cluster.local from pod dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8: the server could not find the requested resource (get pods dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8)
Apr  6 04:05:47.254: INFO: Unable to read jessie_tcp@dns-test-service.dns-4248.svc.cluster.local from pod dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8: the server could not find the requested resource (get pods dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8)
Apr  6 04:05:47.269: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local from pod dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8: the server could not find the requested resource (get pods dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8)
Apr  6 04:05:47.284: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local from pod dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8: the server could not find the requested resource (get pods dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8)
Apr  6 04:05:47.386: INFO: Lookups using dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8 failed for: [wheezy_udp@dns-test-service.dns-4248.svc.cluster.local wheezy_tcp@dns-test-service.dns-4248.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local jessie_udp@dns-test-service.dns-4248.svc.cluster.local jessie_tcp@dns-test-service.dns-4248.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4248.svc.cluster.local]

Apr  6 04:05:52.819: INFO: DNS probes using dns-4248/dns-test-739edf57-cd38-4c23-98e7-6a7888ad4bf8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:05:52.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4248" for this suite.

• [SLOW TEST:10.406 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":233,"skipped":3865,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:05:53.007: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Apr  6 04:05:53.292: INFO: Waiting up to 5m0s for pod "client-containers-95100e92-28f6-4221-b067-21068e2724ea" in namespace "containers-2680" to be "Succeeded or Failed"
Apr  6 04:05:53.307: INFO: Pod "client-containers-95100e92-28f6-4221-b067-21068e2724ea": Phase="Pending", Reason="", readiness=false. Elapsed: 14.997028ms
Apr  6 04:05:55.326: INFO: Pod "client-containers-95100e92-28f6-4221-b067-21068e2724ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033824539s
Apr  6 04:05:57.346: INFO: Pod "client-containers-95100e92-28f6-4221-b067-21068e2724ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054392913s
STEP: Saw pod success
Apr  6 04:05:57.346: INFO: Pod "client-containers-95100e92-28f6-4221-b067-21068e2724ea" satisfied condition "Succeeded or Failed"
Apr  6 04:05:57.385: INFO: Trying to get logs from node 10.241.0.102 pod client-containers-95100e92-28f6-4221-b067-21068e2724ea container agnhost-container: <nil>
STEP: delete the pod
Apr  6 04:05:57.498: INFO: Waiting for pod client-containers-95100e92-28f6-4221-b067-21068e2724ea to disappear
Apr  6 04:05:57.511: INFO: Pod client-containers-95100e92-28f6-4221-b067-21068e2724ea no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:05:57.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2680" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":3878,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:05:57.541: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Apr  6 04:05:57.742: INFO: Waiting up to 5m0s for pod "test-pod-c5f0dddc-a09c-49a7-9681-85ccb682101f" in namespace "svcaccounts-5449" to be "Succeeded or Failed"
Apr  6 04:05:57.755: INFO: Pod "test-pod-c5f0dddc-a09c-49a7-9681-85ccb682101f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.737629ms
Apr  6 04:05:59.774: INFO: Pod "test-pod-c5f0dddc-a09c-49a7-9681-85ccb682101f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032042598s
Apr  6 04:06:01.812: INFO: Pod "test-pod-c5f0dddc-a09c-49a7-9681-85ccb682101f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069300602s
STEP: Saw pod success
Apr  6 04:06:01.812: INFO: Pod "test-pod-c5f0dddc-a09c-49a7-9681-85ccb682101f" satisfied condition "Succeeded or Failed"
Apr  6 04:06:01.836: INFO: Trying to get logs from node 10.241.0.102 pod test-pod-c5f0dddc-a09c-49a7-9681-85ccb682101f container agnhost-container: <nil>
STEP: delete the pod
Apr  6 04:06:01.891: INFO: Waiting for pod test-pod-c5f0dddc-a09c-49a7-9681-85ccb682101f to disappear
Apr  6 04:06:01.904: INFO: Pod test-pod-c5f0dddc-a09c-49a7-9681-85ccb682101f no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:01.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5449" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":235,"skipped":3886,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:01.934: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 04:06:02.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6fbcf3be-dbee-49fe-9a48-bfc5c0e8b1f9" in namespace "projected-6145" to be "Succeeded or Failed"
Apr  6 04:06:02.140: INFO: Pod "downwardapi-volume-6fbcf3be-dbee-49fe-9a48-bfc5c0e8b1f9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.655426ms
Apr  6 04:06:04.155: INFO: Pod "downwardapi-volume-6fbcf3be-dbee-49fe-9a48-bfc5c0e8b1f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026429495s
STEP: Saw pod success
Apr  6 04:06:04.155: INFO: Pod "downwardapi-volume-6fbcf3be-dbee-49fe-9a48-bfc5c0e8b1f9" satisfied condition "Succeeded or Failed"
Apr  6 04:06:04.166: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-6fbcf3be-dbee-49fe-9a48-bfc5c0e8b1f9 container client-container: <nil>
STEP: delete the pod
Apr  6 04:06:04.247: INFO: Waiting for pod downwardapi-volume-6fbcf3be-dbee-49fe-9a48-bfc5c0e8b1f9 to disappear
Apr  6 04:06:04.257: INFO: Pod downwardapi-volume-6fbcf3be-dbee-49fe-9a48-bfc5c0e8b1f9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:04.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6145" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":3898,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:04.301: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  6 04:06:04.761: INFO: Number of nodes with available pods: 0
Apr  6 04:06:04.761: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:06:05.792: INFO: Number of nodes with available pods: 0
Apr  6 04:06:05.792: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:06:06.791: INFO: Number of nodes with available pods: 1
Apr  6 04:06:06.791: INFO: Node 10.241.0.101 is running more than one daemon pod
Apr  6 04:06:07.790: INFO: Number of nodes with available pods: 3
Apr  6 04:06:07.790: INFO: Number of running nodes: 3, number of available pods: 3
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Apr  6 04:06:07.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"108132"},"items":null}

Apr  6 04:06:07.882: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"108135"},"items":[{"metadata":{"name":"daemon-set-gdzqh","generateName":"daemon-set-","namespace":"daemonsets-5958","uid":"e1d14d24-b204-4b7b-a2f1-7f24cf3925f3","resourceVersion":"108134","creationTimestamp":"2022-04-06T04:06:04Z","deletionTimestamp":"2022-04-06T04:06:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"8e00251150fee85c3d01f1aebf73807f9a55f63d67428ad7fa16316a027083ae","cni.projectcalico.org/podIP":"172.17.77.41/32","cni.projectcalico.org/podIPs":"172.17.77.41/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.17.77.41\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.17.77.41\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1bbd541e-ec76-421e-88cc-2c6464c35975","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1bbd541e-ec76-421e-88cc-2c6464c35975\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.77.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g5s6z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g5s6z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.241.0.100","securityContext":{"seLinuxOptions":{"level":"s0:c58,c2"}},"imagePullSecrets":[{"name":"default-dockercfg-d4ptp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.241.0.100"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:04Z"}],"hostIP":"10.241.0.100","podIP":"172.17.77.41","podIPs":[{"ip":"172.17.77.41"}],"startTime":"2022-04-06T04:06:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-04-06T04:06:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://f53b7c9a24748e318118eb81f40fece45d1c430ca437897b454f303901085afa","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-h6df5","generateName":"daemon-set-","namespace":"daemonsets-5958","uid":"a4c5b421-4b2b-4675-9652-eedc14338e48","resourceVersion":"108135","creationTimestamp":"2022-04-06T04:06:04Z","deletionTimestamp":"2022-04-06T04:06:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"aed273191049e06589dcee9996b31cef4be004ab230f98e22ae32759ae5eeeee","cni.projectcalico.org/podIP":"172.17.96.97/32","cni.projectcalico.org/podIPs":"172.17.96.97/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.17.96.97\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.17.96.97\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1bbd541e-ec76-421e-88cc-2c6464c35975","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1bbd541e-ec76-421e-88cc-2c6464c35975\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.96.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jbwpl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jbwpl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.241.0.102","securityContext":{"seLinuxOptions":{"level":"s0:c58,c2"}},"imagePullSecrets":[{"name":"default-dockercfg-d4ptp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.241.0.102"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:04Z"}],"hostIP":"10.241.0.102","podIP":"172.17.96.97","podIPs":[{"ip":"172.17.96.97"}],"startTime":"2022-04-06T04:06:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-04-06T04:06:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://bf1c8d30ee61b5e73dabd11d3bb0b78d4f7b5f95381b6ccbbb0c4df0da44e2f8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mrggr","generateName":"daemon-set-","namespace":"daemonsets-5958","uid":"39718e4a-ea30-429a-a464-545da8bc7bad","resourceVersion":"108132","creationTimestamp":"2022-04-06T04:06:04Z","deletionTimestamp":"2022-04-06T04:06:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d4a40e8d43d22a71382584fbfce2a5b8da86e8e950a83975856ed8cdb9f412ce","cni.projectcalico.org/podIP":"172.17.95.189/32","cni.projectcalico.org/podIPs":"172.17.95.189/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.17.95.189\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.17.95.189\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1bbd541e-ec76-421e-88cc-2c6464c35975","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1bbd541e-ec76-421e-88cc-2c6464c35975\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-04-06T04:06:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.95.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gclqc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gclqc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.241.0.101","securityContext":{"seLinuxOptions":{"level":"s0:c58,c2"}},"imagePullSecrets":[{"name":"default-dockercfg-d4ptp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.241.0.101"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-06T04:06:04Z"}],"hostIP":"10.241.0.101","podIP":"172.17.95.189","podIPs":[{"ip":"172.17.95.189"}],"startTime":"2022-04-06T04:06:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-04-06T04:06:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://c0e3be6eacb823ba15cdd78439566254e1efc4d0e1a5484828c15e9001061002","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:07.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5958" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":237,"skipped":3915,"failed":0}
SSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:07.952: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:08.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2461" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":238,"skipped":3920,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:08.200: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:21.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7072" for this suite.

• [SLOW TEST:13.472 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":239,"skipped":3937,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:21.673: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:22.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4483" for this suite.
STEP: Destroying namespace "nspatchtest-63e3ba97-cb93-461a-8628-dec81ccc965b-5781" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":240,"skipped":3944,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:22.100: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-3bb0b6cd-536b-4c21-84b3-c2973f91d06d
STEP: Creating a pod to test consume secrets
Apr  6 04:06:22.388: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18d47e4f-1b27-4ff1-aa6c-d20883844535" in namespace "projected-7061" to be "Succeeded or Failed"
Apr  6 04:06:22.400: INFO: Pod "pod-projected-secrets-18d47e4f-1b27-4ff1-aa6c-d20883844535": Phase="Pending", Reason="", readiness=false. Elapsed: 11.84182ms
Apr  6 04:06:24.417: INFO: Pod "pod-projected-secrets-18d47e4f-1b27-4ff1-aa6c-d20883844535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028473971s
STEP: Saw pod success
Apr  6 04:06:24.417: INFO: Pod "pod-projected-secrets-18d47e4f-1b27-4ff1-aa6c-d20883844535" satisfied condition "Succeeded or Failed"
Apr  6 04:06:24.430: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-secrets-18d47e4f-1b27-4ff1-aa6c-d20883844535 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 04:06:24.511: INFO: Waiting for pod pod-projected-secrets-18d47e4f-1b27-4ff1-aa6c-d20883844535 to disappear
Apr  6 04:06:24.523: INFO: Pod pod-projected-secrets-18d47e4f-1b27-4ff1-aa6c-d20883844535 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:24.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7061" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":3946,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:24.555: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Apr  6 04:06:24.877: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:06:26.893: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:06:28.889: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr  6 04:06:29.954: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:31.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6701" for this suite.

• [SLOW TEST:6.530 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":242,"skipped":3951,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:31.086: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-39ded30b-70bf-47ba-b27b-0f62572df15d
STEP: Creating a pod to test consume secrets
Apr  6 04:06:31.640: INFO: Waiting up to 5m0s for pod "pod-secrets-68f0816b-8809-45d2-ada9-e0cf3e84eaeb" in namespace "secrets-4723" to be "Succeeded or Failed"
Apr  6 04:06:31.681: INFO: Pod "pod-secrets-68f0816b-8809-45d2-ada9-e0cf3e84eaeb": Phase="Pending", Reason="", readiness=false. Elapsed: 40.676617ms
Apr  6 04:06:33.699: INFO: Pod "pod-secrets-68f0816b-8809-45d2-ada9-e0cf3e84eaeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058455229s
Apr  6 04:06:35.711: INFO: Pod "pod-secrets-68f0816b-8809-45d2-ada9-e0cf3e84eaeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071098982s
STEP: Saw pod success
Apr  6 04:06:35.711: INFO: Pod "pod-secrets-68f0816b-8809-45d2-ada9-e0cf3e84eaeb" satisfied condition "Succeeded or Failed"
Apr  6 04:06:35.724: INFO: Trying to get logs from node 10.241.0.102 pod pod-secrets-68f0816b-8809-45d2-ada9-e0cf3e84eaeb container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 04:06:35.784: INFO: Waiting for pod pod-secrets-68f0816b-8809-45d2-ada9-e0cf3e84eaeb to disappear
Apr  6 04:06:35.795: INFO: Pod pod-secrets-68f0816b-8809-45d2-ada9-e0cf3e84eaeb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:06:35.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4723" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":3958,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:06:35.824: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:12:02.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7159" for this suite.

• [SLOW TEST:326.436 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":244,"skipped":3973,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:12:02.261: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr  6 04:12:02.494: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9865  1b393b03-59dd-43e9-9fb9-eec9e11bb175 110782 0 2022-04-06 04:12:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 04:12:02.494: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9865  1b393b03-59dd-43e9-9fb9-eec9e11bb175 110782 0 2022-04-06 04:12:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr  6 04:12:12.531: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9865  1b393b03-59dd-43e9-9fb9-eec9e11bb175 110862 0 2022-04-06 04:12:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 04:12:12.531: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9865  1b393b03-59dd-43e9-9fb9-eec9e11bb175 110862 0 2022-04-06 04:12:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr  6 04:12:22.580: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9865  1b393b03-59dd-43e9-9fb9-eec9e11bb175 110906 0 2022-04-06 04:12:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 04:12:22.580: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9865  1b393b03-59dd-43e9-9fb9-eec9e11bb175 110906 0 2022-04-06 04:12:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr  6 04:12:32.611: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9865  1b393b03-59dd-43e9-9fb9-eec9e11bb175 110954 0 2022-04-06 04:12:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 04:12:32.611: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9865  1b393b03-59dd-43e9-9fb9-eec9e11bb175 110954 0 2022-04-06 04:12:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr  6 04:12:42.660: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9865  2452e08e-849c-412d-87a9-b0870a19b92d 111000 0 2022-04-06 04:12:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 04:12:42.660: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9865  2452e08e-849c-412d-87a9-b0870a19b92d 111000 0 2022-04-06 04:12:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr  6 04:12:52.689: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9865  2452e08e-849c-412d-87a9-b0870a19b92d 111044 0 2022-04-06 04:12:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 04:12:52.689: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9865  2452e08e-849c-412d-87a9-b0870a19b92d 111044 0 2022-04-06 04:12:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-04-06 04:12:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9865" for this suite.

• [SLOW TEST:60.456 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":245,"skipped":3981,"failed":0}
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:02.717: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Apr  6 04:13:03.106: INFO: The status of Pod annotationupdate9a2af139-a848-4ac3-ae45-e0e2f497a73c is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:13:05.120: INFO: The status of Pod annotationupdate9a2af139-a848-4ac3-ae45-e0e2f497a73c is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:13:07.135: INFO: The status of Pod annotationupdate9a2af139-a848-4ac3-ae45-e0e2f497a73c is Running (Ready = true)
Apr  6 04:13:07.814: INFO: Successfully updated pod "annotationupdate9a2af139-a848-4ac3-ae45-e0e2f497a73c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:09.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4643" for this suite.

• [SLOW TEST:7.249 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":246,"skipped":3981,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:09.966: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr  6 04:13:10.214: INFO: Waiting up to 5m0s for pod "pod-d8e6cd5a-d11c-4d5e-9535-af3937eb135f" in namespace "emptydir-9120" to be "Succeeded or Failed"
Apr  6 04:13:10.227: INFO: Pod "pod-d8e6cd5a-d11c-4d5e-9535-af3937eb135f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.047697ms
Apr  6 04:13:12.248: INFO: Pod "pod-d8e6cd5a-d11c-4d5e-9535-af3937eb135f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033461912s
Apr  6 04:13:14.263: INFO: Pod "pod-d8e6cd5a-d11c-4d5e-9535-af3937eb135f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049244439s
STEP: Saw pod success
Apr  6 04:13:14.263: INFO: Pod "pod-d8e6cd5a-d11c-4d5e-9535-af3937eb135f" satisfied condition "Succeeded or Failed"
Apr  6 04:13:14.276: INFO: Trying to get logs from node 10.241.0.102 pod pod-d8e6cd5a-d11c-4d5e-9535-af3937eb135f container test-container: <nil>
STEP: delete the pod
Apr  6 04:13:14.361: INFO: Waiting for pod pod-d8e6cd5a-d11c-4d5e-9535-af3937eb135f to disappear
Apr  6 04:13:14.374: INFO: Pod pod-d8e6cd5a-d11c-4d5e-9535-af3937eb135f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:14.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9120" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":3995,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:14.407: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-2fd16aae-0f99-441e-b5ce-6ea7e6648ce7
STEP: Creating a pod to test consume configMaps
Apr  6 04:13:14.696: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e9a64e64-f103-4f89-a787-ed9f0ef662f3" in namespace "projected-6645" to be "Succeeded or Failed"
Apr  6 04:13:14.734: INFO: Pod "pod-projected-configmaps-e9a64e64-f103-4f89-a787-ed9f0ef662f3": Phase="Pending", Reason="", readiness=false. Elapsed: 38.323237ms
Apr  6 04:13:16.760: INFO: Pod "pod-projected-configmaps-e9a64e64-f103-4f89-a787-ed9f0ef662f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.063851819s
STEP: Saw pod success
Apr  6 04:13:16.760: INFO: Pod "pod-projected-configmaps-e9a64e64-f103-4f89-a787-ed9f0ef662f3" satisfied condition "Succeeded or Failed"
Apr  6 04:13:16.775: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-configmaps-e9a64e64-f103-4f89-a787-ed9f0ef662f3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 04:13:16.895: INFO: Waiting for pod pod-projected-configmaps-e9a64e64-f103-4f89-a787-ed9f0ef662f3 to disappear
Apr  6 04:13:16.906: INFO: Pod pod-projected-configmaps-e9a64e64-f103-4f89-a787-ed9f0ef662f3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:16.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6645" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4009,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:16.935: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Apr  6 04:13:17.227: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Apr  6 04:13:18.143: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr  6 04:13:20.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:13:22.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:13:24.568: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:13:26.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:13:28.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:13:30.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815198, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 04:13:37.086: INFO: Waited 4.497981432s for the sample-apiserver to be ready to handle requests.
I0406 04:13:38.185750      21 request.go:665] Waited for 1.008923247s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/apiregistration.k8s.io/v1?timeout=32s
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Apr  6 04:13:38.878: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:39.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4409" for this suite.

• [SLOW TEST:22.682 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":249,"skipped":4028,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:39.617: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  6 04:13:39.961: INFO: Number of nodes with available pods: 0
Apr  6 04:13:39.961: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:13:40.989: INFO: Number of nodes with available pods: 0
Apr  6 04:13:40.989: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:13:42.004: INFO: Number of nodes with available pods: 1
Apr  6 04:13:42.004: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:13:42.991: INFO: Number of nodes with available pods: 3
Apr  6 04:13:42.991: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr  6 04:13:43.075: INFO: Number of nodes with available pods: 3
Apr  6 04:13:43.075: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4403, will wait for the garbage collector to delete the pods
Apr  6 04:13:43.206: INFO: Deleting DaemonSet.extensions daemon-set took: 23.546428ms
Apr  6 04:13:43.407: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.225874ms
Apr  6 04:13:46.720: INFO: Number of nodes with available pods: 0
Apr  6 04:13:46.720: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 04:13:46.729: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"111925"},"items":null}

Apr  6 04:13:46.743: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"111925"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:46.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4403" for this suite.

• [SLOW TEST:7.210 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":250,"skipped":4033,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:46.827: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:51.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-190" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":251,"skipped":4050,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:51.667: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:52.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7881" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":252,"skipped":4063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:52.075: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:13:52.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-462" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":253,"skipped":4098,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:13:52.391: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-246
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Apr  6 04:13:52.649: INFO: Found 0 stateful pods, waiting for 3
Apr  6 04:14:02.669: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 04:14:02.669: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 04:14:02.669: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 04:14:02.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-246 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 04:14:03.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 04:14:03.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 04:14:03.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Apr  6 04:14:13.455: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr  6 04:14:23.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-246 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 04:14:23.745: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 04:14:23.745: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 04:14:23.745: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 04:14:33.840: INFO: Waiting for StatefulSet statefulset-246/ss2 to complete update
STEP: Rolling back to a previous revision
Apr  6 04:14:43.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-246 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 04:14:44.140: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 04:14:44.140: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 04:14:44.140: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 04:14:54.245: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr  6 04:15:04.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=statefulset-246 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 04:15:04.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 04:15:04.507: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 04:15:04.507: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 04:15:14.605: INFO: Waiting for StatefulSet statefulset-246/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Apr  6 04:15:24.635: INFO: Deleting all statefulset in ns statefulset-246
Apr  6 04:15:24.647: INFO: Scaling statefulset ss2 to 0
Apr  6 04:15:34.719: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 04:15:34.732: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:15:34.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-246" for this suite.

• [SLOW TEST:102.454 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":254,"skipped":4111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:15:34.845: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:15:35.081: INFO: The status of Pod server-envvars-5d8b3da9-e46f-4238-bb90-4b7832ea733d is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:15:37.102: INFO: The status of Pod server-envvars-5d8b3da9-e46f-4238-bb90-4b7832ea733d is Running (Ready = true)
Apr  6 04:15:37.241: INFO: Waiting up to 5m0s for pod "client-envvars-b61c2051-b5dd-436c-b85e-cc581c9441b6" in namespace "pods-9441" to be "Succeeded or Failed"
Apr  6 04:15:37.254: INFO: Pod "client-envvars-b61c2051-b5dd-436c-b85e-cc581c9441b6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.327153ms
Apr  6 04:15:39.268: INFO: Pod "client-envvars-b61c2051-b5dd-436c-b85e-cc581c9441b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02758976s
STEP: Saw pod success
Apr  6 04:15:39.268: INFO: Pod "client-envvars-b61c2051-b5dd-436c-b85e-cc581c9441b6" satisfied condition "Succeeded or Failed"
Apr  6 04:15:39.280: INFO: Trying to get logs from node 10.241.0.102 pod client-envvars-b61c2051-b5dd-436c-b85e-cc581c9441b6 container env3cont: <nil>
STEP: delete the pod
Apr  6 04:15:39.411: INFO: Waiting for pod client-envvars-b61c2051-b5dd-436c-b85e-cc581c9441b6 to disappear
Apr  6 04:15:39.422: INFO: Pod client-envvars-b61c2051-b5dd-436c-b85e-cc581c9441b6 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:15:39.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9441" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":255,"skipped":4147,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:15:39.455: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Apr  6 04:15:39.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-3001 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr  6 04:15:39.865: INFO: stderr: ""
Apr  6 04:15:39.865: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Apr  6 04:15:39.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-3001 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Apr  6 04:15:43.190: INFO: stderr: ""
Apr  6 04:15:43.190: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Apr  6 04:15:43.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-3001 delete pods e2e-test-httpd-pod'
Apr  6 04:15:45.440: INFO: stderr: ""
Apr  6 04:15:45.440: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:15:45.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3001" for this suite.

• [SLOW TEST:6.024 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:913
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":256,"skipped":4165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:15:45.480: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8456
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8456
STEP: creating replication controller externalsvc in namespace services-8456
I0406 04:15:45.741108      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8456, replica count: 2
I0406 04:15:48.792215      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr  6 04:15:48.860: INFO: Creating new exec pod
Apr  6 04:15:52.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-8456 exec execpoddr285 -- /bin/sh -x -c nslookup nodeport-service.services-8456.svc.cluster.local'
Apr  6 04:15:53.263: INFO: stderr: "+ nslookup nodeport-service.services-8456.svc.cluster.local\n"
Apr  6 04:15:53.263: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-8456.svc.cluster.local\tcanonical name = externalsvc.services-8456.svc.cluster.local.\nName:\texternalsvc.services-8456.svc.cluster.local\nAddress: 172.21.148.20\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8456, will wait for the garbage collector to delete the pods
Apr  6 04:15:53.347: INFO: Deleting ReplicationController externalsvc took: 20.070324ms
Apr  6 04:15:53.448: INFO: Terminating ReplicationController externalsvc pods took: 101.109117ms
Apr  6 04:15:56.488: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:15:56.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8456" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.066 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":257,"skipped":4198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:15:56.546: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-804efe36-1500-4de2-8537-2cd9a5d710ef
STEP: Creating a pod to test consume configMaps
Apr  6 04:15:57.043: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb8bbf39-44ce-48fb-9e16-da7f0c0ca492" in namespace "configmap-5388" to be "Succeeded or Failed"
Apr  6 04:15:57.054: INFO: Pod "pod-configmaps-fb8bbf39-44ce-48fb-9e16-da7f0c0ca492": Phase="Pending", Reason="", readiness=false. Elapsed: 11.492616ms
Apr  6 04:15:59.073: INFO: Pod "pod-configmaps-fb8bbf39-44ce-48fb-9e16-da7f0c0ca492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030184041s
Apr  6 04:16:01.088: INFO: Pod "pod-configmaps-fb8bbf39-44ce-48fb-9e16-da7f0c0ca492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045369726s
STEP: Saw pod success
Apr  6 04:16:01.088: INFO: Pod "pod-configmaps-fb8bbf39-44ce-48fb-9e16-da7f0c0ca492" satisfied condition "Succeeded or Failed"
Apr  6 04:16:01.099: INFO: Trying to get logs from node 10.241.0.102 pod pod-configmaps-fb8bbf39-44ce-48fb-9e16-da7f0c0ca492 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 04:16:01.161: INFO: Waiting for pod pod-configmaps-fb8bbf39-44ce-48fb-9e16-da7f0c0ca492 to disappear
Apr  6 04:16:01.171: INFO: Pod pod-configmaps-fb8bbf39-44ce-48fb-9e16-da7f0c0ca492 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:16:01.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5388" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4228,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:16:01.199: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:16:09.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7071" for this suite.

• [SLOW TEST:8.240 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":259,"skipped":4246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:16:09.439: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr  6 04:16:09.631: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 04:17:09.877: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Apr  6 04:17:09.978: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr  6 04:17:10.015: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr  6 04:17:10.085: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr  6 04:17:10.124: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Apr  6 04:17:10.185: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Apr  6 04:17:10.218: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:17:26.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3725" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:77.215 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":260,"skipped":4303,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:17:26.654: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Apr  6 04:17:26.829: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-9710 proxy --unix-socket=/tmp/kubectl-proxy-unix088389995/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:17:26.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9710" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":261,"skipped":4311,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:17:26.908: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr  6 04:17:30.397: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:17:30.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1936" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":4338,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:17:30.482: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:17:30.730: INFO: The status of Pod pod-secrets-69a6b7ad-7ca2-4e15-8006-9f7aa0d3252c is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:17:32.784: INFO: The status of Pod pod-secrets-69a6b7ad-7ca2-4e15-8006-9f7aa0d3252c is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:17:34.743: INFO: The status of Pod pod-secrets-69a6b7ad-7ca2-4e15-8006-9f7aa0d3252c is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:17:34.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8815" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":263,"skipped":4342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:17:34.870: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5461
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  6 04:17:35.035: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 04:17:35.196: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:17:37.212: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:17:39.214: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:17:41.210: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:17:43.213: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:17:45.214: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:17:47.214: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:17:49.213: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:17:51.214: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:17:53.209: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:17:55.219: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr  6 04:17:55.243: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr  6 04:17:55.269: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr  6 04:17:57.365: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr  6 04:17:57.365: INFO: Breadth first check of 172.17.77.59 on host 10.241.0.100...
Apr  6 04:17:57.377: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.96.123:9080/dial?request=hostname&protocol=http&host=172.17.77.59&port=8083&tries=1'] Namespace:pod-network-test-5461 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:17:57.377: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 04:17:57.557: INFO: Waiting for responses: map[]
Apr  6 04:17:57.560: INFO: reached 172.17.77.59 after 0/1 tries
Apr  6 04:17:57.560: INFO: Breadth first check of 172.17.95.152 on host 10.241.0.101...
Apr  6 04:17:57.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.96.123:9080/dial?request=hostname&protocol=http&host=172.17.95.152&port=8083&tries=1'] Namespace:pod-network-test-5461 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:17:57.572: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 04:17:57.875: INFO: Waiting for responses: map[]
Apr  6 04:17:57.875: INFO: reached 172.17.95.152 after 0/1 tries
Apr  6 04:17:57.875: INFO: Breadth first check of 172.17.96.120 on host 10.241.0.102...
Apr  6 04:17:57.913: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.96.123:9080/dial?request=hostname&protocol=http&host=172.17.96.120&port=8083&tries=1'] Namespace:pod-network-test-5461 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:17:57.913: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 04:17:58.059: INFO: Waiting for responses: map[]
Apr  6 04:17:58.059: INFO: reached 172.17.96.120 after 0/1 tries
Apr  6 04:17:58.059: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:17:58.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5461" for this suite.

• [SLOW TEST:23.222 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:17:58.092: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-bd209149-5d56-4313-b58f-7b2905398a7f
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:17:58.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4736" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":265,"skipped":4391,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:17:58.281: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:17:58.556: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr  6 04:17:58.615: INFO: Number of nodes with available pods: 0
Apr  6 04:17:58.615: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:17:59.651: INFO: Number of nodes with available pods: 0
Apr  6 04:17:59.651: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:18:00.642: INFO: Number of nodes with available pods: 2
Apr  6 04:18:00.642: INFO: Node 10.241.0.102 is running more than one daemon pod
Apr  6 04:18:01.654: INFO: Number of nodes with available pods: 3
Apr  6 04:18:01.654: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr  6 04:18:01.736: INFO: Wrong image for pod: daemon-set-dch2f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:01.736: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:01.736: INFO: Wrong image for pod: daemon-set-xjp7t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:02.770: INFO: Wrong image for pod: daemon-set-dch2f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:02.770: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:03.764: INFO: Wrong image for pod: daemon-set-dch2f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:03.764: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:04.777: INFO: Wrong image for pod: daemon-set-dch2f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:04.777: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:05.763: INFO: Wrong image for pod: daemon-set-dch2f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:05.763: INFO: Pod daemon-set-k7s4j is not available
Apr  6 04:18:05.763: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:06.764: INFO: Wrong image for pod: daemon-set-dch2f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:06.764: INFO: Pod daemon-set-k7s4j is not available
Apr  6 04:18:06.764: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:07.767: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:08.767: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:08.767: INFO: Pod daemon-set-npfdz is not available
Apr  6 04:18:09.768: INFO: Wrong image for pod: daemon-set-m8lvc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Apr  6 04:18:09.768: INFO: Pod daemon-set-npfdz is not available
Apr  6 04:18:12.766: INFO: Pod daemon-set-8977n is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr  6 04:18:12.818: INFO: Number of nodes with available pods: 2
Apr  6 04:18:12.818: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:18:13.847: INFO: Number of nodes with available pods: 3
Apr  6 04:18:13.847: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2995, will wait for the garbage collector to delete the pods
Apr  6 04:18:13.971: INFO: Deleting DaemonSet.extensions daemon-set took: 15.568252ms
Apr  6 04:18:14.172: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.554809ms
Apr  6 04:18:17.092: INFO: Number of nodes with available pods: 0
Apr  6 04:18:17.092: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 04:18:17.103: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"115941"},"items":null}

Apr  6 04:18:17.115: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"115941"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:18:17.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2995" for this suite.

• [SLOW TEST:18.909 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":266,"skipped":4408,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:18:17.191: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr  6 04:18:17.451: INFO: Waiting up to 5m0s for pod "downward-api-f29530d4-4c3c-4458-80c9-23c5efaf1823" in namespace "downward-api-8237" to be "Succeeded or Failed"
Apr  6 04:18:17.461: INFO: Pod "downward-api-f29530d4-4c3c-4458-80c9-23c5efaf1823": Phase="Pending", Reason="", readiness=false. Elapsed: 10.536743ms
Apr  6 04:18:19.475: INFO: Pod "downward-api-f29530d4-4c3c-4458-80c9-23c5efaf1823": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024300804s
STEP: Saw pod success
Apr  6 04:18:19.475: INFO: Pod "downward-api-f29530d4-4c3c-4458-80c9-23c5efaf1823" satisfied condition "Succeeded or Failed"
Apr  6 04:18:19.488: INFO: Trying to get logs from node 10.241.0.102 pod downward-api-f29530d4-4c3c-4458-80c9-23c5efaf1823 container dapi-container: <nil>
STEP: delete the pod
Apr  6 04:18:19.616: INFO: Waiting for pod downward-api-f29530d4-4c3c-4458-80c9-23c5efaf1823 to disappear
Apr  6 04:18:19.627: INFO: Pod downward-api-f29530d4-4c3c-4458-80c9-23c5efaf1823 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:18:19.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8237" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":4411,"failed":0}
S
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:18:19.655: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Apr  6 04:18:19.947: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:18:21.962: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:18:23.963: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.241.0.102 on the node which pod1 resides and expect scheduled
Apr  6 04:18:24.022: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:18:26.036: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:18:28.049: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.241.0.102 but use UDP protocol on the node which pod2 resides
Apr  6 04:18:28.118: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:18:30.155: INFO: The status of Pod pod3 is Running (Ready = false)
Apr  6 04:18:32.137: INFO: The status of Pod pod3 is Running (Ready = true)
Apr  6 04:18:32.183: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:18:34.198: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Apr  6 04:18:34.209: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.241.0.102 http://127.0.0.1:54323/hostname] Namespace:hostport-2451 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:18:34.209: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.241.0.102, port: 54323
Apr  6 04:18:34.372: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.241.0.102:54323/hostname] Namespace:hostport-2451 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:18:34.372: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.241.0.102, port: 54323 UDP
Apr  6 04:18:34.542: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.241.0.102 54323] Namespace:hostport-2451 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:18:34.542: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:18:40.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-2451" for this suite.

• [SLOW TEST:20.413 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":268,"skipped":4412,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:18:40.068: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Apr  6 04:18:40.277: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:19:21.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9603" for this suite.

• [SLOW TEST:41.862 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":269,"skipped":4428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:19:21.930: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-fd3a4c9d-002a-4ce7-8afb-4bda8258dea5
STEP: Creating a pod to test consume secrets
Apr  6 04:19:22.392: INFO: Waiting up to 5m0s for pod "pod-secrets-5e958614-b8b5-4f0d-980c-d08a1aebeabe" in namespace "secrets-7067" to be "Succeeded or Failed"
Apr  6 04:19:22.428: INFO: Pod "pod-secrets-5e958614-b8b5-4f0d-980c-d08a1aebeabe": Phase="Pending", Reason="", readiness=false. Elapsed: 36.03917ms
Apr  6 04:19:24.446: INFO: Pod "pod-secrets-5e958614-b8b5-4f0d-980c-d08a1aebeabe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05460709s
Apr  6 04:19:26.460: INFO: Pod "pod-secrets-5e958614-b8b5-4f0d-980c-d08a1aebeabe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068185351s
STEP: Saw pod success
Apr  6 04:19:26.460: INFO: Pod "pod-secrets-5e958614-b8b5-4f0d-980c-d08a1aebeabe" satisfied condition "Succeeded or Failed"
Apr  6 04:19:26.472: INFO: Trying to get logs from node 10.241.0.102 pod pod-secrets-5e958614-b8b5-4f0d-980c-d08a1aebeabe container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 04:19:26.552: INFO: Waiting for pod pod-secrets-5e958614-b8b5-4f0d-980c-d08a1aebeabe to disappear
Apr  6 04:19:26.563: INFO: Pod pod-secrets-5e958614-b8b5-4f0d-980c-d08a1aebeabe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:19:26.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7067" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":4467,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:19:26.601: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-969f3d1a-fa4f-4892-ab85-94f1b099be86
STEP: Creating a pod to test consume configMaps
Apr  6 04:19:26.874: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-775e235a-8e2e-4dae-8c98-df3447a1f6c3" in namespace "projected-570" to be "Succeeded or Failed"
Apr  6 04:19:26.914: INFO: Pod "pod-projected-configmaps-775e235a-8e2e-4dae-8c98-df3447a1f6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 39.607091ms
Apr  6 04:19:28.956: INFO: Pod "pod-projected-configmaps-775e235a-8e2e-4dae-8c98-df3447a1f6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.081554961s
STEP: Saw pod success
Apr  6 04:19:28.956: INFO: Pod "pod-projected-configmaps-775e235a-8e2e-4dae-8c98-df3447a1f6c3" satisfied condition "Succeeded or Failed"
Apr  6 04:19:28.970: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-configmaps-775e235a-8e2e-4dae-8c98-df3447a1f6c3 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 04:19:29.032: INFO: Waiting for pod pod-projected-configmaps-775e235a-8e2e-4dae-8c98-df3447a1f6c3 to disappear
Apr  6 04:19:29.044: INFO: Pod pod-projected-configmaps-775e235a-8e2e-4dae-8c98-df3447a1f6c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:19:29.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-570" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":4471,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:19:29.081: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 04:19:29.654: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 04:19:31.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815569, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815569, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815569, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815569, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 04:19:34.739: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr  6 04:19:34.847: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:19:34.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5164" for this suite.
STEP: Destroying namespace "webhook-5164-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.018 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":272,"skipped":4472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:19:35.099: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Apr  6 04:19:35.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-8297 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Apr  6 04:19:35.503: INFO: stderr: ""
Apr  6 04:19:35.503: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Apr  6 04:19:35.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-8297 delete pods e2e-test-httpd-pod'
Apr  6 04:19:39.495: INFO: stderr: ""
Apr  6 04:19:39.495: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:19:39.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8297" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":273,"skipped":4548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:19:39.563: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Apr  6 04:19:39.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6607 api-versions'
Apr  6 04:19:39.812: INFO: stderr: ""
Apr  6 04:19:39.812: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:19:39.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6607" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":274,"skipped":4595,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:19:39.864: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 04:19:40.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3391145-11f6-4b8e-a082-c0f9bda98d62" in namespace "projected-3658" to be "Succeeded or Failed"
Apr  6 04:19:40.105: INFO: Pod "downwardapi-volume-f3391145-11f6-4b8e-a082-c0f9bda98d62": Phase="Pending", Reason="", readiness=false. Elapsed: 14.814272ms
Apr  6 04:19:42.122: INFO: Pod "downwardapi-volume-f3391145-11f6-4b8e-a082-c0f9bda98d62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032385077s
Apr  6 04:19:44.141: INFO: Pod "downwardapi-volume-f3391145-11f6-4b8e-a082-c0f9bda98d62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051173635s
STEP: Saw pod success
Apr  6 04:19:44.141: INFO: Pod "downwardapi-volume-f3391145-11f6-4b8e-a082-c0f9bda98d62" satisfied condition "Succeeded or Failed"
Apr  6 04:19:44.163: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-f3391145-11f6-4b8e-a082-c0f9bda98d62 container client-container: <nil>
STEP: delete the pod
Apr  6 04:19:44.236: INFO: Waiting for pod downwardapi-volume-f3391145-11f6-4b8e-a082-c0f9bda98d62 to disappear
Apr  6 04:19:44.248: INFO: Pod downwardapi-volume-f3391145-11f6-4b8e-a082-c0f9bda98d62 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:19:44.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3658" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":4596,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:19:44.290: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Apr  6 04:19:44.681: INFO: The status of Pod labelsupdate4dc1c156-04c3-4589-837f-3ba9f5d8fa49 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:19:46.700: INFO: The status of Pod labelsupdate4dc1c156-04c3-4589-837f-3ba9f5d8fa49 is Running (Ready = true)
Apr  6 04:19:47.403: INFO: Successfully updated pod "labelsupdate4dc1c156-04c3-4589-837f-3ba9f5d8fa49"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:19:49.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2933" for this suite.

• [SLOW TEST:5.215 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":4606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:19:49.505: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3030
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  6 04:19:49.744: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 04:19:50.340: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:19:52.357: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:19:54.354: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:19:56.361: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:19:58.357: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:20:00.355: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:20:02.353: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:20:04.353: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:20:06.356: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:20:08.354: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 04:20:10.358: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr  6 04:20:10.381: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr  6 04:20:10.426: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr  6 04:20:14.580: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr  6 04:20:14.580: INFO: Going to poll 172.17.77.58 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Apr  6 04:20:14.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.77.58:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:20:14.591: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 04:20:14.763: INFO: Found all 1 expected endpoints: [netserver-0]
Apr  6 04:20:14.763: INFO: Going to poll 172.17.95.155 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Apr  6 04:20:14.776: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.95.155:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:20:14.776: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 04:20:14.940: INFO: Found all 1 expected endpoints: [netserver-1]
Apr  6 04:20:14.940: INFO: Going to poll 172.17.96.98 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Apr  6 04:20:14.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.96.98:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 04:20:14.955: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 04:20:15.120: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:20:15.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3030" for this suite.

• [SLOW TEST:25.652 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":277,"skipped":4674,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:20:15.157: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-f0635656-5dc1-4f86-be35-3e70a5307af2
STEP: Creating a pod to test consume secrets
Apr  6 04:20:15.424: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45f08ba4-7f27-41d8-bca8-527941260c55" in namespace "projected-6974" to be "Succeeded or Failed"
Apr  6 04:20:15.435: INFO: Pod "pod-projected-secrets-45f08ba4-7f27-41d8-bca8-527941260c55": Phase="Pending", Reason="", readiness=false. Elapsed: 10.795786ms
Apr  6 04:20:17.447: INFO: Pod "pod-projected-secrets-45f08ba4-7f27-41d8-bca8-527941260c55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022217092s
Apr  6 04:20:19.462: INFO: Pod "pod-projected-secrets-45f08ba4-7f27-41d8-bca8-527941260c55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037157723s
STEP: Saw pod success
Apr  6 04:20:19.462: INFO: Pod "pod-projected-secrets-45f08ba4-7f27-41d8-bca8-527941260c55" satisfied condition "Succeeded or Failed"
Apr  6 04:20:19.472: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-secrets-45f08ba4-7f27-41d8-bca8-527941260c55 container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 04:20:19.538: INFO: Waiting for pod pod-projected-secrets-45f08ba4-7f27-41d8-bca8-527941260c55 to disappear
Apr  6 04:20:19.549: INFO: Pod pod-projected-secrets-45f08ba4-7f27-41d8-bca8-527941260c55 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:20:19.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6974" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":4682,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:20:19.587: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-7811/configmap-test-f5d7ed60-0563-4ced-a508-e2e7bc341289
STEP: Creating a pod to test consume configMaps
Apr  6 04:20:19.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-59b9ace2-332a-48a0-ab1f-9dd4b125980d" in namespace "configmap-7811" to be "Succeeded or Failed"
Apr  6 04:20:19.810: INFO: Pod "pod-configmaps-59b9ace2-332a-48a0-ab1f-9dd4b125980d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.941375ms
Apr  6 04:20:21.828: INFO: Pod "pod-configmaps-59b9ace2-332a-48a0-ab1f-9dd4b125980d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029338231s
STEP: Saw pod success
Apr  6 04:20:21.829: INFO: Pod "pod-configmaps-59b9ace2-332a-48a0-ab1f-9dd4b125980d" satisfied condition "Succeeded or Failed"
Apr  6 04:20:21.840: INFO: Trying to get logs from node 10.241.0.102 pod pod-configmaps-59b9ace2-332a-48a0-ab1f-9dd4b125980d container env-test: <nil>
STEP: delete the pod
Apr  6 04:20:21.998: INFO: Waiting for pod pod-configmaps-59b9ace2-332a-48a0-ab1f-9dd4b125980d to disappear
Apr  6 04:20:22.010: INFO: Pod pod-configmaps-59b9ace2-332a-48a0-ab1f-9dd4b125980d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:20:22.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7811" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":4717,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:20:22.067: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:20:33.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2551" for this suite.

• [SLOW TEST:11.581 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":280,"skipped":4727,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:20:33.648: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:20:33.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7007" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":281,"skipped":4727,"failed":0}
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:20:33.926: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:20:34.275: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8beacd02-d594-4b7e-b3cb-b83d0b8f5bfb" in namespace "security-context-test-3139" to be "Succeeded or Failed"
Apr  6 04:20:34.285: INFO: Pod "alpine-nnp-false-8beacd02-d594-4b7e-b3cb-b83d0b8f5bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.976439ms
Apr  6 04:20:36.299: INFO: Pod "alpine-nnp-false-8beacd02-d594-4b7e-b3cb-b83d0b8f5bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023186138s
Apr  6 04:20:38.315: INFO: Pod "alpine-nnp-false-8beacd02-d594-4b7e-b3cb-b83d0b8f5bfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039722438s
Apr  6 04:20:38.315: INFO: Pod "alpine-nnp-false-8beacd02-d594-4b7e-b3cb-b83d0b8f5bfb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:20:38.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3139" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":282,"skipped":4730,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:20:38.371: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:20:38.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-752" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":283,"skipped":4738,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:20:38.730: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr  6 04:20:38.922: INFO: Waiting up to 5m0s for pod "pod-4488b525-ce5d-4ca9-9386-d617728fc440" in namespace "emptydir-2559" to be "Succeeded or Failed"
Apr  6 04:20:38.932: INFO: Pod "pod-4488b525-ce5d-4ca9-9386-d617728fc440": Phase="Pending", Reason="", readiness=false. Elapsed: 9.99702ms
Apr  6 04:20:40.943: INFO: Pod "pod-4488b525-ce5d-4ca9-9386-d617728fc440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021722121s
STEP: Saw pod success
Apr  6 04:20:40.943: INFO: Pod "pod-4488b525-ce5d-4ca9-9386-d617728fc440" satisfied condition "Succeeded or Failed"
Apr  6 04:20:40.953: INFO: Trying to get logs from node 10.241.0.102 pod pod-4488b525-ce5d-4ca9-9386-d617728fc440 container test-container: <nil>
STEP: delete the pod
Apr  6 04:20:41.021: INFO: Waiting for pod pod-4488b525-ce5d-4ca9-9386-d617728fc440 to disappear
Apr  6 04:20:41.031: INFO: Pod pod-4488b525-ce5d-4ca9-9386-d617728fc440 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:20:41.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2559" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":4741,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:20:41.078: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-1fef9a0a-1a3b-415f-bb02-6b28a7bf1df7 in namespace container-probe-7392
Apr  6 04:20:45.317: INFO: Started pod liveness-1fef9a0a-1a3b-415f-bb02-6b28a7bf1df7 in namespace container-probe-7392
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 04:20:45.327: INFO: Initial restart count of pod liveness-1fef9a0a-1a3b-415f-bb02-6b28a7bf1df7 is 0
Apr  6 04:21:03.474: INFO: Restart count of pod container-probe-7392/liveness-1fef9a0a-1a3b-415f-bb02-6b28a7bf1df7 is now 1 (18.146962292s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:03.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7392" for this suite.

• [SLOW TEST:22.488 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":4753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:03.567: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:21:03.836: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr  6 04:21:06.020: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:07.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7660" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":286,"skipped":4796,"failed":0}

------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:07.092: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr  6 04:21:07.348: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr  6 04:21:07.384: INFO: starting watch
STEP: patching
STEP: updating
Apr  6 04:21:07.456: INFO: waiting for watch events with expected annotations
Apr  6 04:21:07.456: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:07.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-21" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":287,"skipped":4796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:07.569: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:07.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4893" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":288,"skipped":4847,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:07.890: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:08.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-182" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":4860,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Apr  6 04:21:08.441: INFO: Creating simple deployment test-deployment-jrcvf
Apr  6 04:21:08.485: INFO: deployment "test-deployment-jrcvf" doesn't have the required revision set
Apr  6 04:21:10.520: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815668, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815668, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815668, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815668, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-jrcvf-794dd694d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Apr  6 04:21:12.585: INFO: Deployment test-deployment-jrcvf has Conditions: [{Available True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:10 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrcvf-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Apr  6 04:21:12.618: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815670, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815670, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815670, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815668, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jrcvf-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Apr  6 04:21:12.623: INFO: Observed &Deployment event: ADDED
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jrcvf-794dd694d8"}
Apr  6 04:21:12.623: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jrcvf-794dd694d8"}
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr  6 04:21:12.623: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jrcvf-794dd694d8" is progressing.}
Apr  6 04:21:12.623: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:10 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrcvf-794dd694d8" has successfully progressed.}
Apr  6 04:21:12.623: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:10 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr  6 04:21:12.623: INFO: Observed Deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrcvf-794dd694d8" has successfully progressed.}
Apr  6 04:21:12.623: INFO: Found Deployment test-deployment-jrcvf in namespace deployment-9592 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr  6 04:21:12.623: INFO: Deployment test-deployment-jrcvf has an updated status
STEP: patching the Statefulset Status
Apr  6 04:21:12.623: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr  6 04:21:12.646: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Apr  6 04:21:12.649: INFO: Observed &Deployment event: ADDED
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jrcvf-794dd694d8"}
Apr  6 04:21:12.650: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jrcvf-794dd694d8"}
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr  6 04:21:12.650: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:08 +0000 UTC 2022-04-06 04:21:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jrcvf-794dd694d8" is progressing.}
Apr  6 04:21:12.650: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:10 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrcvf-794dd694d8" has successfully progressed.}
Apr  6 04:21:12.650: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:10 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-06 04:21:10 +0000 UTC 2022-04-06 04:21:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrcvf-794dd694d8" has successfully progressed.}
Apr  6 04:21:12.650: INFO: Observed deployment test-deployment-jrcvf in namespace deployment-9592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr  6 04:21:12.650: INFO: Observed &Deployment event: MODIFIED
Apr  6 04:21:12.650: INFO: Found deployment test-deployment-jrcvf in namespace deployment-9592 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Apr  6 04:21:12.650: INFO: Deployment test-deployment-jrcvf has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr  6 04:21:12.663: INFO: Deployment "test-deployment-jrcvf":
&Deployment{ObjectMeta:{test-deployment-jrcvf  deployment-9592  3a605274-e05b-4e88-8439-4b463f823c37 119078 1 2022-04-06 04:21:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-04-06 04:21:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-04-06 04:21:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-04-06 04:21:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e4a478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jrcvf-794dd694d8",LastUpdateTime:2022-04-06 04:21:12 +0000 UTC,LastTransitionTime:2022-04-06 04:21:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 04:21:12.672: INFO: New ReplicaSet "test-deployment-jrcvf-794dd694d8" of Deployment "test-deployment-jrcvf":
&ReplicaSet{ObjectMeta:{test-deployment-jrcvf-794dd694d8  deployment-9592  8d84bc79-5f7b-4bf7-b886-71c2417d8ae9 119051 1 2022-04-06 04:21:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jrcvf 3a605274-e05b-4e88-8439-4b463f823c37 0xc003212b50 0xc003212b51}] []  [{kube-controller-manager Update apps/v1 2022-04-06 04:21:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a605274-e05b-4e88-8439-4b463f823c37\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 04:21:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003212bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 04:21:12.708: INFO: Pod "test-deployment-jrcvf-794dd694d8-j6xdj" is available:
&Pod{ObjectMeta:{test-deployment-jrcvf-794dd694d8-j6xdj test-deployment-jrcvf-794dd694d8- deployment-9592  b1893bf4-b6d0-4da4-b32d-4fe2e844f449 119050 0 2022-04-06 04:21:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/containerID:8eab3e1cf36885e577f0e3440c1addda7e213e972c11b5bbe206944692d53e0d cni.projectcalico.org/podIP:172.17.96.80/32 cni.projectcalico.org/podIPs:172.17.96.80/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.80"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.17.96.80"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-jrcvf-794dd694d8 8d84bc79-5f7b-4bf7-b886-71c2417d8ae9 0xc003212fd7 0xc003212fd8}] []  [{kube-controller-manager Update v1 2022-04-06 04:21:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d84bc79-5f7b-4bf7-b886-71c2417d8ae9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-06 04:21:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-04-06 04:21:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-04-06 04:21:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.96.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdsnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdsnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c62,c39,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n55jv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 04:21:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 04:21:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 04:21:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-06 04:21:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.0.102,PodIP:172.17.96.80,StartTime:2022-04-06 04:21:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-06 04:21:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://6137eff0be53d7284aa83f5a010f9207b2a2d4a0006ef63aaf20731df53d2256,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.96.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:12.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9592" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":290,"skipped":4877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:12.749: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 04:21:13.693: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 04:21:15.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815673, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815673, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815673, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815673, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 04:21:18.771: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:19.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2333" for this suite.
STEP: Destroying namespace "webhook-2333-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.631 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":291,"skipped":4905,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:19.380: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-bc874e7c-9301-4264-86fd-2e38845b4a3a
STEP: Creating a pod to test consume configMaps
Apr  6 04:21:19.622: INFO: Waiting up to 5m0s for pod "pod-configmaps-4972a146-9347-47ad-a746-b6e8aac74236" in namespace "configmap-2611" to be "Succeeded or Failed"
Apr  6 04:21:19.633: INFO: Pod "pod-configmaps-4972a146-9347-47ad-a746-b6e8aac74236": Phase="Pending", Reason="", readiness=false. Elapsed: 11.118971ms
Apr  6 04:21:21.647: INFO: Pod "pod-configmaps-4972a146-9347-47ad-a746-b6e8aac74236": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024827759s
Apr  6 04:21:23.673: INFO: Pod "pod-configmaps-4972a146-9347-47ad-a746-b6e8aac74236": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050591798s
STEP: Saw pod success
Apr  6 04:21:23.673: INFO: Pod "pod-configmaps-4972a146-9347-47ad-a746-b6e8aac74236" satisfied condition "Succeeded or Failed"
Apr  6 04:21:23.685: INFO: Trying to get logs from node 10.241.0.102 pod pod-configmaps-4972a146-9347-47ad-a746-b6e8aac74236 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 04:21:23.777: INFO: Waiting for pod pod-configmaps-4972a146-9347-47ad-a746-b6e8aac74236 to disappear
Apr  6 04:21:23.801: INFO: Pod pod-configmaps-4972a146-9347-47ad-a746-b6e8aac74236 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:23.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2611" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":4912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:23.839: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Apr  6 04:21:25.241: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0406 04:21:25.241364      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 04:21:25.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6744" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":293,"skipped":4940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:25.273: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:21:25.596: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: creating the pod
STEP: submitting the pod to kubernetes
Apr  6 04:21:25.664: INFO: The status of Pod pod-exec-websocket-0773f4ca-652a-43a3-a2a9-30cdcc53f512 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:21:27.678: INFO: The status of Pod pod-exec-websocket-0773f4ca-652a-43a3-a2a9-30cdcc53f512 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:21:29.688: INFO: The status of Pod pod-exec-websocket-0773f4ca-652a-43a3-a2a9-30cdcc53f512 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:29.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8389" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":4967,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:29.862: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 04:21:30.374: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 04:21:33.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:33.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4308" for this suite.
STEP: Destroying namespace "webhook-4308-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":295,"skipped":4969,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:33.876: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-6b3ed143-5ce7-4f87-8510-6cb87e11b079
STEP: Creating a pod to test consume secrets
Apr  6 04:21:34.101: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29d35912-db53-4b18-8eea-d96f82762177" in namespace "projected-5105" to be "Succeeded or Failed"
Apr  6 04:21:34.119: INFO: Pod "pod-projected-secrets-29d35912-db53-4b18-8eea-d96f82762177": Phase="Pending", Reason="", readiness=false. Elapsed: 18.087353ms
Apr  6 04:21:36.134: INFO: Pod "pod-projected-secrets-29d35912-db53-4b18-8eea-d96f82762177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032703027s
STEP: Saw pod success
Apr  6 04:21:36.134: INFO: Pod "pod-projected-secrets-29d35912-db53-4b18-8eea-d96f82762177" satisfied condition "Succeeded or Failed"
Apr  6 04:21:36.147: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-secrets-29d35912-db53-4b18-8eea-d96f82762177 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 04:21:36.207: INFO: Waiting for pod pod-projected-secrets-29d35912-db53-4b18-8eea-d96f82762177 to disappear
Apr  6 04:21:36.219: INFO: Pod pod-projected-secrets-29d35912-db53-4b18-8eea-d96f82762177 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:36.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5105" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":4982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:36.256: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Apr  6 04:21:36.432: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1593 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:36.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1593" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":297,"skipped":5028,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:36.570: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr  6 04:21:36.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1087  64aa38ef-b1f2-4bb0-ba60-e53877178f68 120027 0 2022-04-06 04:21:36 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-04-06 04:21:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 04:21:36.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1087  64aa38ef-b1f2-4bb0-ba60-e53877178f68 120028 0 2022-04-06 04:21:36 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-04-06 04:21:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:21:36.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1087" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":298,"skipped":5032,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:21:36.876: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:21:37.089: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr  6 04:21:46.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-1507 --namespace=crd-publish-openapi-1507 create -f -'
Apr  6 04:21:49.103: INFO: stderr: ""
Apr  6 04:21:49.103: INFO: stdout: "e2e-test-crd-publish-openapi-3882-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr  6 04:21:49.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-1507 --namespace=crd-publish-openapi-1507 delete e2e-test-crd-publish-openapi-3882-crds test-cr'
Apr  6 04:21:49.204: INFO: stderr: ""
Apr  6 04:21:49.204: INFO: stdout: "e2e-test-crd-publish-openapi-3882-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr  6 04:21:49.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-1507 --namespace=crd-publish-openapi-1507 apply -f -'
Apr  6 04:21:51.257: INFO: stderr: ""
Apr  6 04:21:51.257: INFO: stdout: "e2e-test-crd-publish-openapi-3882-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr  6 04:21:51.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-1507 --namespace=crd-publish-openapi-1507 delete e2e-test-crd-publish-openapi-3882-crds test-cr'
Apr  6 04:21:51.361: INFO: stderr: ""
Apr  6 04:21:51.362: INFO: stdout: "e2e-test-crd-publish-openapi-3882-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr  6 04:21:51.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-1507 explain e2e-test-crd-publish-openapi-3882-crds'
Apr  6 04:21:51.616: INFO: stderr: ""
Apr  6 04:21:51.616: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3882-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:22:01.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1507" for this suite.

• [SLOW TEST:24.489 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":299,"skipped":5040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:22:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:22:01.656: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-dc084bd5-40a9-4237-a675-3988d4759b1f
STEP: Creating secret with name s-test-opt-upd-aee98012-4793-4bbb-a9f4-89bda13f899f
STEP: Creating the pod
Apr  6 04:22:01.808: INFO: The status of Pod pod-secrets-a804a0bd-fd5b-49bc-813d-ff9f9ca7114e is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:22:03.819: INFO: The status of Pod pod-secrets-a804a0bd-fd5b-49bc-813d-ff9f9ca7114e is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:22:05.825: INFO: The status of Pod pod-secrets-a804a0bd-fd5b-49bc-813d-ff9f9ca7114e is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-dc084bd5-40a9-4237-a675-3988d4759b1f
STEP: Updating secret s-test-opt-upd-aee98012-4793-4bbb-a9f4-89bda13f899f
STEP: Creating secret with name s-test-opt-create-799eaad6-40ca-4ba5-862b-ee88f87be644
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:22:08.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6377" for this suite.

• [SLOW TEST:6.816 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:22:08.183: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr  6 04:22:08.420: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 04:22:17.324: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:22:50.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2361" for this suite.

• [SLOW TEST:42.170 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":301,"skipped":5159,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:22:50.352: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:22:50.553: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr  6 04:22:59.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7331 --namespace=crd-publish-openapi-7331 create -f -'
Apr  6 04:23:01.648: INFO: stderr: ""
Apr  6 04:23:01.648: INFO: stdout: "e2e-test-crd-publish-openapi-7420-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr  6 04:23:01.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7331 --namespace=crd-publish-openapi-7331 delete e2e-test-crd-publish-openapi-7420-crds test-cr'
Apr  6 04:23:01.731: INFO: stderr: ""
Apr  6 04:23:01.731: INFO: stdout: "e2e-test-crd-publish-openapi-7420-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr  6 04:23:01.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7331 --namespace=crd-publish-openapi-7331 apply -f -'
Apr  6 04:23:03.579: INFO: stderr: ""
Apr  6 04:23:03.579: INFO: stdout: "e2e-test-crd-publish-openapi-7420-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr  6 04:23:03.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7331 --namespace=crd-publish-openapi-7331 delete e2e-test-crd-publish-openapi-7420-crds test-cr'
Apr  6 04:23:03.671: INFO: stderr: ""
Apr  6 04:23:03.671: INFO: stdout: "e2e-test-crd-publish-openapi-7420-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr  6 04:23:03.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7331 explain e2e-test-crd-publish-openapi-7420-crds'
Apr  6 04:23:05.711: INFO: stderr: ""
Apr  6 04:23:05.711: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7420-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:23:14.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7331" for this suite.

• [SLOW TEST:24.293 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":302,"skipped":5166,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:23:14.645: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-73ffa492-902f-4d56-8669-7a87931d9a27
STEP: Creating a pod to test consume configMaps
Apr  6 04:23:15.007: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ab25998-f38a-438a-9232-87bea50269e7" in namespace "configmap-5641" to be "Succeeded or Failed"
Apr  6 04:23:15.025: INFO: Pod "pod-configmaps-6ab25998-f38a-438a-9232-87bea50269e7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.498032ms
Apr  6 04:23:17.047: INFO: Pod "pod-configmaps-6ab25998-f38a-438a-9232-87bea50269e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039878768s
Apr  6 04:23:19.077: INFO: Pod "pod-configmaps-6ab25998-f38a-438a-9232-87bea50269e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07022849s
STEP: Saw pod success
Apr  6 04:23:19.077: INFO: Pod "pod-configmaps-6ab25998-f38a-438a-9232-87bea50269e7" satisfied condition "Succeeded or Failed"
Apr  6 04:23:19.089: INFO: Trying to get logs from node 10.241.0.102 pod pod-configmaps-6ab25998-f38a-438a-9232-87bea50269e7 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 04:23:19.163: INFO: Waiting for pod pod-configmaps-6ab25998-f38a-438a-9232-87bea50269e7 to disappear
Apr  6 04:23:19.194: INFO: Pod pod-configmaps-6ab25998-f38a-438a-9232-87bea50269e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:23:19.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5641" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5182,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:23:19.417: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:23:19.579: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr  6 04:23:28.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 --namespace=crd-publish-openapi-7638 create -f -'
Apr  6 04:23:30.444: INFO: stderr: ""
Apr  6 04:23:30.444: INFO: stdout: "e2e-test-crd-publish-openapi-6187-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr  6 04:23:30.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 --namespace=crd-publish-openapi-7638 delete e2e-test-crd-publish-openapi-6187-crds test-foo'
Apr  6 04:23:30.639: INFO: stderr: ""
Apr  6 04:23:30.639: INFO: stdout: "e2e-test-crd-publish-openapi-6187-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr  6 04:23:30.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 --namespace=crd-publish-openapi-7638 apply -f -'
Apr  6 04:23:30.944: INFO: stderr: ""
Apr  6 04:23:30.944: INFO: stdout: "e2e-test-crd-publish-openapi-6187-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr  6 04:23:30.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 --namespace=crd-publish-openapi-7638 delete e2e-test-crd-publish-openapi-6187-crds test-foo'
Apr  6 04:23:31.100: INFO: stderr: ""
Apr  6 04:23:31.100: INFO: stdout: "e2e-test-crd-publish-openapi-6187-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr  6 04:23:31.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 --namespace=crd-publish-openapi-7638 create -f -'
Apr  6 04:23:33.774: INFO: rc: 1
Apr  6 04:23:33.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 --namespace=crd-publish-openapi-7638 apply -f -'
Apr  6 04:23:35.700: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr  6 04:23:35.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 --namespace=crd-publish-openapi-7638 create -f -'
Apr  6 04:23:35.945: INFO: rc: 1
Apr  6 04:23:35.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 --namespace=crd-publish-openapi-7638 apply -f -'
Apr  6 04:23:36.203: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr  6 04:23:36.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 explain e2e-test-crd-publish-openapi-6187-crds'
Apr  6 04:23:36.465: INFO: stderr: ""
Apr  6 04:23:36.465: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6187-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr  6 04:23:36.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 explain e2e-test-crd-publish-openapi-6187-crds.metadata'
Apr  6 04:23:36.790: INFO: stderr: ""
Apr  6 04:23:36.790: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6187-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr  6 04:23:36.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 explain e2e-test-crd-publish-openapi-6187-crds.spec'
Apr  6 04:23:37.045: INFO: stderr: ""
Apr  6 04:23:37.045: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6187-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr  6 04:23:37.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 explain e2e-test-crd-publish-openapi-6187-crds.spec.bars'
Apr  6 04:23:37.310: INFO: stderr: ""
Apr  6 04:23:37.310: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6187-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr  6 04:23:37.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=crd-publish-openapi-7638 explain e2e-test-crd-publish-openapi-6187-crds.spec.bars2'
Apr  6 04:23:37.566: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:23:46.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7638" for this suite.

• [SLOW TEST:27.402 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":304,"skipped":5189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:23:46.819: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9489
STEP: creating service affinity-clusterip in namespace services-9489
STEP: creating replication controller affinity-clusterip in namespace services-9489
I0406 04:23:47.315316      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9489, replica count: 3
I0406 04:23:50.365971      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 04:23:50.392: INFO: Creating new exec pod
Apr  6 04:23:55.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9489 exec execpod-affinity4drh4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Apr  6 04:23:55.759: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr  6 04:23:55.760: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:23:55.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9489 exec execpod-affinity4drh4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.141.216 80'
Apr  6 04:23:56.176: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.141.216 80\nConnection to 172.21.141.216 80 port [tcp/http] succeeded!\n"
Apr  6 04:23:56.176: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:23:56.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-9489 exec execpod-affinity4drh4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.141.216:80/ ; done'
Apr  6 04:23:56.508: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.141.216:80/\n"
Apr  6 04:23:56.508: INFO: stdout: "\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5\naffinity-clusterip-9p6n5"
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Received response from host: affinity-clusterip-9p6n5
Apr  6 04:23:56.508: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9489, will wait for the garbage collector to delete the pods
Apr  6 04:23:56.635: INFO: Deleting ReplicationController affinity-clusterip took: 11.770855ms
Apr  6 04:23:56.736: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.041736ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:24:00.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9489" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.224 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":305,"skipped":5221,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:24:00.044: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 04:24:00.779: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr  6 04:24:02.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815840, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815840, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815841, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815840, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 04:24:05.926: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr  6 04:24:10.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=webhook-7177 attach --namespace=webhook-7177 to-be-attached-pod -i -c=container1'
Apr  6 04:24:10.176: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:24:10.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7177" for this suite.
STEP: Destroying namespace "webhook-7177-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.368 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":306,"skipped":5234,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:24:10.412: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr  6 04:24:21.054: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr  6 04:24:21.054: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qrmb" in namespace "gc-9567"
W0406 04:24:21.054022      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 04:24:21.087: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2bfw" in namespace "gc-9567"
Apr  6 04:24:21.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw4ft" in namespace "gc-9567"
Apr  6 04:24:21.162: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsxxp" in namespace "gc-9567"
Apr  6 04:24:21.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-km7rw" in namespace "gc-9567"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:24:21.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9567" for this suite.

• [SLOW TEST:10.880 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":307,"skipped":5247,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:24:21.292: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 04:24:22.360: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr  6 04:24:24.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815862, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815862, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815862, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784815862, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 04:24:27.498: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:24:27.508: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:24:31.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2945" for this suite.
STEP: Destroying namespace "webhook-2945-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.055 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":308,"skipped":5257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:24:31.347: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Apr  6 04:24:31.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 create -f -'
Apr  6 04:24:35.188: INFO: stderr: ""
Apr  6 04:24:35.188: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  6 04:24:35.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:24:35.254: INFO: stderr: ""
Apr  6 04:24:35.254: INFO: stdout: "update-demo-nautilus-fgtfn "
STEP: Replicas for name=update-demo: expected=2 actual=1
Apr  6 04:24:40.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:24:40.329: INFO: stderr: ""
Apr  6 04:24:40.329: INFO: stdout: "update-demo-nautilus-fgtfn update-demo-nautilus-slzt7 "
Apr  6 04:24:40.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:24:40.410: INFO: stderr: ""
Apr  6 04:24:40.410: INFO: stdout: ""
Apr  6 04:24:40.410: INFO: update-demo-nautilus-fgtfn is created but not running
Apr  6 04:24:45.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:24:45.480: INFO: stderr: ""
Apr  6 04:24:45.480: INFO: stdout: "update-demo-nautilus-fgtfn update-demo-nautilus-slzt7 "
Apr  6 04:24:45.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:24:45.540: INFO: stderr: ""
Apr  6 04:24:45.540: INFO: stdout: "true"
Apr  6 04:24:45.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 04:24:45.602: INFO: stderr: ""
Apr  6 04:24:45.602: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Apr  6 04:24:45.602: INFO: validating pod update-demo-nautilus-fgtfn
Apr  6 04:24:45.616: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 04:24:45.616: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 04:24:45.616: INFO: update-demo-nautilus-fgtfn is verified up and running
Apr  6 04:24:45.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-slzt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:24:45.684: INFO: stderr: ""
Apr  6 04:24:45.684: INFO: stdout: "true"
Apr  6 04:24:45.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-slzt7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 04:24:45.754: INFO: stderr: ""
Apr  6 04:24:45.754: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Apr  6 04:24:45.754: INFO: validating pod update-demo-nautilus-slzt7
Apr  6 04:24:45.771: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 04:24:45.771: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 04:24:45.771: INFO: update-demo-nautilus-slzt7 is verified up and running
STEP: scaling down the replication controller
Apr  6 04:24:45.773: INFO: scanned /root for discovery docs: <nil>
Apr  6 04:24:45.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr  6 04:24:46.877: INFO: stderr: ""
Apr  6 04:24:46.877: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  6 04:24:46.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:24:46.954: INFO: stderr: ""
Apr  6 04:24:46.954: INFO: stdout: "update-demo-nautilus-fgtfn update-demo-nautilus-slzt7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr  6 04:24:51.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:24:52.027: INFO: stderr: ""
Apr  6 04:24:52.027: INFO: stdout: "update-demo-nautilus-fgtfn "
Apr  6 04:24:52.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:24:52.095: INFO: stderr: ""
Apr  6 04:24:52.095: INFO: stdout: "true"
Apr  6 04:24:52.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 04:24:52.168: INFO: stderr: ""
Apr  6 04:24:52.168: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Apr  6 04:24:52.168: INFO: validating pod update-demo-nautilus-fgtfn
Apr  6 04:24:52.183: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 04:24:52.183: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 04:24:52.183: INFO: update-demo-nautilus-fgtfn is verified up and running
STEP: scaling up the replication controller
Apr  6 04:24:52.185: INFO: scanned /root for discovery docs: <nil>
Apr  6 04:24:52.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr  6 04:24:53.293: INFO: stderr: ""
Apr  6 04:24:53.293: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  6 04:24:53.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:24:53.394: INFO: stderr: ""
Apr  6 04:24:53.394: INFO: stdout: "update-demo-nautilus-fgtfn update-demo-nautilus-zsvlj "
Apr  6 04:24:53.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:24:53.461: INFO: stderr: ""
Apr  6 04:24:53.461: INFO: stdout: "true"
Apr  6 04:24:53.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 04:24:53.536: INFO: stderr: ""
Apr  6 04:24:53.536: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Apr  6 04:24:53.536: INFO: validating pod update-demo-nautilus-fgtfn
Apr  6 04:24:53.572: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 04:24:53.572: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 04:24:53.572: INFO: update-demo-nautilus-fgtfn is verified up and running
Apr  6 04:24:53.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-zsvlj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:24:53.638: INFO: stderr: ""
Apr  6 04:24:53.638: INFO: stdout: ""
Apr  6 04:24:53.638: INFO: update-demo-nautilus-zsvlj is created but not running
Apr  6 04:24:58.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:24:58.715: INFO: stderr: ""
Apr  6 04:24:58.715: INFO: stdout: "update-demo-nautilus-fgtfn update-demo-nautilus-zsvlj "
Apr  6 04:24:58.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:24:58.798: INFO: stderr: ""
Apr  6 04:24:58.798: INFO: stdout: "true"
Apr  6 04:24:58.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-fgtfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 04:24:58.866: INFO: stderr: ""
Apr  6 04:24:58.866: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Apr  6 04:24:58.866: INFO: validating pod update-demo-nautilus-fgtfn
Apr  6 04:24:58.913: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 04:24:58.913: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 04:24:58.913: INFO: update-demo-nautilus-fgtfn is verified up and running
Apr  6 04:24:58.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-zsvlj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:24:58.982: INFO: stderr: ""
Apr  6 04:24:58.982: INFO: stdout: "true"
Apr  6 04:24:58.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods update-demo-nautilus-zsvlj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 04:24:59.081: INFO: stderr: ""
Apr  6 04:24:59.081: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Apr  6 04:24:59.081: INFO: validating pod update-demo-nautilus-zsvlj
Apr  6 04:24:59.142: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 04:24:59.142: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 04:24:59.142: INFO: update-demo-nautilus-zsvlj is verified up and running
STEP: using delete to clean up resources
Apr  6 04:24:59.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 delete --grace-period=0 --force -f -'
Apr  6 04:24:59.238: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 04:24:59.238: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  6 04:24:59.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get rc,svc -l name=update-demo --no-headers'
Apr  6 04:24:59.336: INFO: stderr: "No resources found in kubectl-1299 namespace.\n"
Apr  6 04:24:59.336: INFO: stdout: ""
Apr  6 04:24:59.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-1299 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 04:24:59.417: INFO: stderr: ""
Apr  6 04:24:59.417: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:24:59.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1299" for this suite.

• [SLOW TEST:28.094 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":309,"skipped":5324,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:24:59.442: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:29:59.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5979" for this suite.

• [SLOW TEST:300.262 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":310,"skipped":5327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:29:59.704: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  6 04:29:59.926: INFO: Number of nodes with available pods: 0
Apr  6 04:29:59.926: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:30:00.956: INFO: Number of nodes with available pods: 0
Apr  6 04:30:00.956: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:30:01.956: INFO: Number of nodes with available pods: 1
Apr  6 04:30:01.956: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:30:02.954: INFO: Number of nodes with available pods: 2
Apr  6 04:30:02.954: INFO: Node 10.241.0.102 is running more than one daemon pod
Apr  6 04:30:03.948: INFO: Number of nodes with available pods: 3
Apr  6 04:30:03.948: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr  6 04:30:04.008: INFO: Number of nodes with available pods: 2
Apr  6 04:30:04.008: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:30:05.032: INFO: Number of nodes with available pods: 2
Apr  6 04:30:05.032: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:30:06.063: INFO: Number of nodes with available pods: 2
Apr  6 04:30:06.063: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:30:07.027: INFO: Number of nodes with available pods: 2
Apr  6 04:30:07.027: INFO: Node 10.241.0.100 is running more than one daemon pod
Apr  6 04:30:08.081: INFO: Number of nodes with available pods: 3
Apr  6 04:30:08.081: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3254, will wait for the garbage collector to delete the pods
Apr  6 04:30:08.159: INFO: Deleting DaemonSet.extensions daemon-set took: 12.544205ms
Apr  6 04:30:08.259: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.422273ms
Apr  6 04:30:10.971: INFO: Number of nodes with available pods: 0
Apr  6 04:30:10.971: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 04:30:10.985: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"124989"},"items":null}

Apr  6 04:30:10.996: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"124989"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:30:11.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3254" for this suite.

• [SLOW TEST:11.347 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":311,"skipped":5351,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:30:11.050: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:30:11.330: INFO: The status of Pod busybox-scheduling-e89f962e-050d-4b5a-8f2a-2392ff594cde is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:30:13.344: INFO: The status of Pod busybox-scheduling-e89f962e-050d-4b5a-8f2a-2392ff594cde is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:30:13.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7454" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":5355,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:30:13.418: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:30:13.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6189" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":313,"skipped":5360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:30:13.821: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 04:30:14.540: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr  6 04:30:16.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784816214, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784816214, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63784816214, loc:(*time.Location)(0xa0a4dc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784816214, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 04:30:19.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:30:19.630: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6145-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:30:23.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7362" for this suite.
STEP: Destroying namespace "webhook-7362-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.421 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":314,"skipped":5386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:30:23.242: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-7abcfd2f-b7ab-48ad-94d7-24e0a899de5e
STEP: Creating a pod to test consume secrets
Apr  6 04:30:23.485: INFO: Waiting up to 5m0s for pod "pod-secrets-007952c5-0e4f-4360-bd96-aecde624dfcd" in namespace "secrets-7019" to be "Succeeded or Failed"
Apr  6 04:30:23.496: INFO: Pod "pod-secrets-007952c5-0e4f-4360-bd96-aecde624dfcd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.333201ms
Apr  6 04:30:25.507: INFO: Pod "pod-secrets-007952c5-0e4f-4360-bd96-aecde624dfcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02136801s
Apr  6 04:30:27.517: INFO: Pod "pod-secrets-007952c5-0e4f-4360-bd96-aecde624dfcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031711347s
STEP: Saw pod success
Apr  6 04:30:27.517: INFO: Pod "pod-secrets-007952c5-0e4f-4360-bd96-aecde624dfcd" satisfied condition "Succeeded or Failed"
Apr  6 04:30:27.525: INFO: Trying to get logs from node 10.241.0.102 pod pod-secrets-007952c5-0e4f-4360-bd96-aecde624dfcd container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 04:30:27.600: INFO: Waiting for pod pod-secrets-007952c5-0e4f-4360-bd96-aecde624dfcd to disappear
Apr  6 04:30:27.609: INFO: Pod pod-secrets-007952c5-0e4f-4360-bd96-aecde624dfcd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:30:27.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7019" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":5436,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:30:27.631: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5382
Apr  6 04:30:27.862: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:30:29.874: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Apr  6 04:30:29.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5382 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr  6 04:30:30.144: INFO: rc: 7
Apr  6 04:30:30.176: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr  6 04:30:30.184: INFO: Pod kube-proxy-mode-detector no longer exists
Apr  6 04:30:30.184: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5382 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-5382
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5382
I0406 04:30:30.231751      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5382, replica count: 3
I0406 04:30:33.282901      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 04:30:33.304: INFO: Creating new exec pod
Apr  6 04:30:36.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5382 exec execpod-affinity945mj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Apr  6 04:30:36.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Apr  6 04:30:36.611: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:30:36.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5382 exec execpod-affinity945mj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.29.213 80'
Apr  6 04:30:36.838: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.29.213 80\nConnection to 172.21.29.213 80 port [tcp/http] succeeded!\n"
Apr  6 04:30:36.838: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:30:36.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5382 exec execpod-affinity945mj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.29.213:80/ ; done'
Apr  6 04:30:37.147: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n"
Apr  6 04:30:37.147: INFO: stdout: "\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d\naffinity-clusterip-timeout-vv24d"
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Received response from host: affinity-clusterip-timeout-vv24d
Apr  6 04:30:37.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5382 exec execpod-affinity945mj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.29.213:80/'
Apr  6 04:30:37.386: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n"
Apr  6 04:30:37.386: INFO: stdout: "affinity-clusterip-timeout-vv24d"
Apr  6 04:30:57.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-5382 exec execpod-affinity945mj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.29.213:80/'
Apr  6 04:30:57.655: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.29.213:80/\n"
Apr  6 04:30:57.655: INFO: stdout: "affinity-clusterip-timeout-dp99g"
Apr  6 04:30:57.655: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5382, will wait for the garbage collector to delete the pods
Apr  6 04:30:57.764: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 12.120682ms
Apr  6 04:30:57.965: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 201.042737ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:31:00.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5382" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:33.322 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":316,"skipped":5444,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:31:00.954: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 04:31:01.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b4d9990-4cc8-46f7-9a51-9f848b0f3756" in namespace "downward-api-7848" to be "Succeeded or Failed"
Apr  6 04:31:01.229: INFO: Pod "downwardapi-volume-6b4d9990-4cc8-46f7-9a51-9f848b0f3756": Phase="Pending", Reason="", readiness=false. Elapsed: 9.086892ms
Apr  6 04:31:03.241: INFO: Pod "downwardapi-volume-6b4d9990-4cc8-46f7-9a51-9f848b0f3756": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021050295s
STEP: Saw pod success
Apr  6 04:31:03.241: INFO: Pod "downwardapi-volume-6b4d9990-4cc8-46f7-9a51-9f848b0f3756" satisfied condition "Succeeded or Failed"
Apr  6 04:31:03.251: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-6b4d9990-4cc8-46f7-9a51-9f848b0f3756 container client-container: <nil>
STEP: delete the pod
Apr  6 04:31:03.322: INFO: Waiting for pod downwardapi-volume-6b4d9990-4cc8-46f7-9a51-9f848b0f3756 to disappear
Apr  6 04:31:03.332: INFO: Pod downwardapi-volume-6b4d9990-4cc8-46f7-9a51-9f848b0f3756 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:31:03.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7848" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":317,"skipped":5446,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:31:03.369: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Apr  6 04:31:03.590: INFO: PodSpec: initContainers in spec.initContainers
Apr  6 04:31:49.318: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-943bf786-3825-40c8-b825-b65941a102f3", GenerateName:"", Namespace:"init-container-4237", SelfLink:"", UID:"c0894d21-e8de-4403-887c-a5fa12f9767a", ResourceVersion:"126367", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63784816263, loc:(*time.Location)(0xa0a4dc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"590353956"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"d24df581c40ffbce28fd67eed0dcb1ee848ae34b78169d60b3c603074f240f00", "cni.projectcalico.org/podIP":"172.17.96.127/32", "cni.projectcalico.org/podIPs":"172.17.96.127/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.17.96.127\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.17.96.127\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002c06078), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002c06090), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002c060a8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002c060c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002c06120), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002c06138), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002c06150), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002c06168), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pglcm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003c04020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pglcm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003d98240), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pglcm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003d98300), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pglcm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003d98180), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027e8208), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.241.0.102", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003f32070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027e82c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027e82e0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027e82fc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027e8300), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004a84060), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784816263, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784816263, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784816263, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63784816263, loc:(*time.Location)(0xa0a4dc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.241.0.102", PodIP:"172.17.96.127", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.17.96.127"}}, StartTime:(*v1.Time)(0xc002c06198), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003f321c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003f32230)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:244bdbdf4b8d368b5836e9d2c7808a280a73ad72ae321d644e9f220da503218f", ContainerID:"cri-o://d7dfb35a7718e2e0814126ade5baa3f985600fa2857e0b8adeffffbce54ef404", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c040c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c040a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc0027e837f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:31:49.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4237" for this suite.

• [SLOW TEST:45.979 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":318,"skipped":5489,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:31:49.348: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2546, will wait for the garbage collector to delete the pods
Apr  6 04:31:53.750: INFO: Deleting Job.batch foo took: 15.373085ms
Apr  6 04:31:53.951: INFO: Terminating Job.batch foo pods took: 201.025656ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:32:25.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2546" for this suite.

• [SLOW TEST:36.541 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":319,"skipped":5503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:32:25.890: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Apr  6 04:32:26.087: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:32:30.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4166" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":320,"skipped":5607,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:32:30.504: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Apr  6 04:32:32.729: INFO: pods: 0 < 3
Apr  6 04:32:34.739: INFO: running pods: 1 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Apr  6 04:32:40.963: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:32:43.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9323" for this suite.

• [SLOW TEST:12.634 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":321,"skipped":5608,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:32:43.137: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-ec63f712-32de-4aa6-b376-9d57229cb81e
STEP: Creating a pod to test consume configMaps
Apr  6 04:32:43.381: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d0254ea2-a362-4bea-987f-1e3303f01cd3" in namespace "projected-3440" to be "Succeeded or Failed"
Apr  6 04:32:43.390: INFO: Pod "pod-projected-configmaps-d0254ea2-a362-4bea-987f-1e3303f01cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.442048ms
Apr  6 04:32:45.400: INFO: Pod "pod-projected-configmaps-d0254ea2-a362-4bea-987f-1e3303f01cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019138251s
Apr  6 04:32:47.426: INFO: Pod "pod-projected-configmaps-d0254ea2-a362-4bea-987f-1e3303f01cd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045221196s
STEP: Saw pod success
Apr  6 04:32:47.426: INFO: Pod "pod-projected-configmaps-d0254ea2-a362-4bea-987f-1e3303f01cd3" satisfied condition "Succeeded or Failed"
Apr  6 04:32:47.435: INFO: Trying to get logs from node 10.241.0.102 pod pod-projected-configmaps-d0254ea2-a362-4bea-987f-1e3303f01cd3 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 04:32:47.558: INFO: Waiting for pod pod-projected-configmaps-d0254ea2-a362-4bea-987f-1e3303f01cd3 to disappear
Apr  6 04:32:47.598: INFO: Pod pod-projected-configmaps-d0254ea2-a362-4bea-987f-1e3303f01cd3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:32:47.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3440" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":5609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:32:47.629: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Apr  6 04:32:48.028: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 04:32:48.028: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 04:32:48.062: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 04:32:48.062: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 04:32:48.100: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 04:32:48.100: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 04:32:48.315: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 04:32:48.315: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 04:32:50.221: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr  6 04:32:50.221: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr  6 04:32:50.578: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Apr  6 04:32:50.606: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Apr  6 04:32:50.609: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0
Apr  6 04:32:50.609: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0
Apr  6 04:32:50.609: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0
Apr  6 04:32:50.609: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 0
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:50.610: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:50.643: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:50.643: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:50.704: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:50.704: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:50.733: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:50.733: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:50.755: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:50.755: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:52.678: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:52.678: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:52.765: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
STEP: listing Deployments
Apr  6 04:32:52.798: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Apr  6 04:32:52.824: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Apr  6 04:32:52.838: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:52.857: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:52.898: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:52.935: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:52.998: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:54.643: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:54.679: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:54.702: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:54.723: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:54.751: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 04:32:56.235: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Apr  6 04:32:56.328: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:56.328: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:56.328: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:56.328: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:56.328: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 1
Apr  6 04:32:56.329: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:56.329: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:56.329: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:56.329: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:56.329: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 2
Apr  6 04:32:56.329: INFO: observed Deployment test-deployment in namespace deployment-9947 with ReadyReplicas 3
STEP: deleting the Deployment
Apr  6 04:32:56.364: INFO: observed event type MODIFIED
Apr  6 04:32:56.364: INFO: observed event type MODIFIED
Apr  6 04:32:56.364: INFO: observed event type MODIFIED
Apr  6 04:32:56.365: INFO: observed event type MODIFIED
Apr  6 04:32:56.365: INFO: observed event type MODIFIED
Apr  6 04:32:56.365: INFO: observed event type MODIFIED
Apr  6 04:32:56.365: INFO: observed event type MODIFIED
Apr  6 04:32:56.365: INFO: observed event type MODIFIED
Apr  6 04:32:56.365: INFO: observed event type MODIFIED
Apr  6 04:32:56.365: INFO: observed event type MODIFIED
Apr  6 04:32:56.365: INFO: observed event type MODIFIED
Apr  6 04:32:56.366: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr  6 04:32:56.378: INFO: Log out all the ReplicaSets if there is no deployment created
Apr  6 04:32:56.391: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-9947  dfdae2a5-aa43-41a1-8f4d-c8caa924a101 127278 3 2022-04-06 04:32:48 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 76f21933-9762-4361-b858-a6c2c30a4d37 0xc005ca0c47 0xc005ca0c48}] []  [{kube-controller-manager Update apps/v1 2022-04-06 04:32:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76f21933-9762-4361-b858-a6c2c30a4d37\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-06 04:32:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ca0cd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:32:56.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9947" for this suite.

• [SLOW TEST:8.792 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":323,"skipped":5643,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:32:56.421: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:32:56.830: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-2b828d78-d6dc-4150-a797-f4c00bc61059
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:33:00.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1185" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":324,"skipped":5651,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:33:00.954: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 04:33:01.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f3eb4ad-6270-4e00-8b20-1fb29a9b91ab" in namespace "projected-8136" to be "Succeeded or Failed"
Apr  6 04:33:01.176: INFO: Pod "downwardapi-volume-3f3eb4ad-6270-4e00-8b20-1fb29a9b91ab": Phase="Pending", Reason="", readiness=false. Elapsed: 11.331272ms
Apr  6 04:33:03.188: INFO: Pod "downwardapi-volume-3f3eb4ad-6270-4e00-8b20-1fb29a9b91ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023502641s
Apr  6 04:33:05.203: INFO: Pod "downwardapi-volume-3f3eb4ad-6270-4e00-8b20-1fb29a9b91ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038454559s
STEP: Saw pod success
Apr  6 04:33:05.203: INFO: Pod "downwardapi-volume-3f3eb4ad-6270-4e00-8b20-1fb29a9b91ab" satisfied condition "Succeeded or Failed"
Apr  6 04:33:05.212: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-3f3eb4ad-6270-4e00-8b20-1fb29a9b91ab container client-container: <nil>
STEP: delete the pod
Apr  6 04:33:05.251: INFO: Waiting for pod downwardapi-volume-3f3eb4ad-6270-4e00-8b20-1fb29a9b91ab to disappear
Apr  6 04:33:05.260: INFO: Pod downwardapi-volume-3f3eb4ad-6270-4e00-8b20-1fb29a9b91ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:33:05.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8136" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":325,"skipped":5671,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:33:05.286: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Apr  6 04:33:06.626: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0406 04:33:06.626136      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:33:06.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6853" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":326,"skipped":5674,"failed":0}
S
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:33:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-3417
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3417
STEP: Deleting pre-stop pod
Apr  6 04:33:18.018: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:33:18.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3417" for this suite.

• [SLOW TEST:11.440 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":327,"skipped":5675,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:33:18.092: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-10849571-db82-4415-afc8-86b9323b9495
STEP: Creating a pod to test consume configMaps
Apr  6 04:33:18.324: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cfba2de-c804-4aae-8cc8-a955fde6d528" in namespace "configmap-252" to be "Succeeded or Failed"
Apr  6 04:33:18.340: INFO: Pod "pod-configmaps-1cfba2de-c804-4aae-8cc8-a955fde6d528": Phase="Pending", Reason="", readiness=false. Elapsed: 15.633909ms
Apr  6 04:33:20.354: INFO: Pod "pod-configmaps-1cfba2de-c804-4aae-8cc8-a955fde6d528": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030311339s
Apr  6 04:33:22.367: INFO: Pod "pod-configmaps-1cfba2de-c804-4aae-8cc8-a955fde6d528": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042630729s
STEP: Saw pod success
Apr  6 04:33:22.367: INFO: Pod "pod-configmaps-1cfba2de-c804-4aae-8cc8-a955fde6d528" satisfied condition "Succeeded or Failed"
Apr  6 04:33:22.375: INFO: Trying to get logs from node 10.241.0.102 pod pod-configmaps-1cfba2de-c804-4aae-8cc8-a955fde6d528 container agnhost-container: <nil>
STEP: delete the pod
Apr  6 04:33:22.431: INFO: Waiting for pod pod-configmaps-1cfba2de-c804-4aae-8cc8-a955fde6d528 to disappear
Apr  6 04:33:22.439: INFO: Pod pod-configmaps-1cfba2de-c804-4aae-8cc8-a955fde6d528 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:33:22.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-252" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":328,"skipped":5688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:33:22.459: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7251
STEP: creating service affinity-clusterip-transition in namespace services-7251
STEP: creating replication controller affinity-clusterip-transition in namespace services-7251
I0406 04:33:22.795322      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7251, replica count: 3
I0406 04:33:25.845982      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 04:33:25.889: INFO: Creating new exec pod
Apr  6 04:33:28.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-7251 exec execpod-affinityw9vmm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Apr  6 04:33:29.223: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr  6 04:33:29.223: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:33:29.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-7251 exec execpod-affinityw9vmm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.144.57 80'
Apr  6 04:33:29.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.144.57 80\nConnection to 172.21.144.57 80 port [tcp/http] succeeded!\n"
Apr  6 04:33:29.455: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:33:29.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-7251 exec execpod-affinityw9vmm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.144.57:80/ ; done'
Apr  6 04:33:29.811: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n"
Apr  6 04:33:29.811: INFO: stdout: "\naffinity-clusterip-transition-sgzl2\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-sgzl2\naffinity-clusterip-transition-sgzl2\naffinity-clusterip-transition-9wwvg\naffinity-clusterip-transition-9wwvg\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-9wwvg\naffinity-clusterip-transition-9wwvg\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-9wwvg\naffinity-clusterip-transition-sgzl2\naffinity-clusterip-transition-sgzl2\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv"
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-sgzl2
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-sgzl2
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-sgzl2
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-9wwvg
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-9wwvg
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-9wwvg
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-9wwvg
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-9wwvg
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-sgzl2
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-sgzl2
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:29.811: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:29.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-7251 exec execpod-affinityw9vmm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.144.57:80/ ; done'
Apr  6 04:33:30.167: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.144.57:80/\n"
Apr  6 04:33:30.167: INFO: stdout: "\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv\naffinity-clusterip-transition-w4rcv"
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Received response from host: affinity-clusterip-transition-w4rcv
Apr  6 04:33:30.167: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7251, will wait for the garbage collector to delete the pods
Apr  6 04:33:30.275: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.370973ms
Apr  6 04:33:30.375: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.808566ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:33:33.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7251" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.245 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":329,"skipped":5716,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:33:33.704: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Apr  6 04:33:33.969: INFO: observed Pod pod-test in namespace pods-9268 in phase Pending with labels: map[test-pod-static:true] & conditions []
Apr  6 04:33:33.988: INFO: observed Pod pod-test in namespace pods-9268 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  }]
Apr  6 04:33:34.026: INFO: observed Pod pod-test in namespace pods-9268 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  }]
Apr  6 04:33:34.818: INFO: observed Pod pod-test in namespace pods-9268 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  }]
Apr  6 04:33:34.911: INFO: observed Pod pod-test in namespace pods-9268 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  }]
Apr  6 04:33:35.810: INFO: Found Pod pod-test in namespace pods-9268 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-06 04:33:33 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Apr  6 04:33:35.845: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Apr  6 04:33:35.937: INFO: observed event type ADDED
Apr  6 04:33:35.937: INFO: observed event type MODIFIED
Apr  6 04:33:35.938: INFO: observed event type MODIFIED
Apr  6 04:33:35.938: INFO: observed event type MODIFIED
Apr  6 04:33:35.938: INFO: observed event type MODIFIED
Apr  6 04:33:35.938: INFO: observed event type MODIFIED
Apr  6 04:33:35.938: INFO: observed event type MODIFIED
Apr  6 04:33:35.939: INFO: observed event type MODIFIED
Apr  6 04:33:35.939: INFO: observed event type MODIFIED
Apr  6 04:33:37.845: INFO: observed event type MODIFIED
Apr  6 04:33:38.218: INFO: observed event type MODIFIED
Apr  6 04:33:38.817: INFO: observed event type MODIFIED
Apr  6 04:33:39.139: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:33:39.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9268" for this suite.

• [SLOW TEST:5.477 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":330,"skipped":5734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:33:39.181: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr  6 04:33:39.329: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 04:33:39.347: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 04:33:39.362: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.100 before test
Apr  6 04:33:39.417: INFO: calico-kube-controllers-b6474b6d6-v54x8 from calico-system started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr  6 04:33:39.417: INFO: calico-node-tnbgq from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 04:33:39.417: INFO: calico-typha-5fc4c7b7c7-rjrk4 from calico-system started at 2022-04-06 01:23:03 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 04:33:39.417: INFO: managed-storage-validation-webhooks-86b89bd6d-25bg6 from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Apr  6 04:33:39.417: INFO: managed-storage-validation-webhooks-86b89bd6d-7bfgh from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Apr  6 04:33:39.417: INFO: managed-storage-validation-webhooks-86b89bd6d-q5wr7 from ibm-odf-validation-webhook started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Apr  6 04:33:39.417: INFO: ibm-keepalived-watcher-c7h76 from kube-system started at 2022-04-06 01:20:27 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 04:33:39.417: INFO: ibm-master-proxy-static-10.241.0.100 from kube-system started at 2022-04-06 01:20:21 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container pause ready: true, restart count 0
Apr  6 04:33:39.417: INFO: ibm-storage-metrics-agent-77bc4fb5c9-dck7l from kube-system started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Apr  6 04:33:39.417: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2022-04-06 01:25:08 +0000 UTC (6 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container csi-resizer ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 04:33:39.417: INFO: ibm-vpc-block-csi-node-n4fgh from kube-system started at 2022-04-06 01:20:27 +0000 UTC (4 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 04:33:39.417: INFO: cluster-node-tuning-operator-847fd957bd-kmksx from openshift-cluster-node-tuning-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: tuned-lxrqd from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container tuned ready: true, restart count 0
Apr  6 04:33:39.417: INFO: cluster-samples-operator-67d667cb6c-5hvmq from openshift-cluster-samples-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr  6 04:33:39.417: INFO: cluster-storage-operator-77cf9bb8c7-zwgsg from openshift-cluster-storage-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Apr  6 04:33:39.417: INFO: csi-snapshot-controller-7ffc756fcb-ld98k from openshift-cluster-storage-operator started at 2022-04-06 03:48:57 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container snapshot-controller ready: true, restart count 0
Apr  6 04:33:39.417: INFO: csi-snapshot-controller-operator-55d76bfc74-fxv2z from openshift-cluster-storage-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: csi-snapshot-webhook-574644677c-gnlg5 from openshift-cluster-storage-operator started at 2022-04-06 03:48:57 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container webhook ready: true, restart count 0
Apr  6 04:33:39.417: INFO: console-operator-7c7f956448-rt9cz from openshift-console-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container console-operator ready: true, restart count 1
Apr  6 04:33:39.417: INFO: console-84457c4b7f-l4zfr from openshift-console started at 2022-04-06 01:38:56 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container console ready: true, restart count 0
Apr  6 04:33:39.417: INFO: dns-operator-8d8fb8787-xmhn7 from openshift-dns-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container dns-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: dns-default-gx9pd from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container dns ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: node-resolver-8cqjf from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 04:33:39.417: INFO: cluster-image-registry-operator-6bcb795945-5qbrw from openshift-image-registry started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: node-ca-gvpjv from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 04:33:39.417: INFO: ingress-canary-jd288 from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 04:33:39.417: INFO: ingress-operator-58b79c98c4-9q2fc from openshift-ingress-operator started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container ingress-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: router-default-79f7bb79b4-z2h89 from openshift-ingress started at 2022-04-06 03:48:57 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container router ready: true, restart count 0
Apr  6 04:33:39.417: INFO: openshift-kube-proxy-pqrfn from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: kube-storage-version-migrator-operator-5cfccbf8c7-c9h68 from openshift-kube-storage-version-migrator-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Apr  6 04:33:39.417: INFO: marketplace-operator-6cf6b95b7c-vjlnm from openshift-marketplace started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: cluster-monitoring-operator-6c8f74c5d5-9lsz4 from openshift-monitoring started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Apr  6 04:33:39.417: INFO: node-exporter-wbftb from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 04:33:39.417: INFO: prometheus-adapter-54744554d8-7jmd6 from openshift-monitoring started at 2022-04-06 03:48:58 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr  6 04:33:39.417: INFO: thanos-querier-6797965cb7-gfd6x from openshift-monitoring started at 2022-04-06 03:48:57 +0000 UTC (5 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container thanos-query ready: true, restart count 0
Apr  6 04:33:39.417: INFO: multus-additional-cni-plugins-xpz8b from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 04:33:39.417: INFO: multus-admission-controller-qwnbg from openshift-multus started at 2022-04-06 01:25:08 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 04:33:39.417: INFO: multus-fjdss from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 04:33:39.417: INFO: network-metrics-daemon-4m7nv from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 04:33:39.417: INFO: network-check-source-544bd4cb64-g5s5s from openshift-network-diagnostics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container check-endpoints ready: true, restart count 0
Apr  6 04:33:39.417: INFO: network-check-target-5lxc6 from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 04:33:39.417: INFO: network-operator-6c8789b55f-ntzrr from openshift-network-operator started at 2022-04-06 01:21:14 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container network-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: catalog-operator-699fc547c5-l5qkl from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container catalog-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: olm-operator-864d7cb959-4fbrf from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container olm-operator ready: true, restart count 0
Apr  6 04:33:39.417: INFO: package-server-manager-6f44bc74b7-c9pcp from openshift-operator-lifecycle-manager started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container package-server-manager ready: true, restart count 0
Apr  6 04:33:39.417: INFO: packageserver-77bb6bdcd6-hqmlj from openshift-operator-lifecycle-manager started at 2022-04-06 01:35:31 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container packageserver ready: true, restart count 0
Apr  6 04:33:39.417: INFO: metrics-b6fbdf747-4gdlt from openshift-roks-metrics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container metrics ready: true, restart count 2
Apr  6 04:33:39.417: INFO: push-gateway-f7897c967-v6cxg from openshift-roks-metrics started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container push-gateway ready: true, restart count 0
Apr  6 04:33:39.417: INFO: service-ca-operator-77fd55df89-nkd65 from openshift-service-ca-operator started at 2022-04-06 01:25:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container service-ca-operator ready: true, restart count 1
Apr  6 04:33:39.417: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-htb29 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 04:33:39.417: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.101 before test
Apr  6 04:33:39.468: INFO: calico-node-cs6hj from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 04:33:39.468: INFO: calico-typha-5fc4c7b7c7-67lgl from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 04:33:39.468: INFO: ibm-keepalived-watcher-rppx4 from kube-system started at 2022-04-06 01:20:09 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 04:33:39.468: INFO: ibm-master-proxy-static-10.241.0.101 from kube-system started at 2022-04-06 01:20:00 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container pause ready: true, restart count 0
Apr  6 04:33:39.468: INFO: ibm-vpc-block-csi-node-qh8rl from kube-system started at 2022-04-06 01:20:09 +0000 UTC (4 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 04:33:39.468: INFO: tuned-46gbx from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container tuned ready: true, restart count 0
Apr  6 04:33:39.468: INFO: csi-snapshot-controller-7ffc756fcb-xzddf from openshift-cluster-storage-operator started at 2022-04-06 01:36:18 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container snapshot-controller ready: true, restart count 0
Apr  6 04:33:39.468: INFO: csi-snapshot-webhook-574644677c-dmmpc from openshift-cluster-storage-operator started at 2022-04-06 01:36:15 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container webhook ready: true, restart count 0
Apr  6 04:33:39.468: INFO: console-84457c4b7f-8gp5t from openshift-console started at 2022-04-06 01:39:26 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container console ready: true, restart count 0
Apr  6 04:33:39.468: INFO: downloads-dbb5d5764-jhtbl from openshift-console started at 2022-04-06 01:35:23 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container download-server ready: true, restart count 0
Apr  6 04:33:39.468: INFO: dns-default-v242c from openshift-dns started at 2022-04-06 01:37:47 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container dns ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: node-resolver-lsbp7 from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 04:33:39.468: INFO: image-registry-54fc9d45f8-spl7j from openshift-image-registry started at 2022-04-06 03:48:58 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container registry ready: true, restart count 0
Apr  6 04:33:39.468: INFO: node-ca-ccn7d from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 04:33:39.468: INFO: ingress-canary-wklcl from openshift-ingress-canary started at 2022-04-06 01:38:13 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 04:33:39.468: INFO: router-default-79f7bb79b4-vnvq6 from openshift-ingress started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container router ready: true, restart count 0
Apr  6 04:33:39.468: INFO: openshift-kube-proxy-lsknc from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: community-operators-c2jdx from openshift-marketplace started at 2022-04-06 01:38:07 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 04:33:39.468: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-04-06 01:38:53 +0000 UTC (5 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: node-exporter-vx9fl from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 04:33:39.468: INFO: openshift-state-metrics-8dcbc5f76-cx6zj from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (3 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr  6 04:33:39.468: INFO: prometheus-adapter-54744554d8-95snn from openshift-monitoring started at 2022-04-06 01:39:52 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr  6 04:33:39.468: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-04-06 01:38:59 +0000 UTC (7 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container prometheus ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr  6 04:33:39.468: INFO: thanos-querier-6797965cb7-gjqm5 from openshift-monitoring started at 2022-04-06 01:38:55 +0000 UTC (5 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container thanos-query ready: true, restart count 0
Apr  6 04:33:39.468: INFO: multus-additional-cni-plugins-d5x4n from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 04:33:39.468: INFO: multus-admission-controller-6c4s6 from openshift-multus started at 2022-04-06 01:27:47 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 04:33:39.468: INFO: multus-cwlpj from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 04:33:39.468: INFO: network-metrics-daemon-hgtsb from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 04:33:39.468: INFO: network-check-target-rmb8p from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 04:33:39.468: INFO: packageserver-77bb6bdcd6-cjmz7 from openshift-operator-lifecycle-manager started at 2022-04-06 03:48:58 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container packageserver ready: true, restart count 0
Apr  6 04:33:39.468: INFO: sonobuoy from sonobuoy started at 2022-04-06 02:56:18 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 04:33:39.468: INFO: sonobuoy-e2e-job-9989f36257d44400 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container e2e ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 04:33:39.468: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-ffjt6 from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.468: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 04:33:39.468: INFO: 
Logging pods the apiserver thinks is on node 10.241.0.102 before test
Apr  6 04:33:39.520: INFO: calico-node-t786f from calico-system started at 2022-04-06 01:22:56 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 04:33:39.520: INFO: ibm-keepalived-watcher-bv7pb from kube-system started at 2022-04-06 01:20:29 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr  6 04:33:39.520: INFO: ibm-master-proxy-static-10.241.0.102 from kube-system started at 2022-04-06 01:20:23 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container pause ready: true, restart count 0
Apr  6 04:33:39.520: INFO: ibm-vpc-block-csi-node-nk2wg from kube-system started at 2022-04-06 01:20:29 +0000 UTC (4 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Apr  6 04:33:39.520: INFO: vpn-849cbbd4f5-2qlk5 from kube-system started at 2022-04-06 03:48:57 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container vpn ready: true, restart count 0
Apr  6 04:33:39.520: INFO: tuned-r57x6 from openshift-cluster-node-tuning-operator started at 2022-04-06 01:37:41 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container tuned ready: true, restart count 0
Apr  6 04:33:39.520: INFO: downloads-dbb5d5764-q86mx from openshift-console started at 2022-04-06 03:48:57 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container download-server ready: true, restart count 0
Apr  6 04:33:39.520: INFO: dns-default-v5g6d from openshift-dns started at 2022-04-06 03:49:37 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container dns ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: node-resolver-9gtvm from openshift-dns started at 2022-04-06 01:37:48 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr  6 04:33:39.520: INFO: node-ca-w42rx from openshift-image-registry started at 2022-04-06 01:38:14 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container node-ca ready: true, restart count 0
Apr  6 04:33:39.520: INFO: ingress-canary-8gm9c from openshift-ingress-canary started at 2022-04-06 03:49:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Apr  6 04:33:39.520: INFO: openshift-kube-proxy-9c8jc from openshift-kube-proxy started at 2022-04-06 01:21:38 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: migrator-c84bfd698-cc2lp from openshift-kube-storage-version-migrator started at 2022-04-06 03:48:57 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container migrator ready: true, restart count 0
Apr  6 04:33:39.520: INFO: certified-operators-dbr2v from openshift-marketplace started at 2022-04-06 03:49:07 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 04:33:39.520: INFO: redhat-marketplace-dcgkb from openshift-marketplace started at 2022-04-06 03:49:08 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 04:33:39.520: INFO: redhat-operators-4p8kc from openshift-marketplace started at 2022-04-06 03:49:09 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container registry-server ready: true, restart count 0
Apr  6 04:33:39.520: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-04-06 03:49:09 +0000 UTC (5 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container alertmanager ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: grafana-65d7ff4ff4-k42bt from openshift-monitoring started at 2022-04-06 03:48:57 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container grafana ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: kube-state-metrics-c4c8f95d8-ffkcb from openshift-monitoring started at 2022-04-06 03:48:57 +0000 UTC (3 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 04:33:39.520: INFO: node-exporter-nnrpz from openshift-monitoring started at 2022-04-06 01:37:27 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 04:33:39.520: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-04-06 03:49:09 +0000 UTC (7 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container config-reloader ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container prometheus ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr  6 04:33:39.520: INFO: prometheus-operator-785898db99-gshzq from openshift-monitoring started at 2022-04-06 03:48:57 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr  6 04:33:39.520: INFO: telemeter-client-bdc7d9995-ch76w from openshift-monitoring started at 2022-04-06 03:48:58 +0000 UTC (3 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container reload ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container telemeter-client ready: true, restart count 0
Apr  6 04:33:39.520: INFO: multus-additional-cni-plugins-v2thd from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Apr  6 04:33:39.520: INFO: multus-admission-controller-nb7b9 from openshift-multus started at 2022-04-06 03:49:37 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr  6 04:33:39.520: INFO: multus-mhr5m from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-multus ready: true, restart count 0
Apr  6 04:33:39.520: INFO: network-metrics-daemon-qt77t from openshift-multus started at 2022-04-06 01:21:37 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Apr  6 04:33:39.520: INFO: network-check-target-m4pg9 from openshift-network-diagnostics started at 2022-04-06 01:21:38 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container network-check-target-container ready: true, restart count 0
Apr  6 04:33:39.520: INFO: collect-profiles-27486960--1-xq8lk from openshift-operator-lifecycle-manager started at 2022-04-06 04:00:00 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 04:33:39.520: INFO: collect-profiles-27486975--1-bhxmd from openshift-operator-lifecycle-manager started at 2022-04-06 04:15:00 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 04:33:39.520: INFO: collect-profiles-27486990--1-gdccp from openshift-operator-lifecycle-manager started at 2022-04-06 04:30:00 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container collect-profiles ready: false, restart count 0
Apr  6 04:33:39.520: INFO: service-ca-c77965566-kfdm4 from openshift-service-ca started at 2022-04-06 03:48:57 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container service-ca-controller ready: false, restart count 0
Apr  6 04:33:39.520: INFO: tester from prestop-3417 started at 2022-04-06 04:33:10 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container tester ready: true, restart count 0
Apr  6 04:33:39.520: INFO: sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-84flv from sonobuoy started at 2022-04-06 02:56:22 +0000 UTC (2 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 04:33:39.520: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 04:33:39.520: INFO: tigera-operator-5d4d8f956c-pcz9l from tigera-operator started at 2022-04-06 03:48:58 +0000 UTC (1 container statuses recorded)
Apr  6 04:33:39.520: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 10.241.0.100
STEP: verifying the node has the label node 10.241.0.101
STEP: verifying the node has the label node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod calico-kube-controllers-b6474b6d6-v54x8 requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod calico-node-cs6hj requesting resource cpu=250m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod calico-node-t786f requesting resource cpu=250m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod calico-node-tnbgq requesting resource cpu=250m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod calico-typha-5fc4c7b7c7-67lgl requesting resource cpu=250m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod calico-typha-5fc4c7b7c7-rjrk4 requesting resource cpu=250m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod managed-storage-validation-webhooks-86b89bd6d-25bg6 requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod managed-storage-validation-webhooks-86b89bd6d-7bfgh requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod managed-storage-validation-webhooks-86b89bd6d-q5wr7 requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod ibm-keepalived-watcher-bv7pb requesting resource cpu=5m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod ibm-keepalived-watcher-c7h76 requesting resource cpu=5m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod ibm-keepalived-watcher-rppx4 requesting resource cpu=5m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod ibm-master-proxy-static-10.241.0.100 requesting resource cpu=26m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod ibm-master-proxy-static-10.241.0.101 requesting resource cpu=26m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod ibm-master-proxy-static-10.241.0.102 requesting resource cpu=26m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod ibm-storage-metrics-agent-77bc4fb5c9-dck7l requesting resource cpu=50m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod ibm-vpc-block-csi-controller-0 requesting resource cpu=138m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod ibm-vpc-block-csi-node-n4fgh requesting resource cpu=48m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod ibm-vpc-block-csi-node-nk2wg requesting resource cpu=48m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod ibm-vpc-block-csi-node-qh8rl requesting resource cpu=48m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod vpn-849cbbd4f5-2qlk5 requesting resource cpu=5m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod cluster-node-tuning-operator-847fd957bd-kmksx requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod tuned-46gbx requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod tuned-lxrqd requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod tuned-r57x6 requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod cluster-samples-operator-67d667cb6c-5hvmq requesting resource cpu=20m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod cluster-storage-operator-77cf9bb8c7-zwgsg requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod csi-snapshot-controller-7ffc756fcb-ld98k requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod csi-snapshot-controller-7ffc756fcb-xzddf requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod csi-snapshot-controller-operator-55d76bfc74-fxv2z requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod csi-snapshot-webhook-574644677c-dmmpc requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod csi-snapshot-webhook-574644677c-gnlg5 requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod console-operator-7c7f956448-rt9cz requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod console-84457c4b7f-8gp5t requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod console-84457c4b7f-l4zfr requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod downloads-dbb5d5764-jhtbl requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod downloads-dbb5d5764-q86mx requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod dns-operator-8d8fb8787-xmhn7 requesting resource cpu=20m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod dns-default-gx9pd requesting resource cpu=60m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod dns-default-v242c requesting resource cpu=60m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod dns-default-v5g6d requesting resource cpu=60m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod node-resolver-8cqjf requesting resource cpu=5m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod node-resolver-9gtvm requesting resource cpu=5m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod node-resolver-lsbp7 requesting resource cpu=5m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod cluster-image-registry-operator-6bcb795945-5qbrw requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod image-registry-54fc9d45f8-spl7j requesting resource cpu=100m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod node-ca-ccn7d requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod node-ca-gvpjv requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod node-ca-w42rx requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod ingress-canary-8gm9c requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod ingress-canary-jd288 requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod ingress-canary-wklcl requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod ingress-operator-58b79c98c4-9q2fc requesting resource cpu=20m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod router-default-79f7bb79b4-vnvq6 requesting resource cpu=100m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod router-default-79f7bb79b4-z2h89 requesting resource cpu=100m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod openshift-kube-proxy-9c8jc requesting resource cpu=110m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod openshift-kube-proxy-lsknc requesting resource cpu=110m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod openshift-kube-proxy-pqrfn requesting resource cpu=110m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod kube-storage-version-migrator-operator-5cfccbf8c7-c9h68 requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod migrator-c84bfd698-cc2lp requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod certified-operators-dbr2v requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod community-operators-c2jdx requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod marketplace-operator-6cf6b95b7c-vjlnm requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod redhat-marketplace-dcgkb requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod redhat-operators-4p8kc requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod cluster-monitoring-operator-6c8f74c5d5-9lsz4 requesting resource cpu=11m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod grafana-65d7ff4ff4-k42bt requesting resource cpu=5m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod kube-state-metrics-c4c8f95d8-ffkcb requesting resource cpu=4m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod node-exporter-nnrpz requesting resource cpu=9m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod node-exporter-vx9fl requesting resource cpu=9m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod node-exporter-wbftb requesting resource cpu=9m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod openshift-state-metrics-8dcbc5f76-cx6zj requesting resource cpu=3m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod prometheus-adapter-54744554d8-7jmd6 requesting resource cpu=1m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod prometheus-adapter-54744554d8-95snn requesting resource cpu=1m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod prometheus-operator-785898db99-gshzq requesting resource cpu=6m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod telemeter-client-bdc7d9995-ch76w requesting resource cpu=3m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod thanos-querier-6797965cb7-gfd6x requesting resource cpu=14m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod thanos-querier-6797965cb7-gjqm5 requesting resource cpu=14m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod multus-additional-cni-plugins-d5x4n requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod multus-additional-cni-plugins-v2thd requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod multus-additional-cni-plugins-xpz8b requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.799: INFO: Pod multus-admission-controller-6c4s6 requesting resource cpu=20m on Node 10.241.0.101
Apr  6 04:33:39.799: INFO: Pod multus-admission-controller-nb7b9 requesting resource cpu=20m on Node 10.241.0.102
Apr  6 04:33:39.799: INFO: Pod multus-admission-controller-qwnbg requesting resource cpu=20m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod multus-cwlpj requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.800: INFO: Pod multus-fjdss requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod multus-mhr5m requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.800: INFO: Pod network-metrics-daemon-4m7nv requesting resource cpu=20m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod network-metrics-daemon-hgtsb requesting resource cpu=20m on Node 10.241.0.101
Apr  6 04:33:39.800: INFO: Pod network-metrics-daemon-qt77t requesting resource cpu=20m on Node 10.241.0.102
Apr  6 04:33:39.800: INFO: Pod network-check-source-544bd4cb64-g5s5s requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod network-check-target-5lxc6 requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod network-check-target-m4pg9 requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.800: INFO: Pod network-check-target-rmb8p requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.800: INFO: Pod network-operator-6c8789b55f-ntzrr requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod catalog-operator-699fc547c5-l5qkl requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod olm-operator-864d7cb959-4fbrf requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod package-server-manager-6f44bc74b7-c9pcp requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod packageserver-77bb6bdcd6-cjmz7 requesting resource cpu=10m on Node 10.241.0.101
Apr  6 04:33:39.800: INFO: Pod packageserver-77bb6bdcd6-hqmlj requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod metrics-b6fbdf747-4gdlt requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod push-gateway-f7897c967-v6cxg requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod service-ca-operator-77fd55df89-nkd65 requesting resource cpu=10m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod service-ca-c77965566-kfdm4 requesting resource cpu=10m on Node 10.241.0.102
Apr  6 04:33:39.800: INFO: Pod tester requesting resource cpu=0m on Node 10.241.0.102
Apr  6 04:33:39.800: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.241.0.101
Apr  6 04:33:39.800: INFO: Pod sonobuoy-e2e-job-9989f36257d44400 requesting resource cpu=0m on Node 10.241.0.101
Apr  6 04:33:39.800: INFO: Pod sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-84flv requesting resource cpu=0m on Node 10.241.0.102
Apr  6 04:33:39.800: INFO: Pod sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-ffjt6 requesting resource cpu=0m on Node 10.241.0.101
Apr  6 04:33:39.800: INFO: Pod sonobuoy-systemd-logs-daemon-set-18ba63bf63574c1c-htb29 requesting resource cpu=0m on Node 10.241.0.100
Apr  6 04:33:39.800: INFO: Pod tigera-operator-5d4d8f956c-pcz9l requesting resource cpu=100m on Node 10.241.0.102
STEP: Starting Pods to consume most of the cluster CPU.
Apr  6 04:33:39.800: INFO: Creating a pod which consumes cpu=1879m on Node 10.241.0.101
Apr  6 04:33:39.859: INFO: Creating a pod which consumes cpu=2121m on Node 10.241.0.102
Apr  6 04:33:39.898: INFO: Creating a pod which consumes cpu=1704m on Node 10.241.0.100
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56e37915-16e6-4af0-8423-364c5154ff2c.16e33481a0217461], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9219/filler-pod-56e37915-16e6-4af0-8423-364c5154ff2c to 10.241.0.101]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56e37915-16e6-4af0-8423-364c5154ff2c.16e33481d12b9f69], Reason = [AddedInterface], Message = [Add eth0 [172.17.95.168/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56e37915-16e6-4af0-8423-364c5154ff2c.16e33481df21c054], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56e37915-16e6-4af0-8423-364c5154ff2c.16e33481e8999dd8], Reason = [Created], Message = [Created container filler-pod-56e37915-16e6-4af0-8423-364c5154ff2c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56e37915-16e6-4af0-8423-364c5154ff2c.16e33481ea1055db], Reason = [Started], Message = [Started container filler-pod-56e37915-16e6-4af0-8423-364c5154ff2c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9adcf75-d9e1-499b-aad7-c7e472f4908a.16e33481a363368e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9219/filler-pod-a9adcf75-d9e1-499b-aad7-c7e472f4908a to 10.241.0.102]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9adcf75-d9e1-499b-aad7-c7e472f4908a.16e33481d82ae897], Reason = [AddedInterface], Message = [Add eth0 [172.17.96.92/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9adcf75-d9e1-499b-aad7-c7e472f4908a.16e33481e920a808], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9adcf75-d9e1-499b-aad7-c7e472f4908a.16e33481f4bfdd50], Reason = [Created], Message = [Created container filler-pod-a9adcf75-d9e1-499b-aad7-c7e472f4908a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9adcf75-d9e1-499b-aad7-c7e472f4908a.16e33481f6f67523], Reason = [Started], Message = [Started container filler-pod-a9adcf75-d9e1-499b-aad7-c7e472f4908a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d98f1128-9639-4cf3-bbe1-348c444eaadc.16e33481a4f4ffa5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9219/filler-pod-d98f1128-9639-4cf3-bbe1-348c444eaadc to 10.241.0.100]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d98f1128-9639-4cf3-bbe1-348c444eaadc.16e33481d6b37ec4], Reason = [AddedInterface], Message = [Add eth0 [172.17.77.50/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d98f1128-9639-4cf3-bbe1-348c444eaadc.16e33481e7569222], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d98f1128-9639-4cf3-bbe1-348c444eaadc.16e33481f4bf2191], Reason = [Created], Message = [Created container filler-pod-d98f1128-9639-4cf3-bbe1-348c444eaadc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d98f1128-9639-4cf3-bbe1-348c444eaadc.16e33481f6fbb470], Reason = [Started], Message = [Started container filler-pod-d98f1128-9639-4cf3-bbe1-348c444eaadc]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16e3348220b6b9a6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.241.0.101
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.241.0.102
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.241.0.100
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:33:43.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9219" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":331,"skipped":5756,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:33:43.211: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr  6 04:33:43.403: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr  6 04:34:14.216: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
Apr  6 04:34:24.000: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:34:58.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7590" for this suite.

• [SLOW TEST:74.925 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":332,"skipped":5762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:34:58.136: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 04:34:58.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24b36af8-dd21-4ddf-8828-6c4838300bde" in namespace "projected-1509" to be "Succeeded or Failed"
Apr  6 04:34:58.577: INFO: Pod "downwardapi-volume-24b36af8-dd21-4ddf-8828-6c4838300bde": Phase="Pending", Reason="", readiness=false. Elapsed: 10.665355ms
Apr  6 04:35:00.600: INFO: Pod "downwardapi-volume-24b36af8-dd21-4ddf-8828-6c4838300bde": Phase="Running", Reason="", readiness=true. Elapsed: 2.033147445s
Apr  6 04:35:02.631: INFO: Pod "downwardapi-volume-24b36af8-dd21-4ddf-8828-6c4838300bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064369395s
STEP: Saw pod success
Apr  6 04:35:02.631: INFO: Pod "downwardapi-volume-24b36af8-dd21-4ddf-8828-6c4838300bde" satisfied condition "Succeeded or Failed"
Apr  6 04:35:02.641: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-24b36af8-dd21-4ddf-8828-6c4838300bde container client-container: <nil>
STEP: delete the pod
Apr  6 04:35:02.730: INFO: Waiting for pod downwardapi-volume-24b36af8-dd21-4ddf-8828-6c4838300bde to disappear
Apr  6 04:35:02.740: INFO: Pod downwardapi-volume-24b36af8-dd21-4ddf-8828-6c4838300bde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:02.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1509" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":5793,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:02.786: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr  6 04:35:03.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67ef7088-9bec-4d81-8213-f9c4e09c2ff8" in namespace "downward-api-3287" to be "Succeeded or Failed"
Apr  6 04:35:03.105: INFO: Pod "downwardapi-volume-67ef7088-9bec-4d81-8213-f9c4e09c2ff8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.962202ms
Apr  6 04:35:05.120: INFO: Pod "downwardapi-volume-67ef7088-9bec-4d81-8213-f9c4e09c2ff8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030098056s
Apr  6 04:35:07.138: INFO: Pod "downwardapi-volume-67ef7088-9bec-4d81-8213-f9c4e09c2ff8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048306471s
STEP: Saw pod success
Apr  6 04:35:07.138: INFO: Pod "downwardapi-volume-67ef7088-9bec-4d81-8213-f9c4e09c2ff8" satisfied condition "Succeeded or Failed"
Apr  6 04:35:07.152: INFO: Trying to get logs from node 10.241.0.102 pod downwardapi-volume-67ef7088-9bec-4d81-8213-f9c4e09c2ff8 container client-container: <nil>
STEP: delete the pod
Apr  6 04:35:07.312: INFO: Waiting for pod downwardapi-volume-67ef7088-9bec-4d81-8213-f9c4e09c2ff8 to disappear
Apr  6 04:35:07.324: INFO: Pod downwardapi-volume-67ef7088-9bec-4d81-8213-f9c4e09c2ff8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:07.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3287" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":5804,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:07.379: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Apr  6 04:35:07.795: INFO: The status of Pod annotationupdate7d7b3048-50d7-4cf5-b347-860bf074ba51 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:35:09.835: INFO: The status of Pod annotationupdate7d7b3048-50d7-4cf5-b347-860bf074ba51 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:35:11.812: INFO: The status of Pod annotationupdate7d7b3048-50d7-4cf5-b347-860bf074ba51 is Running (Ready = true)
Apr  6 04:35:12.477: INFO: Successfully updated pod "annotationupdate7d7b3048-50d7-4cf5-b347-860bf074ba51"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:14.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8236" for this suite.

• [SLOW TEST:7.234 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":5814,"failed":0}
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:14.613: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:35:14.811: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: creating the pod
STEP: submitting the pod to kubernetes
Apr  6 04:35:14.884: INFO: The status of Pod pod-logs-websocket-c554c53c-e474-431e-9642-2358cc66246a is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:35:16.896: INFO: The status of Pod pod-logs-websocket-c554c53c-e474-431e-9642-2358cc66246a is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:35:18.900: INFO: The status of Pod pod-logs-websocket-c554c53c-e474-431e-9642-2358cc66246a is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:18.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5653" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":5814,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:18.995: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Apr  6 04:35:19.185: INFO: Major version: 1
STEP: Confirm minor version
Apr  6 04:35:19.185: INFO: cleanMinorVersion: 22
Apr  6 04:35:19.185: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:19.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-71" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":337,"skipped":5890,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:19.229: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-56b93751-f4c2-4661-83d1-e35d2e3db6d9
STEP: Creating a pod to test consume secrets
Apr  6 04:35:19.696: INFO: Waiting up to 5m0s for pod "pod-secrets-4a9664c7-8d70-49b7-b071-142267a9a701" in namespace "secrets-2175" to be "Succeeded or Failed"
Apr  6 04:35:19.708: INFO: Pod "pod-secrets-4a9664c7-8d70-49b7-b071-142267a9a701": Phase="Pending", Reason="", readiness=false. Elapsed: 11.997117ms
Apr  6 04:35:21.722: INFO: Pod "pod-secrets-4a9664c7-8d70-49b7-b071-142267a9a701": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02611463s
Apr  6 04:35:23.740: INFO: Pod "pod-secrets-4a9664c7-8d70-49b7-b071-142267a9a701": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043889149s
STEP: Saw pod success
Apr  6 04:35:23.740: INFO: Pod "pod-secrets-4a9664c7-8d70-49b7-b071-142267a9a701" satisfied condition "Succeeded or Failed"
Apr  6 04:35:23.751: INFO: Trying to get logs from node 10.241.0.102 pod pod-secrets-4a9664c7-8d70-49b7-b071-142267a9a701 container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 04:35:23.825: INFO: Waiting for pod pod-secrets-4a9664c7-8d70-49b7-b071-142267a9a701 to disappear
Apr  6 04:35:23.839: INFO: Pod pod-secrets-4a9664c7-8d70-49b7-b071-142267a9a701 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:23.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2175" for this suite.
STEP: Destroying namespace "secret-namespace-1151" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":338,"skipped":5890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-3668
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3668 to expose endpoints map[]
Apr  6 04:35:24.187: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Apr  6 04:35:25.219: INFO: successfully validated that service endpoint-test2 in namespace services-3668 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3668
Apr  6 04:35:25.402: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:35:27.414: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3668 to expose endpoints map[pod1:[80]]
Apr  6 04:35:27.460: INFO: successfully validated that service endpoint-test2 in namespace services-3668 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Apr  6 04:35:27.460: INFO: Creating new exec pod
Apr  6 04:35:30.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-3668 exec execpodwv855 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr  6 04:35:31.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr  6 04:35:31.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:35:31.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-3668 exec execpodwv855 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.8.190 80'
Apr  6 04:35:31.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.8.190 80\nConnection to 172.21.8.190 80 port [tcp/http] succeeded!\n"
Apr  6 04:35:31.359: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3668
Apr  6 04:35:31.412: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:35:33.430: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3668 to expose endpoints map[pod1:[80] pod2:[80]]
Apr  6 04:35:33.482: INFO: successfully validated that service endpoint-test2 in namespace services-3668 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Apr  6 04:35:34.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-3668 exec execpodwv855 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr  6 04:35:34.711: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr  6 04:35:34.711: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:35:34.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-3668 exec execpodwv855 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.8.190 80'
Apr  6 04:35:34.941: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.8.190 80\nConnection to 172.21.8.190 80 port [tcp/http] succeeded!\n"
Apr  6 04:35:34.941: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3668
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3668 to expose endpoints map[pod2:[80]]
Apr  6 04:35:35.021: INFO: successfully validated that service endpoint-test2 in namespace services-3668 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Apr  6 04:35:36.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-3668 exec execpodwv855 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr  6 04:35:36.260: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr  6 04:35:36.260: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 04:35:36.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=services-3668 exec execpodwv855 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.8.190 80'
Apr  6 04:35:36.484: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.8.190 80\nConnection to 172.21.8.190 80 port [tcp/http] succeeded!\n"
Apr  6 04:35:36.484: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3668
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3668 to expose endpoints map[]
Apr  6 04:35:36.552: INFO: successfully validated that service endpoint-test2 in namespace services-3668 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:36.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3668" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.733 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":339,"skipped":5923,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:36.641: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:43.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4196" for this suite.

• [SLOW TEST:6.448 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":340,"skipped":5933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:43.089: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Apr  6 04:35:43.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 create -f -'
Apr  6 04:35:45.578: INFO: stderr: ""
Apr  6 04:35:45.578: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  6 04:35:45.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:35:45.657: INFO: stderr: ""
Apr  6 04:35:45.657: INFO: stdout: "update-demo-nautilus-mnhqd "
STEP: Replicas for name=update-demo: expected=2 actual=1
Apr  6 04:35:50.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 04:35:50.736: INFO: stderr: ""
Apr  6 04:35:50.736: INFO: stdout: "update-demo-nautilus-mnhqd update-demo-nautilus-rhl7m "
Apr  6 04:35:50.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 get pods update-demo-nautilus-mnhqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:35:50.801: INFO: stderr: ""
Apr  6 04:35:50.801: INFO: stdout: "true"
Apr  6 04:35:50.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 get pods update-demo-nautilus-mnhqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 04:35:50.869: INFO: stderr: ""
Apr  6 04:35:50.869: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Apr  6 04:35:50.869: INFO: validating pod update-demo-nautilus-mnhqd
Apr  6 04:35:50.889: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 04:35:50.889: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 04:35:50.889: INFO: update-demo-nautilus-mnhqd is verified up and running
Apr  6 04:35:50.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 get pods update-demo-nautilus-rhl7m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 04:35:50.957: INFO: stderr: ""
Apr  6 04:35:50.957: INFO: stdout: "true"
Apr  6 04:35:50.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 get pods update-demo-nautilus-rhl7m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 04:35:51.017: INFO: stderr: ""
Apr  6 04:35:51.017: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Apr  6 04:35:51.017: INFO: validating pod update-demo-nautilus-rhl7m
Apr  6 04:35:51.037: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 04:35:51.037: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 04:35:51.037: INFO: update-demo-nautilus-rhl7m is verified up and running
STEP: using delete to clean up resources
Apr  6 04:35:51.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 delete --grace-period=0 --force -f -'
Apr  6 04:35:51.125: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 04:35:51.125: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  6 04:35:51.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 get rc,svc -l name=update-demo --no-headers'
Apr  6 04:35:51.209: INFO: stderr: "No resources found in kubectl-6211 namespace.\n"
Apr  6 04:35:51.209: INFO: stdout: ""
Apr  6 04:35:51.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-583620336 --namespace=kubectl-6211 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 04:35:51.280: INFO: stderr: ""
Apr  6 04:35:51.280: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:51.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6211" for this suite.

• [SLOW TEST:8.226 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":341,"skipped":6023,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:51.315: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:35:51.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3884" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":342,"skipped":6044,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:35:51.713: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-7431
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-7431
Apr  6 04:35:52.004: INFO: Found 0 stateful pods, waiting for 1
Apr  6 04:36:02.025: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Apr  6 04:36:02.160: INFO: Deleting all statefulset in ns statefulset-7431
Apr  6 04:36:02.174: INFO: Scaling statefulset ss to 0
Apr  6 04:36:12.280: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 04:36:12.288: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:36:12.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7431" for this suite.

• [SLOW TEST:20.655 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":343,"skipped":6045,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:36:12.369: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:36:12.563: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-44a08777-3c28-4839-935e-e87415d48355
STEP: Creating secret with name s-test-opt-upd-6b52216d-917d-4e46-933e-703c6f9da358
STEP: Creating the pod
Apr  6 04:36:12.647: INFO: The status of Pod pod-projected-secrets-723ea81e-2238-4afb-9d77-832b02c27ab8 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 04:36:14.661: INFO: The status of Pod pod-projected-secrets-723ea81e-2238-4afb-9d77-832b02c27ab8 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-44a08777-3c28-4839-935e-e87415d48355
STEP: Updating secret s-test-opt-upd-6b52216d-917d-4e46-933e-703c6f9da358
STEP: Creating secret with name s-test-opt-create-173d06ad-181d-4984-bc0e-14d0915420dc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:36:17.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1634" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":344,"skipped":6058,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:36:17.091: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Apr  6 04:36:17.365: INFO: Pod name sample-pod: Found 0 pods out of 3
Apr  6 04:36:22.378: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Apr  6 04:36:22.386: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:36:22.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5942" for this suite.

• [SLOW TEST:5.413 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":345,"skipped":6071,"failed":0}
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr  6 04:36:22.505: INFO: >>> kubeConfig: /tmp/kubeconfig-583620336
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Apr  6 04:36:22.722: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 04:37:22.934: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr  6 04:37:22.952: INFO: Starting informer...
STEP: Starting pods...
Apr  6 04:37:23.242: INFO: Pod1 is running on 10.241.0.102. Tainting Node
Apr  6 04:37:25.547: INFO: Pod2 is running on 10.241.0.102. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Apr  6 04:37:32.248: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr  6 04:37:52.327: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr  6 04:37:52.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8106" for this suite.

• [SLOW TEST:89.956 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":346,"skipped":6075,"failed":0}
SSSSSSSSSSSSApr  6 04:37:52.526: INFO: Running AfterSuite actions on all nodes
Apr  6 04:37:52.526: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Apr  6 04:37:52.526: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Apr  6 04:37:52.526: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Apr  6 04:37:52.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Apr  6 04:37:52.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Apr  6 04:37:52.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Apr  6 04:37:52.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Apr  6 04:37:52.537: INFO: Running AfterSuite actions on node 1
Apr  6 04:37:52.537: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6087,"failed":0}

Ran 346 of 6433 Specs in 6067.950 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6087 Skipped
PASS

Ginkgo ran 1 suite in 1h41m9.579810819s
Test Suite Passed
